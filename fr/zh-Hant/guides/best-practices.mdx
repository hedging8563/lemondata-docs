---
title: "Bonnes pratiques"
description: "Optimisez votre utilisation de l'API LemonData pour améliorer la rentabilité, les performances et la fiabilité"
---

## Sélection du modèle

Le choix du modèle approprié a un impact significatif sur le coût et la qualité.

### Recommandations par tâche

| Tâche | Modèle recommandé | Raison |
|------|-------------------|-----------|
| **Questions-réponses simples** | `gpt-4o-mini`, `gemini-2.5-flash` | Rapide, économique, suffisant |
| **Raisonnement complexe** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Meilleures capacités de logique et de planification |
| **Programmation** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimisé pour le code |
| **Écriture créative** | `claude-sonnet-4-5`, `gpt-4o` | Meilleure qualité de prose |
| **Vision / Image** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Support natif de la vision |
| **Contexte long** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Fenêtre de 1M+ tokens |
| **Sensible au coût** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Meilleur rapport qualité-prix |

### Niveaux de coût

```
$$$$ Premium: o3, claude-opus-4-5, gpt-4o
$$$  Standard: claude-sonnet-4-5, gpt-4o
$$   Budget:   gpt-4o-mini, gemini-2.5-flash
$    Economy:  deepseek-v3.2, deepseek-r1
```

## Optimisation des coûts

### 1. Privilégier les modèles légers

```python
def smart_query(question: str, complexity: str = "auto"):
    """Use cheaper models for simple tasks."""

    if complexity == "simple":
        model = "gpt-4o-mini"
    elif complexity == "complex":
        model = "gpt-4o"
    else:
        # Start cheap, escalate if needed
        model = "gpt-4o-mini"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": question}]
    )
    return response
```

### 2. Définir max_tokens

Assurez-vous de définir une limite `max_tokens` raisonnable :

```python
# ❌ Erreur : Aucune limite définie, peut générer des milliers de tokens
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}]
)

# ✅ Correct : Limiter la longueur de la réponse
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}],
    max_tokens=500  # Limite raisonnable pour un résumé
)
```

### 3. Optimiser le Prompt

```python
# ❌ Prompt verbeux (consomme plus de tokens d'entrée)
prompt = """
I would like you to please help me by analyzing the following text
and providing a comprehensive summary of the main points. Please be
thorough but also concise in your response. The text is as follows:
{text}
"""

# ✅ Prompt concis (consomme moins de tokens)
prompt = "Summarize the key points:\n{text}"
```

### 4. Activer la mise en cache

Tirez parti du [cache sémantique](/guides/caching) :

```python
# Pour les requêtes similaires répétées, le cache permet de réaliser des économies importantes
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is machine learning?"}],
    temperature=0  # Déterminisme = meilleur taux de réussite du cache
)
```

### 5. Traitement par lots des requêtes similaires

```python
# ❌ Nombreuses petites requêtes
for question in questions:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": question}]
    )

# ✅ Moins de requêtes, mais plus volumineuses
combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"Answer each question:\n{combined_prompt}"}]
)
```

## Optimisation des performances

### 1. Utiliser le streaming pour améliorer l'expérience utilisateur

Le streaming peut améliorer la performance perçue :

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 2. Choisir des modèles rapides pour les usages interactifs

| Cas d'utilisation | Modèle recommandé | Latence |
|----------|-------------|---------|
| Interface de chat | `gpt-4o-mini`, `gemini-2.5-flash` | Premier token environ 200ms |
| Autocomplétion | `claude-haiku-4-5` | Premier token environ 150ms |
| Traitement en arrière-plan | `gpt-4o`, `claude-sonnet-4-5` | Premier token environ 500ms |

### 3. Définir des délais d'expiration (Timeouts)

```python
client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    timeout=60.0  # Délai de 60 secondes
)
```

## Fiabilité

### 1. Implémenter un mécanisme de tentative (Retry)

```python
import time
from openai import RateLimitError, APIError

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError:
            wait = 2 ** attempt
            print(f"Rate limited, waiting {wait}s...")
            time.sleep(wait)
        except APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(1)
    raise Exception("Max retries exceeded")
```

### 2. Gérer les erreurs avec élégance

```python
from openai import APIError, AuthenticationError, RateLimitError

try:
    response = client.chat.completions.create(...)
except AuthenticationError:
    # Vérifier la clé API
    notify_admin("Invalid API key")
except RateLimitError:
    # Ajouter à la file d'attente pour traitement ultérieur ou utiliser un secours
    add_to_queue(request)
except APIError as e:
    if e.status_code == 402:
        notify_admin("Balance low")
    elif e.status_code >= 500:
        # Erreur serveur, réessayer plus tard
        schedule_retry(request)
```

### 3. Utiliser des modèles de secours (Fallback Models)

```python
FALLBACK_CHAIN = ["gpt-4o", "claude-sonnet-4-5", "gemini-2.5-flash"]

def chat_with_fallback(messages):
    for model in FALLBACK_CHAIN:
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages
            )
        except APIError:
            continue
    raise Exception("All models failed")
```

## Sécurité

### 1. Protéger la clé API

```python
# ❌ Ne jamais coder la clé en dur dans le code
client = OpenAI(api_key="sk-abc123...")

# ✅ Utiliser des variables d'environnement
import os
client = OpenAI(api_key=os.environ["LEMONDATA_API_KEY"])
```

### 2. Valider les entrées utilisateur

```python
def validate_message(content: str) -> bool:
    """Valider l'entrée utilisateur avant de l'envoyer à l'API."""
    if len(content) > 100000:
        raise ValueError("Message too long")
    # Ajouter d'autres validations selon les besoins
    return True
```

### 3. Définir des restrictions de clé API

Créez des clés API distinctes avec des limites de dépenses pour les usages suivants :
- Développement / Test
- Production
- Différentes applications

## Surveillance

### 1. Suivre l'utilisation

Consultez régulièrement votre tableau de bord pour comprendre :
- Consommation de tokens par modèle
- Détails des coûts
- Taux de réussite du cache
- Taux d'erreur

### 2. Enregistrer les indicateurs clés

```python
import logging

response = client.chat.completions.create(...)

logging.info({
    "model": response.model,
    "prompt_tokens": response.usage.prompt_tokens,
    "completion_tokens": response.usage.completion_tokens,
    "total_tokens": response.usage.total_tokens,
})
```

### 3. Configurer des alertes

Configurez des alertes de solde insuffisant dans le tableau de bord pour éviter les interruptions de service.

## Liste de contrôle

<AccordionGroup>
  <Accordion title="Optimisation des coûts">
    - [ ] Utiliser le modèle approprié pour chaque tâche
    - [ ] Définir une limite max_tokens
    - [ ] Garder les prompts concis
    - [ ] Activer la mise en cache là où c'est approprié
    - [ ] Traiter les requêtes similaires par lots
  </Accordion>

  <Accordion title="Performance">
    - [ ] Utiliser le streaming pour les expériences interactives
    - [ ] Utiliser des modèles rapides pour les usages en temps réel
    - [ ] Paramètres de délai d'expiration configurés
  </Accordion>

  <Accordion title="Fiabilité">
    - [ ] Logique de tentative implémentée
    - [ ] Gestion des erreurs en place
    - [ ] Modèles de secours configurés
  </Accordion>

  <Accordion title="Sécurité">
    - [ ] Clé API stockée dans les variables d'environnement
    - [ ] Validation des entrées
    - [ ] Utiliser des clés distinctes pour le développement et la production
    - [ ] Limites de dépenses définies
  </Accordion>
</AccordionGroup>