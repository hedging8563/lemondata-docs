---
title: "LlamaIndex"
description: "Intégrez LemonData avec LlamaIndex pour les applications RAG"
---

## Aperçu

LlamaIndex est un framework de données pour les applications LLM, particulièrement puissant pour la construction de systèmes RAG (Retrieval Augmented Generation). LemonData fonctionne de manière fluide avec l'intégration OpenAI de LlamaIndex.

## Installation

```bash
pip install llama-index llama-index-llms-openai llama-index-embeddings-openai
```

## Configuration de base

```python
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# Configure LLM
llm = OpenAI(
    model="gpt-4o",
    api_key="sk-your-lemondata-key",
    api_base="https://api.lemondata.cc/v1"
)

# Set as default
Settings.llm = llm

# Simple query
response = llm.complete("What is LemonData?")
print(response.text)
```

## Utilisation de différents modèles

```python
# OpenAI GPT-4o
gpt4 = OpenAI(
    model="gpt-4o",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)

# Anthropic Claude (via OpenAI-compatible endpoint)
claude = OpenAI(
    model="claude-sonnet-4-5",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)

# Google Gemini
gemini = OpenAI(
    model="gemini-2.5-flash",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)
```

## Interface de chat

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(role="system", content="You are a helpful assistant."),
    ChatMessage(role="user", content="What is the capital of France?")
]

response = llm.chat(messages)
print(response.message.content)
```

## Streaming

```python
# Streaming completion
for chunk in llm.stream_complete("Write a poem about AI"):
    print(chunk.delta, end="", flush=True)

# Streaming chat
for chunk in llm.stream_chat(messages):
    print(chunk.delta, end="", flush=True)
```

## Embeddings

```python
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key="sk-your-lemondata-key",
    api_base="https://api.lemondata.cc/v1"
)

# Set as default
Settings.embed_model = embed_model

# Get embeddings
embeddings = embed_model.get_text_embedding("Hello, world!")
print(f"Embedding dimension: {len(embeddings)}")
```

## RAG avec des documents

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Configure settings
Settings.llm = llm
Settings.embed_model = embed_model

# Load documents
documents = SimpleDirectoryReader("./data").load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Query
query_engine = index.as_query_engine()
response = query_engine.query("What is in my documents?")
print(response)
```

## Moteur de chat

```python
# Create chat engine with memory
chat_engine = index.as_chat_engine(chat_mode="condense_question")

# Multi-turn conversation
response = chat_engine.chat("What is LemonData?")
print(response)

response = chat_engine.chat("How many models does it support?")
print(response)
```

## Utilisation asynchrone

```python
import asyncio

async def main():
    response = await llm.acomplete("Hello!")
    print(response.text)

asyncio.run(main())
```

## Variables d'environnement

Pour un code plus propre, utilisez des variables d'environnement :

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
export OPENAI_API_BASE="https://api.lemondata.cc/v1"
```

```python
from llama_index.llms.openai import OpenAI

# Will automatically use environment variables
llm = OpenAI(model="gpt-4o")
```

## Bonnes pratiques

<AccordionGroup>
  <Accordion title="Choisir le bon modèle">
    Utilisez des modèles plus rapides (GPT-4o-mini) pour les tâches d'embedding et de résumé, réservez les modèles puissants (GPT-4o, Claude) pour les réponses finales.
  </Accordion>

  <Accordion title="Optimiser la taille des segments">
    Ajustez la taille des segments (chunk size) en fonction de vos types de documents. Des segments plus petits pour les documents techniques denses, plus grands pour le contenu narratif.
  </Accordion>

  <Accordion title="Utiliser la mise en cache">
    Activez la mise en cache de LlamaIndex pour éviter les appels API redondants pendant le développement.
  </Accordion>
</AccordionGroup>