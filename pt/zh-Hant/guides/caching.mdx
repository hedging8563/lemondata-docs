---
title: "✨ Cache Inteligente"
description: "Reduza custos e latência com cache semântico sensível ao contexto"
---

## Visão Geral

A LemonData oferece um sistema de cache inteligente que reduz significativamente seus custos de API e a latência de resposta. Nosso cache vai além da simples correspondência de requisições; ele compreende o **significado semântico (semantic meaning)** dos seus prompts.

<CardGroup cols={2}>
  <Card title="Economia de Custos" icon="piggy-bank">
    Cache hits são cobrados por apenas uma fração do custo normal.
  </Card>
  <Card title="Respostas Mais Rápidas" icon="bolt">
    Respostas em cache são retornadas instantaneamente, sem necessidade de inferência do modelo.
  </Card>
  <Card title="Sensível ao Contexto" icon="brain">
    A correspondência semântica encontra requisições semelhantes mesmo quando formuladas de forma diferente.
  </Card>
  <Card title="Controle de Privacidade" icon="shield">
    Controle total sobre o que é armazenado em cache e compartilhado.
  </Card>
</CardGroup>

## Como Funciona

A LemonData utiliza um sistema de cache de duas camadas:

### Camada 1: Cache de Resposta (Correspondência Exata)

Para requisições determinísticas (`temperature=0`), armazenamos a resposta exata:

- **Condições de correspondência**: Mesmo modelo, mensagens e parâmetros
- **Velocidade**: Instantânea (nível de microssegundos)
- **Ideal para**: Consultas idênticas repetidas

### Camada 2: Cache Semântico (Correspondência de Similaridade)

Para todas as requisições, também verificamos a similaridade semântica usando um algoritmo de correspondência de duas etapas:

- **Etapa 1 (Apenas consulta)**: Similaridade da consulta do usuário ≥95%
- **Etapa 2 (Contexto completo)**: Similaridade incluindo o contexto da conversa ≥85%
- **Ideal para**: Consultas do tipo FAQ, perguntas frequentes

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
→ Same cached response (high semantic similarity)
```

## Controle de Cache

### Controle em Nível de Requisição

Use o parâmetro `cache_control` no corpo da requisição para controlar o comportamento do cache para cada requisição:

```bash
# Pula a consulta ao cache e sempre obtém uma nova resposta
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello"}],
    "cache_control": {"type": "no_cache"}
  }'
```

| Tipo | Efeito |
|------|------|
| `no_cache` | Pula a consulta ao cache e sempre obtém uma nova resposta |
| `no_store` | Não armazena esta resposta no cache |
| `response_only` | Usa apenas o cache de correspondência exata (pula o cache semântico) |
| `semantic_only` | Usa apenas o cache semântico (pula a correspondência exata) |

### Headers de Resposta

Cada resposta inclui o status do cache:

```
X-Cache-Status: HIT    # Resposta vinda do cache
X-Cache-Status: MISS   # Nova resposta vinda do modelo
```

## Verificando o Status do Cache

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# Verifica o status do cache a partir dos headers da resposta
# (Disponível na resposta HTTP bruta)
print(f"Cache: {response._raw_response.headers.get('X-Cache-Status')}")
```

## Faturamento de Cache

O custo de um cache hit é significativamente menor do que uma nova requisição:

| Tipo | Custo |
|------|------|
| Cache Hit (HIT) | **90% de desconto** |
| Cache Miss (MISS) | Preço original |

O desconto exato é exibido nos logs de uso do seu dashboard.

## Controle de Privacidade

### Nível de Organização / Usuário

Configure o comportamento do cache nas configurações do dashboard:

| Modo | Descrição |
|------|-------------|
| **Compartilhado (Shared)** | Cache ativado, as respostas podem ser compartilhadas entre usuários (padrão para contas pessoais) |
| **Isolado (Isolated)** | Cache ativado, mas as respostas são privadas apenas para sua organização (padrão para contas de organização) |
| **Desativado (Disabled)** | Não utiliza cache de forma alguma |

Outros itens configuráveis:
- **Limiar de similaridade**: Ajusta a sensibilidade da correspondência semântica (padrão: 92%)
- **TTL personalizado**: Sobrescreve o tempo de expiração do cache
- **Excluir modelos**: Desativa o cache para modelos específicos

### Nível de Requisição

Use o parâmetro `cache_control` para sobrescrever as configurações de uma única requisição:

```bash
# Desativa o cache para esta requisição
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "..."}],
    "cache_control": {"type": "no_store"}
  }'
```

## Feedback de Cache

Se você receber uma resposta em cache incorreta, pode reportá-la:

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**Tipos de feedback:**
- `wrong_answer` - Erro factual
- `outdated` - Informação desatualizada
- `irrelevant` - Irrelevante para a pergunta
- `other` - Outros problemas

Quando uma entrada de cache recebe feedback negativo suficiente, ela será invalidada automaticamente.

## Melhores Práticas

<AccordionGroup>
  <Accordion title="Use temperature=0 para consultas cacheáveis">
    Configurações determinísticas maximizam a taxa de cache hit.
  </Accordion>

  <Accordion title="Padronize o formato dos prompts">
    A formatação consistente melhora a correspondência semântica.
  </Accordion>

  <Accordion title="Use no-cache para consultas sensíveis ao tempo">
    Eventos atuais e dados em tempo real devem pular o cache.
  </Accordion>

  <Accordion title="Monitore a taxa de cache hit">
    Veja as estatísticas de cache e o valor economizado no dashboard.
  </Accordion>
</AccordionGroup>

## Quando não usar cache

Desative o cache para as seguintes situações:

- **Informações em tempo real**: Preços de ações, clima, notícias
- **Conteúdo personalizado**: Recomendações para usuários específicos
- **Tarefas criativas**: Quando a diversidade é necessária
- **Dados sensíveis**: Informações confidenciais

```python
# Para consultas sensíveis ao tempo
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_body={"cache_control": {"type": "no_cache"}}
)
```