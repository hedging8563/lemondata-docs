---
title: "LangChain"
description: "Integre o LemonData com o LangChain"
---

## Visão Geral

O LangChain é um framework popular para a construção de aplicações de LLM. O LemonData funciona perfeitamente com a integração OpenAI do LangChain.

## Instalação

```bash
pip install langchain langchain-openai
```

## Configuração Básica

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o",
    api_key="sk-your-lemondata-key",
    base_url="https://api.lemondata.cc/v1"
)

response = llm.invoke("Hello, how are you?")
print(response.content)
```

## Usando Diferentes Modelos

Acesse qualquer modelo do LemonData:

```python
# OpenAI GPT-4o
gpt4 = ChatOpenAI(
    model="gpt-4o",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# Anthropic Claude
claude = ChatOpenAI(
    model="claude-sonnet-4-5",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# Google Gemini
gemini = ChatOpenAI(
    model="gemini-2.5-flash",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# DeepSeek
deepseek = ChatOpenAI(
    model="deepseek-r1",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)
```

## Chat com Histórico de Mensagens

```python
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is the capital of France?")
]

response = llm.invoke(messages)
print(response.content)
```

## Streaming

```python
for chunk in llm.stream("Write a poem about coding"):
    print(chunk.content, end="", flush=True)
```

## Uso Assíncrono

```python
import asyncio

async def main():
    response = await llm.ainvoke("Hello!")
    print(response.content)

asyncio.run(main())
```

## Chains

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that translates {input_language} to {output_language}."),
    ("