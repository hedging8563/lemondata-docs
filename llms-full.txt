# LemonData - Full Documentation

> Unified AI API Gateway - Access 300+ AI models through a single API

Generated: 2026-02-08T16:23:24.084Z
Source: https://docs.lemondata.cc

---


# Documentation


## Getting Started

### Introduction

> Access 300+ AI models through a unified API

URL: https://docs.lemondata.cc/introduction

## What is LemonData?

LemonData is an AI API aggregation platform that provides unified access to **300+ AI models** from leading providers including OpenAI, Anthropic, Google, DeepSeek, and more.

  
  
  

## Supported Capabilities

| Capability | Description | Example Models |
|------------|-------------|----------------|
| **Chat** | Text generation and conversation | GPT-4o, Claude 3.5, Gemini 2.0 |
| **Vision** | Image understanding and analysis | GPT-4o, Claude 3.5 Sonnet, Gemini |
| **Image Generation** | Create images from text | DALL-E 3, Midjourney, Flux, Ideogram |
| **Video Generation** | Create videos from text/images | Sora, Runway Gen-3, Kling |
| **Audio** | Text-to-speech and transcription | Whisper, TTS-1, MiniMax |
| **Embeddings** | Text vectorization | text-embedding-3-small |
| **Music** | AI music generation | Suno |
| **3D Generation** | Create 3D models | Tripo3D |

## Key Features

  
  
  

## Quick Example

## Next Steps

---

### Quickstart

> Get started with LemonData API in 2 minutes

URL: https://docs.lemondata.cc/quickstart

## Step 1: Get Your API Key

  
  

## Step 2: Install SDK

## Step 3: Make Your First Request

## Try Different Models

LemonData supports 300+ models. Just change the `model` parameter:

```python
# OpenAI GPT-4o
response = client.chat.completions.create(model="gpt-4o", messages=messages)

# Anthropic Claude Sonnet 4.5
response = client.chat.completions.create(model="claude-sonnet-4-5", messages=messages)

# Google Gemini 2.5 Flash
response = client.chat.completions.create(model="gemini-2.5-flash", messages=messages)

# DeepSeek R1
response = client.chat.completions.create(model="deepseek-r1", messages=messages)
```

## Enable Streaming

For real-time responses, enable streaming:

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

## What's Next?

---

### Authentication

> Secure your API requests with API keys

URL: https://docs.lemondata.cc/authentication

## API Keys

All API requests require authentication using an API key. Include your key in the `Authorization` header:

```bash
Authorization: Bearer sk-your-api-key
```

## Getting Your API Key

1. Log in to your [LemonData Dashboard](https://lemondata.cc/dashboard)
2. Navigate to **API Keys** section
3. Click **Create New Key**
4. Give your key a descriptive name
5. Copy the key immediately - it's only shown once

## Using API Keys

## API Key Features

### Usage Limits

You can set a usage limit on each API key to control spending:

| Setting | Description |
|---------|-------------|
| **No Limit** | Key uses your account balance without restrictions |
| **Fixed Limit** | Key stops working after reaching the specified amount |

### Key Prefix

All LemonData API keys start with `sk-` prefix. The key format is:

```
sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

## Anthropic API Compatibility

For the `/v1/messages` endpoint, you can use the `x-api-key` header (Anthropic SDK compatible):

```bash
curl https://api.lemondata.cc/v1/messages \
  -H "x-api-key: sk-your-api-key" \
  -H "anthropic-version: 2023-06-01" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Error Responses

| Status Code | Type | Code | Description |
|-------------|------|------|-------------|
| 401 | `invalid_request_error` | `invalid_api_key` | Missing or invalid API key |
| 401 | `invalid_request_error` | `expired_api_key` | API key has been revoked |
| 402 | `insufficient_quota` | `insufficient_quota` | Account balance is insufficient |
| 402 | `insufficient_quota` | `quota_exceeded` | API key usage limit reached |

Example error response:

```json
{
  "error": {
    "message": "Invalid API key provided",
    "type": "invalid_api_key",
    "code": "invalid_api_key"
  }
}
```

---

### âœ¨ Claude Code Skill

> 5-minute quick start guide - Let Claude Code automatically integrate any LemonData API for you

URL: https://docs.lemondata.cc/integrations/claude-code-skill

## What Can This Skill Do?

When you want to use AI features in your code (like GPT-4, image generation, speech recognition, etc.), this skill will:

1. âœ… **Automatically search** LemonData's hundreds of APIs
2. âœ… **Find the best API** for your needs
3. âœ… **Generate complete, runnable code**
4. âœ… **Configure your API Key**
5. âœ… **Provide usage examples** and best practices

**In short: Just describe your needs in natural language, and get complete API integration code!**

## Step 1: Install the Skill

  
  

### Verify Installation

Ask Claude Code:
```
What skills are available?
```

If you see `lemondata-api-integration`, the installation was successful!

## Step 2: Get Your API Key

Before using the skill, you need a LemonData API Key.

  
  
  

## Step 3: Start Using

Using the skill is as simple as chatting!

### Example 1: Using GPT-4

**You say:**
```
I want to use GPT-4 in my Python project
```

**Claude Code will:**
1. Ask for your API Key
2. Search for GPT-4 related APIs
3. Show available API options
4. Generate complete Python code
5. Explain how to use it

**You get:**
```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

### Example 2: Generate Images

**You say:**
```
How do I generate images with Flux in Node.js?
```

**Claude Code will:**
- Search for image generation APIs
- Generate JavaScript code
- Include image download and save code

### Example 3: Speech Recognition

**You say:**
```
Integrate a speech-to-text API
```

**Claude Code will:**
- Ask what programming language you're using
- Search for speech recognition APIs
- Generate code in your chosen language
- Provide audio file handling examples

## Supported Features

This skill can help you integrate all these AI capabilities:

| Feature Type | Examples |
|-------------|----------|
| ðŸ’¬ Chat | GPT-4o, Claude, Gemini, DeepSeek |
| ðŸŽ¨ Image Generation | Midjourney, Flux, Stable Diffusion |
| ðŸŽ¬ Video Generation | Sora, Runway, Kling, Luma AI |
| ðŸŽµ Music Generation | Suno |
| ðŸ—¿ 3D Models | Tripo3D |
| ðŸŽ¤ Audio | Text-to-Speech, Speech-to-Text |
| ðŸ“Š Embeddings | text-embedding-3 |
| ðŸ”„ Rerank | bce-reranker, qwen3-rerank |

## Usage Tips

### 1. Just State Your Needs

  

### 2. Specify Your Language

If you have a specific language requirement, just say it:

```
Use JavaScript to call GPT-4
Use Python for speech recognition
Use Go to generate images
```

### 3. Describe Your Scenario

If you have specific needs, describe them in detail:

```
I'm building a chatbot, need to use GPT-4
My website needs to let users upload and transform images
I'm making a voice notes app, need speech-to-text
```

## Important Considerations

### API Key Security

  

### Cost Management

- Each API call consumes credits
- Set usage limits in your [dashboard](https://lemondata.cc/dashboard)
- Use small data volumes for testing

### Error Handling

Generated code includes basic error handling, but you may want to:
- Add retry mechanisms
- Log errors
- Provide user-friendly error messages

## FAQ

  

  

  

## Resources

  
  
  

## Get Started!

Now you know how to use this skill. Try asking Claude Code:

```
I want to use GPT-4 in my project
```

or

```
Help me integrate image generation
```

Claude Code will automatically use this skill to complete all the work for you!

---


## Guides

### âœ¨ Multi-Format API

> Use OpenAI, Anthropic, or Gemini formats with a single API key

URL: https://docs.lemondata.cc/guides/api-formats

## Overview

LemonData supports **three native API formats** with a single API key. Choose the format that best fits your use case - no configuration changes needed.

  
  

## Why Multi-Format?

| Benefit | Description |
|---------|-------------|
| **No SDK switching** | Use any model with your preferred SDK |
| **Native features** | Access format-specific capabilities |
| **Easy migration** | Switch from official APIs with just a base URL change |
| **Single billing** | One account, one API key, all formats |

## Format Comparison

| Feature | OpenAI | Anthropic | Gemini |
|---------|--------|-----------|--------|
| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |
| **Auth Header** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |
| **System Prompt** | In messages array | Separate `system` field | In `systemInstruction` |
| **Extended Thinking** | âŒ | âœ… | âŒ |
| **Streaming** | âœ… SSE | âœ… SSE | âœ… SSE |
| **Tool Calling** | âœ… | âœ… | âœ… |
| **Vision** | âœ… | âœ… | âœ… |

## OpenAI Format

The most widely compatible format. Works with all LemonData models.

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-lemondata-key",
    base_url="https://api.lemondata.cc/v1"
)

# Works with ANY model
response = client.chat.completions.create(
    model="claude-sonnet-4-5",  # Claude via OpenAI format
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
```

**Best for:**
- General use
- Existing OpenAI SDK integrations
- Maximum compatibility

## Anthropic Format

Native Anthropic Messages API. Required for Claude-specific features like extended thinking.

```python
from anthropic import Anthropic

client = Anthropic(
    api_key="sk-your-lemondata-key",
    base_url="https://api.lemondata.cc"  # No /v1 suffix!
)

message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system="You are a helpful assistant.",  # Separate system field
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```

### Extended Thinking (Claude Opus 4.5)

Only available in Anthropic format:

```python
message = client.messages.create(
    model="claude-opus-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{"role": "user", "content": "Solve this complex problem..."}]
)

# Access thinking process
for block in message.content:
    if block.type == "thinking":
        print(f"Thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Answer: {block.text}")
```

**Best for:**
- Claude-specific features
- Extended thinking mode
- Native Anthropic SDK users

## Gemini Format

Native Google Gemini API format for Google ecosystem integration.

```bash
curl "https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent" \
  -H "Authorization: Bearer sk-your-lemondata-key" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{
      "parts": [{"text": "Hello!"}]
    }],
    "systemInstruction": {
      "parts": [{"text": "You are a helpful assistant."}]
    }
  }'
```

### Streaming

```bash
curl "https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse" \
  -H "Authorization: Bearer sk-your-lemondata-key" \
  -H "Content-Type: application/json" \
  -d '{
    "contents": [{"parts": [{"text": "Write a story"}]}]
  }'
```

**Best for:**
- Google Cloud integrations
- Existing Gemini SDK code
- Native Gemini features

## Choosing the Right Format

```mermaid
graph TD
    A[Which format?] --> B{Need Claude extended thinking?}
    B -->|Yes| C[Use Anthropic Format]
    B -->|No| D{Existing codebase?}
    D -->|OpenAI SDK| E[Use OpenAI Format]
    D -->|Anthropic SDK| C
    D -->|Gemini SDK| F[Use Gemini Format]
    D -->|New project| E
```

## Migration Guides

### From OpenAI Official API

```python
# Before (OpenAI)
client = OpenAI(api_key="sk-openai-key")

# After (LemonData)
client = OpenAI(
    api_key="sk-lemondata-key",
    base_url="https://api.lemondata.cc/v1"  # Add this line
)
# That's it! Same code works
```

### From Anthropic Official API

```python
# Before (Anthropic)
client = Anthropic(api_key="sk-ant-key")

# After (LemonData)
client = Anthropic(
    api_key="sk-lemondata-key",
    base_url="https://api.lemondata.cc"  # Add this line (no /v1!)
)
```

### From Google AI Studio

```python
# Before (Google)
import google.generativeai as genai
genai.configure(api_key="google-api-key")

# After (LemonData) - Use REST API
import requests

response = requests.post(
    "https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent",
    headers={"Authorization": "Bearer sk-lemondata-key"},
    json={"contents": [{"parts": [{"text": "Hello"}]}]}
)
```

## Cross-Model Compatibility

The magic of LemonData: use **any SDK** with **any model**. The gateway automatically handles format conversion.

### Any SDK â†’ Any Model

```python
# Anthropic SDK with GPT-4o (auto-converts to OpenAI format)
from anthropic import Anthropic

client = Anthropic(
    api_key="sk-lemondata-key",
    base_url="https://api.lemondata.cc"
)

response = client.messages.create(
    model="gpt-4o",  # âœ… Works! Auto-converted
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}]
)

# Same SDK, different models - no code changes
response = client.messages.create(model="gemini-2.5-flash", ...)  # âœ… Works!
response = client.messages.create(model="deepseek-r1", ...)       # âœ… Works!
```

### OpenAI SDK â†’ All Models

```python
from openai import OpenAI

client = OpenAI(base_url="https://api.lemondata.cc/v1", api_key="sk-...")

# All these work with the same SDK:
response = client.chat.completions.create(model="gpt-4o", ...)
response = client.chat.completions.create(model="claude-sonnet-4-5", ...)
response = client.chat.completions.create(model="gemini-2.5-flash", ...)
```

### Industry Comparison

| Platform | OpenAI Format | Anthropic Format | Gemini Format | Responses API |
|----------|:---:|:---:|:---:|:---:|
| **LemonData** | âœ… All models | âœ… All models | âœ… All models | âœ… All models |
| OpenRouter | âœ… All models | âŒ | âŒ | âŒ |
| Together AI | âœ… All models | âŒ | âŒ | âŒ |
| Fireworks | âœ… All models | âŒ | âŒ | âŒ |

---

### âœ¨ Intelligent Caching

> Reduce costs and latency with context-aware semantic caching

URL: https://docs.lemondata.cc/guides/caching

## Overview

LemonData provides an intelligent caching system that can significantly reduce your API costs and response latency. Our caching goes beyond simple request matching - it understands the **semantic meaning** of your prompts.

  
  
  

## How It Works

LemonData uses a two-layer caching system:

### Layer 1: Response Cache (Exact Match)

For deterministic requests (`temperature=0`), we cache the exact response:

- **Match**: Identical model, messages, and parameters
- **Speed**: Instant (microseconds)
- **Best for**: Repeated identical queries

### Layer 2: Semantic Cache (Similarity Match)

For all requests, we also check semantic similarity using a two-stage matching algorithm:

- **Stage 1 (Query-only)**: â‰¥95% similarity on user query
- **Stage 2 (Full context)**: â‰¥85% similarity including conversation context
- **Best for**: FAQ-style queries, common questions

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
â†’ Same cached response (high semantic similarity)
```

## Cache Control

### Request-Level Control

Control caching behavior per-request using the `cache_control` parameter in the request body:

```bash
# Skip cache lookup, always call the model
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello"}],
    "cache_control": {"type": "no_cache"}
  }'
```

| Type | Effect |
|------|--------|
| `no_cache` | Skip cache lookup, always get fresh response |
| `no_store` | Don't store this response in cache |
| `response_only` | Only use exact match cache (skip semantic) |
| `semantic_only` | Only use semantic cache (skip exact match) |

### Response Headers

Every response includes cache status:

```
X-Cache-Status: HIT    # Response served from cache
X-Cache-Status: MISS   # Fresh response from model
```

## Checking Cache Status

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# Check cache status from response headers
# (Available in raw HTTP response)
print(f"Cache: {response._raw_response.headers.get('X-Cache-Status')}")
```

## Cache Billing

Cache hits are significantly cheaper than fresh requests:

| Type | Cost |
|------|------|
| Cache HIT | **90% off** |
| Cache MISS | Full price |

The exact discount is shown in your dashboard usage logs.

## Privacy Controls

### Organization / User Level

Configure caching behavior in your dashboard settings:

| Mode | Description |
|------|-------------|
| **Shared** | Cache enabled, responses may be shared across users (default for personal accounts) |
| **Isolated** | Cache enabled, but responses are private to your organization (default for organizations) |
| **Disabled** | No caching at all |

Additional settings available:
- **Similarity Threshold**: Adjust semantic matching sensitivity (default: 92%)
- **Custom TTL**: Override cache expiration time
- **Excluded Models**: Disable caching for specific models

### Request Level

Override per-request using the `cache_control` parameter:

```bash
# Disable caching for this request
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "..."}],
    "cache_control": {"type": "no_store"}
  }'
```

## Cache Feedback

If you receive an incorrect cached response, you can report it:

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**Feedback types:**
- `wrong_answer` - Factually incorrect
- `outdated` - Information is stale
- `irrelevant` - Doesn't match the question
- `other` - Other issues

When a cache entry receives enough negative feedback, it's automatically invalidated.

## Best Practices

  

  

  

## When NOT to Cache

Disable caching for:

- **Real-time information**: Stock prices, weather, news
- **Personalized content**: User-specific recommendations
- **Creative tasks**: When variety is desired
- **Sensitive data**: Confidential information

```python
# For time-sensitive queries
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_body={"cache_control": {"type": "no_cache"}}
)
```

---

### âœ¨ Upstream Prompt Cache

> Understand provider-level prompt caching and how it reduces costs

URL: https://docs.lemondata.cc/guides/prompt-cache

## Overview

In addition to LemonData's [platform semantic cache](/guides/caching), many AI providers offer their own **prompt caching** feature. This is a separate caching mechanism that operates at the provider level (Anthropic, OpenAI, DeepSeek, etc.).

## How Provider Prompt Cache Works

Provider prompt caching stores the processed representation of your prompt prefix on the provider's servers. When you send a request with the same prefix, the provider can skip reprocessing those tokens.

### Key Characteristics

- **Prefix-based**: Only the beginning of your prompt can be cached
- **Exact match**: Requires identical tokens (not semantic similarity)
- **Time-limited**: Cache entries expire (typically 5-60 minutes)
- **Automatic**: No special configuration needed

```
Request 1: [System prompt + Context A + Question 1]
           ^^^^^^^^^^^^^^^^^^^^^^^^
           This prefix gets cached

Request 2: [System prompt + Context A + Question 2]
           ^^^^^^^^^^^^^^^^^^^^^^^^
           Cache hit! Only Question 2 is processed
```

## Supported Providers

| Provider | Cache Read Discount | Cache Write Cost | Min Tokens |
|----------|---------------------|------------------|------------|
| **Anthropic** | 90% off | Same as input | 1024 |
| **OpenAI** | 50% off | Same as input | 1024 |
| **DeepSeek** | 90% off | Same as input | 64 |
| **Google** | 75% off | 25% premium | 32768 |

## Identifying Cache Usage

### In Usage Logs

Your usage logs show detailed cache token breakdown:

| Field | Description |
|-------|-------------|
| `cacheReadTokens` | Tokens served from provider cache (discounted) |
| `cacheWriteTokens` | Tokens written to cache (for future requests) |
| `nonCachedPromptTokens` | Tokens processed without cache |

### In Transactions

Transactions show a **Provider Cache** label when upstream caching was used:

- **Cache** (sky blue): Platform semantic cache hit - 90% discount
- **Provider Cache** (teal): Upstream prompt cache hit - discounted rates

## Cost Calculation Example

For a request with 10,000 input tokens to Claude (Anthropic):

**Without cache:**
```
10,000 tokens Ã— $3.00/1M = $0.030
```

**With provider cache (8,000 cached + 2,000 new):**
```
Cache read:  8,000 tokens Ã— $0.30/1M = $0.0024  (90% off)
Cache write: 2,000 tokens Ã— $3.00/1M = $0.0060
Total: $0.0084 (72% savings)
```

## Best Practices

  

  

  

## Platform Cache vs Provider Cache

| Aspect | Platform Cache | Provider Cache |
|--------|----------------|----------------|
| **Matching** | Semantic similarity | Exact prefix match |
| **Cost** | 10% of normal price | Discounted rates |
| **Latency** | Instant (~1ms) | Reduced (skip processing) |
| **Control** | Dashboard settings | Automatic |
| **Scope** | Cross-user (optional) | Per-API-key |

### When Each Applies

```
Request arrives
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Platform Cache Hit? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ Yes              â”‚ No
    â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return  â”‚    â”‚ Call Upstream API   â”‚
â”‚ Cached  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ (10%)   â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Provider Cache Hit? â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Yes        â”‚ No
                   â–¼            â–¼
               Discounted    Full Price
               Token Rate    Token Rate
```

## Checking Cache Status

### Response Headers

```
X-Cache-Status: HIT           # Platform cache hit
X-Cache-Status: MISS          # No platform cache
X-Upstream-Cache-Read: 8000   # Provider cache read tokens
X-Upstream-Cache-Write: 2000  # Provider cache write tokens
```

### Usage API

Query your usage logs to see cache breakdown:

```bash
curl https://api.lemondata.cc/v1/usage/logs \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json"
```

Response includes:
```json
{
  "promptTokens": 10000,
  "cacheReadTokens": 8000,
  "cacheWriteTokens": 2000,
  "nonCachedPromptTokens": 0,
  "completionTokens": 500,
  "cost": 0.0084
}
```

## FAQ

---

### SDKs & Libraries

> Official and community SDKs for LemonData

URL: https://docs.lemondata.cc/guides/sdks

## Overview

LemonData is **OpenAI-compatible**, which means you can use any OpenAI SDK with just a base URL change. We also support native Anthropic and Google Gemini SDKs.

## Official SDKs

Since LemonData is OpenAI-compatible, use the official OpenAI SDKs:

  
  
  

## Configuration

### Python

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc/v1"
)

# Use any model
response = client.chat.completions.create(
    model="gpt-4o",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### JavaScript / TypeScript

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: 'sk-your-api-key',
  baseURL: 'https://api.lemondata.cc/v1'
});

const response = await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'Hello!' }]
});
```

### Go

```go
package main

import (
    "context"
    "fmt"
    openai "github.com/sashabaranov/go-openai"
)

func main() {
    config := openai.DefaultConfig("sk-your-api-key")
    config.BaseURL = "https://api.lemondata.cc/v1"

    client := openai.NewClientWithConfig(config)

    resp, err := client.CreateChatCompletion(
        context.Background(),
        openai.ChatCompletionRequest{
            Model: "gpt-4o",
            Messages: []openai.ChatCompletionMessage{
                {Role: "user", Content: "Hello!"},
            },
        },
    )
    if err != nil {
        panic(err)
    }
    fmt.Println(resp.Choices[0].Message.Content)
}
```

### .NET / C#

```csharp
using OpenAI;

var options = new OpenAIClientOptions
{
    Endpoint = new Uri("https://api.lemondata.cc/v1")
};

var client = new OpenAIClient("sk-your-api-key", options);

var chat = client.GetChatClient("gpt-4o");
var response = await chat.CompleteChatAsync("Hello!");

Console.WriteLine(response.Value.Content[0].Text);
```

### cURL

```bash
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Environment Variables

We recommend using environment variables for API keys:

```bash
# .env or shell profile
export LEMONDATA_API_KEY="sk-your-api-key"
export LEMONDATA_BASE_URL="https://api.lemondata.cc/v1"
```

```python
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.environ.get("LEMONDATA_API_KEY"),
    base_url=os.environ.get("LEMONDATA_BASE_URL")
)
```

## Anthropic SDK

For Anthropic-native requests, use the Anthropic SDK:

```python
from anthropic import Anthropic

client = Anthropic(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc"
)

message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## LangChain Integration

See the [LangChain Integration](/integrations/langchain) guide for framework integration.

---

### Streaming

> Implement real-time streaming responses

URL: https://docs.lemondata.cc/guides/streaming

## Overview

Streaming allows you to receive partial responses as they're generated, providing a better user experience for chat applications.

## Enable Streaming

Set `stream: true` in your request:

## Stream Response Format

Each chunk in the stream follows this format:

```
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4o","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4o","choices":[{"index":0,"delta":{"content":" world"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1234567890,"model":"gpt-4o","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

## Handling Stream End

The stream ends with:
- `finish_reason: "stop"` - Normal completion
- `finish_reason: "length"` - Hit max_tokens limit
- `finish_reason: "tool_calls"` - Model wants to call a tool
- `data: [DONE]` - Final message

## Collecting Full Response

To collect the complete response while streaming:

```python
full_response = ""

for chunk in stream:
    if chunk.choices[0].delta.content:
        content = chunk.choices[0].delta.content
        full_response += content
        print(content, end="", flush=True)

print(f"\n\nFull response: {full_response}")
```

## Async Streaming

For async applications:

```python
import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI(
        api_key="sk-your-api-key",
        base_url="https://api.lemondata.cc/v1"
    )

    stream = await client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}],
        stream=True
    )

    async for chunk in stream:
        if chunk.choices[0].delta.content:
            print(chunk.choices[0].delta.content, end="")

asyncio.run(main())
```

## Web Application Example

For a web chat interface:

```javascript
async function streamChat(message) {
  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': 'Bearer sk-your-api-key',
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: message }],
      stream: true
    })
  });

  const reader = response.body.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n').filter(line => line.startsWith('data: '));

    for (const line of lines) {
      const data = line.slice(6);
      if (data === '[DONE]') return;

      const parsed = JSON.parse(data);
      const content = parsed.choices[0]?.delta?.content;
      if (content) {
        // Append to your UI
        document.getElementById('output').textContent += content;
      }
    }
  }
}
```

---

### Error Handling

> Handle API errors gracefully

URL: https://docs.lemondata.cc/guides/error-handling

## Error Response Format

All errors return a consistent JSON format:

```json
{
  "error": {
    "message": "Human-readable error description",
    "type": "error_type",
    "code": "error_code",
    "param": "parameter_name"  // Optional, for validation errors
  }
}
```

## HTTP Status Codes

| Code | Description |
|------|-------------|
| 400 | Bad Request - Invalid parameters |
| 401 | Unauthorized - Invalid or missing API key |
| 402 | Payment Required - Insufficient balance |
| 403 | Forbidden - Access denied or model not allowed |
| 404 | Not Found - Model or resource not found |
| 413 | Payload Too Large - Input or file size exceeded |
| 429 | Too Many Requests - Rate limit exceeded |
| 500 | Internal Server Error |
| 502 | Bad Gateway - Upstream provider error |
| 503 | Service Unavailable - All channels failed |
| 504 | Gateway Timeout - Request timed out |

## Error Types

### Authentication Errors (401)

| Type | Code | Description |
|------|------|-------------|
| `invalid_api_key` | `invalid_api_key` | API key is missing or invalid |
| `expired_api_key` | `expired_api_key` | API key has been revoked |

```python
from openai import OpenAI, AuthenticationError

try:
    response = client.chat.completions.create(...)
except AuthenticationError as e:
    print(f"Authentication failed: {e.message}")
```

### Payment Errors (402)

| Type | Code | Description |
|------|------|-------------|
| `insufficient_quota` | `insufficient_quota` | Account balance is too low |
| `quota_exceeded` | `quota_exceeded` | API key usage limit reached |

```python
from openai import OpenAI, APIStatusError

try:
    response = client.chat.completions.create(...)
except APIStatusError as e:
    if e.status_code == 402:
        print("Please top up your account balance")
```

### Access Errors (403)

| Type | Code | Description |
|------|------|-------------|
| `access_denied` | `access_denied` | Access to resource denied |
| `access_denied` | `model_not_allowed` | Model not allowed for this API key |

```json
{
  "error": {
    "message": "You don't have permission to access this model",
    "type": "access_denied",
    "code": "model_not_allowed"
  }
}
```

### Validation Errors (400)

| Type | Description |
|------|-------------|
| `invalid_request_error` | Request parameters are invalid |
| `context_length_exceeded` | Input too long for model |
| `model_not_found` | Requested model doesn't exist |

```json
{
  "error": {
    "message": "model: 'invalid-model' is not a valid model",
    "type": "model_not_found",
    "param": "model"
  }
}
```

### Rate Limit Errors (429)

When you exceed rate limits:

```json
{
  "error": {
    "message": "Rate limit exceeded. Please retry later.",
    "type": "rate_limit_error",
    "code": "rate_limit_exceeded"
  }
}
```

**Header included:**

```
Retry-After: 60
```

The `Retry-After` header indicates how many seconds to wait before retrying.

### Payload Too Large (413)

When input or file size exceeds limits:

```json
{
  "error": {
    "message": "Input size exceeds maximum allowed",
    "type": "invalid_request_error",
    "code": "payload_too_large"
  }
}
```

Common causes:
- Image file too large (max 20MB)
- Audio file too large (max 25MB)
- Input text exceeds model context length

### Upstream Errors (502, 503)

| Type | Description |
|------|-------------|
| `upstream_error` | Provider returned an error |
| `all_channels_failed` | No available providers |
| `timeout_error` | Request timed out |

## Handling Errors in Python

```python
from openai import OpenAI, APIError, RateLimitError, APIConnectionError

client = OpenAI(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc/v1"
)

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError as e:
            if attempt  setTimeout(r, 2 ** attempt * 1000));
          continue;
        }
      }
      throw error;
    }
  }
}
```

## Best Practices

---

### Rate Limits

> Understanding and handling rate limits

URL: https://docs.lemondata.cc/guides/rate-limits

## Overview

LemonData implements rate limits to ensure fair usage and platform stability. Limits vary by account tier.

## Rate Limit Tiers

| Tier | Requests/min | Description |
|------|-------------|-------------|
| **User** | 1,000 | Default tier for all accounts |
| **Partner** | 3,000 | For integration partners |
| **VIP** | 10,000 | High-volume users |

## Rate Limit Response

When you exceed the rate limit, the API returns a `429` status code with a `Retry-After` header indicating how long to wait before retrying.

## Rate Limit Exceeded

When you exceed the limit, you'll receive a `429` response:

```json
{
  "error": {
    "message": "Rate limit exceeded. Please retry later.",
    "type": "rate_limit_error",
    "code": "rate_limit_exceeded"
  }
}
```

The response includes a `Retry-After` header:
```
Retry-After: 60  # Seconds to wait before retrying
```

## Handling Rate Limits

### Exponential Backoff

Implement exponential backoff for automatic retries:

```python
import time
from openai import OpenAI, RateLimitError

client = OpenAI(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc/v1"
)

def make_request_with_backoff(messages, max_retries=5):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError as e:
            if attempt == max_retries - 1:
                raise

            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds
            print(f"Rate limited. Waiting {wait_time}s...")
            time.sleep(wait_time)
```

### Request Queuing

For high-volume applications, implement a request queue:

```python
import asyncio
from collections import deque

class RateLimitedClient:
    def __init__(self, requests_per_minute=60):
        self.rpm = requests_per_minute
        self.interval = 60 / requests_per_minute
        self.last_request = 0

    async def request(self, messages):
        # Wait if needed to respect rate limit
        now = asyncio.get_event_loop().time()
        wait_time = max(0, self.last_request + self.interval - now)
        if wait_time > 0:
            await asyncio.sleep(wait_time)

        self.last_request = asyncio.get_event_loop().time()
        return await self.client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )
```

### Batch Processing

For bulk operations, process in batches with delays:

```python
def process_batch(items, batch_size=50, delay=1):
    results = []
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        for item in batch:
            result = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": item}]
            )
            results.append(result)
        time.sleep(delay)  # Pause between batches
    return results
```

## Best Practices

  

  

  

## Upgrading Your Tier

To request a tier upgrade:

1. Log in to your [Dashboard](https://lemondata.cc/dashboard)
2. Go to **Settings â†’ Account**
3. Contact support with your use case

Or email support@lemondata.cc with:
- Your account email
- Expected request volume
- Use case description

---

### Billing & Pricing

> Understand LemonData

URL: https://docs.lemondata.cc/guides/billing

## Overview

LemonData uses **pay-as-you-go pricing**. You only pay for what you use, with no subscriptions or minimum commitments.

## How Billing Works

1. **Add credits** to your account
2. **Use the API** - costs are deducted per request
3. **Monitor usage** in your dashboard
4. **Top up** when your balance is low

## Pricing Models

### Per-Token Pricing

Most text generation models are priced per million tokens:

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| GPT-4o | $1.75 | $7.00 |
| GPT-4o-mini | $0.105 | $0.42 |
| Claude Opus 4.5 | $3.50 | $17.50 |
| Claude Sonnet 4.5 | $2.10 | $10.50 |
| Claude Haiku 4.5 | $0.70 | $3.50 |
| Gemini 2.5 Pro | $0.875 | $7.00 |
| Gemini 2.5 Flash | $0.1225 | $0.525 |
| DeepSeek R1 | $0.385 | $1.533 |
| DeepSeek V3.2 | $0.196 | $0.294 |

### Per-Request Pricing

Image, video, and audio models are priced per request:

| Model | Price per Request |
|-------|-------------------|
| DALL-E 3 (1024x1024) | $0.04 |
| Midjourney | $0.05 |
| Sora Video (5s) | $0.20 |
| Whisper (per minute) | $0.006 |
| TTS-1 | $0.015 |

### Async Task Billing (Video/Music/3D)

For async generation tasks (video, music, 3D models):

1. **Billing occurs immediately** when you submit the task
2. You receive a `task_id` to poll for status
3. **No automatic refunds** if the task fails

This is because upstream providers charge us when the task is submitted, regardless of the outcome. If you experience task failures, please contact support@lemondata.cc for assistance.

```python
# Example: Video generation billing
response = client.post("/v1/video/generations", json={
    "model": "kling-v1",
    "prompt": "A sunset over the ocean"
})
# âš ï¸ You are billed NOW, not when the video is ready

task_id = response.json()["task_id"]
# Poll for status...
```

## Token Counting

Tokens are the basic units of text processing:

- ~4 characters = 1 token (English)
- ~1-2 characters = 1 token (Chinese)
- 1 image = varies by size and detail

### Estimating Tokens

```python
# Rough estimation
def estimate_tokens(text):
    return len(text) / 4  # Approximate for English

# Actual count (for OpenAI models)
import tiktoken
encoder = tiktoken.encoding_for_model("gpt-4o")
tokens = encoder.encode("Your text here")
print(f"Token count: {len(tokens)}")
```

## Usage Tracking

### Dashboard

Monitor your usage in the [Dashboard](https://lemondata.cc/dashboard):

- Real-time balance
- Usage history by model
- Cost breakdown
- API key usage

### API Response

Each response includes usage information:

```json
{
  "usage": {
    "prompt_tokens": 50,
    "completion_tokens": 100,
    "total_tokens": 150
  }
}
```

## Cost Optimization

  

  

  

  

## Low Balance Alerts

Configure alerts when your balance drops:

1. Go to **Dashboard â†’ Settings â†’ Notifications**
2. Set your threshold amount
3. Receive email notifications

## Adding Credits

### Payment Methods

- Alipay (æ”¯ä»˜å®)
- WeChat Pay (å¾®ä¿¡æ”¯ä»˜)
- Stripe (Visa, Mastercard)

### Steps

1. Log in to [Dashboard](https://lemondata.cc/dashboard)
2. Click **Add Credits**
3. Select amount and payment method
4. Complete payment

Credits are added instantly after payment confirmation.

## API Key Limits

You can set spending limits on individual API keys:

1. Go to **Dashboard â†’ API Keys**
2. Click on a key to edit
3. Set **Usage Limit**

When the limit is reached, requests with that key will return `402 Payment Required`.

## Invoices

For business accounts, invoices are available:

1. Go to **Dashboard â†’ Billing**
2. View transaction history
3. Download invoices as PDF

## Questions?

Contact support@lemondata.cc for billing inquiries.

---

### Best Practices

> Optimize your LemonData API usage for cost, performance, and reliability

URL: https://docs.lemondata.cc/guides/best-practices

## Model Selection

Choosing the right model can significantly impact cost and quality.

### Task-Based Recommendations

| Task | Recommended Models | Reasoning |
|------|-------------------|-----------|
| **Simple Q&A** | `gpt-4o-mini`, `gemini-2.5-flash` | Fast, cheap, good enough |
| **Complex reasoning** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Better logic and planning |
| **Coding** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimized for code |
| **Creative writing** | `claude-sonnet-4-5`, `gpt-4o` | Better prose quality |
| **Vision/Images** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Native vision support |
| **Long context** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 1M+ token windows |
| **Cost-sensitive** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Best value |

### Cost Tiers

```
$$$$ Premium: o3, claude-opus-4-5, gpt-4o
$$$  Standard: claude-sonnet-4-5, gpt-4o
$$   Budget:   gpt-4o-mini, gemini-2.5-flash
$    Economy:  deepseek-v3.2, deepseek-r1
```

## Cost Optimization

### 1. Use Smaller Models First

```python
def smart_query(question: str, complexity: str = "auto"):
    """Use cheaper models for simple tasks."""

    if complexity == "simple":
        model = "gpt-4o-mini"
    elif complexity == "complex":
        model = "gpt-4o"
    else:
        # Start cheap, escalate if needed
        model = "gpt-4o-mini"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": question}]
    )
    return response
```

### 2. Set max_tokens

Always set a reasonable `max_tokens` limit:

```python
# âŒ Bad: No limit, could generate thousands of tokens
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}]
)

# âœ… Good: Limit response length
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}],
    max_tokens=500  # Reasonable limit for a summary
)
```

### 3. Optimize Prompts

```python
# âŒ Verbose prompt (more input tokens)
prompt = """
I would like you to please help me by analyzing the following text
and providing a comprehensive summary of the main points. Please be
thorough but also concise in your response. The text is as follows:
{text}
"""

# âœ… Concise prompt (fewer tokens)
prompt = "Summarize the key points:\n{text}"
```

### 4. Enable Caching

Take advantage of [semantic caching](/guides/caching):

```python
# For repeated similar queries, caching provides major savings
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is machine learning?"}],
    temperature=0  # Deterministic = better cache hits
)
```

### 5. Batch Similar Requests

```python
# âŒ Many small requests
for question in questions:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": question}]
    )

# âœ… Fewer larger requests
combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"Answer each question:\n{combined_prompt}"}]
)
```

## Performance Optimization

### 1. Use Streaming for UX

Streaming improves perceived performance:

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 2. Choose Fast Models for Interactive Use

| Use Case | Recommended | Latency |
|----------|-------------|---------|
| Chat UI | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms first token |
| Tab completion | `claude-haiku-4-5` | ~150ms first token |
| Background processing | `gpt-4o`, `claude-sonnet-4-5` | ~500ms first token |

### 3. Set Timeouts

```python
client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    timeout=60.0  # 60 second timeout
)
```

## Reliability

### 1. Implement Retries

```python
import time
from openai import RateLimitError, APIError

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError:
            wait = 2 ** attempt
            print(f"Rate limited, waiting {wait}s...")
            time.sleep(wait)
        except APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(1)
    raise Exception("Max retries exceeded")
```

### 2. Handle Errors Gracefully

```python
from openai import APIError, AuthenticationError, RateLimitError

try:
    response = client.chat.completions.create(...)
except AuthenticationError:
    # Check API key
    notify_admin("Invalid API key")
except RateLimitError:
    # Queue for later or use backup
    add_to_queue(request)
except APIError as e:
    if e.status_code == 402:
        notify_admin("Balance low")
    elif e.status_code >= 500:
        # Server error, retry later
        schedule_retry(request)
```

### 3. Use Fallback Models

```python
FALLBACK_CHAIN = ["gpt-4o", "claude-sonnet-4-5", "gemini-2.5-flash"]

def chat_with_fallback(messages):
    for model in FALLBACK_CHAIN:
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages
            )
        except APIError:
            continue
    raise Exception("All models failed")
```

## Security

### 1. Protect API Keys

```python
# âŒ Never hardcode keys
client = OpenAI(api_key="sk-abc123...")

# âœ… Use environment variables
import os
client = OpenAI(api_key=os.environ["LEMONDATA_API_KEY"])
```

### 2. Validate User Input

```python
def validate_message(content: str) -> bool:
    """Validate user input before sending to API."""
    if len(content) > 100000:
        raise ValueError("Message too long")
    # Add other validation as needed
    return True
```

### 3. Set API Key Limits

Create separate API keys with spending limits for:
- Development/testing
- Production
- Different applications

## Monitoring

### 1. Track Usage

Check your dashboard regularly for:
- Token usage by model
- Cost breakdown
- Cache hit rates
- Error rates

### 2. Log Important Metrics

```python
import logging

response = client.chat.completions.create(...)

logging.info({
    "model": response.model,
    "prompt_tokens": response.usage.prompt_tokens,
    "completion_tokens": response.usage.completion_tokens,
    "total_tokens": response.usage.total_tokens,
})
```

### 3. Set Up Alerts

Configure low balance alerts in your dashboard to avoid service interruption.

## Checklist

---

### IDE & SDK Compatibility

> Full compatibility reference for AI coding tools, SDKs, and frameworks

URL: https://docs.lemondata.cc/guides/ide-sdk-compatibility

## Overview

LemonData API is designed for **drop-in compatibility** with all major AI development tools. This guide documents supported parameters and verified integrations.

## Supported API Formats

| Endpoint | Format | Use Case |
|----------|--------|----------|
| `/v1/chat/completions` | OpenAI Chat | Universal compatibility |
| `/v1/responses` | OpenAI Responses | Stateful conversations |
| `/v1/messages` | Anthropic Messages | Claude native features |
| `/v1beta/models/:model:generateContent` | Google Gemini | Gemini native features |

## IDE & CLI Compatibility

### Verified Tools

| Tool | Status | Format | Notes |
|------|--------|--------|-------|
| **Cursor** | âœ… Full | OpenAI | Anthropic tool format supported |
| **Claude Code CLI** | âœ… Full | Anthropic | Extended thinking, tool_choice |
| **Windsurf** | âœ… Full | OpenAI | Standard OpenAI format |
| **Aider** | âœ… Full | OpenAI | All models supported |
| **Continue.dev** | âœ… Full | OpenAI/Anthropic | Dual format support |
| **OpenCode** | âœ… Full | OpenAI | Multi-provider support |
| **Cline/Roo Code** | âœ… Full | OpenAI | Via OpenRouter format |
| **GitHub Copilot** | âœ… Full | OpenAI | Standard format |
| **Codex CLI** | âœ… Full | OpenAI | OpenAI Responses API |
| **Gemini CLI** | âœ… Full | Gemini | Native Gemini format |

### Configuration Examples

  
  
  

## SDK Compatibility

### Verified SDKs

| SDK | Language | Status | Notes |
|-----|----------|--------|-------|
| **OpenAI SDK** | Python/JS/Go | âœ… Full | All parameters supported |
| **Anthropic SDK** | Python/JS | âœ… Full | Extended thinking, tools |
| **Vercel AI SDK** | TypeScript | âœ… Full | streamText, generateObject |
| **LangChain** | Python/JS | âœ… Full | ChatOpenAI, bind_tools |
| **LlamaIndex** | Python | âœ… Full | OpenAI-compatible |
| **Dify** | - | âœ… Full | OpenAI format |

## Chat Completions Parameters

### Core Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | string | Model identifier (required) |
| `messages` | array | Conversation messages (required) |
| `max_tokens` | integer | Maximum output tokens |
| `temperature` | number | Sampling temperature (0-2) |
| `top_p` | number | Nucleus sampling (0-1) |
| `stream` | boolean | Enable streaming |

### Tool Calling

```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": { "type": "string" }
          }
        },
        "strict": true
      }
    }
  ],
  "tool_choice": "auto",
  "parallel_tool_calls": true
}
```

### Tool Choice Options

| Format | Example | Description |
|--------|---------|-------------|
| String | `"auto"`, `"none"`, `"required"` | Simple selection |
| OpenAI Object | `{ "type": "function", "function": { "name": "fn" } }` | Force specific function |
| Anthropic Object | `{ "type": "tool", "name": "fn", "disable_parallel_tool_use": true }` | Anthropic native format |

### Advanced Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `stream_options` | object | `{ include_usage: true }` for token counts |
| `reasoning_effort` | string | `"low"`, `"medium"`, `"high"` for o1/o3 models |
| `service_tier` | string | `"auto"` or `"default"` |
| `seed` | integer | Deterministic outputs |
| `logprobs` | boolean | Return log probabilities |
| `top_logprobs` | integer | Number of top logprobs (0-20) |
| `logit_bias` | object | Token bias map (-100 to 100) |
| `frequency_penalty` | number | Repetition penalty (-2 to 2) |
| `presence_penalty` | number | Topic penalty (-2 to 2) |
| `stop` | string/array | Stop sequences |
| `n` | integer | Number of completions (1-128) |
| `user` | string | User identifier for tracking |

### OpenAI Advanced Features

| Parameter | Type | Description |
|-----------|------|-------------|
| `modalities` | array | `["text", "audio"]` for multimodal |
| `audio` | object | Audio output config (voice, format) |
| `prediction` | object | Predicted output for faster completion |
| `metadata` | object | Key-value pairs for tracking |
| `store` | boolean | Store for later retrieval |

### Provider-Specific Options

```json
{
  "anthropic_options": {
    "thinking": {
      "type": "enabled",
      "budget_tokens": 10000
    },
    "prompt_caching": true
  },
  "google_options": {
    "safety_settings": [...],
    "google_search": true,
    "code_execution": true
  }
}
```

## Anthropic Messages Parameters

### Core Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | string | Model identifier |
| `messages` | array | Conversation messages |
| `max_tokens` | integer | Maximum output (up to 128000) |
| `system` | string/array | System prompt |
| `stream` | boolean | Enable streaming |

### Tool Calling

```json
{
  "tools": [
    {
      "name": "get_weather",
      "description": "Get weather",
      "input_schema": {
        "type": "object",
        "properties": {
          "location": { "type": "string" }
        }
      }
    }
  ],
  "tool_choice": {
    "type": "auto",
    "disable_parallel_tool_use": false
  }
}
```

### Extended Thinking

```json
{
  "model": "claude-opus-4-5",
  "thinking": {
    "type": "enabled",
    "budget_tokens": 10000
  }
}
```

## Responses API Parameters

### Core Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | string | Model identifier |
| `input` | string/array | Input content |
| `instructions` | string | System instructions |
| `max_output_tokens` | integer | Maximum output tokens |
| `previous_response_id` | string | Continue conversation |

### Advanced Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `truncation_strategy` | string | `"auto"` or `"disabled"` |
| `include` | array | `["reasoning.encrypted_content"]` |
| `reasoning_effort` | string | For reasoning models |
| `service_tier` | string | Priority tier |

### Tool Format

Supports both OpenAI and Anthropic tool formats:

```json
// OpenAI format
{ "type": "function", "name": "fn", "parameters": {...} }

// Anthropic format (Cursor compatibility)
{ "name": "fn", "input_schema": {...} }
```

## Gemini API Parameters

### Core Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `contents` | array | Conversation content |
| `systemInstruction` | object | System prompt |
| `generationConfig` | object | Generation settings |

### Tools

```json
{
  "tools": [{
    "functionDeclarations": [{
      "name": "search",
      "description": "Search the web",
      "parameters": {...}
    }],
    "codeExecution": {},
    "googleSearch": {}
  }],
  "toolConfig": {
    "functionCallingConfig": {
      "mode": "AUTO"
    }
  }
}
```

### Safety Settings

```json
{
  "safetySettings": [
    {
      "category": "HARM_CATEGORY_HARASSMENT",
      "threshold": "BLOCK_MEDIUM_AND_ABOVE"
    }
  ]
}
```

### Additional Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `cachedContent` | string | Cached content reference |
| `responseMimeType` | string | `"text/plain"` or `"application/json"` |
| `responseSchema` | object | JSON schema for structured output |

## Streaming

All endpoints support Server-Sent Events (SSE) streaming:

```bash
# Chat Completions
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-xxx" \
  -d '{"model": "gpt-4o", "messages": [...], "stream": true}'

# With usage tracking
-d '{"...", "stream_options": {"include_usage": true}}'
```

## Error Handling

LemonData returns OpenAI-compatible error responses:

```json
{
  "error": {
    "message": "Invalid API key",
    "type": "invalid_api_key",
    "code": "invalid_api_key"
  }
}
```

See [Error Handling Guide](/guides/error-handling) for details.

## Best Practices

---


# API Reference


## Overview

### API Reference

> Complete reference for the LemonData API

URL: https://docs.lemondata.cc/api-reference/introduction

## Overview

The LemonData API is **OpenAI-compatible**, which means you can use the official OpenAI SDK with just a base URL change. We also support native **Anthropic** and **Gemini** request formats.

## Base URL

```
https://api.lemondata.cc
```

## Authentication

All API endpoints require authentication using a Bearer token:

```bash
Authorization: Bearer sk-your-api-key
```

Get your API key from the [Dashboard](https://lemondata.cc/dashboard).

## Supported Endpoints

### Chat & Text Generation

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/chat/completions` | POST | OpenAI-compatible chat completions |
| `/v1/messages` | POST | Anthropic-compatible messages API |
| `/v1/responses` | POST | OpenAI Responses API |

### Embeddings & Rerank

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/embeddings` | POST | Create text embeddings |
| `/v1/rerank` | POST | Rerank documents |

### Images

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/images/generations` | POST | Generate images from text |
| `/v1/images/edits` | POST | Edit images |

### Audio

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/audio/speech` | POST | Text-to-speech (TTS) |
| `/v1/audio/transcriptions` | POST | Speech-to-text (STT) |

### Video

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/videos/generations` | POST | Create video generation task |
| `/v1/videos/generations/{id}` | GET | Get video task status |

### Music

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/music/generations` | POST | Create music generation task |
| `/v1/music/generations/{id}` | GET | Get music task status |

### 3D Generation

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/3d/generations` | POST | Create 3D model generation task |
| `/v1/3d/generations/{id}` | GET | Get 3D task status |

### Models

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/models` | GET | List all available models |
| `/v1/models/{model}` | GET | Get specific model info |

### Gemini (v1beta)

Native Google Gemini API format support:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1beta/models/{model}:generateContent` | POST | Generate content (Gemini format) |
| `/v1beta/models/{model}:streamGenerateContent` | POST | Stream generate content (Gemini format) |

## Response Format

All responses follow a consistent format:

### Success Response

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gpt-4o",
  "choices": [...],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

### Routing Transparency

All responses include a `_routing` field with channel information:

```json
{
  "id": "chatcmpl-abc123",
  ...,
  "_routing": {
    "channel": {
      "id": "ch_xxx",
      "name": "channel-name",
      "provider": "openai",
      "channelType": "PLATFORM"
    },
    "cached": false,
    "retryCount": 0
  }
}
```

| Field | Description |
|-------|-------------|
| `channel.id` | Channel identifier used |
| `channel.provider` | Upstream provider (openai, anthropic, etc.) |
| `channel.channelType` | `PLATFORM` (LemonData) or `PRIVATE` (BYOK) |
| `cached` | Whether response was served from cache |
| `retryCount` | Number of retry attempts (if any) |

### Error Response

```json
{
  "error": {
    "message": "Invalid API key provided",
    "type": "invalid_api_key",
    "code": "invalid_api_key"
  }
}
```

## Rate Limits

Rate limits are role-based and configurable by administrators. Default values:

| Role | Requests/min |
|------|-------------|
| User | 60 |
| Partner | 300 |
| VIP | 1,000 |

When rate limits are exceeded, the API returns a `429` status code with a `Retry-After` header indicating how long to wait.

## OpenAPI Specification

---


## Chat

### Create Chat Completion

> Creates a completion for the chat message

URL: https://docs.lemondata.cc/api-reference/chat/create-completion

## Request Body

## Response

---


## Messages

### Create Message

> Creates a message using the Anthropic Messages API format

URL: https://docs.lemondata.cc/api-reference/messages/create-message

## Overview

This endpoint provides native Anthropic Messages API compatibility. Use this for Claude models with features like extended thinking.

## Request Headers

## Request Body

## Response

## Extended Thinking Example

```python
message = client.messages.create(
    model="claude-opus-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{"role": "user", "content": "Solve this math problem..."}]
)

for block in message.content:
    if block.type == "thinking":
        print(f"Thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Response: {block.text}")
```

---


## Responses

### Create Response

> Creates a response using the OpenAI Responses API format

URL: https://docs.lemondata.cc/api-reference/responses/create-response

The Responses API is OpenAI's newer stateful conversation API. LemonData supports this format for compatible models.

## Request Body

## Response

---


## Gemini

### Generate Content

> Generates content using Google Gemini API format

URL: https://docs.lemondata.cc/api-reference/gemini/generate-content

LemonData supports the native Google Gemini API format for Gemini models. This allows direct compatibility with Google AI SDKs.

## Path Parameters

## Query Parameters

## Authentication

Gemini endpoints support multiple authentication methods:
- `?key=YOUR_API_KEY` query parameter
- `x-goog-api-key: YOUR_API_KEY` header
- `Authorization: Bearer YOUR_API_KEY` header

## Request Body

## Response

---

### Stream Generate Content

> Streams content generation using Google Gemini API format

URL: https://docs.lemondata.cc/api-reference/gemini/stream-generate-content

Streaming version of the Gemini generateContent endpoint. Returns Server-Sent Events.

## Path Parameters

## Query Parameters

## Request Body

Same as [Generate Content](/api-reference/gemini/generate-content).

## Response

Returns a stream of JSON objects, each containing a partial response.

---


## Images

### Create Image

> Creates an image given a prompt

URL: https://docs.lemondata.cc/api-reference/images/create-image

## Request Body

## Response

### Synchronous Response (DALL-E, Flux Schnell, etc.)

### Asynchronous Response (Midjourney, Flux Pro, Ideogram, etc.)

Some models require longer processing time and return an async response:

## Available Models

| Model | Type | Features |
|-------|------|----------|
| `dall-e-3` | Sync | Best quality, prompt enhancement |
| `dall-e-2` | Sync | Faster, more affordable |
| `flux-pro` | **Async** | Photorealistic, high quality |
| `flux-schnell` | Sync | Very fast |
| `midjourney` | **Async** | Artistic style |
| `ideogram-v3` | **Async** | Best text rendering |
| `stable-diffusion-3` | Sync | Open source, customizable |

## Handling Async Responses

For async models, check if the response contains `status: "pending"`:

```python
import requests
import time

def generate_image(prompt, model="midjourney"):
    # Create image request
    response = requests.post(
        "https://api.lemondata.cc/v1/images/generations",
        headers={"Authorization": "Bearer sk-your-api-key"},
        json={"model": model, "prompt": prompt}
    )
    data = response.json()

    # Check if async
    if data.get("status") == "pending":
        task_id = data["task_id"]
        print(f"Async task started: {task_id}")

        # Poll for result
        while True:
            status_resp = requests.get(
                f"https://api.lemondata.cc/v1/images/generations/{task_id}",
                headers={"Authorization": "Bearer sk-your-api-key"}
            )
            status_data = status_resp.json()

            if status_data["status"] == "completed":
                return status_data["data"][0]["url"]
            elif status_data["status"] == "failed":
                raise Exception(status_data.get("error", "Generation failed"))

            time.sleep(3)
    else:
        # Sync response
        return data["data"][0]["url"]

# Usage
url = generate_image("a beautiful sunset over mountains", model="midjourney")
print(f"Generated image: {url}")
```

---

### Edit Image

> Edit an image given a prompt and source image

URL: https://docs.lemondata.cc/api-reference/images/edit-image

## Overview

Creates an edited or extended image given an original image and a prompt. Requires `multipart/form-data` content type.

## Request Body

## Response

## Notes

---

### Create Image Variation

> Creates a variation of a given image

URL: https://docs.lemondata.cc/api-reference/images/create-variation

## Overview

Creates a variation of a given image. Requires `multipart/form-data` content type.

## Request Body

## Response

## Notes

---

### Get Image Status

> Get the status and result of an async image generation task

URL: https://docs.lemondata.cc/api-reference/images/get-image-status

## Path Parameters

## Response

## Polling Best Practices

```python
import requests
import time

def poll_image_task(task_id, api_key, max_wait=300, interval=3):
    """Poll for image generation result with timeout."""
    url = f"https://api.lemondata.cc/v1/images/generations/{task_id}"
    headers = {"Authorization": f"Bearer {api_key}"}

    start_time = time.time()
    while time.time() - start_time < max_wait:
        response = requests.get(url, headers=headers)
        data = response.json()

        if data["status"] == "completed":
            return data["data"][0]["url"]
        elif data["status"] == "failed":
            raise Exception(data.get("error", "Generation failed"))

        time.sleep(interval)

    raise TimeoutError(f"Task {task_id} did not complete within {max_wait}s")

# Usage
image_url = poll_image_task("img_abc123def456", "sk-your-api-key")
print(f"Generated image: {image_url}")
```

---


## Video

### Create Video

> Creates a video generation task

URL: https://docs.lemondata.cc/api-reference/video/create-video

## Overview

Video generation is asynchronous. You submit a request and receive a task ID, then poll for the result.

## Request Body

## Response

## Image to Video

```python
response = requests.post(
    "https://api.lemondata.cc/v1/videos/generations",
    headers={"Authorization": "Bearer sk-your-api-key"},
    json={
        "model": "wan2.6-i2v",
        "prompt": "The person starts walking forward",
        "image_url": "https://example.com/image.jpg"
    }
)
```

## Available Models

| Model | Type | Description |
|-------|------|-------------|
| `sora-2` | T2V | OpenAI's video model (default) |
| `kling-video` | T2V/I2V | Realistic motion, multiple versions (v1.0-v2.6) |
| `veo3.1` | T2V | Google's latest |
| `veo3.1-pro` | T2V | Professional quality |
| `minimax/video-01` | T2V | High rhythm and stability |
| `wan2.6-i2v` | I2V | Image to video |

---

### Get Video Status

> Retrieves the status and result of a video generation task

URL: https://docs.lemondata.cc/api-reference/video/get-video-status

## Path Parameters

## Response

## Polling Best Practices

- Poll every 5-10 seconds
- Implement exponential backoff for long tasks
- Set a maximum timeout (e.g., 10 minutes)
- Handle `failed` status gracefully

```python
import time

def wait_for_video(task_id, max_wait=600, interval=5):
    """Wait for video with timeout."""
    start = time.time()

    while time.time() - start < max_wait:
        response = requests.get(
            f"https://api.lemondata.cc/v1/videos/generations/{task_id}",
            headers={"Authorization": "Bearer sk-your-api-key"}
        )
        data = response.json()

        if data["status"] == "completed":
            return data["video_url"]
        elif data["status"] == "failed":
            raise Exception(data.get("error", "Video generation failed"))

        time.sleep(interval)

    raise TimeoutError("Video generation timed out")
```

---


## Audio

### Create Speech

> Generates audio from the input text

URL: https://docs.lemondata.cc/api-reference/audio/create-speech

## Request Body

## Response

Returns the audio file in the requested format.

## Voice Samples

| Voice | Description |
|-------|-------------|
| `alloy` | Neutral, balanced |
| `ash` | Calm, measured |
| `ballad` | Melodic, expressive |
| `coral` | Warm, inviting |
| `echo` | Warm, conversational |
| `fable` | Expressive, narrative |
| `nova` | Friendly, clear |
| `onyx` | Deep, authoritative |
| `sage` | Wise, thoughtful |
| `shimmer` | Soft, gentle |
| `verse` | Dynamic, versatile |

---

### Create Transcription

> Transcribes audio into the input language

URL: https://docs.lemondata.cc/api-reference/audio/create-transcription

## Request Body

## Response

For `verbose_json`:

## Translation

To translate audio to English, use the translations endpoint:

```python
response = client.audio.translations.create(
    model="whisper-1",
    file=audio_file
)
```

---

### Create Translation

> Translates audio into English text

URL: https://docs.lemondata.cc/api-reference/audio/create-translation

## Overview

Translates audio in any supported language into English text. Unlike transcription, this endpoint always outputs English text regardless of the input language.

## Request Body

## Response

For `verbose_json` format, the response also includes:

## Translation vs Transcription

| Feature | Translation | Transcription |
|---------|-------------|---------------|
| Output language | Always English | Same as input |
| Use case | Convert foreign audio to English | Preserve original language |
| Language parameter | Not applicable | Optional hint |

---


## Music

### Create Music

> Creates a music generation task using Suno

URL: https://docs.lemondata.cc/api-reference/music/create-music

Generate music and lyrics using AI. This is an asynchronous API - you'll receive a task ID to poll for completion.

## Request Body

## Response

---

### Get Music Status

> Retrieves the status and result of a music generation task

URL: https://docs.lemondata.cc/api-reference/music/get-music-status

Poll this endpoint to check the status of your music generation task.

## Path Parameters

## Response

---


## 3D

### Create 3D Model

> Creates a 3D model generation task

URL: https://docs.lemondata.cc/api-reference/3d/create-3d

Generate 3D models from text or images using Tripo3D and other providers. This is an asynchronous API.

## Request Body

## Response

---

### Get 3D Model Status

> Retrieves the status and result of a 3D generation task

URL: https://docs.lemondata.cc/api-reference/3d/get-3d-status

Poll this endpoint to check the status of your 3D model generation task.

## Path Parameters

## Response

---


## Embeddings

### Create Embedding

> Creates an embedding vector representing the input text

URL: https://docs.lemondata.cc/api-reference/embeddings/create-embedding

## Request Body

## Available Models

| Model | Dimensions | Description |
|-------|------------|-------------|
| `text-embedding-3-large` | 3072 | Best quality |
| `text-embedding-3-small` | 1536 | Balanced |
| `text-embedding-ada-002` | 1536 | Legacy |

## Response

## Batch Embeddings

```python
# Embed multiple texts at once
response = client.embeddings.create(
    model="text-embedding-3-small",
    input=[
        "First document text",
        "Second document text",
        "Third document text"
    ]
)

for i, data in enumerate(response.data):
    print(f"Document {i}: {len(data.embedding)} dimensions")
```

---


## Rerank

### Rerank Documents

> Reranks documents by relevance to a query

URL: https://docs.lemondata.cc/api-reference/rerank/create-rerank

Rerank documents using semantic similarity models. Useful for improving search results and RAG applications.

## Request Body

## Response

---


## Cache

### Cache Management

> Control caching behavior for your API requests

URL: https://docs.lemondata.cc/api-reference/cache/cache-management

## Overview

LemonData automatically manages caching to optimize performance and reduce costs. While there is no public endpoint to clear cache entries, you have full control over caching behavior through request-level controls.

## Bypassing Cache

To get fresh responses without using cache, use the `cache_control` parameter in your request:

## Cache Control Options

| Type | Effect |
|------|--------|
| `no_cache` | Skip cache lookup, always get fresh response |
| `no_store` | Don't store this response in cache |
| `response_only` | Only use exact match cache (skip semantic) |
| `semantic_only` | Only use semantic cache (skip exact match) |

## Cache Feedback

If you receive an incorrect cached response, you can report it:

```bash
curl -X POST "https://api.lemondata.cc/v1/cache/feedback" \
  -H "Authorization: Bearer sk-your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

When a cache entry receives enough negative feedback, it is automatically invalidated.

## Use Cases

---


## Models

### List Models

> Lists all available models

URL: https://docs.lemondata.cc/api-reference/models/list-models

## Response

## Filtering by Provider

```python
# Get all OpenAI models
openai_models = [m for m in models.data if m.owned_by == "openai"]

# Get all Anthropic models
anthropic_models = [m for m in models.data if m.owned_by == "anthropic"]
```

## Model Categories

| Provider | Example Models |
|----------|----------------|
| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |
| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |
| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |
| `deepseek` | deepseek-r1, deepseek-v3-2 |
| `meta` | llama-3.3-70b, llama-3.1-405b |

---

### Get Model

> Retrieves a model instance

URL: https://docs.lemondata.cc/api-reference/models/get-model

## Path Parameters

## Response

## Error Handling

If the model doesn't exist, you'll receive a 404 error:

```json
{
  "error": {
    "message": "Model 'invalid-model' not found",
    "type": "invalid_request_error",
    "code": "model_not_found"
  }
}
```

---


## Pricing

### Get Pricing

> Get pricing information for models

URL: https://docs.lemondata.cc/api-reference/pricing/get-pricing

## Query Parameters

## Response

## Pricing Types

| Type | Description | Example Models |
|------|-------------|----------------|
| **Token-based** | Charged per input/output tokens | GPT-4o, Claude, Gemini |
| **Per-request** | Fixed price per request | DALL-E 3, Sora, Suno |

---


# âœ¨ Claw


## Getting Started

### âœ¨ Claw - AI on Every Chat Platform

> Deploy your personal AI assistant to Telegram, Discord, WhatsApp, Slack, and more â€” powered by 300+ models

URL: https://docs.lemondata.cc/claw/introduction

## Why Claw?

  
  
  

## Pricing

| Plan | Price | Duration | Includes |
|------|-------|----------|----------|
| **Claw Instance** | $20 | 30 days | Unlimited messages, all 8 IM platforms, persistent storage |

## Quick Start

  
  
  

## Supported Platforms

  
  
  
  
  
  
  

## How Auto-Pairing Works

Most platforms (Telegram, Discord, Slack, Feishu, DingTalk, QQ) use an **auto-pairing** mechanism for security:

1. The bot sends you a **6-digit pairing code**
2. Enter the code in your Claw WebUI at `claw-{subdomain}.lemondata.cc`
3. Once paired, you can chat freely â€” the pairing is permanent

## Managing Your Instance

Access your instance's WebUI at `https://claw-{subdomain}.lemondata.cc` to:

- View and manage connected users
- Monitor conversation history
- Configure AI model settings
- Manage IM platform connections

---

### API Reference

> Programmatically manage Claw instances via the /v1/claw API

URL: https://docs.lemondata.cc/claw/api

All Claw API endpoints require authentication via `Authorization: Bearer sk-...` header.

**Base URL:** `https://api.lemondata.cc/v1/claw`

## List Instances

```
GET /v1/claw/instances
```

Returns all instances for the authenticated organization.

### Instance Status Values

| Status | Description |
|--------|-------------|
| `PENDING` | Just created, waiting for provisioning |
| `PROVISIONING` | Container is being deployed |
| `RUNNING` | Instance is active and healthy |
| `STOPPED` | Stopped due to expiration (data retained 7 days) |
| `FAILED` | Deployment failed (auto-refunded) |
| `DELETED` | Permanently removed |

---

## Create Instance

```
POST /v1/claw/instances
```

Creates a new instance and charges **$20** for 30 days from your organization balance.

---

## Get Instance

```
GET /v1/claw/instances/:id
```

Returns details for a specific instance.

---

## Update Instance

```
PATCH /v1/claw/instances/:id
```

Update instance configuration.

---

## Rebuild Instance

```
POST /v1/claw/instances/:id/rebuild
```

Destroys and recreates the container while preserving data (PVC). Limited to **10 rebuilds per day**.

Returns **429 Too Many Requests** if the daily limit is exceeded.

---

## Renew Instance

```
POST /v1/claw/instances/:id/renew
```

Extends the billing period by 30 days ($20). If the instance was stopped due to expiration, it will automatically restart.

---

## Health Check

```
GET /v1/claw/health
```

Returns Claw service health status. No authentication required.

```json
{"status": "ok", "timestamp": "2026-02-08T15:00:00.000Z"}
```

---


## Chat Platforms

### Telegram

> Connect your Claw instance to Telegram â€” the easiest platform to set up

URL: https://docs.lemondata.cc/claw/telegram

Telegram is the **simplest** platform to connect. You only need a bot token from BotFather, and you'll be chatting with AI in under 2 minutes.

## Prerequisites

- A [Telegram](https://telegram.org/) account
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  

## Optional: Customize Bot Profile

After creating your bot, you can customize it with BotFather:

| Command | Purpose |
|---------|---------|
| `/setdescription` | Set the bot's bio/description |
| `/setabouttext` | Set the "About" section |
| `/setuserpic` | Upload a profile picture |
| `/setcommands` | Add command suggestions |

## Troubleshooting

---

### Discord

> Connect your Claw instance to Discord with a bot application

URL: https://docs.lemondata.cc/claw/discord

Connect your Claw instance to Discord and chat with AI directly in your server or DMs.

## Prerequisites

- A [Discord](https://discord.com/) account
- A Discord server where you have **Manage Server** permission
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  
  

## Troubleshooting

---

### WhatsApp

> Connect your Claw instance to WhatsApp via QR code pairing

URL: https://docs.lemondata.cc/claw/whatsapp

Connect WhatsApp to your Claw instance by scanning a QR code â€” just like WhatsApp Web.

## Prerequisites

- A phone with [WhatsApp](https://www.whatsapp.com/) installed and an active account
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  

## Troubleshooting

---

### Slack

> Connect your Claw instance to a Slack workspace using Socket Mode

URL: https://docs.lemondata.cc/claw/slack

Add an AI assistant to your Slack workspace using Socket Mode â€” no public URL required.

## Prerequisites

- A [Slack](https://slack.com/) workspace where you have admin access
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  
  

## Troubleshooting

---

### Feishu / Lark

> Connect your Claw instance to Feishu (Lark) using WebSocket events

URL: https://docs.lemondata.cc/claw/feishu

Connect Feishu (é£žä¹¦) or Lark to your Claw instance. Uses WebSocket mode â€” no public callback URL needed.

## Prerequisites

- A [Feishu](https://www.feishu.cn/) or [Lark](https://www.larksuite.com/) account with developer access
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  
  
  
  
  

## Troubleshooting

---

### WeCom

> Connect your Claw instance to WeCom (WeChat Work) via webhook callback

URL: https://docs.lemondata.cc/claw/wecom

Integrate WeCom (ä¼ä¸šå¾®ä¿¡ / WeChat Work) with your Claw instance to bring AI to your enterprise workspace.

## Prerequisites

- A [WeCom](https://work.weixin.qq.com/) admin account
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  
  

## Troubleshooting

---

### DingTalk

> Connect your Claw instance to DingTalk using Stream mode

URL: https://docs.lemondata.cc/claw/dingtalk

Connect DingTalk (é’‰é’‰) to your Claw instance using Stream mode â€” a WebSocket-based connection that requires no public URL.

## Prerequisites

- A [DingTalk](https://www.dingtalk.com/) account with enterprise admin access
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  
  
  

## Troubleshooting

---

### QQ

> Connect your Claw instance to QQ using the QQ Open Platform Bot API

URL: https://docs.lemondata.cc/claw/qq

Connect QQ (the most popular messaging app in China) to your Claw instance using the official QQ Open Platform Bot API.

## Prerequisites

- A [QQ](https://im.qq.com/) account
- Access to [QQ Open Platform](https://q.qq.com/) (developer account)
- A Claw instance ([create one here](https://lemondata.cc/dashboard/claw))

## Setup Guide

  
  
  
  
  

## Supported Message Types

| Type | Description |
|------|-------------|
| **C2C (Private)** | Direct messages to the bot |
| **Group @mention** | @mention the bot in a QQ group |
| **Channel** | Messages in QQ channels (é¢‘é“) |

## Troubleshooting

---


# Integrations


## IDE & CLI

### Cursor

> Use LemonData with Cursor AI code editor

URL: https://docs.lemondata.cc/integrations/cursor

## Overview

[Cursor](https://cursor.sh) is an AI-powered code editor. You can use LemonData as a custom API provider to access 300+ models.

## Configuration

1. Open Cursor Settings (`Cmd/Ctrl + ,`)
2. Navigate to **Models** in the left sidebar
3. Scroll down to the **OpenAI API Key** section
4. Enter your LemonData API key
5. Enable **Override OpenAI Base URL**
6. Set the Base URL to:

```
https://api.lemondata.cc/v1
```

## Model Selection

After configuration, you can select any LemonData model in Cursor:

### Recommended Models for Coding

| Model | Best For |
|-------|----------|
| `claude-opus-4-5` | Complex coding, architecture |
| `claude-sonnet-4-5` | General coding, debugging |
| `gpt-4o` | Code generation, explanations |
| `deepseek-r1` | Reasoning-heavy tasks |
| `gpt-4o-mini` | Fast completions, simple tasks |

## Features

### Chat

Use the AI chat panel (`Cmd/Ctrl + L`) with any LemonData model:

- Code explanations
- Bug fixing
- Refactoring suggestions
- Documentation generation

### Composer

The Composer feature (`Cmd/Ctrl + I`) works with LemonData models for:

- Multi-file edits
- Feature implementation
- Code migrations

### Tab Completion

For tab completions, we recommend using fast models:

- `gpt-4o-mini`
- `claude-haiku-4-5`
- `gemini-2.5-flash`

## Troubleshooting

  

  

## Tips

---

### Claude Code

> Use LemonData with Anthropic

URL: https://docs.lemondata.cc/integrations/claude-code

## Overview

[Claude Code](https://docs.anthropic.com/en/docs/claude-code) is Anthropic's official CLI tool for AI-assisted coding. You can configure it to use LemonData as the API provider.

## Configuration

### Environment Variables

Set the following environment variables:

```bash
export ANTHROPIC_API_KEY="sk-your-lemondata-key"
export ANTHROPIC_BASE_URL="https://api.lemondata.cc"
```

Add these to your shell profile (`~/.bashrc`, `~/.zshrc`, etc.) for persistence.

### Configuration File

Alternatively, create or edit `~/.claude/settings.json`:

```json
{
  "env": {
    "ANTHROPIC_API_KEY": "sk-your-lemondata-key",
    "ANTHROPIC_BASE_URL": "https://api.lemondata.cc"
  }
}
```

## Usage

After configuration, use Claude Code normally:

```bash
# Start an interactive session
claude

# Run a single command
claude "Explain this code" 

## Benefits

---

### Codex CLI

> Configure OpenAI Codex CLI to use LemonData API

URL: https://docs.lemondata.cc/integrations/codex-cli

## Overview

OpenAI Codex is an open-source command-line tool (CLI) that serves as a lightweight coding agent, capable of reading, modifying, and running code in the terminal. It's built on GPT models and optimized for code generation.

## System Requirements

- **OS**: macOS, Linux (official support), Windows via WSL
- **Node.js**: Version 18+
- **npm**: Version 10.x.x or higher

## Installation

```bash
sudo npm install -g @openai/codex@latest
```

Verify installation:

```bash
codex --version
```

## Configuration

### Step 1: Set API Key

**Temporary (current session):**

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
```

**Permanent configuration:**

Add to `~/.bashrc`, `~/.zshrc`, or `~/.bash_profile`:

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
```

Then reload:

```bash
source ~/.zshrc  # or source ~/.bashrc
```

### Step 2: Configure config.toml

Edit `~/.codex/config.toml`:

```toml
model = "gpt-4o"
model_provider = "lemondata"

[model_providers.lemondata]
name = "LemonData"
base_url = "https://api.lemondata.cc/v1"
env_key = "OPENAI_API_KEY"
wire_api = "chat"
```

## Basic Usage

**Start interactive mode:**

```bash
codex
```

**Direct command:**

```bash
codex "Fix the bug in main.py line 42"
```

**Specify model:**

```bash
codex -m gpt-4o "Build a REST API server"
```

## Available Models

| Model | Best For |
|-------|----------|
| `gpt-4o` | Complex coding tasks, architecture |
| `gpt-4o-mini` | Quick fixes, simple tasks |
| `claude-sonnet-4-5` | Code review, documentation |
| `deepseek-r1` | Algorithm design, reasoning |

## Interactive Commands

| Command | Description |
|---------|-------------|
| `/help` | Display help |
| `/exit` or `Ctrl+C` | Exit |
| `/clear` | Clear conversation |
| `/config` | View configuration |
| `/model ` | Switch model |
| `/tokens` | View token usage |

## Verify Configuration

```bash
# Check environment variable
echo $OPENAI_API_KEY

# Test API connection
codex "Hello, Codex!"

# View configuration
cat ~/.codex/config.toml
```

## Common Use Cases

**Code review:**

```bash
git diff | codex "Review these code changes"
```

**Generate commit messages:**

```bash
git diff --staged | codex "Generate a commit message for these changes"
```

**Fix errors:**

```bash
codex "Fix the TypeScript errors in src/components/"
```

**Explain code:**

```bash
cat main.py | codex "Explain what this code does"
```

## Troubleshooting

---

### Gemini CLI

> Configure Google Gemini CLI to use LemonData API

URL: https://docs.lemondata.cc/integrations/gemini-cli

## Overview

Google Gemini CLI is a command-line tool for interacting with Gemini models. LemonData provides a compatible endpoint that allows you to use Gemini CLI with access to 300+ models.

## System Requirements

- **Node.js**: Version 20.0+
- **OS**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+, or Debian 10+

## Installation

```bash
npm install -g @google/gemini-cli
```

Verify installation:

```bash
gemini --version
```

## Configuration

### Step 1: Get Your API Key

1. Log into [LemonData Dashboard](https://lemondata.cc/dashboard)
2. Navigate to [API Keys](https://lemondata.cc/dashboard/api)
3. Create and copy your API key (format: `sk-...`)

### Step 2: Set Environment Variables

**Temporary (current session):**

```bash
export GEMINI_API_KEY="sk-your-lemondata-key"
export GOOGLE_GEMINI_BASE_URL="https://api.lemondata.cc"
```

**Permanent configuration:**

Add to your shell configuration file:

  
  

## Basic Usage

Start Gemini CLI from your project directory:

```bash
cd your-project
gemini
```

On first run, you'll:
1. Choose a theme
2. Confirm safety notice
3. Trust the working directory

## Available Models

| Model | Description |
|-------|-------------|
| `gemini-2.5-pro` | Most capable Gemini model |
| `gemini-2.5-flash` | Fast, efficient for most tasks |
| `gemini-2.0-flash` | Previous generation, stable |

## Common Commands

**Ask a question:**

```
> What is the best way to structure a React app?
```

**Analyze code:**

```
> Explain the code in src/main.ts
```

**Generate code:**

```
> Create a Python function to parse JSON files
```

**Review changes:**

```
> Review the recent git changes and suggest improvements
```

## Verify Configuration

```bash
# Check environment variables
echo $GEMINI_API_KEY
echo $GOOGLE_GEMINI_BASE_URL

# Test connection
gemini
```

## Troubleshooting

  

  

## Best Practices

---

### OpenCode

> Configure OpenCode to use LemonData API

URL: https://docs.lemondata.cc/integrations/opencode

## Overview

OpenCode is an open-source AI coding assistant that runs in your terminal. It supports multiple LLM providers and can be configured to use LemonData's API for access to 300+ models.

## Installation

  
  

Verify installation:

```bash
opencode --version
```

## Configuration

### Step 1: Set Environment Variables

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
export LOCAL_ENDPOINT="https://api.lemondata.cc/v1"
```

For permanent configuration, add to `~/.bashrc` or `~/.zshrc`:

```bash
echo 'export OPENAI_API_KEY="sk-your-lemondata-key"' >> ~/.zshrc
echo 'export LOCAL_ENDPOINT="https://api.lemondata.cc/v1"' >> ~/.zshrc
source ~/.zshrc
```

### Step 2: Configure OpenCode

Create or edit `~/.config/opencode/opencode.json` (global) or `opencode.json` in your project root:

```json
{
  "provider": {
    "openai": {
      "options": {
        "apiKey": "sk-your-lemondata-key",
        "baseURL": "https://api.lemondata.cc/v1"
      }
    }
  },
  "model": "gpt-4o",
  "agent": {
    "coder": {
      "model": "gpt-4o"
    }
  }
}
```

## Basic Usage

**Start interactive mode:**

```bash
opencode
```

**Run with a prompt:**

```bash
opencode "Explain this codebase"
```

**Specify model:**

```bash
opencode --model gpt-4o "Fix the bugs in main.py"
```

## Available Models

| Model | Best For |
|-------|----------|
| `gpt-4o` | Complex tasks, code architecture |
| `gpt-4o-mini` | Quick fixes, simple queries |
| `claude-sonnet-4-5` | Code review, documentation |
| `claude-opus-4-5` | Complex reasoning |
| `gemini-2.5-flash` | Fast responses |
| `deepseek-r1` | Algorithm design |

## Common Commands

**Analyze code:**

```bash
opencode "What does this function do?" ` | Switch to a different model |
| `/clear` | Clear conversation history |
| `/exit` | Exit OpenCode |

## Troubleshooting

  

  

## Best Practices

---


## SDKs

### OpenAI SDK

> Use LemonData with the official OpenAI SDK

URL: https://docs.lemondata.cc/integrations/openai-sdk

## Overview

LemonData is fully compatible with the OpenAI SDK. Just change the base URL and you can access 300+ models.

## Installation

## Configuration

## Chat Completions

Works exactly like the OpenAI API:

```python
response = client.chat.completions.create(
    model="gpt-4o",  # Or any LemonData model
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=1000
)

print(response.choices[0].message.content)
```

## Streaming

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

## Function Calling / Tools

```python
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the weather in Tokyo?"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                },
                "required": ["location"]
            }
        }
    }]
)

# Check if model wants to call a function
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    print(f"Function: {tool_call.function.name}")
    print(f"Arguments: {tool_call.function.arguments}")
```

## Vision

```python
response = client.chat.completions.create(
    model="gpt-4o",  # Or claude-sonnet-4-5
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
        ]
    }]
)
```

## Image Generation

```python
response = client.images.generate(
    model="dall-e-3",
    prompt="A white siamese cat",
    size="1024x1024",
    quality="standard",
    n=1
)

print(response.data[0].url)
```

## Embeddings

```python
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="Hello world"
)

print(response.data[0].embedding[:5])  # First 5 dimensions
```

## Audio - Text to Speech

```python
response = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input="Hello, welcome to LemonData!"
)

response.stream_to_file("output.mp3")
```

## Audio - Transcription

```python
with open("audio.mp3", "rb") as audio_file:
    response = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file
    )

print(response.text)
```

## Using Different Models

The key advantage of LemonData is accessing multiple providers:

```python
# OpenAI
response = client.chat.completions.create(model="gpt-4o", messages=messages)

# Anthropic Claude
response = client.chat.completions.create(model="claude-sonnet-4-5", messages=messages)

# Google Gemini
response = client.chat.completions.create(model="gemini-2.5-flash", messages=messages)

# DeepSeek
response = client.chat.completions.create(model="deepseek-r1", messages=messages)
```

## Error Handling

```python
from openai import APIError, RateLimitError, AuthenticationError

try:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello"}]
    )
except AuthenticationError:
    print("Invalid API key")
except RateLimitError:
    print("Rate limit exceeded, please wait")
except APIError as e:
    print(f"API error: {e.status_code} - {e.message}")
```

---

### Anthropic SDK

> Use LemonData with the Anthropic SDK for Claude models

URL: https://docs.lemondata.cc/integrations/anthropic-sdk

## Overview

LemonData supports the native Anthropic Messages API format. Use the official Anthropic SDK with LemonData to access Claude models.

## Installation

## Configuration

## Basic Usage

```python
message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Hello, Claude!"}
    ]
)

print(message.content[0].text)
```

## With System Prompt

```python
message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system="You are a helpful coding assistant.",
    messages=[
        {"role": "user", "content": "Write a Python function to reverse a string"}
    ]
)
```

## Streaming

```python
with client.messages.stream(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Tell me a story"}]
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

## Vision

```python
import base64

# From URL
message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {
                "type": "image",
                "source": {
                    "type": "url",
                    "url": "https://example.com/image.jpg"
                }
            }
        ]
    }]
)

# From base64
with open("image.png", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this image"},
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/png",
                    "data": image_data
                }
            }
        ]
    }]
)
```

## Tool Use

```python
message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    tools=[{
        "name": "get_weather",
        "description": "Get the weather for a location",
        "input_schema": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City name"}
            },
            "required": ["location"]
        }
    }],
    messages=[{"role": "user", "content": "What's the weather in Tokyo?"}]
)

# Check for tool use
for block in message.content:
    if block.type == "tool_use":
        print(f"Tool: {block.name}")
        print(f"Input: {block.input}")
```

## Extended Thinking (Claude Opus 4.5)

For models that support extended thinking:

```python
message = client.messages.create(
    model="claude-opus-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    messages=[{"role": "user", "content": "Solve this complex math problem..."}]
)

# Access thinking blocks
for block in message.content:
    if block.type == "thinking":
        print(f"Thinking: {block.thinking}")
    elif block.type == "text":
        print(f"Response: {block.text}")
```

## Available Claude Models

| Model | Best For |
|-------|----------|
| `claude-opus-4-5` | Complex reasoning, extended thinking |
| `claude-sonnet-4-5` | General purpose, coding |
| `claude-haiku-4-5` | Fast responses |

## Error Handling

```python
from anthropic import APIError, APIStatusError, APIConnectionError

try:
    message = client.messages.create(
        model="claude-sonnet-4-5",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
except APIStatusError as e:
    if e.status_code == 401:
        print("Invalid API key")
    elif e.status_code == 429:
        print("Rate limit exceeded")
    else:
        print(f"API error: {e.status_code}")
except APIConnectionError:
    print("Connection error")
except APIError as e:
    print(f"Unexpected error: {e}")
```

## Comparison: OpenAI SDK vs Anthropic SDK

Both work with LemonData for Claude models:

| Feature | OpenAI SDK | Anthropic SDK |
|---------|-----------|---------------|
| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |
| Endpoint | `/chat/completions` | `/v1/messages` |
| System prompt | In messages array | Separate `system` parameter |
| Extended thinking | Not supported | Supported |

Choose based on your preference or existing codebase.

---

### Vercel AI SDK

> Integrate LemonData with Vercel AI SDK for React and Next.js applications

URL: https://docs.lemondata.cc/integrations/vercel-ai-sdk

## Overview

The Vercel AI SDK is a TypeScript toolkit for building AI-powered streaming applications with React, Next.js, Vue, Svelte, and more. LemonData works seamlessly through the OpenAI-compatible provider.

## Installation

```bash
npm install ai @ai-sdk/openai
```

## Basic Configuration

```typescript
import { createOpenAI } from '@ai-sdk/openai';

const lemondata = createOpenAI({
  apiKey: 'sk-your-lemondata-key',
  baseURL: 'https://api.lemondata.cc/v1',
});
```

## Generate Text

```typescript
import { generateText } from 'ai';

const { text } = await generateText({
  model: lemondata('gpt-4o'),
  prompt: 'What is LemonData?',
});

console.log(text);
```

## Streaming Text

```typescript
import { streamText } from 'ai';

const result = await streamText({
  model: lemondata('gpt-4o'),
  prompt: 'Write a poem about AI.',
});

for await (const textPart of result.textStream) {
  process.stdout.write(textPart);
}
```

## Chat Messages

```typescript
import { generateText } from 'ai';

const { text } = await generateText({
  model: lemondata('gpt-4o'),
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'What is the capital of France?' },
  ],
});
```

## Using Different Models

```typescript
// OpenAI GPT-4o
const gpt4Response = await generateText({
  model: lemondata('gpt-4o'),
  prompt: 'Hello!',
});

// Anthropic Claude
const claudeResponse = await generateText({
  model: lemondata('claude-sonnet-4-5'),
  prompt: 'Hello!',
});

// Google Gemini
const geminiResponse = await generateText({
  model: lemondata('gemini-2.5-flash'),
  prompt: 'Hello!',
});

// DeepSeek
const deepseekResponse = await generateText({
  model: lemondata('deepseek-r1'),
  prompt: 'Hello!',
});
```

## Next.js API Route

```typescript
// app/api/chat/route.ts
import { createOpenAI } from '@ai-sdk/openai';
import { streamText } from 'ai';

const lemondata = createOpenAI({
  apiKey: process.env.LEMONDATA_API_KEY,
  baseURL: 'https://api.lemondata.cc/v1',
});

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: lemondata('gpt-4o'),
    messages,
  });

  return result.toDataStreamResponse();
}
```

## React Chat Component

```tsx
'use client';

import { useChat } from 'ai/react';

export default function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat',
  });

  return (
    
      {messages.map((m) => (
        
          {m.role}: {m.content}
        
      ))}

      
        
        Send
      
    
  );
}
```

## Tool Calling

```typescript
import { generateText, tool } from 'ai';
import { z } from 'zod';

const { text, toolCalls } = await generateText({
  model: lemondata('gpt-4o'),
  tools: {
    weather: tool({
      description: 'Get the weather in a location',
      parameters: z.object({
        location: z.string().describe('The location to get weather for'),
      }),
      execute: async ({ location }) => {
        return { temperature: 72, condition: 'sunny' };
      },
    }),
  },
  prompt: 'What is the weather in San Francisco?',
});
```

## Structured Output

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

const { object } = await generateObject({
  model: lemondata('gpt-4o'),
  schema: z.object({
    name: z.string(),
    age: z.number(),
    email: z.string().email(),
  }),
  prompt: 'Generate a fake user profile.',
});

console.log(object);
```

## Environment Variables

```bash
# .env.local
LEMONDATA_API_KEY=sk-your-lemondata-key
```

```typescript
const lemondata = createOpenAI({
  apiKey: process.env.LEMONDATA_API_KEY,
  baseURL: 'https://api.lemondata.cc/v1',
});
```

## Best Practices

---


## Frameworks

### LangChain

> Integrate LemonData with LangChain

URL: https://docs.lemondata.cc/integrations/langchain

## Overview

LangChain is a popular framework for building LLM applications. LemonData works seamlessly with LangChain's OpenAI integration.

## Installation

```bash
pip install langchain langchain-openai
```

## Basic Configuration

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o",
    api_key="sk-your-lemondata-key",
    base_url="https://api.lemondata.cc/v1"
)

response = llm.invoke("Hello, how are you?")
print(response.content)
```

## Using Different Models

Access any LemonData model:

```python
# OpenAI GPT-4o
gpt4 = ChatOpenAI(
    model="gpt-4o",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# Anthropic Claude
claude = ChatOpenAI(
    model="claude-sonnet-4-5",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# Google Gemini
gemini = ChatOpenAI(
    model="gemini-2.5-flash",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# DeepSeek
deepseek = ChatOpenAI(
    model="deepseek-r1",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)
```

## Chat with Message History

```python
from langchain_core.messages import HumanMessage, SystemMessage

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is the capital of France?")
]

response = llm.invoke(messages)
print(response.content)
```

## Streaming

```python
for chunk in llm.stream("Write a poem about coding"):
    print(chunk.content, end="", flush=True)
```

## Async Usage

```python
import asyncio

async def main():
    response = await llm.ainvoke("Hello!")
    print(response.content)

asyncio.run(main())
```

## Chains

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that translates {input_language} to {output_language}."),
    ("human", "{text}")
])

chain = prompt | llm | StrOutputParser()

result = chain.invoke({
    "input_language": "English",
    "output_language": "French",
    "text": "Hello, how are you?"
})
print(result)
```

## RAG (Retrieval Augmented Generation)

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# Embeddings
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

# Create vector store
texts = ["LemonData supports 300+ AI models", "API is OpenAI compatible"]
vectorstore = FAISS.from_texts(texts, embeddings)
retriever = vectorstore.as_retriever()

# RAG chain
template = """Answer based on context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

response = rag_chain.invoke("How many models does LemonData support?")
print(response.content)
```

## Agents

```python
from langchain.agents import create_openai_tools_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool

@tool
def search(query: str) -> str:
    """Search for information."""
    return f"Search results for: {query}"

tools = [search]

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant with access to tools."),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}")
])

agent = create_openai_tools_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools)

result = executor.invoke({"input": "Search for LemonData pricing"})
print(result["output"])
```

## Environment Variables

For cleaner code, use environment variables:

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
export OPENAI_API_BASE="https://api.lemondata.cc/v1"
```

```python
from langchain_openai import ChatOpenAI

# Will automatically use environment variables
llm = ChatOpenAI(model="gpt-4o")
```

## Callbacks and Tracing

```python
from langchain_core.callbacks import StdOutCallbackHandler

llm = ChatOpenAI(
    model="gpt-4o",
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    callbacks=[StdOutCallbackHandler()]
)
```

## Best Practices

---

### LlamaIndex

> Integrate LemonData with LlamaIndex for RAG applications

URL: https://docs.lemondata.cc/integrations/llamaindex

## Overview

LlamaIndex is a data framework for LLM applications, especially powerful for building RAG (Retrieval Augmented Generation) systems. LemonData works seamlessly with LlamaIndex's OpenAI integration.

## Installation

```bash
pip install llama-index llama-index-llms-openai llama-index-embeddings-openai
```

## Basic Configuration

```python
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# Configure LLM
llm = OpenAI(
    model="gpt-4o",
    api_key="sk-your-lemondata-key",
    api_base="https://api.lemondata.cc/v1"
)

# Set as default
Settings.llm = llm

# Simple query
response = llm.complete("What is LemonData?")
print(response.text)
```

## Using Different Models

```python
# OpenAI GPT-4o
gpt4 = OpenAI(
    model="gpt-4o",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)

# Anthropic Claude (via OpenAI-compatible endpoint)
claude = OpenAI(
    model="claude-sonnet-4-5",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)

# Google Gemini
gemini = OpenAI(
    model="gemini-2.5-flash",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)
```

## Chat Interface

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(role="system", content="You are a helpful assistant."),
    ChatMessage(role="user", content="What is the capital of France?")
]

response = llm.chat(messages)
print(response.message.content)
```

## Streaming

```python
# Streaming completion
for chunk in llm.stream_complete("Write a poem about AI"):
    print(chunk.delta, end="", flush=True)

# Streaming chat
for chunk in llm.stream_chat(messages):
    print(chunk.delta, end="", flush=True)
```

## Embeddings

```python
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key="sk-your-lemondata-key",
    api_base="https://api.lemondata.cc/v1"
)

# Set as default
Settings.embed_model = embed_model

# Get embeddings
embeddings = embed_model.get_text_embedding("Hello, world!")
print(f"Embedding dimension: {len(embeddings)}")
```

## RAG with Documents

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Configure settings
Settings.llm = llm
Settings.embed_model = embed_model

# Load documents
documents = SimpleDirectoryReader("./data").load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Query
query_engine = index.as_query_engine()
response = query_engine.query("What is in my documents?")
print(response)
```

## Chat Engine

```python
# Create chat engine with memory
chat_engine = index.as_chat_engine(chat_mode="condense_question")

# Multi-turn conversation
response = chat_engine.chat("What is LemonData?")
print(response)

response = chat_engine.chat("How many models does it support?")
print(response)
```

## Async Usage

```python
import asyncio

async def main():
    response = await llm.acomplete("Hello!")
    print(response.text)

asyncio.run(main())
```

## Environment Variables

For cleaner code, use environment variables:

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
export OPENAI_API_BASE="https://api.lemondata.cc/v1"
```

```python
from llama_index.llms.openai import OpenAI

# Will automatically use environment variables
llm = OpenAI(model="gpt-4o")
```

## Best Practices

---

### Dify

> Integrate LemonData with Dify for building LLM applications

URL: https://docs.lemondata.cc/integrations/dify

## Overview

Dify is an open-source LLM application development platform. It provides visual prompt orchestration, RAG pipelines, agent frameworks, and LLMOps capabilities. LemonData can be configured as a custom model provider in Dify.

## Benefits

- Access 300+ AI models through one interface
- Switch between models without changing application logic
- Optimize costs by selecting the best model for each task
- Unified billing and usage tracking

## Prerequisites

- LemonData account with API access
- Dify installation (cloud or self-hosted)

## Configuration Steps

### Step 1: Get Your API Key

1. Log into [LemonData Dashboard](https://lemondata.cc/dashboard)
2. Navigate to [API Keys](https://lemondata.cc/dashboard/api)
3. Create and copy your API key (format: `sk-...`)

### Step 2: Add Custom Model Provider

  
  
  

### Step 3: Test Connection

1. Select a model (e.g., `gpt-4o-mini`)
2. Send a test message
3. Verify you receive a response

## Using in Applications

### Chatbot

1. Create a new Chatbot application
2. Select LemonData as the model provider
3. Choose your preferred model
4. Configure system prompt and parameters

### Agent

1. Create an Agent application
2. Select a capable model (GPT-4o, Claude)
3. Add tools and knowledge bases
4. Configure agent behavior

### Workflow

1. Create a Workflow
2. Add LLM nodes
3. Select LemonData models for each node
4. Connect nodes and configure data flow

## Available Models

| Category | Models |
|----------|--------|
| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |
| Embeddings | text-embedding-3-small, text-embedding-3-large |
| Vision | GPT-4o (with images), Claude Sonnet (with images) |

## RAG Configuration

For RAG applications, configure embeddings:

1. Go to **Settings** â†’ **Model Provider**
2. Add embedding model: `text-embedding-3-small`
3. Set as default embedding model in knowledge base settings

## Best Practices

  

  

## Troubleshooting

---


## Chat Apps

### AI Chat Applications

> Configure popular AI chat applications to use LemonData API

URL: https://docs.lemondata.cc/integrations/ai-chat-apps

## Overview

LemonData works with any OpenAI-compatible chat application. This guide covers configuration for popular desktop and mobile chat apps.

## Supported Applications

| Application | Platform | Features |
|-------------|----------|----------|
| LobeChat | Web, Desktop, Self-hosted | Multi-provider, plugins, RAG |
| CherryStudio | Windows, macOS, Linux | Rich plugins, multiple providers |
| Chatbox | Windows, macOS, Linux | Lightweight, fast |
| BotGem | iOS, macOS | Native Apple experience |
| TypingMind | Web, Desktop | Custom personas, plugins |
| OpenCat | iOS, macOS | Native, offline support |
| ChatWise | Windows, macOS | Team collaboration |

## Configuration Steps

### Step 1: Get Your API Key

1. Log into [LemonData Dashboard](https://lemondata.cc/dashboard)
2. Navigate to [API Keys](https://lemondata.cc/dashboard/api)
3. Create and copy your API key (format: `sk-...`)

### Step 2: Configure Your App

  
  
  
  

### Step 3: Add Models

After configuring the provider, add the models you want to use:

| Category | Models |
|----------|--------|
| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |
| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |
| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |
| DeepSeek | `deepseek-r1`, `deepseek-chat` |

## Configuration Reference

| Field | Value |
|-------|-------|
| Base URL | `https://api.lemondata.cc/v1` |
| API Key | `sk-your-lemondata-key` |
| API Type | OpenAI Compatible |

**JSON Configuration (for apps that support it):**

```json
{
  "provider": "LemonData",
  "baseURL": "https://api.lemondata.cc/v1",
  "apiKey": "sk-your-lemondata-key",
  "models": [
    "gpt-4o",
    "gpt-4o-mini",
    "claude-sonnet-4-5",
    "gemini-2.5-flash"
  ]
}
```

## Troubleshooting

  

  

  

  

## Tips

---
