{
  "api-reference/3d/create-3d.mdx": {
    "sourceHash": "1a4b06c0963b1dc9",
    "translations": {
      "zh": "---\ntitle: \"创建 3D 模型\"\napi: \"POST /v1/3d/generations\"\ndescription: \"创建一个 3D 模型生成任务\"\n---\n\n使用 Tripo3D 和其他提供商通过文本或图像生成 3D 模型。这是一个异步 API。\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  要使用的模型（例如 `tripo3d-v2.5`，`tripo3d-v2`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  要生成的 3D 模型的文本描述。\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  用于图像转 3D 生成的 Base64 编码图像。\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  用于图像转 3D 生成的图像 URL。\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  输出格式：`glb`、`fbx`、`obj` 或 `usdz`。\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  质量等级：`draft`、`standard` 或 `high`。\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  模型的风格预设。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  用于可复现生成的种子。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  终端用户的唯一标识符。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  用于轮询状态的任务 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任务状态：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  任务创建的 Unix 时间戳。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"建立 3D 模型\"\napi: \"POST /v1/3d/generations\"\ndescription: \"建立 3D 模型生成任務\"\n---\n\n使用 Tripo3D 及其他供應商，透過文字或圖片生成 3D 模型。這是一個非同步 API。\n\n## 請求主體 (Request Body)\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  要使用的模型（例如：`tripo3d-v2.5`、`tripo3d-v2`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  要生成的 3D 模型文字描述。\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  用於圖片轉 3D 生成的 Base64 編碼圖片。\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  用於圖片轉 3D 生成的圖片 URL。\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  輸出格式：`glb`、`fbx`、`obj` 或 `usdz`。\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  品質等級：`draft`、`standard` 或 `high`。\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  模型的風格預設。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  用於可重現生成的種子值。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  終端用戶的唯一識別碼。\n</ParamField>\n\n## 回應 (Response)\n\n<ResponseField name=\"id\" type=\"string\">\n  用於輪詢狀態的任務 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任務狀態：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  任務建立的 Unix 時間戳記。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"3Dモデルの作成\"\napi: \"POST /v1/3d/generations\"\ndescription: \"3Dモデル生成タスクを作成します\"\n---\n\nTripo3Dやその他のプロバイダーを使用して、テキストまたは画像から3Dモデルを生成します。これは非同期APIです。\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  使用するモデル（例：`tripo3d-v2.5`、`tripo3d-v2`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  生成する3Dモデルのテキスト説明。\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Image-to-3D生成用のBase64エンコード済み画像。\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Image-to-3D生成用の画像のURL。\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  出力形式：`glb`、`fbx`、`obj`、または`usdz`。\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  品質レベル：`draft`、`standard`、または`high`。\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  モデルのスタイルプリセット。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  再現可能な生成のためのシード値。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  エンドユーザーの一意識別子。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  ステータス確認（ポーリング）用のタスクID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  タスクのステータス：`pending`、`processing`、`completed`、または`failed`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  タスク作成時のUnixタイムスタンプ。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"3D 모델 생성\"\napi: \"POST /v1/3d/generations\"\ndescription: \"3D 모델 생성 작업을 생성합니다\"\n---\n\nTripo3D 및 기타 제공업체를 사용하여 텍스트 또는 이미지에서 3D 모델을 생성합니다. 이 API는 비동기 방식으로 작동합니다.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  사용할 모델 (예: `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  생성할 3D 모델에 대한 텍스트 설명입니다.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  image-to-3D 생성을 위한 Base64 인코딩된 이미지입니다.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  image-to-3D 생성을 위한 이미지 URL입니다.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  출력 형식: `glb`, `fbx`, `obj` 또는 `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  품질 수준: `draft`, `standard` 또는 `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  모델의 스타일 프리셋입니다.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  재현 가능한 생성을 위한 시드(Seed) 값입니다.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  최종 사용자를 위한 고유 식별자입니다.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  상태 폴링을 위한 작업 ID입니다.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  작업 상태: `pending`, `processing`, `completed` 또는 `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  작업이 생성된 Unix 타임스탬프입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"3D-Modell erstellen\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Erstellt eine Aufgabe zur Generierung eines 3D-Modells\"\n---\n\nGenerieren Sie 3D-Modelle aus Text oder Bildern mit Tripo3D und anderen Anbietern. Dies ist eine asynchrone API.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Zu verwendendes Modell (z. B. `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Textbeschreibung des zu generierenden 3D-Modells.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Base64-kodiertes Bild für die Image-to-3D-Generierung.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  URL des Bildes für die Image-to-3D-Generierung.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Ausgabeformat: `glb`, `fbx`, `obj` oder `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Qualitätsstufe: `draft`, `standard` oder `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Stil-Voreinstellung für das Modell.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed für reproduzierbare Generierung.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Eine eindeutige Kennung für den Endbenutzer.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  Task-ID zur Statusabfrage.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Task-Status: `pending`, `processing`, `completed` oder `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Unix-Zeitstempel der Task-Erstellung.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Créer un modèle 3D\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Crée une tâche de génération de modèle 3D\"\n---\n\nGénérez des modèles 3D à partir de texte ou d'images en utilisant Tripo3D et d'autres fournisseurs. Il s'agit d'une API asynchrone.\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Modèle à utiliser (par ex., `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Description textuelle du modèle 3D à générer.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Image encodée en Base64 pour la génération image-vers-3D.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  URL de l'image pour la génération image-vers-3D.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Format de sortie : `glb`, `fbx`, `obj` ou `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Niveau de qualité : `draft`, `standard` ou `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Préréglage de style pour le modèle.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed pour une génération reproductible.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identifiant unique pour l'utilisateur final.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tâche pour le suivi du statut.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Statut de la tâche : `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Horodatage Unix de la création de la tâche.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Crear modelo 3D\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Crea una tarea de generación de modelos 3D\"\n---\n\nGenere modelos 3D a partir de texto o imágenes utilizando Tripo3D y otros proveedores. Esta es una API asíncrona.\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Modelo a utilizar (por ejemplo, `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Descripción de texto del modelo 3D a generar.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Imagen codificada en Base64 para la generación de imagen a 3D.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  URL de la imagen para la generación de imagen a 3D.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Formato de salida: `glb`, `fbx`, `obj` o `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Nivel de calidad: `draft`, `standard` o `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Preajuste de estilo para el modelo.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Semilla para una generación reproducible.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identificador único para el usuario final.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tarea para consultar el estado.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Estado de la tarea: `pending`, `processing`, `completed` o `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Marca de tiempo Unix de la creación de la tarea.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Criar Modelo 3D\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Cria uma tarefa de geração de modelo 3D\"\n---\n\nGere modelos 3D a partir de texto ou imagens usando Tripo3D e outros provedores. Esta é uma API assíncrona.\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Modelo a ser usado (ex: `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Descrição em texto do modelo 3D a ser gerado.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Imagem codificada em Base64 para geração de imagem para 3D.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  URL da imagem para geração de imagem para 3D.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Formato de saída: `glb`, `fbx`, `obj` ou `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Nível de qualidade: `draft`, `standard` ou `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Predefinição de estilo para o modelo.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed para geração reproduzível.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Um identificador exclusivo para o usuário final.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID da tarefa para consulta de status (polling).\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status da tarefa: `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Timestamp Unix da criação da tarefa.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"إنشاء نموذج ثلاثي الأبعاد\"\napi: \"POST /v1/3d/generations\"\ndescription: \"إنشاء مهمة لتوليد نموذج ثلاثي الأبعاد\"\n---\n\nتوليد نماذج ثلاثية الأبعاد من النصوص أو الصور باستخدام Tripo3D ومزودين آخرين. هذه API غير متزامنة.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  النموذج المراد استخدامه (على سبيل المثال، `tripo3d-v2.5` أو `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  وصف نصي للنموذج ثلاثي الأبعاد المراد توليده.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  صورة بترميز Base64 لتوليد نموذج ثلاثي الأبعاد من صورة.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  رابط (URL) الصورة لتوليد نموذج ثلاثي الأبعاد من صورة.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  تنسيق المخرجات: `glb` أو `fbx` أو `obj` أو `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  مستوى الجودة: `draft` أو `standard` أو `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  نمط مسبق الإعداد للنموذج.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  بذرة (seed) لضمان توليد نتائج متطابقة.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  معرف فريد للمستخدم النهائي.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  معرف المهمة (Task ID) للاستعلام عن الحالة.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  حالة المهمة: `pending` أو `processing` أو `completed` أو `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  طابع زمني بتنسيق Unix لوقت إنشاء المهمة.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Tạo Mô hình 3D\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Tạo một tác vụ tạo mô hình 3D\"\n---\n\nTạo các mô hình 3D từ văn bản hoặc hình ảnh bằng cách sử dụng Tripo3D và các nhà cung cấp khác. Đây là một API bất đồng bộ.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Model để sử dụng (ví dụ: `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Mô tả văn bản của mô hình 3D cần tạo.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Hình ảnh được mã hóa Base64 để tạo mô hình 3D từ hình ảnh.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  URL của hình ảnh để tạo mô hình 3D từ hình ảnh.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Định dạng đầu ra: `glb`, `fbx`, `obj`, hoặc `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Mức chất lượng: `draft`, `standard`, hoặc `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Phong cách thiết lập sẵn cho mô hình.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed để tạo kết quả có thể tái lập.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Một mã định danh duy nhất cho người dùng cuối.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tác vụ để kiểm tra trạng thái (polling).\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Trạng thái tác vụ: `pending`, `processing`, `completed`, hoặc `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Dấu thời gian Unix của thời điểm tạo tác vụ.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\n",
      "id": "---\ntitle: \"Buat Model 3D\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Membuat tugas pembuatan model 3D\"\n---\n\nHasilkan model 3D dari teks atau gambar menggunakan Tripo3D dan penyedia lainnya. Ini adalah API asinkron.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Model yang akan digunakan (misalnya, `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Deskripsi teks dari model 3D yang akan dihasilkan.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Gambar dengan encoding Base64 untuk pembuatan image-to-3D.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  URL gambar untuk pembuatan image-to-3D.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Format output: `glb`, `fbx`, `obj`, atau `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Tingkat kualitas: `draft`, `standard`, atau `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Preset gaya untuk model tersebut.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed untuk pembuatan yang dapat direproduksi.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Pengidentifikasi unik untuk pengguna akhir.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tugas untuk polling status.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status tugas: `pending`, `processing`, `completed`, atau `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Timestamp Unix dari pembuatan tugas.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>",
      "tr": "---\ntitle: \"3D Model Oluştur\"\napi: \"POST /v1/3d/generations\"\ndescription: \"Bir 3D model oluşturma görevi oluşturur\"\n---\n\nTripo3D ve diğer sağlayıcıları kullanarak metin veya görsellerden 3D modeller oluşturun. Bu asenkron bir API'dir.\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" default=\"tripo3d-v2.5\">\n  Kullanılacak model (örneğin, `tripo3d-v2.5`, `tripo3d-v2`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Oluşturulacak 3D modelin metin açıklaması.\n</ParamField>\n\n<ParamField body=\"image\" type=\"string\">\n  Görselden 3D'ye üretim için Base64 kodlu görsel.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Görselden 3D'ye üretim için görselin URL'si.\n</ParamField>\n\n<ParamField body=\"format\" type=\"string\" default=\"glb\">\n  Çıktı formatı: `glb`, `fbx`, `obj` veya `usdz`.\n</ParamField>\n\n<ParamField body=\"quality\" type=\"string\" default=\"standard\">\n  Kalite seviyesi: `draft`, `standard` veya `high`.\n</ParamField>\n\n<ParamField body=\"style\" type=\"string\">\n  Model için stil ön ayarı.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Tekrarlanabilir üretim için seed değeri.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Son kullanıcı için benzersiz bir tanımlayıcı.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Durum sorgulama için görev kimliği (Task ID).\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Görev durumu: `pending`, `processing`, `completed` veya `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Görevin oluşturulma zamanına ait Unix zaman damgası.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/3d/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tripo3d-v2.5\",\n    \"prompt\": \"A detailed medieval castle with towers\",\n    \"format\": \"glb\",\n    \"quality\": \"high\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/3d/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"tripo3d-v2.5\",\n        \"prompt\": \"A detailed medieval castle with towers\",\n        \"format\": \"glb\",\n        \"quality\": \"high\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/3d/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'tripo3d-v2.5',\n    prompt: 'A detailed medieval castle with towers',\n    format: 'glb',\n    quality: 'high'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":   \"tripo3d-v2.5\",\n        \"prompt\":  \"A detailed medieval castle with towers\",\n        \"format\":  \"glb\",\n        \"quality\": \"high\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/3d/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/3d/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tripo3d-v2.5',\n        'prompt' => 'A detailed medieval castle with towers',\n        'format' => 'glb',\n        'quality' => 'high'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"3d_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"tripo3d-v2.5\"\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:19:33.289Z"
  },
  "api-reference/3d/get-3d-status.mdx": {
    "sourceHash": "a237fe53e74aa0fe",
    "translations": {
      "zh": "---\ntitle: \"获取 3D 模型状态\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"获取 3D 生成任务的状态和结果\"\n---\n\n轮询此端点以检查 3D 模型生成任务的状态。\n\n## 路径参数\n\n<ParamField path=\"id\" type=\"string\" required>\n  3D 生成任务 ID。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  任务 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任务状态：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  进度百分比 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  下载 3D 模型文件的 URL（完成后）。\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  下载 GLB 格式模型的 URL（可用时）。\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  下载 FBX 格式模型的 URL（可用时）。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  错误信息（失败时）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json 响应 (已完成)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"獲取 3D 模型狀態\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"獲取 3D 生成任務的狀態與結果\"\n---\n\n輪詢此端點以檢查您的 3D 模型生成任務狀態。\n\n## 路徑參數\n\n<ParamField path=\"id\" type=\"string\" required>\n  3D 生成任務 ID。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"id\" type=\"string\">\n  任務 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任務狀態：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  進度百分比 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  下載 3D 模型檔案的 URL（完成時）。\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  下載 GLB 格式模型的 URL（可用時）。\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  下載 FBX 格式模型的 URL（可用時）。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  錯誤訊息（失敗時）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json 回應 (已完成)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"3Dモデルステータスの取得\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"3D生成タスクのステータスと結果を取得します\"\n---\n\nこのエンドポイントをポーリングして、3Dモデル生成タスクのステータスを確認します。\n\n## パスパラメータ\n\n<ParamField path=\"id\" type=\"string\" required>\n  3D生成タスクのID。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  タスクID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  タスクのステータス：`pending`、`processing`、`completed`、または`failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  進捗率 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  3DモデルファイルをダウンロードするためのURL（完了時）。\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  GLB形式のモデルをダウンロードするためのURL（利用可能な場合）。\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  FBX形式のモデルをダウンロードするためのURL（利用可能な場合）。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  エラーメッセージ（失敗時）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json レスポンス (完了)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"3D 모델 상태 조회\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"3D 생성 작업의 상태 및 결과를 조회합니다.\"\n---\n\n이 엔드포인트를 폴링하여 3D 모델 생성 작업의 상태를 확인하십시오.\n\n## 경로 파라미터\n\n<ParamField path=\"id\" type=\"string\" required>\n  3D 생성 작업 ID입니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"id\" type=\"string\">\n  작업 ID.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  작업 상태: `pending`, `processing`, `completed` 또는 `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  진행률 백분율 (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  3D 모델 파일을 다운로드할 URL (완료 시).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  GLB 형식 모델을 다운로드할 URL (사용 가능한 경우).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  FBX 형식 모델을 다운로드할 URL (사용 가능한 경우).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  에러 메시지 (실패 시).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json 응답 (완료)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"3D-Modell-Status abrufen\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Ruft den Status und das Ergebnis einer 3D-Generierungsaufgabe ab\"\n---\n\nFragen Sie diesen Endpunkt ab, um den Status Ihrer 3D-Modell-Generierungsaufgabe zu überprüfen.\n\n## Pfadparameter\n\n<ParamField path=\"id\" type=\"string\" required>\n  Die ID der 3D-Generierungsaufgabe.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"id\" type=\"string\">\n  Aufgaben-ID.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Aufgabenstatus: `pending`, `processing`, `completed` oder `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Fortschritt in Prozent (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  URL zum Herunterladen der 3D-Modelldatei (nach Abschluss).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  URL zum Herunterladen des Modells im GLB-Format (sofern verfügbar).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  URL zum Herunterladen des Modells im FBX-Format (sofern verfügbar).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Fehlermeldung (im Fehlerfall).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Obtenir le statut du modèle 3D\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Récupère le statut et le résultat d'une tâche de génération 3D\"\n---\n\nInterrogez cet endpoint pour vérifier le statut de votre tâche de génération de modèle 3D.\n\n## Paramètres de chemin\n\n<ParamField path=\"id\" type=\"string\" required>\n  L'ID de la tâche de génération 3D.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tâche.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Statut de la tâche : `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Pourcentage de progression (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  URL pour télécharger le fichier du modèle 3D (une fois terminé).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  URL pour télécharger le modèle au format GLB (si disponible).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  URL pour télécharger le modèle au format FBX (si disponible).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Message d'erreur (en cas d'échec).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Réponse (Terminée)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Obtener estado del modelo 3D\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Recupera el estado y el resultado de una tarea de generación 3D\"\n---\n\nRealice peticiones periódicas (poll) a este endpoint para verificar el estado de su tarea de generación de modelos 3D.\n\n## Parámetros de ruta\n\n<ParamField path=\"id\" type=\"string\" required>\n  El ID de la tarea de generación 3D.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tarea.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Estado de la tarea: `pending`, `processing`, `completed` o `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Porcentaje de progreso (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  URL para descargar el archivo del modelo 3D (cuando se haya completado).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  URL para descargar el modelo en formato GLB (cuando esté disponible).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  URL para descargar el modelo en formato FBX (cuando esté disponible).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Mensaje de error (cuando falle).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Obter Status do Modelo 3D\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Recupera o status e o resultado de uma tarefa de geração 3D\"\n---\n\nConsulte este endpoint para verificar o status da sua tarefa de geração de modelo 3D.\n\n## Parâmetros de Path\n\n<ParamField path=\"id\" type=\"string\" required>\n  O ID da tarefa de geração 3D.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID da tarefa.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status da tarefa: `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Porcentagem de progresso (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  URL para baixar o arquivo do modelo 3D (quando concluído).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  URL para baixar o modelo no formato GLB (quando disponível).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  URL para baixar o modelo no formato FBX (quando disponível).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Mensagem de erro (quando houver falha).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Resposta (Concluída)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"الحصول على حالة نموذج 3D\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"يسترجع حالة ونتيجة مهمة إنشاء نموذج 3D\"\n---\n\nقم بالاستعلام (Poll) عن نقطة النهاية هذه للتحقق من حالة مهمة إنشاء نموذج 3D الخاص بك.\n\n## معلمات المسار (Path Parameters)\n\n<ParamField path=\"id\" type=\"string\" required>\n  معرف مهمة إنشاء نموذج 3D.\n</ParamField>\n\n## الاستجابة (Response)\n\n<ResponseField name=\"id\" type=\"string\">\n  معرف المهمة.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  حالة المهمة: `pending` أو `processing` أو `completed` أو `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  نسبة التقدم (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  رابط (URL) لتحميل ملف نموذج 3D (عند الاكتمال).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  رابط (URL) لتحميل النموذج بتنسيق GLB (عند توفره).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  رابط (URL) لتحميل النموذج بتنسيق FBX (عند توفره).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  رسالة الخطأ (عند الفشل).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Lấy trạng thái mô hình 3D\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Truy xuất trạng thái và kết quả của tác vụ tạo mô hình 3D\"\n---\n\nThực hiện poll endpoint này để kiểm tra trạng thái của tác vụ tạo mô hình 3D của bạn.\n\n## Tham số đường dẫn\n\n<ParamField path=\"id\" type=\"string\" required>\n  ID của tác vụ tạo mô hình 3D.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tác vụ.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Trạng thái tác vụ: `pending`, `processing`, `completed`, hoặc `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Phần trăm tiến độ (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  URL để tải xuống tệp mô hình 3D (khi hoàn tất).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  URL để tải xuống mô hình định dạng GLB (khi có sẵn).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  URL để tải xuống mô hình định dạng FBX (khi có sẵn).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Thông báo lỗi (khi thất bại).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Dapatkan Status Model 3D\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Mengambil status dan hasil dari tugas pembuatan model 3D\"\n---\n\nLakukan polling pada endpoint ini untuk memeriksa status tugas pembuatan model 3D Anda.\n\n## Parameter Path\n\n<ParamField path=\"id\" type=\"string\" required>\n  ID tugas pembuatan model 3D.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tugas.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status tugas: `pending`, `processing`, `completed`, atau `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Persentase progres (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  URL untuk mengunduh file model 3D (saat selesai).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  URL untuk mengunduh model format GLB (saat tersedia).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  URL untuk mengunduh model format FBX (saat tersedia).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Pesan kesalahan (saat gagal).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>",
      "tr": "---\ntitle: \"3D Model Durumunu Al\"\napi: \"GET /v1/3d/generations/{id}\"\ndescription: \"Bir 3D oluşturma görevinin durumunu ve sonucunu getirir\"\n---\n\n3D model oluşturma görevinizin durumunu kontrol etmek için bu uç noktayı sorgulayın.\n\n## Yol Parametreleri\n\n<ParamField path=\"id\" type=\"string\" required>\n  3D oluşturma görevi kimliği (ID).\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Görev kimliği (ID).\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Görev durumu: `pending`, `processing`, `completed` veya `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  İlerleme yüzdesi (0-100).\n</ResponseField>\n\n<ResponseField name=\"model_url\" type=\"string\">\n  3D model dosyasını indirmek için URL (tamamlandığında).\n</ResponseField>\n\n<ResponseField name=\"glb_url\" type=\"string\">\n  GLB formatındaki modeli indirmek için URL (mevcut olduğunda).\n</ResponseField>\n\n<ResponseField name=\"fbx_url\" type=\"string\">\n  FBX formatındaki modeli indirmek için URL (mevcut olduğunda).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Hata mesajı (başarısız olduğunda).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/3d/generations/3d_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"3d_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/3d/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"3D Model URL: {result['model_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = '3d_abc123';\n\nasync function pollFor3DModel() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/3d/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`3D Model URL: ${result.model_url}`);\n      return result.model_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"3d_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/3d/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"3D Model URL: %s\\n\", result[\"model_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = '3d_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/3d/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"3D Model URL: {$result['model_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Yanıt (Tamamlandı)\n{\n  \"id\": \"gptproto_v3:abc123\",\n  \"status\": \"completed\",\n  \"model_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"glb_url\": \"https://cdn.tripo3d.ai/abc123.glb\",\n  \"fbx_url\": \"https://cdn.tripo3d.ai/abc123.fbx\"\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:19:54.988Z"
  },
  "api-reference/audio/create-speech.mdx": {
    "sourceHash": "e3f01a43e5cfcea9",
    "translations": {
      "zh": "---\ntitle: \"创建语音\"\napi: \"POST /v1/audio/speech\"\ndescription: \"根据输入文本生成音频\"\n---\n\n## 请求正文\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  TTS 模型：`tts-1`（标准）或 `tts-1-hd`（高质量）。\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  要生成音频的文本。最大长度为 4096 个字符。\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  使用的音色：`alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  音频格式：`mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`。\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  音频语速（0.25 到 4.0）。\n</ParamField>\n\n## 响应\n\n以请求的格式返回音频文件。\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## 音色示例\n\n| 音色 | 描述 |\n|-------|-------------|\n| `alloy` | 中性，均衡 |\n| `ash` | 冷静，沉稳 |\n| `ballad` | 悦耳，富有表现力 |\n| `coral` | 温暖，亲切 |\n| `echo` | 温暖，口语化 |\n| `fable` | 富有表现力，叙述性 |\n| `nova` | 友好，清晰 |\n| `onyx` | 深沉，权威 |\n| `sage` | 睿智，深思熟虑 |\n| `shimmer` | 柔和，温柔 |\n| `verse` | 动态，多变 |",
      "zh-TW": "---\ntitle: \"建立語音\"\napi: \"POST /v1/audio/speech\"\ndescription: \"根據輸入文字生成音訊\"\n---\n\n## 請求主體\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  TTS 模型：`tts-1`（標準）或 `tts-1-hd`（高品質）。\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  要生成音訊的文字。最多 4096 個字元。\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  使用的聲音：`alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  音訊格式：`mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`。\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  音訊速度（0.25 至 4.0）。\n</ParamField>\n\n## 回應\n\n以要求的格式傳回音訊檔案。\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## 聲音範例\n\n| 聲音 | 描述 |\n|-------|-------------|\n| `alloy` | 中性、平衡 |\n| `ash` | 冷靜、沉穩 |\n| `ballad` | 悅耳、富有表現力 |\n| `coral` | 溫暖、親切 |\n| `echo` | 溫暖、對話式 |\n| `fable` | 富有表現力、敘事性 |\n| `nova` | 友好、清晰 |\n| `onyx` | 深沉、權威 |\n| `sage` | 睿智、深思熟慮 |\n| `shimmer` | 柔和、溫柔 |\n| `verse` | 動態、多才多藝 |",
      "ja": "---\ntitle: \"音声の作成\"\napi: \"POST /v1/audio/speech\"\ndescription: \"入力テキストから音声を生成します\"\n---\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  TTSモデル：`tts-1`（標準）または `tts-1-hd`（高品質）。\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  音声を生成するテキスト。最大4096文字。\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  使用する音声：`alloy`、`ash`、`ballad`、`coral`、`echo`、`fable`、`nova`、`onyx`、`sage`、`shimmer`、`verse`。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  音声形式：`mp3`、`opus`、`aac`、`flac`、`wav`、`pcm`。\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  音声の速度（0.25から4.0）。\n</ParamField>\n\n## レスポンス\n\nリクエストされた形式で音声ファイルを返します。\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## 音声サンプル\n\n| 音声 | 説明 |\n|-------|-------------|\n| `alloy` | ニュートラル、バランスが良い |\n| `ash` | 穏やか、落ち着いた |\n| `ballad` | メロディック、表現力豊か |\n| `coral` | 温かい、親しみやすい |\n| `echo` | 温かい、対話的 |\n| `fable` | 表現力豊か、ナラティブ |\n| `nova` | フレンドリー、明快 |\n| `onyx` | 深く、威厳がある |\n| `sage` | 賢明、思慮深い |\n| `shimmer` | ソフト、優しい |\n| `verse` | ダイナミック、多才 |",
      "ko": "---\ntitle: \"음성 생성\"\napi: \"POST /v1/audio/speech\"\ndescription: \"입력 텍스트로부터 오디오를 생성합니다\"\n---\n\n## 요청 본문\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  TTS 모델: `tts-1` (표준) 또는 `tts-1-hd` (고품질).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  오디오를 생성할 텍스트입니다. 최대 4096자까지 가능합니다.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  사용할 음성: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  오디오 형식: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  오디오 속도 (0.25에서 4.0).\n</ParamField>\n\n## 응답\n\n요청된 형식의 오디오 파일을 반환합니다.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## 음성 샘플\n\n| 음성 | 설명 |\n|-------|-------------|\n| `alloy` | 중립적이고 균형 잡힌 |\n| `ash` | 차분하고 신중한 |\n| `ballad` | 멜로디가 있고 표현력이 풍부한 |\n| `coral` | 따뜻하고 매력적인 |\n| `echo` | 따뜻하고 대화체인 |\n| `fable` | 표현력이 풍부하고 서사적인 |\n| `nova` | 친근하고 명확한 |\n| `onyx` | 깊고 권위 있는 |\n| `sage` | 현명하고 사려 깊은 |\n| `shimmer` | 부드럽고 온화한 |\n| `verse` | 역동적이고 다재다능한 |",
      "de": "---\ntitle: \"Sprache erstellen\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Generiert Audio aus dem Eingabetext\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  TTS-Modell: `tts-1` (Standard) oder `tts-1-hd` (höhere Qualität).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  Der Text, für den Audio generiert werden soll. Maximal 4096 Zeichen.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Zu verwendende Stimme: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Audioformat: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Audiogeschwindigkeit (0.25 bis 4.0).\n</ParamField>\n\n## Antwort\n\nGibt die Audiodatei im angeforderten Format zurück.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## Sprachbeispiele\n\n| Stimme | Beschreibung |\n|-------|-------------|\n| `alloy` | Neutral, ausgewogen |\n| `ash` | Ruhig, besonnen |\n| `ballad` | Melodisch, ausdrucksstark |\n| `coral` | Warm, einladend |\n| `echo` | Warm, gesprächig |\n| `fable` | Ausdrucksstark, erzählend |\n| `nova` | Freundlich, klar |\n| `onyx` | Tief, autoritär |\n| `sage` | Weise, nachdenklich |\n| `shimmer` | Weich, sanft |\n| `verse` | Dynamisch, vielseitig |",
      "fr": "---\ntitle: \"Créer une synthèse vocale\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Génère de l'audio à partir du texte d'entrée\"\n---\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  Modèle TTS : `tts-1` (standard) ou `tts-1-hd` (haute qualité).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  Le texte pour lequel générer l'audio. Maximum 4096 caractères.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Voix à utiliser : `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Format audio : `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Vitesse de l'audio (0.25 à 4.0).\n</ParamField>\n\n## Réponse\n\nRenvoie le fichier audio dans le format demandé.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## Échantillons de voix\n\n| Voix | Description |\n|-------|-------------|\n| `alloy` | Neutre, équilibrée |\n| `ash` | Calme, mesurée |\n| `ballad` | Mélodique, expressive |\n| `coral` | Chaleureuse, invitante |\n| `echo` | Chaleureuse, conversationnelle |\n| `fable` | Expressive, narrative |\n| `nova` | Amicale, claire |\n| `onyx` | Profonde, autoritaire |\n| `sage` | Sage, réfléchie |\n| `shimmer` | Douce, délicate |\n| `verse` | Dynamique, polyvalente |",
      "es": "---\ntitle: \"Crear Voz\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Genera audio a partir del texto de entrada\"\n---\n\n## Cuerpo de la Solicitud\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  Modelo TTS: `tts-1` (estándar) o `tts-1-hd` (mayor calidad).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  El texto para el cual se generará el audio. Máximo 4096 caracteres.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Voz a utilizar: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Formato de audio: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Velocidad del audio (0.25 a 4.0).\n</ParamField>\n\n## Respuesta\n\nDevuelve el archivo de audio en el formato solicitado.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## Muestras de Voz\n\n| Voz | Descripción |\n|-------|-------------|\n| `alloy` | Neutral, equilibrada |\n| `ash` | Calma, pausada |\n| `ballad` | Melódica, expresiva |\n| `coral` | Cálida, acogedora |\n| `echo` | Cálida, conversacional |\n| `fable` | Expresiva, narrativa |\n| `nova` | Amigable, clara |\n| `onyx` | Profunda, autoritaria |\n| `sage` | Sabia, reflexiva |\n| `shimmer` | Suave, delicada |\n| `verse` | Dinámica, versátil |",
      "pt": "---\ntitle: \"Criar Fala\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Gera áudio a partir do texto de entrada\"\n---\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  Modelo TTS: `tts-1` (padrão) ou `tts-1-hd` (alta qualidade).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  O texto para o qual o áudio será gerado. Máximo de 4096 caracteres.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Voz a ser utilizada: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Formato de áudio: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Velocidade do áudio (0.25 a 4.0).\n</ParamField>\n\n## Resposta\n\nRetorna o arquivo de áudio no formato solicitado.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## Amostras de Voz\n\n| Voz | Descrição |\n|-------|-------------|\n| `alloy` | Neutra, equilibrada |\n| `ash` | Calma, compassada |\n| `ballad` | Melódica, expressiva |\n| `coral` | Acolhedora, convidativa |\n| `echo` | Acolhedora, coloquial |\n| `fable` | Expressiva, narrativa |\n| `nova` | Amigável, clara |\n| `onyx` | Grave, autoritária |\n| `sage` | Sábia, ponderada |\n| `shimmer` | Suave, gentil |\n| `verse` | Dinâmica, versátil |",
      "ar": "---\ntitle: \"إنشاء الكلام\"\napi: \"POST /v1/audio/speech\"\ndescription: \"توليد صوت من النص المدخل\"\n---\n\n## جسم الطلب\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  نموذج TTS: `tts-1` (قياسي) أو `tts-1-hd` (جودة أعلى).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  النص المراد توليد الصوت له. بحد أقصى 4096 حرفاً.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  الصوت المراد استخدامه: `alloy`، `ash`، `ballad`، `coral`، `echo`، `fable`، `nova`، `onyx`، `sage`، `shimmer`، `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  تنسيق الصوت: `mp3`، `opus`، `aac`، `flac`، `wav`، `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  سرعة الصوت (من 0.25 إلى 4.0).\n</ParamField>\n\n## الاستجابة\n\nيُرجع ملف الصوت بالتنسيق المطلوب.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## عينات الأصوات\n\n| الصوت | الوصف |\n|-------|-------------|\n| `alloy` | محايد، متوازن |\n| `ash` | هادئ، رزين |\n| `ballad` | لحني، معبر |\n| `coral` | دافئ، جذاب |\n| `echo` | دافئ، حواري |\n| `fable` | معبر، سردي |\n| `nova` | ودود، واضح |\n| `onyx` | عميق، رسمي |\n| `sage` | حكيم، متأمل |\n| `shimmer` | ناعم، لطيف |\n| `verse` | ديناميكي، متعدد الاستخدامات |",
      "vi": "---\ntitle: \"Tạo giọng nói\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Tạo âm thanh từ văn bản đầu vào\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  Mô hình TTS: `tts-1` (tiêu chuẩn) hoặc `tts-1-hd` (chất lượng cao).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  Văn bản cần tạo âm thanh. Tối đa 4096 ký tự.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Giọng nói sử dụng: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Định dạng âm thanh: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Tốc độ âm thanh (0.25 đến 4.0).\n</ParamField>\n\n## Phản hồi\n\nTrả về tệp âm thanh theo định dạng đã yêu cầu.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## Mẫu giọng nói\n\n| Giọng nói | Mô tả |\n|-------|-------------|\n| `alloy` | Trung tính, cân bằng |\n| `ash` | Điềm tĩnh, chừng mực |\n| `ballad` | Truyền cảm, biểu cảm |\n| `coral` | Ấm áp, lôi cuốn |\n| `echo` | Ấm áp, tự nhiên như trò chuyện |\n| `fable` | Biểu cảm, mang tính tự sự |\n| `nova` | Thân thiện, rõ ràng |\n| `onyx` | Trầm, uy quyền |\n| `sage` | Thông thái, sâu sắc |\n| `shimmer` | Nhẹ nhàng, dịu dàng |\n| `verse` | Năng động, linh hoạt |",
      "id": "---\ntitle: \"Buat Ucapan\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Menghasilkan audio dari teks input\"\n---\n\n## Body Permintaan\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  Model TTS: `tts-1` (standar) atau `tts-1-hd` (kualitas lebih tinggi).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  Teks untuk menghasilkan audio. Maksimum 4096 karakter.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Suara yang digunakan: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Format audio: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Kecepatan audio (0,25 hingga 4,0).\n</ParamField>\n\n## Respons\n\nMengembalikan file audio dalam format yang diminta.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport",
      "tr": "---\ntitle: \"Konuşma Oluştur\"\napi: \"POST /v1/audio/speech\"\ndescription: \"Giriş metninden ses oluşturur\"\n---\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" default=\"tts-1\">\n  TTS modeli: `tts-1` (standart) veya `tts-1-hd` (daha yüksek kalite).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string\" required>\n  Ses oluşturulacak metin. Maksimum 4096 karakter.\n</ParamField>\n\n<ParamField body=\"voice\" type=\"string\" required>\n  Kullanılacak ses: `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, `shimmer`, `verse`.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"mp3\">\n  Ses formatı: `mp3`, `opus`, `aac`, `flac`, `wav`, `pcm`.\n</ParamField>\n\n<ParamField body=\"speed\" type=\"number\" default=\"1.0\">\n  Ses hızı (0.25 ile 4.0 arası).\n</ParamField>\n\n## Yanıt\n\nSes dosyasını istenen formatta döndürür.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/speech\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"tts-1-hd\",\n    \"voice\": \"nova\",\n    \"input\": \"Hello, welcome to LemonData!\"\n  }' \\\n  --output speech.mp3\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.audio.speech.create(\n    model=\"tts-1-hd\",\n    voice=\"nova\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"speech.mp3\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.speech.create({\n  model: 'tts-1-hd',\n  voice: 'nova',\n  input: 'Hello, welcome to LemonData!'\n});\n\nconst buffer = Buffer.from(await response.arrayBuffer());\nfs.writeFileSync('speech.mp3', buffer);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"io\"\n    \"os\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateSpeech(\n        context.Background(),\n        openai.CreateSpeechRequest{\n            Model: openai.TTSModel1HD,\n            Voice: openai.VoiceNova,\n            Input: \"Hello, welcome to LemonData!\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    defer resp.Close()\n\n    file, _ := os.Create(\"speech.mp3\")\n    defer file.Close()\n    io.Copy(file, resp)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/speech');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'tts-1-hd',\n        'voice' => 'nova',\n        'input' => 'Hello, welcome to LemonData!'\n    ])\n]);\n\n$audio = curl_exec($ch);\ncurl_close($ch);\n\nfile_put_contents('speech.mp3', $audio);\n```\n</RequestExample>\n\n## Ses Örnekleri\n\n| Ses | Açıklama |\n|-------|-------------|\n| `alloy` | Nötr, dengeli |\n| `ash` | Sakin, ölçülü |\n| `ballad` | Melodik, etkileyici |\n| `coral` | Sıcak, davetkar |\n| `echo` | Sıcak, konuşkan |\n| `fable` | Etkileyici, anlatısal |\n| `nova` | Arkadaş canlısı, net |\n| `onyx` | Derin, otoriter |\n| `sage` | Bilge, düşünceli |\n| `shimmer` | Yumuşak, nazik |\n| `verse` | Dinamik, çok yönlü |"
    },
    "updatedAt": "2026-01-26T05:20:15.911Z"
  },
  "api-reference/audio/create-transcription.mdx": {
    "sourceHash": "a8d61058c76a2727",
    "translations": {
      "zh": "---\ntitle: \"创建转录\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"将音频转录为输入语言\"\n---\n\n## 请求体\n\n<ParamField body=\"file\" type=\"file\" required>\n  要转录的音频文件。支持的格式：flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm。\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  要使用的模型。目前仅支持 `whisper-1`。\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  音频语言，采用 ISO-639-1 格式（例如：`en`, `zh`, `ja`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  可选文本，用于引导模型的风格或延续之前的片段。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  输出格式：`json`, `text`, `srt`, `verbose_json`, `vtt`。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  采样温度（0 到 1）。\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  时间戳粒度：`word` 和/或 `segment`。需要 `verbose_json`。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"text\" type=\"string\">\n  转录后的文本。\n</ResponseField>\n\n对于 `verbose_json`：\n\n<ResponseField name=\"task\" type=\"string\">\n  始终为 `transcribe`。\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  检测到的语言。\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  音频时长（秒）。\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  带有时间戳的转录片段。\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  词级时间戳（如果已请求）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 翻译\n\n要将音频翻译为英文，请使用翻译端点：\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "zh-TW": "---\ntitle: \"建立逐字稿\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"將音訊轉錄為輸入語言\"\n---\n\n## 請求主體\n\n<ParamField body=\"file\" type=\"file\" required>\n  要轉錄的音訊檔案。支援的格式：flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm。\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  要使用的模型。目前僅支援 `whisper-1`。\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  音訊的語言，採用 ISO-639-1 格式（例如：`en`、`zh`、`ja`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  選填文字，用於引導模型的風格或延續之前的片段。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  輸出格式：`json`、`text`、`srt`、`verbose_json`、`vtt`。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  取樣溫度（0 到 1）。\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  時間戳記精細度：`word` 和/或 `segment`。需要 `verbose_json`。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"text\" type=\"string\">\n  轉錄的文字。\n</ResponseField>\n\n針對 `verbose_json`：\n\n<ResponseField name=\"task\" type=\"string\">\n  固定為 `transcribe`。\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  偵測到的語言。\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  音訊長度（秒）。\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  包含時間戳記的轉錄片段。\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  單詞級別的時間戳記（如果請求）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 翻譯\n\n若要將音訊翻譯為英文，請使用翻譯端點：\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "ja": "---\ntitle: \"文字起こしの作成\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"音声を入力言語で文字起こしします\"\n---\n\n## リクエストボディ\n\n<ParamField body=\"file\" type=\"file\" required>\n  文字起こしする音声ファイル。サポートされている形式：flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm。\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  使用するモデル。現在は `whisper-1` のみがサポートされています。\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  ISO-639-1 形式の音声の言語（例：`en`, `zh`, `ja`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  モデルのスタイルをガイドしたり、前のセグメントを継続したりするためのオプションのテキスト。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  出力形式：`json`, `text`, `srt`, `verbose_json`, `vtt`。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  サンプリング温度（0 から 1）。\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  タイムスタンプの粒度：`word` または `segment`、あるいはその両方。`verbose_json` が必要です。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"text\" type=\"string\">\n  文字起こしされたテキスト。\n</ResponseField>\n\n`verbose_json` の場合：\n\n<ResponseField name=\"task\" type=\"string\">\n  常に `transcribe`。\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  検出された言語。\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  音声の長さ（秒）。\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  タイムスタンプ付きの文字起こしセグメント。\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  単語レベルのタイムスタンプ（リクエストされた場合）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 翻訳\n\n音声を英語に翻訳するには、translations エンドポイントを使用します：\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "ko": "---\ntitle: \"전사 생성\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"오디오를 입력 언어로 전사합니다.\"\n---\n\n## 요청 본문\n\n<ParamField body=\"file\" type=\"file\" required>\n  전사할 오디오 파일입니다. 지원되는 형식: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  사용할 모델입니다. 현재 `whisper-1`만 지원됩니다.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  ISO-639-1 형식의 오디오 언어입니다 (예: `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  모델의 스타일을 가이드하거나 이전 세그먼트를 이어가기 위한 선택적 텍스트입니다.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  출력 형식: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  샘플링 온도 (0에서 1 사이).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  타임스탬프 세분화: `word` 및/또는 `segment`. `verbose_json`이 필요합니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"text\" type=\"string\">\n  전사된 텍스트입니다.\n</ResponseField>\n\n`verbose_json`의 경우:\n\n<ResponseField name=\"task\" type=\"string\">\n  항상 `transcribe`입니다.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  감지된 언어입니다.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  초 단위의 오디오 길이입니다.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  타임스탬프가 포함된 전사 세그먼트입니다.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  단어 단위 타임스탬프 (요청 시).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 번역\n\n오디오를 영어로 번역하려면 번역 엔드포인트를 사용하세요:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "de": "---\ntitle: \"Transkription erstellen\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Transkribiert Audio in die Eingabesprache\"\n---\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  Audiodatei zum Transkribieren. Unterstützte Formate: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Zu verwendendes Modell. Derzeit wird nur `whisper-1` unterstützt.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  Sprache des Audios im ISO-639-1-Format (z. B. `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Optionaler Text, um den Stil des Modells zu steuern oder ein vorheriges Segment fortzusetzen.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Ausgabeformat: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Sampling-Temperatur (0 bis 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Zeitstempel-Granularität: `word` und/oder `segment`. Erfordert `verbose_json`.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"text\" type=\"string\">\n  Der transkribierte Text.\n</ResponseField>\n\nFür `verbose_json`:\n\n<ResponseField name=\"task\" type=\"string\">\n  Immer `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Erkannte Sprache.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Audiodauer in Sekunden.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Transkriptionssegmente mit Zeitstempeln.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Zeitstempel auf Wortebene (falls angefordert).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Übersetzung\n\nUm Audio ins Englische zu übersetzen, verwenden Sie den Translations-Endpunkt:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "fr": "---\ntitle: \"Créer une transcription\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Transcrit l'audio dans la langue d'entrée\"\n---\n\n## Corps de la requête\n\n<ParamField body=\"file\" type=\"file\" required>\n  Fichier audio à transcrire. Formats pris en charge : flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Modèle à utiliser. Actuellement, seul `whisper-1` est pris en charge.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  Langue de l'audio au format ISO-639-1 (par ex., `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Texte facultatif pour guider le style du modèle ou continuer un segment précédent.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Format de sortie : `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Température d'échantillonnage (0 à 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Granularité de l'horodatage : `word` et/ou `segment`. Nécessite `verbose_json`.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"text\" type=\"string\">\n  Le texte transcrit.\n</ResponseField>\n\nPour `verbose_json` :\n\n<ResponseField name=\"task\" type=\"string\">\n  Toujours `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Langue détectée.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Durée de l'audio en secondes.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segments de transcription avec horodatages.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Horodatages au niveau des mots (si demandés).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Réponse (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Réponse (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Traduction\n\nPour traduire de l'audio vers l'anglais, utilisez l'endpoint de traduction :\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "es": "---\ntitle: \"Crear transcripción\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Transcribe audio al idioma de entrada\"\n---\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"file\" type=\"file\" required>\n  Archivo de audio a transcribir. Formatos soportados: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Modelo a utilizar. Actualmente solo se admite `whisper-1`.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  Idioma del audio en formato ISO-639-1 (por ejemplo, `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Texto opcional para guiar el estilo del modelo o continuar un segmento anterior.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Formato de salida: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Temperatura de muestreo (0 a 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Granularidad de la marca de tiempo: `word` y/o `segment`. Requiere `verbose_json`.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"text\" type=\"string\">\n  El texto transcrito.\n</ResponseField>\n\nPara `verbose_json`:\n\n<ResponseField name=\"task\" type=\"string\">\n  Siempre `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Idioma detectado.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Duración del audio en segundos.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmentos de transcripción con marcas de tiempo.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Marcas de tiempo a nivel de palabra (si se solicitan).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Respuesta (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Respuesta (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Traducción\n\nPara traducir audio al inglés, utilice el endpoint de traducciones:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "pt": "---\ntitle: \"Criar Transcrição\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Transcreve áudio para o idioma de entrada\"\n---\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  Arquivo de áudio para transcrever. Formatos suportados: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Modelo a ser usado. Atualmente, apenas `whisper-1` é suportado.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  Idioma do áudio no formato ISO-639-1 (ex: `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Texto opcional para guiar o estilo do modelo ou continuar um segmento anterior.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Formato de saída: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Temperatura de amostragem (0 a 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Granularidade do timestamp: `word` e/ou `segment`. Requer `verbose_json`.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"text\" type=\"string\">\n  O texto transcrito.\n</ResponseField>\n\nPara `verbose_json`:\n\n<ResponseField name=\"task\" type=\"string\">\n  Sempre `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Idioma detectado.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Duração do áudio em segundos.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmentos de transcrição com timestamps.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Timestamps em nível de palavra (se solicitado).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Tradução\n\nPara traduzir áudio para o inglês, use o endpoint de traduções:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "ar": "---\ntitle: \"إنشاء تفريغ صوتي\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"يقوم بتفريغ الصوت إلى لغة الإدخال\"\n---\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  ملف الصوت المراد تفريغه. التنسيقات المدعومة: flac، mp3، mp4، mpeg، mpga، m4a، ogg، wav، webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  النموذج المراد استخدامه. حالياً، يتم دعم `whisper-1` فقط.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  لغة الصوت بتنسيق ISO-639-1 (مثل `en`، `zh`، `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  نص اختياري لتوجيه أسلوب النموذج أو متابعة مقطع سابق.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  تنسيق المخرجات: `json`، `text`، `srt`، `verbose_json`، `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  درجة حرارة العينات (Sampling temperature) (من 0 إلى 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  دقة الطابع الزمني: `word` و/أو `segment`. يتطلب `verbose_json`.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"text\" type=\"string\">\n  النص المفرغ.\n</ResponseField>\n\nبالنسبة لـ `verbose_json`:\n\n<ResponseField name=\"task\" type=\"string\">\n  دائماً `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  اللغة المكتشفة.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  مدة الصوت بالثواني.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  مقاطع التفريغ مع الطوابع الزمنية.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  طوابع زمنية على مستوى الكلمة (إذا تم طلب ذلك).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en",
      "vi": "---\ntitle: \"Tạo Bản Phiên Âm\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Phiên âm âm thanh sang ngôn ngữ đầu vào\"\n---\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  Tệp âm thanh cần phiên âm. Các định dạng được hỗ trợ: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Mô hình sử dụng. Hiện tại chỉ hỗ trợ `whisper-1`.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  Ngôn ngữ của âm thanh ở định dạng ISO-639-1 (ví dụ: `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Văn bản tùy chọn để hướng dẫn phong cách của mô hình hoặc tiếp tục một phân đoạn trước đó.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Định dạng đầu ra: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Nhiệt độ lấy mẫu (0 đến 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Độ chi tiết của dấu thời gian: `word` và/hoặc `segment`. Yêu cầu `verbose_json`.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"text\" type=\"string\">\n  Văn bản đã được phiên âm.\n</ResponseField>\n\nĐối với `verbose_json`:\n\n<ResponseField name=\"task\" type=\"string\">\n  Luôn là `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Ngôn ngữ được phát hiện.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Thời lượng âm thanh tính bằng giây.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Các phân đoạn phiên âm kèm theo dấu thời gian.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Dấu thời gian cấp độ từ (nếu được yêu cầu).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Dịch thuật\n\nĐể dịch âm thanh sang tiếng Anh, hãy sử dụng endpoint translations:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "id": "---\ntitle: \"Buat Transkripsi\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Mentranskripsi audio ke dalam bahasa input\"\n---\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  File audio untuk ditranskripsi. Format yang didukung: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Model yang digunakan. Saat ini hanya `whisper-1` yang didukung.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  Bahasa audio dalam format ISO-639-1 (misalnya, `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Teks opsional untuk memandu gaya model atau melanjutkan segmen sebelumnya.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Format output: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Temperatur sampling (0 hingga 1).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Granularitas timestamp: `word` dan/atau `segment`. Memerlukan `verbose_json`.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"text\" type=\"string\">\n  Teks hasil transkripsi.\n</ResponseField>\n\nUntuk `verbose_json`:\n\n<ResponseField name=\"task\" type=\"string\">\n  Selalu `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Bahasa yang terdeteksi.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Durasi audio dalam detik.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmen transkripsi dengan timestamp.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Timestamp tingkat kata (jika diminta).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Response (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Terjemahan\n\nUntuk menerjemahkan audio ke bahasa Inggris, gunakan endpoint translations:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```",
      "tr": "---\ntitle: \"Transkripsiyon Oluştur\"\napi: \"POST /v1/audio/transcriptions\"\ndescription: \"Sesi giriş dilinde metne dönüştürür\"\n---\n\n## İstek Gövdesi\n\n<ParamField body=\"file\" type=\"file\" required>\n  Metne dönüştürülecek ses dosyası. Desteklenen formatlar: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Kullanılacak model. Şu anda yalnızca `whisper-1` desteklenmektedir.\n</ParamField>\n\n<ParamField body=\"language\" type=\"string\">\n  ISO-639-1 formatında sesin dili (örneğin, `en`, `zh`, `ja`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Modelin stilini yönlendirmek veya önceki bir bölümü devam ettirmek için isteğe bağlı metin.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Çıktı formatı: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"0\">\n  Örnekleme sıcaklığı (0 ile 1 arası).\n</ParamField>\n\n<ParamField body=\"timestamp_granularities\" type=\"array\">\n  Zaman damgası ayrıntı düzeyi: `word` ve/veya `segment`. `verbose_json` gerektirir.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"text\" type=\"string\">\n  Metne dönüştürülen metin.\n</ResponseField>\n\n`verbose_json` için:\n\n<ResponseField name=\"task\" type=\"string\">\n  Her zaman `transcribe`.\n</ResponseField>\n\n<ResponseField name=\"language\" type=\"string\">\n  Algılanan dil.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Saniye cinsinden ses süresi.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Zaman damgalı transkripsiyon bölümleri.\n</ResponseField>\n\n<ResponseField name=\"words\" type=\"array\">\n  Kelime düzeyinde zaman damgaları (istenirse).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/transcriptions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F file=\"@audio.mp3\" \\\n  -F model=\"whisper-1\" \\\n  -F language=\"en\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file,\n        language=\"en\"\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.transcriptions.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('audio.mp3'),\n  language: 'en'\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranscription(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"audio.mp3\",\n            Language: \"en\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/transcriptions');\n\n$file = new CURLFile('audio.mp3', 'audio/mpeg', 'audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1',\n        'language' => 'en'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Yanıt (json)\n{\n  \"text\": \"Hello, this is a test of the transcription API.\"\n}\n```\n\n```json Yanıt (verbose_json)\n{\n  \"task\": \"transcribe\",\n  \"language\": \"english\",\n  \"duration\": 5.5,\n  \"text\": \"Hello, this is a test of the transcription API.\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"start\": 0.0,\n      \"end\": 2.5,\n      \"text\": \"Hello, this is a test\",\n      \"tokens\": [...]\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Çeviri\n\nSesi İngilizceye çevirmek için translations uç noktasını kullanın:\n\n```python\nresponse = client.audio.translations.create(\n    model=\"whisper-1\",\n    file=audio_file\n)\n```"
    },
    "updatedAt": "2026-01-26T05:20:32.958Z"
  },
  "api-reference/audio/create-translation.mdx": {
    "sourceHash": "3def301faa9f0915",
    "translations": {
      "zh": "---\ntitle: \"创建翻译\"\napi: \"POST /v1/audio/translations\"\ndescription: \"将音频翻译为英文文本\"\n---\n\n## 概览\n\n将任何支持的语言的音频翻译为英文文本。与转录不同，无论输入语言是什么，此端点始终输出英文文本。\n\n## 请求体\n\n<ParamField body=\"file\" type=\"file\" required>\n  要翻译的音频文件。支持的格式：`flac`、`mp3`、`mp4`、`mpeg`、`mpga`、`m4a`、`ogg`、`wav`、`webm`。最大文件大小为 25 MB。\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  要使用的模型。目前仅支持 `whisper-1`。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  可选文本，用于引导模型的风格或延续之前的片段。应为英文。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  输出的格式。选项：`json`、`text`、`srt`、`verbose_json`、`vtt`。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  采样温度，介于 0 和 1 之间。较高的值（如 0.8）会产生更随机的输出，而较低的值（如 0.2）会使输出更集中且更具确定性。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"text\" type=\"string\">\n  翻译后的英文文本。\n</ResponseField>\n\n对于 `verbose_json` 格式，响应还包括：\n\n<ResponseField name=\"language\" type=\"string\">\n  检测到的输入音频语言。\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  输入音频的时长（以秒为单位）。\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  带有时间戳的翻译文本片段。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 翻译与转录\n\n| 特性 | 翻译 | 转录 |\n|---------|-------------|---------------|\n| 输出语言 | 始终为英文 | 与输入相同 |\n| 使用场景 | 将外语音频转换为英文 | 保留原始语言 |\n| 语言参数 | 不适用 | 可选提示 |\n\n<Note>\n  翻译端点会自动检测源语言并翻译为英文。转录中的 `language` 参数将被忽略。\n</Note>",
      "zh-TW": "---\ntitle: \"建立翻譯\"\napi: \"POST /v1/audio/translations\"\ndescription: \"將音訊翻譯為英文文本\"\n---\n\n## 概覽\n\n將任何支援語言的音訊翻譯為英文文本。與逐字稿（transcription）不同，無論輸入語言為何，此端點始終輸出英文文本。\n\n## 請求主體\n\n<ParamField body=\"file\" type=\"file\" required>\n  要翻譯的音訊檔案。支援的格式：`flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`。檔案大小上限為 25 MB。\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  要使用的模型。目前僅支援 `whisper-1`。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  選填文本，用於引導模型的風格或延續先前的片段。應使用英文。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  輸出格式。選項：`json`, `text`, `srt`, `verbose_json`, `vtt`。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  取樣溫度，介於 0 與 1 之間。較高的值（如 0.8）會產生更隨機的輸出，而較低的值（如 0.2）則會使輸出更集中且具確定性。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"text\" type=\"string\">\n  翻譯後的英文文本。\n</ResponseField>\n\n對於 `verbose_json` 格式，回應還包含：\n\n<ResponseField name=\"language\" type=\"string\">\n  偵測到的輸入音訊語言。\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  輸入音訊的時長（以秒為單位）。\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  帶有時間戳記的翻譯文本片段。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 翻譯 vs 逐字稿\n\n| 特性 | 翻譯 | 逐字稿 |\n|---------|-------------|---------------|\n| 輸出語言 | 始終為英文 | 與輸入相同 |\n| 使用場景 | 將外語音訊轉換為英文 | 保留原始語言 |\n| 語言參數 | 不適用 | 選填提示 |\n\n<Note>\n  翻譯端點會自動偵測來源語言並翻譯為英文。逐字稿中的 `language` 參數將被忽略。\n</Note>",
      "ja": "---\ntitle: \"翻訳の作成\"\napi: \"POST /v1/audio/translations\"\ndescription: \"オーディオを英語のテキストに翻訳します\"\n---\n\n## 概要\n\nサポートされている任意の言語のオーディオを英語のテキストに翻訳します。文字起こし（transcription）とは異なり、このエンドポイントは入力言語に関係なく、常に英語のテキストを出力します。\n\n## リクエストボディ\n\n<ParamField body=\"file\" type=\"file\" required>\n  翻訳するオーディオファイル。サポートされている形式：`flac`、`mp3`、`mp4`、`mpeg`、`mpga`、`m4a`、`ogg`、`wav`、`webm`。最大ファイルサイズは 25 MB です。\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  使用するモデル。現在、`whisper-1` のみがサポートされています。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  モデルのスタイルをガイドしたり、前のセグメントを継続したりするためのオプションのテキスト。英語である必要があります。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  出力の形式。オプション：`json`、`text`、`srt`、`verbose_json`、`vtt`。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  サンプリング温度。0 から 1 の間です。0.8 のような高い値はよりランダムな出力を生成し、0.2 のような低い値は出力をより集中させ、決定論的にします。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"text\" type=\"string\">\n  翻訳された英語のテキスト。\n</ResponseField>\n\n`verbose_json` 形式の場合、レスポンスには以下も含まれます：\n\n<ResponseField name=\"language\" type=\"string\">\n  検出された入力オーディオの言語。\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  入力オーディオの長さ（秒単位）。\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  タイムスタンプ付きの翻訳テキストのセグメント。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 翻訳 vs 文字起こし\n\n| 機能 | 翻訳 | 文字起こし |\n|---------|-------------|---------------|\n| 出力言語 | 常に英語 | 入力と同じ |\n| ユースケース | 外国語のオーディオを英語に変換 | 元の言語を保持 |\n| 言語パラメータ | 該当なし | オプションのヒント |\n\n<Note>\n  翻訳エンドポイントはソース言語を自動的に検出し、英語に翻訳します。文字起こしの `language` パラメータは無視されます。\n</Note>",
      "ko": "---\ntitle: \"번역 생성\"\napi: \"POST /v1/audio/translations\"\ndescription: \"오디오를 영어 텍스트로 번역합니다\"\n---\n\n## 개요\n\n지원되는 모든 언어의 오디오를 영어 텍스트로 번역합니다. 전사(transcription)와 달리, 이 엔드포인트는 입력 언어에 관계없이 항상 영어 텍스트를 출력합니다.\n\n## 요청 본문\n\n<ParamField body=\"file\" type=\"file\" required>\n  번역할 오디오 파일입니다. 지원되는 형식: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. 최대 파일 크기는 25 MB입니다.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  사용할 모델입니다. 현재 `whisper-1`만 지원됩니다.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  모델의 스타일을 가이드하거나 이전 세그먼트를 이어가기 위한 선택적 텍스트입니다. 영어로 작성해야 합니다.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  출력 형식입니다. 옵션: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  샘플링 온도로, 0에서 1 사이의 값입니다. 0.8과 같이 높은 값은 더 무작위한 출력을 생성하며, 0.2와 같이 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"text\" type=\"string\">\n  영어로 번역된 텍스트입니다.\n</ResponseField>\n\n`verbose_json` 형식의 경우, 응답에 다음 항목도 포함됩니다:\n\n<ResponseField name=\"language\" type=\"string\">\n  입력 오디오에서 감지된 언어입니다.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  입력 오디오의 길이(초)입니다.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  타임스탬프가 포함된 번역된 텍스트의 세그먼트들입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 번역 vs 전사\n\n| 기능 | 번역 | 전사 |\n|---------|-------------|---------------|\n| 출력 언어 | 항상 영어 | 입력과 동일 |\n| 사용 사례 | 외국어 오디오를 영어로 변환 | 원본 언어 유지 |\n| 언어 파라미터 | 해당 없음 | 선택적 힌트 |\n\n<Note>\n  번역 엔드포인트는 소스 언어를 자동으로 감지하고 영어로 번역합니다. 전사에서 사용되는 `language` 파라미터는 무시됩니다.\n</Note>",
      "de": "---\ntitle: \"Übersetzung erstellen\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Übersetzt Audio in englischen Text\"\n---\n\n## Übersicht\n\nÜbersetzt Audio in jeder unterstützten Sprache in englischen Text. Im Gegensatz zur Transkription gibt dieser Endpunkt unabhängig von der Eingabesprache immer englischen Text aus.\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  Die zu übersetzende Audiodatei. Unterstützte Formate: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. Die maximale Dateigröße beträgt 25 MB.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Das zu verwendende Modell. Derzeit wird nur `whisper-1` unterstützt.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Ein optionaler Text, um den Stil des Modells zu steuern oder ein vorheriges Segment fortzusetzen. Sollte auf Englisch sein.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Das Format der Ausgabe. Optionen: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  Die Sampling-Temperatur, zwischen 0 und 1. Höhere Werte wie 0,8 erzeugen eine zufälligere Ausgabe, während niedrigere Werte wie 0,2 die Ausgabe fokussierter und deterministischer machen.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"text\" type=\"string\">\n  Der übersetzte Text auf Englisch.\n</ResponseField>\n\nFür das `verbose_json`-Format enthält die Antwort zusätzlich:\n\n<ResponseField name=\"language\" type=\"string\">\n  Die erkannte Sprache des Eingabe-Audios.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Die Dauer des Eingabe-Audios in Sekunden.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmente des übersetzten Textes mit Zeitstempeln.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:",
      "fr": "---\ntitle: \"Créer une traduction\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Traduit de l'audio en texte anglais\"\n---\n\n## Aperçu\n\nTraduit de l'audio dans n'importe quelle langue prise en charge en texte anglais. Contrairement à la transcription, ce point de terminaison produit toujours du texte en anglais, quelle que soit la langue d'entrée.\n\n## Corps de la requête\n\n<ParamField body=\"file\" type=\"file\" required>\n  Le fichier audio à traduire. Formats pris en charge : `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. La taille maximale du fichier est de 25 Mo.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Le modèle à utiliser. Actuellement, seul `whisper-1` est pris en charge.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Un texte optionnel pour guider le style du modèle ou continuer un segment précédent. Doit être en anglais.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Le format de la sortie. Options : `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  La température d'échantillonnage, entre 0 et 1. Des valeurs plus élevées comme 0,8 produisent une sortie plus aléatoire, tandis que des valeurs plus basses comme 0,2 rendent la sortie plus concentrée et déterministe.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"text\" type=\"string\">\n  Le texte traduit en anglais.\n</ResponseField>\n\nPour le format `verbose_json`, la réponse inclut également :\n\n<ResponseField name=\"language\" type=\"string\">\n  La langue détectée de l'audio d'entrée.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  La durée de l'audio d'entrée en secondes.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segments du texte traduit avec horodatages.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Traduction vs Transcription\n\n| Fonctionnalité | Traduction | Transcription |\n|---------|-------------|---------------|\n| Langue de sortie | Toujours anglais | Identique à l'entrée |\n| Cas d'utilisation | Convertir de l'audio étranger en anglais | Préserver la langue d'origine |\n| Paramètre de langue | Non applicable | Indice optionnel |\n\n<Note>\n  Le point de terminaison de traduction détecte automatiquement la langue source et traduit en anglais. Le paramètre `language` de la transcription est ignoré.\n</Note>",
      "es": "---\ntitle: \"Crear Traducción\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Traduce audio a texto en inglés\"\n---\n\n## Resumen\n\nTraduce audio en cualquier idioma compatible a texto en inglés. A diferencia de la transcripción, este endpoint siempre genera texto en inglés, independientemente del idioma de entrada.\n\n## Cuerpo de la Solicitud\n\n<ParamField body=\"file\" type=\"file\" required>\n  El archivo de audio a traducir. Formatos compatibles: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. El tamaño máximo de archivo es de 25 MB.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  El modelo a utilizar. Actualmente solo se admite `whisper-1`.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Un texto opcional para guiar el estilo del modelo o continuar un segmento anterior. Debe estar en inglés.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  El formato de la salida. Opciones: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  La temperatura de muestreo, entre 0 y 1. Valores más altos como 0.8 producen una salida más aleatoria, mientras que valores más bajos como 0.2 hacen que la salida sea más enfocada y determinista.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"text\" type=\"string\">\n  El texto traducido en inglés.\n</ResponseField>\n\nPara el formato `verbose_json`, la respuesta también incluye:\n\n<ResponseField name=\"language\" type=\"string\">\n  El idioma detectado del audio de entrada.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  La duración del audio de entrada en segundos.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmentos del texto traducido con marcas de tiempo.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Traducción vs Transcripción\n\n| Característica | Traducción | Transcripción |\n|---------|-------------|---------------|\n| Idioma de salida | Siempre inglés | Igual al de entrada |\n| Caso de uso | Convertir audio extranjero a inglés | Preservar el idioma original |\n| Parámetro de idioma | No aplicable | Sugerencia opcional |\n\n<Note>\n  El endpoint de traducción detecta automáticamente el idioma de origen y lo traduce al inglés. El parámetro `language` de la transcripción se ignora.\n</Note>",
      "pt": "---\ntitle: \"Criar Tradução\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Traduz áudio para texto em inglês\"\n---\n\n## Visão Geral\n\nTraduz áudio em qualquer idioma suportado para texto em inglês. Diferente da transcrição, este endpoint sempre gera texto em inglês, independentemente do idioma de entrada.\n\n## Corpo da Requisição\n\n<ParamField body=\"file\" type=\"file\" required>\n  O arquivo de áudio a ser traduzido. Formatos suportados: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. O tamanho máximo do arquivo é 25 MB.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  O modelo a ser utilizado. Atualmente, apenas `whisper-1` é suportado.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Um texto opcional para orientar o estilo do modelo ou continuar um segmento anterior. Deve estar em inglês.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  O formato da saída. Opções: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  A temperatura de amostragem, entre 0 e 1. Valores mais altos, como 0.8, produzem uma saída mais aleatória, enquanto valores mais baixos, como 0.2, tornam a saída mais focada e determinística.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"text\" type=\"string\">\n  O texto traduzido em inglês.\n</ResponseField>\n\nPara o formato `verbose_json`, a resposta também inclui:\n\n<ResponseField name=\"language\" type=\"string\">\n  O idioma detectado do áudio de entrada.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  A duração do áudio de entrada em segundos.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmentos do texto traduzido com timestamps.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Tradução vs Transcrição\n\n| Recurso | Tradução | Transcrição |\n|---------|-------------|---------------|\n| Idioma de saída | Sempre inglês | O mesmo da entrada |\n| Caso de uso | Converter áudio estrangeiro para inglês | Preservar o idioma original |\n| Parâmetro de idioma | Não aplicável | Dica opcional |\n\n<Note>\n  O endpoint de tradução detecta automaticamente o idioma de origem e traduz para o inglês. O parâmetro `language` da transcrição é ignorado.\n</Note>",
      "ar": "---\ntitle: \"إنشاء ترجمة\"\napi: \"POST /v1/audio/translations\"\ndescription: \"ترجمة الصوت إلى نص باللغة الإنجليزية\"\n---\n\n## نظرة عامة\n\nيقوم بترجمة الصوت من أي لغة مدعومة إلى نص باللغة الإنجليزية. على عكس عملية التفريغ الصوتي (transcription)، يقوم هذا المسار (endpoint) دائمًا بإخراج نص باللغة الإنجليزية بغض النظر عن لغة الإدخال.\n\n## جسم الطلب (Request Body)\n\n<ParamField body=\"file\" type=\"file\" required>\n  ملف الصوت المراد ترجمته. التنسيقات المدعومة: `flac`، `mp3`، `mp4`، `mpeg`، `mpga`، `m4a`، `ogg`، `wav`، `webm`. الحد الأقصى لحجم الملف هو 25 ميجابايت.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  النموذج المستخدم. حاليًا، يتم دعم `whisper-1` فقط.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  نص اختياري لتوجيه أسلوب النموذج أو إكمال مقطع سابق. يجب أن يكون باللغة الإنجليزية.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  تنسيق المخرجات. الخيارات: `json`، `text`، `srt`، `verbose_json`، `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  درجة حرارة العينات (sampling temperature)، بين 0 و 1. القيم الأعلى مثل 0.8 تؤدي إلى مخرجات أكثر عشوائية، بينما القيم الأقل مثل 0.2 تجعل المخرجات أكثر تركيزًا وحتمية.\n</ParamField>\n\n## الاستجابة (Response)\n\n<ResponseField name=\"text\" type=\"string\">\n  النص المترجم باللغة الإنجليزية.\n</ResponseField>\n\nبالنسبة لتنسيق `verbose_json`، تتضمن الاستجابة أيضًا:\n\n<ResponseField name=\"language\" type=\"string\">\n  اللغة المكتشفة للصوت المدخل.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  مدة الصوت المدخل بالثواني.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  مقاطع من النص المترجم مع طوابع زمنية.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## الترجمة مقابل التفريغ الصوتي (Translation vs Transcription)\n\n| الميزة | الترجمة (Translation) | التفريغ الصوتي (Transcription) |\n|---------|-------------|---------------|\n| لغة المخرجات | دائمًا الإنجليزية | نفس لغة الإدخال |\n| حالة الاستخدام | تحويل الصوت الأجنبي إلى الإنجليزية | الحفاظ على اللغة الأصلية |\n| معلمة اللغة (`language`) | غير قابلة للتطبيق | تلميح اختياري |\n\n<Note>\n  يقوم مسار الترجمة (translation endpoint) تلقائيًا باكتشاف لغة المصدر والترجمة إلى الإنجليزية. يتم تجاهل معلمة `language` المستخدمة في التفريغ الصوتي.\n</Note>",
      "vi": "---\ntitle: \"Tạo Bản dịch\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Dịch âm thanh sang văn bản tiếng Anh\"\n---\n\n## Tổng quan\n\nDịch âm thanh từ bất kỳ ngôn ngữ nào được hỗ trợ sang văn bản tiếng Anh. Khác với chuyển ký âm (transcription), endpoint này luôn xuất ra văn bản tiếng Anh bất kể ngôn ngữ đầu vào là gì.\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  Tệp âm thanh cần dịch. Các định dạng được hỗ trợ: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. Kích thước tệp tối đa là 25 MB.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Mô hình được sử dụng. Hiện tại chỉ hỗ trợ `whisper-1`.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Một đoạn văn bản tùy chọn để định hướng phong cách của mô hình hoặc tiếp tục một phân đoạn trước đó. Phải bằng tiếng Anh.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Định dạng của đầu ra. Các tùy chọn: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  Nhiệt độ lấy mẫu (sampling temperature), trong khoảng từ 0 đến 1. Các giá trị cao hơn như 0.8 sẽ tạo ra đầu ra ngẫu nhiên hơn, trong khi các giá trị thấp hơn như 0.2 sẽ làm cho đầu ra tập trung và xác định hơn.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"text\" type=\"string\">\n  Văn bản đã được dịch sang tiếng Anh.\n</ResponseField>\n\nĐối với định dạng `verbose_json`, phản hồi cũng bao gồm:\n\n<ResponseField name=\"language\" type=\"string\">\n  Ngôn ngữ được phát hiện của âm thanh đầu vào.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Thời lượng của âm thanh đầu vào tính bằng giây.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Các phân đoạn của văn bản đã dịch kèm theo dấu thời gian (timestamps).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Dịch thuật (Translation) so với Chuyển ký âm (Transcription)\n\n| Tính năng | Dịch thuật | Chuyển ký âm |\n|---------|-------------|---------------|\n| Ngôn ngữ đầu ra | Luôn là tiếng Anh | Giống với đầu vào |\n| Trường hợp sử dụng | Chuyển đổi âm thanh tiếng nước ngoài sang tiếng Anh | Giữ nguyên ngôn ngữ gốc |\n| Tham số ngôn ngữ | Không áp dụng | Gợi ý tùy chọn |\n\n<Note>\n  Endpoint dịch thuật tự động phát hiện ngôn ngữ nguồn và dịch sang tiếng Anh. Tham số `language` từ chuyển ký âm sẽ bị bỏ qua.\n</Note>",
      "id": "---\ntitle: \"Buat Terjemahan\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Menerjemahkan audio ke dalam teks bahasa Inggris\"\n---\n\n## Gambaran Umum\n\nMenerjemahkan audio dalam bahasa apa pun yang didukung ke dalam teks bahasa Inggris. Berbeda dengan transkripsi, endpoint ini selalu menghasilkan teks bahasa Inggris terlepas dari bahasa inputnya.\n\n## Request Body\n\n<ParamField body=\"file\" type=\"file\" required>\n  File audio yang akan diterjemahkan. Format yang didukung: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. Ukuran file maksimum adalah 25 MB.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Model yang akan digunakan. Saat ini hanya `whisper-1` yang didukung.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Teks opsional untuk memandu gaya model atau melanjutkan segmen sebelumnya. Harus dalam bahasa Inggris.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Format output. Opsi: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  Suhu sampling, antara 0 dan 1. Nilai yang lebih tinggi seperti 0,8 menghasilkan output yang lebih acak, sedangkan nilai yang lebih rendah seperti 0,2 membuat output lebih terfokus dan deterministik.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"text\" type=\"string\">\n  Teks hasil terjemahan dalam bahasa Inggris.\n</ResponseField>\n\nUntuk format `verbose_json`, respons juga menyertakan:\n\n<ResponseField name=\"language\" type=\"string\">\n  Bahasa yang terdeteksi dari audio input.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Durasi audio input dalam detik.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Segmen dari teks yang diterjemahkan dengan timestamp.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Terjemahan vs Transkripsi\n\n| Fitur | Terjemahan | Transkripsi |\n|---------|-------------|---------------|\n| Bahasa output | Selalu bahasa Inggris | Sama dengan input |\n| Kasus penggunaan | Mengonversi audio asing ke bahasa Inggris | Mempertahankan bahasa asli |\n| Parameter bahasa | Tidak berlaku | Petunjuk opsional |\n\n<Note>\n  Endpoint terjemahan secara otomatis mendeteksi bahasa sumber dan menerjemahkannya ke bahasa Inggris. Parameter `language` dari transkripsi akan diabaikan.\n</Note>",
      "tr": "---\ntitle: \"Çeviri Oluştur\"\napi: \"POST /v1/audio/translations\"\ndescription: \"Sesi İngilizce metne çevirir\"\n---\n\n## Genel Bakış\n\nDesteklenen herhangi bir dildeki sesi İngilizce metne çevirir. Transkripsiyonun aksine, bu uç nokta giriş dilinden bağımsız olarak her zaman İngilizce metin çıktısı verir.\n\n## İstek Gövdesi\n\n<ParamField body=\"file\" type=\"file\" required>\n  Çevrilecek ses dosyası. Desteklenen formatlar: `flac`, `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `ogg`, `wav`, `webm`. Maksimum dosya boyutu 25 MB'dır.\n</ParamField>\n\n<ParamField body=\"model\" type=\"string\" default=\"whisper-1\">\n  Kullanılacak model. Şu anda yalnızca `whisper-1` desteklenmektedir.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\">\n  Modelin stilini yönlendirmek veya önceki bir bölümü devam ettirmek için isteğe bağlı bir metin. İngilizce olmalıdır.\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"string\" default=\"json\">\n  Çıktı formatı. Seçenekler: `json`, `text`, `srt`, `verbose_json`, `vtt`.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\">\n  0 ile 1 arasında örnekleme sıcaklığı (temperature). 0.8 gibi daha yüksek değerler daha rastgele çıktılar üretirken, 0.2 gibi daha düşük değerler çıktıyı daha odaklı ve deterministik hale getirir.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"text\" type=\"string\">\n  İngilizceye çevrilmiş metin.\n</ResponseField>\n\n`verbose_json` formatı için yanıt şunları da içerir:\n\n<ResponseField name=\"language\" type=\"string\">\n  Giriş sesinin tespit edilen dili.\n</ResponseField>\n\n<ResponseField name=\"duration\" type=\"number\">\n  Giriş sesinin saniye cinsinden süresi.\n</ResponseField>\n\n<ResponseField name=\"segments\" type=\"array\">\n  Zaman damgalı çevrilmiş metin bölümleri.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/audio/translations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -F \"file=@german_audio.mp3\" \\\n  -F \"model=whisper-1\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nwith open(\"german_audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.translations.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\nimport fs from 'fs';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.audio.translations.create({\n  model: 'whisper-1',\n  file: fs.createReadStream('german_audio.mp3')\n});\n\nconsole.log(response.text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateTranslation(\n        context.Background(),\n        openai.AudioRequest{\n            Model:    openai.Whisper1,\n            FilePath: \"german_audio.mp3\",\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Text)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/audio/translations');\n\n$file = new CURLFile('german_audio.mp3', 'audio/mpeg', 'german_audio.mp3');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => [\n        'file' => $file,\n        'model' => 'whisper-1'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json json\n{\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\"\n}\n```\n\n```json verbose_json\n{\n  \"task\": \"translate\",\n  \"language\": \"german\",\n  \"duration\": 8.470000267028809,\n  \"text\": \"Hello, my name is Wolfgang and I come from Germany. Where are you from?\",\n  \"segments\": [\n    {\n      \"id\": 0,\n      \"seek\": 0,\n      \"start\": 0.0,\n      \"end\": 4.0,\n      \"text\": \" Hello, my name is Wolfgang and I come from Germany.\",\n      \"tokens\": [50364, 2425, 11, 452, 1315, 307, 25329, 293, 286, 808, 490, 5765, 13, 50564],\n      \"temperature\": 0.0,\n      \"avg_logprob\": -0.45,\n      \"compression_ratio\": 1.0,\n      \"no_speech_prob\": 0.0\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Çeviri vs Transkripsiyon\n\n| Özellik | Çeviri | Transkripsiyon |\n|---------|-------------|---------------|\n| Çıktı dili | Her zaman İngilizce | Girişle aynı |\n| Kullanım durumu | Yabancı dildeki sesi İngilizceye dönüştürme | Orijinal dili koruma |\n| Dil parametresi | Uygulanamaz | İsteğe bağlı ipucu |\n\n<Note>\n  Çeviri uç noktası kaynak dili otomatik olarak algılar ve İngilizceye çevirir. Transkripsiyondaki `language` parametresi yoksayılır.\n</Note>"
    },
    "updatedAt": "2026-01-26T05:20:51.145Z"
  },
  "api-reference/cache/clear-cache.mdx": {
    "sourceHash": "8679b52f1336df91",
    "translations": {
      "zh": "---\ntitle: \"清除缓存\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"清除当前用户或组织的语义缓存\"\n---\n\n## 概览\n\n清除与您的 API key 相关的所有缓存响应。这将同时影响响应缓存和语义缓存。\n\n<Note>\n  缓存是自动管理的。仅在需要强制获取新鲜响应时使用此端点。\n</Note>\n\n## 响应\n\n<ResponseField name=\"success\" type=\"boolean\">\n  缓存是否已成功清除。\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  已清除的缓存条目数量。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## 使用场景\n\n<AccordionGroup>\n  <Accordion title=\"测试新鲜响应\">\n    在开发过程中，清除缓存以确保您获取的是最新的 API 响应。\n  </Accordion>\n\n  <Accordion title=\"更新提示词后\">\n    如果您大幅更改了提示词，请清除缓存以避免陈旧的语义匹配。\n  </Accordion>\n\n  <Accordion title=\"调试\">\n    在调试意外响应时，清除缓存以排除缓存结果的影响。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"清除快取\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"清除目前使用者或組織的語義快取\"\n---\n\n## 概覽\n\n清除與您的 API key 相關的所有快取回應。這會同時影響回應快取（response cache）與語義快取（semantic cache）。\n\n<Note>\n  快取是自動管理的。僅在需要強制獲取全新回應時才使用此端點。\n</Note>\n\n## 回應\n\n<ResponseField name=\"success\" type=\"boolean\">\n  快取是否已成功清除。\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  已清除的快取項目數量。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## 使用場景\n\n<AccordionGroup>\n  <Accordion title=\"測試全新回應\">\n    在開發過程中，清除快取以確保您獲得的是最新的 API 回應。\n  </Accordion>\n\n  <Accordion title=\"更新提示詞（Prompts）後\">\n    如果您大幅更改了提示詞，請清除快取以避免過時的語義比對。\n  </Accordion>\n\n  <Accordion title=\"除錯（Debugging）\">\n    在針對非預期回應進行除錯時，清除快取以排除快取結果的干擾。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"キャッシュのクリア\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"現在のユーザーまたは組織のセマンティックキャッシュをクリアします\"\n---\n\n## 概要\n\nAPIキーに関連付けられたすべてのキャッシュ済みレスポンスをクリアします。これはレスポンスキャッシュとセマンティックキャッシュの両方に影響します。\n\n<Note>\n  キャッシュは自動的に管理されます。最新のレスポンスを強制的に取得する必要がある場合にのみ、このエンドポイントを使用してください。\n</Note>\n\n## レスポンス\n\n<ResponseField name=\"success\" type=\"boolean\">\n  キャッシュが正常にクリアされたかどうか。\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  クリアされたキャッシュエントリの数。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## ユースケース\n\n<AccordionGroup>\n  <Accordion title=\"最新レスポンスのテスト\">\n    開発中にキャッシュをクリアすることで、最新のAPIレスポンスを確実に取得できます。\n  </Accordion>\n\n  <Accordion title=\"プロンプト更新後\">\n    プロンプトを大幅に変更した場合は、古いセマンティックマッチを避けるためにキャッシュをクリアしてください。\n  </Accordion>\n\n  <Accordion title=\"デバッグ\">\n    予期しないレスポンスをデバッグする際、キャッシュされた結果を除外するためにキャッシュをクリアします。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"캐시 삭제\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"현재 사용자 또는 조직의 시맨틱 캐시를 삭제합니다.\"\n---\n\n## 개요\n\n사용자의 API key에 대한 모든 캐시된 응답을 삭제합니다. 이는 응답 캐시와 시맨틱 캐시 모두에 영향을 미칩니다.\n\n<Note>\n  캐시는 자동으로 관리됩니다. 새로운 응답을 강제로 받아야 하는 경우에만 이 엔드포인트를 사용하세요.\n</Note>\n\n## 응답\n\n<ResponseField name=\"success\" type=\"boolean\">\n  캐시가 성공적으로 삭제되었는지 여부입니다.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  삭제된 캐시 항목의 수입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## 활용 사례\n\n<AccordionGroup>\n  <Accordion title=\"새로운 응답 테스트\">\n    개발 중에 새로운 API 응답을 받고 있는지 확인하기 위해 캐시를 삭제합니다.\n  </Accordion>\n\n  <Accordion title=\"프롬프트 업데이트 후\">\n    프롬프트를 크게 변경한 경우, 오래된 시맨틱 매칭을 방지하기 위해 캐시를 삭제합니다.\n  </Accordion>\n\n  <Accordion title=\"디버깅\">\n    예기치 않은 응답을 디버깅할 때, 캐시된 결과의 가능성을 배제하기 위해 캐시를 삭제합니다.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Cache leeren\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Leert den semantischen Cache für den aktuellen Benutzer oder die Organisation\"\n---\n\n## Übersicht\n\nLeert alle zwischengespeicherten Antworten für Ihren API-Key. Dies betrifft sowohl den Response-Cache als auch den semantischen Cache.\n\n<Note>\n  Der Cache wird automatisch verwaltet. Verwenden Sie diesen Endpunkt nur, wenn Sie aktuelle Antworten erzwingen müssen.\n</Note>\n\n## Antwort\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Gibt an, ob der Cache erfolgreich geleert wurde.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Anzahl der geleerten Cache-Einträge.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n",
      "fr": "---\ntitle: \"Vider le cache\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Vide le cache sémantique pour l'utilisateur ou l'organisation actuelle\"\n---\n\n## Aperçu\n\nEfface toutes les réponses mises en cache pour votre clé API. Cela affecte à la fois le cache de réponse et le cache sémantique.\n\n<Note>\n  Le cache est géré automatiquement. Utilisez cet endpoint uniquement lorsque vous avez besoin de forcer des réponses fraîches.\n</Note>\n\n## Réponse\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Indique si le cache a été vidé avec succès.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Nombre d'entrées de cache effacées.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## Cas d'utilisation\n\n<AccordionGroup>\n  <Accordion title=\"Tester des réponses fraîches\">\n    Pendant le développement, videz le cache pour vous assurer d'obtenir des réponses API fraîches.\n  </Accordion>\n\n  <Accordion title=\"Après la mise à jour des prompts\">\n    Si vous avez modifié vos prompts de manière significative, videz le cache pour éviter les correspondances sémantiques obsolètes.\n  </Accordion>\n\n  <Accordion title=\"Débogage\">\n    Lors du débogage de réponses inattendues, videz le cache pour écarter les résultats mis en cache.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Limpiar Caché\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Limpia la caché semántica para el usuario u organización actual\"\n---\n\n## Descripción general\n\nLimpia todas las respuestas almacenadas en caché para su clave API. Esto afecta tanto a la caché de respuestas como a la caché semántica.\n\n<Note>\n  La caché se gestiona automáticamente. Utilice este endpoint solo cuando necesite forzar respuestas actualizadas.\n</Note>\n\n## Respuesta\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Indica si la caché se limpió correctamente.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Número de entradas de caché eliminadas.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## Casos de uso\n\n<AccordionGroup>\n  <Accordion title=\"Probar respuestas actualizadas\">\n    Durante el desarrollo, limpie la caché para asegurarse de obtener respuestas de la API actualizadas.\n  </Accordion>\n\n  <Accordion title=\"Después de actualizar prompts\">\n    Si ha cambiado significativamente sus prompts, limpie la caché para evitar coincidencias semánticas obsoletas.\n  </Accordion>\n\n  <Accordion title=\"Depuración\">\n    Al depurar respuestas inesperadas, limpie la caché para descartar resultados almacenados en caché.\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Limpar Cache\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Limpa o cache semântico para o usuário ou organização atual\"\n---\n\n## Visão Geral\n\nLimpa todas as respostas em cache para sua chave de API. Isso afeta tanto o cache de resposta quanto o cache semântico.\n\n<Note>\n  O cache é gerenciado automaticamente. Use este endpoint apenas quando precisar forçar respostas novas.\n</Note>\n\n## Resposta\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Indica se o cache foi limpo com sucesso.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Número de entradas de cache limpas.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## Casos de Uso\n\n<AccordionGroup>\n  <Accordion title=\"Testando novas respostas\">\n    Durante o desenvolvimento, limpe o cache para garantir que você está recebendo respostas de API novas.\n  </Accordion>\n\n  <Accordion title=\"Após atualizar prompts\">\n    Se você alterou significativamente seus prompts, limpe o cache para evitar correspondências semânticas obsoletas.\n  </Accordion>\n\n  <Accordion title=\"Depuração\">\n    Ao depurar respostas inesperadas, limpe o cache para descartar resultados em cache.\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"مسح ذاكرة التخزين المؤقت (Cache)\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"يمسح ذاكرة التخزين المؤقت الدلالية (semantic cache) للمستخدم الحالي أو المؤسسة\"\n---\n\n## نظرة عامة\n\nيمسح جميع الاستجابات المخزنة مؤقتاً لمفتاح الـ API الخاص بك. يؤثر هذا على كل من ذاكرة تخزين الاستجابات (response cache) وذاكرة التخزين الدلالية (semantic cache).\n\n<Note>\n  تتم إدارة الـ Cache تلقائياً. استخدم نقطة النهاية (endpoint) هذه فقط عندما تحتاج إلى فرض الحصول على استجابات جديدة.\n</Note>\n\n## الاستجابة\n\n<ResponseField name=\"success\" type=\"boolean\">\n  ما إذا كان قد تم مسح ذاكرة التخزين المؤقت بنجاح.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  عدد مدخلات ذاكرة التخزين المؤقت التي تم مسحها.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## حالات الاستخدام\n\n<AccordionGroup>\n  <Accordion title=\"اختبار الاستجابات الجديدة\">\n    أثناء التطوير، قم بمسح الـ cache للتأكد من حصولك على استجابات API جديدة.\n  </Accordion>\n\n  <Accordion title=\"بعد تحديث الـ prompts\">\n    إذا قمت بتغيير الـ prompts بشكل كبير، فقم بمسح الـ cache لتجنب المطابقات الدلالية القديمة.\n  </Accordion>\n\n  <Accordion title=\"تصحيح الأخطاء (Debugging)\">\n    عند تصحيح أخطاء الاستجابات غير المتوقعة، قم بمسح الـ cache لاستبعاد النتائج المخزنة مؤقتاً.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Xóa Cache\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Xóa semantic cache cho người dùng hoặc tổ chức hiện tại\"\n---\n\n## Tổng quan\n\nXóa tất cả các phản hồi đã lưu trong bộ nhớ đệm (cache) cho API key của bạn. Điều này ảnh hưởng đến cả response cache và semantic cache.\n\n<Note>\n  Cache được quản lý tự động. Chỉ sử dụng endpoint này khi bạn cần bắt buộc nhận các phản hồi mới.\n</Note>\n\n## Phản hồi\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Cho biết cache đã được xóa thành công hay chưa.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Số lượng mục cache đã được xóa.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## Các trường hợp sử dụng\n\n<AccordionGroup>\n  <Accordion title=\"Kiểm tra các phản hồi mới\">\n    Trong quá trình phát triển, hãy xóa cache để đảm bảo bạn nhận được các phản hồi API mới nhất.\n  </Accordion>\n\n  <Accordion title=\"Sau khi cập nhật prompt\">\n    Nếu bạn đã thay đổi đáng kể các prompt của mình, hãy xóa cache để tránh các kết quả khớp semantic cũ.\n  </Accordion>\n\n  <Accordion title=\"Gỡ lỗi (Debugging)\">\n    Khi gỡ lỗi các phản hồi không mong muốn, hãy xóa cache để loại trừ khả năng do kết quả đã lưu trong bộ nhớ đệm.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Hapus Cache\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Menghapus cache semantik untuk pengguna atau organisasi saat ini\"\n---\n\n## Gambaran Umum\n\nMenghapus semua respons yang tersimpan di cache untuk API key Anda. Ini memengaruhi cache respons dan cache semantik.\n\n<Note>\n  Cache dikelola secara otomatis. Gunakan endpoint ini hanya saat Anda perlu memaksa respons baru.\n</Note>\n\n## Respons\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Apakah cache berhasil dihapus.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Jumlah entri cache yang dihapus.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## Kasus Penggunaan\n\n<AccordionGroup>\n  <Accordion title=\"Menguji respons baru\">\n    Selama pengembangan, hapus cache untuk memastikan Anda mendapatkan respons API yang baru.\n  </Accordion>\n\n  <Accordion title=\"Setelah memperbarui prompt\">\n    Jika Anda telah mengubah prompt secara signifikan, hapus cache untuk menghindari kecocokan semantik yang usang.\n  </Accordion>\n\n  <Accordion title=\"Debugging\">\n    Saat melakukan debugging pada respons yang tidak terduga, hapus cache untuk menyingkirkan hasil yang tersimpan di cache.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"Cache'i Temizle\"\napi: \"DELETE /v1/cache/clear\"\ndescription: \"Mevcut kullanıcı veya organizasyon için semantik cache'i temizler\"\n---\n\n## Genel Bakış\n\nAPI anahtarınız için tüm cache'lenmiş yanıtları temizler. Bu, hem yanıt cache'ini hem de semantik cache'i etkiler.\n\n<Note>\n  Cache otomatik olarak yönetilir. Bu endpoint'i yalnızca yeni yanıtları zorlamanız gerektiğinde kullanın.\n</Note>\n\n## Yanıt\n\n<ResponseField name=\"success\" type=\"boolean\">\n  Cache'in başarıyla temizlenip temizlenmediği.\n</ResponseField>\n\n<ResponseField name=\"cleared\" type=\"integer\">\n  Temizlenen cache girişi sayısı.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X DELETE \"https://api.lemondata.cc/v1/cache/clear\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.delete(\n    \"https://api.lemondata.cc/v1/cache/clear\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/cache/clear',\n  {\n    method: 'DELETE',\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"DELETE\", \"https://api.lemondata.cc/v1/cache/clear\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/cache/clear');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_CUSTOMREQUEST => 'DELETE',\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"success\": true,\n  \"cleared\": 42\n}\n```\n</ResponseExample>\n\n## Kullanım Durumları\n\n<AccordionGroup>\n  <Accordion title=\"Yeni yanıtları test etme\">\n    Geliştirme sırasında, yeni API yanıtları aldığınızdan emin olmak için cache'i temizleyin.\n  </Accordion>\n\n  <Accordion title=\"Prompt'ları güncelledikten sonra\">\n    Prompt'larınızı önemli ölçüde değiştirdiyseniz, eski semantik eşleşmeleri önlemek için cache'i temizleyin.\n  </Accordion>\n\n  <Accordion title=\"Hata Ayıklama\">\n    Beklenmedik yanıtları ayıklarken, cache'lenmiş sonuçları elemek için cache'i temizleyin.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:21:11.392Z"
  },
  "api-reference/chat/create-completion.mdx": {
    "sourceHash": "c3cc332a98b49c5c",
    "translations": {
      "zh": "---\ntitle: \"创建对话补全 (Chat Completion)\"\napi: \"POST /v1/chat/completions\"\ndescription: \"为对话消息创建补全\"\n---\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的模型 ID。请参阅 [Models](https://lemondata.cc/zh/models) 以获取可用选项。\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  包含对话的消息列表。\n\n  每个消息对象包含：\n  - `role` (string): `system`, `user`, 或 `assistant`\n  - `content` (string | array): 消息内容\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  介于 0 到 2 之间的采样温度。较高的值会使输出更具随机性。\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  要生成的最大 token 数量。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  如果为 true，部分消息增量将作为 SSE 事件发送。\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  流式传输选项。设置 `include_usage: true` 以在流数据块中接收 token 使用情况。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  核采样参数。我们建议修改此参数或 `temperature`，不要同时修改两者。\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  介于 -2.0 到 2.0 之间的数值。正值会惩罚重复的 token。\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  介于 -2.0 到 2.0 之间的数值。正值会惩罚文本中已出现的 token。\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  API 将停止生成 token 的最多 4 个序列。\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  模型可能调用的工具列表（函数调用）。\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  控制模型如何使用工具。选项：`auto`, `none`, `required`, 或特定的工具对象。\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  是否启用并行函数调用。设置为 false 以按顺序调用函数。\n</ParamField>\n\n<ParamField body=\"max_completion_tokens\" type=\"integer\">\n  补全的最大 token 数量。`max_tokens` 的替代方案，更适用于 `o1`/`o3` 等较新模型。\n</ParamField>\n\n<ParamField body=\"reasoning_effort\" type=\"string\">\n  `o1`/`o3` 模型的推理力度。选项：`low`, `medium`, `high`。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  用于确定性采样的随机种子。\n</ParamField>\n\n<ParamField body=\"n\" type=\"integer\" default=\"1\">\n  要生成的补全数量 (1-128)。\n</ParamField>\n\n<ParamField body=\"logprobs\" type=\"boolean\">\n  是否返回对数概率。\n</ParamField>\n\n<ParamField body=\"top_logprobs\" type=\"integer\">\n  要返回的前几个对数概率的数量 (0-20)。需要设置 `logprobs: true`。\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Top-K 采样参数（适用于 Anthropic/Gemini 模型）。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"object\">\n  响应格式规范。使用 `{\"type\": \"json_object\"}` 开启 JSON 模式，或使用 `{\"type\": \"json_schema\", \"json_schema\": {...}}` 获取结构化输出。\n</ParamField>\n\n<ParamField body=\"logit_bias\" type=\"object\">\n  修改指定 token 出现的可能性。将 token ID（作为字符串）映射到 -100 到 100 之间的偏置值。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  代表终端用户的唯一标识符，用于滥用监控。\n</ParamField>\n\n<ParamField body=\"cache_control\" type=\"object\">\n  LemonData 缓存控制选项。\n\n  - `type` (string): 缓存策略 - `default`, `no_cache`, `no_store`, `response_only`, `semantic_only`\n  - `max_age` (integer): 缓存 TTL（以秒为单位，最大 86400）\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  补全的唯一标识符。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  始终为 `chat.completion`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  补全创建时的 Unix 时间戳。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  用于补全的模型。\n</ResponseField>\n\n<ResponseField name=\"choices\" type=\"array\">\n  补全选项列表。\n\n  每个选项包含：\n  - `index` (integer): 选项的索引\n  - `message` (object): 生成的消息\n  - `finish_reason` (string): 模型停止的原因 (`stop`, `length`, `tool_calls`)\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token 使用统计。\n\n  - `prompt_tokens` (integer): 提示词中的 token 数量\n  - `completion_tokens` (integer): 补全中的 token 数量\n  - `total_tokens` (integer): 使用的总 token 数量\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/chat/completions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Hello!' }\n  ],\n  temperature: 0.7,\n  max_tokens: 1000\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n            Temperature: 0.7,\n            MaxTokens:   1000,\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'Hello!']\n        ],\n        'temperature' => 0.7,\n        'max_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 20,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 29\n  }\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"建立對話補全 (Create Chat Completion)\"\napi: \"POST /v1/chat/completions\"\ndescription: \"為對話訊息建立補全\"\n---\n\n## 請求主體 (Request Body)\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的模型 ID。請參閱 [Models](https://lemondata.cc/zh-TW/models) 以查看可用選項。\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  組成對話的訊息列表。\n\n  每個訊息物件包含：\n  - `role` (string)：`system`、`user` 或 `assistant`\n  - `content` (string | array)：訊息內容\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  介於 0 到 2 之間的取樣溫度。較高的數值會使輸出更具隨機性。\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  要生成的最大 token 數量。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  若為 true，部分訊息增量將以 SSE 事件形式發送。\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  串流選項。設置 `include_usage: true` 以在串流區塊中接收 token 使用情況。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  核取樣 (Nucleus sampling) 參數。我們建議調整此參數或 temperature，而非兩者同時調整。\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  介於 -2.0 到 2.0 之間的數值。正值會懲罰重複的 token。\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  介於 -2.0 到 2.0 之間的數值。正值會懲罰已在文本中出現過的 token。\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  最多 4 個序列，API 將在這些序列處停止生成 token。\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  模型可能呼叫的工具列表 (function calling)。\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  控制模型如何使用工具。選項：`auto`、`none`、`required` 或特定的工具物件。\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  是否啟用並行函式呼叫。設置為 false 則按順序呼叫函式。\n</ParamField>\n\n<ParamField body=\"max_completion_tokens\" type=\"integer\">\n  補全的最大 token 數量。`max_tokens` 的替代方案，建議用於 o1/o3 等較新的模型。\n</ParamField>\n\n<ParamField body=\"reasoning_effort\" type=\"string\">\n  o1/o3 模型的推理強度。選項：`low`、`medium`、`high`。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  用於確定性取樣的隨機種子。\n</ParamField>\n\n<ParamField body=\"n\" type=\"integer\" default=\"1\">\n  要生成的補全數量 (1-128)。\n</ParamField>\n\n<ParamField body=\"logprobs\" type=\"boolean\">\n  是否返回對數機率 (log probabilities)。\n</ParamField>\n\n<ParamField body=\"top_logprobs\" type=\"integer\">\n  要返回的前幾個對數機率數量 (0-20)。需要設置 `logprobs: true`。\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Top-K 取樣參數（適用於 Anthropic/Gemini 模型）。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"object\">\n  回應格式規範。使用 `{\"type\": \"json_object\"}` 開啟 JSON 模式，或使用 `{\"type\": \"json_schema\", \"json_schema\": {...}}` 進行結構化輸出。\n</ParamField>\n\n<ParamField body=\"logit_bias\" type=\"object\">\n  修改指定 token 出現的可能性。將 token ID（字串形式）映射到 -100 到 100 之間的偏差值。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  代表您終端用戶的唯一識別碼，用於濫用監控。\n</ParamField>\n\n<ParamField body=\"cache_control\" type=\"object\">\n  LemonData 快取控制選項。\n\n  - `type` (string)：快取策略 - `default`、`no_cache`、`no_store`、`response_only`、`semantic_only`\n  - `max_age` (integer)：快取存活時間 (TTL)，以秒為單位（最大 86400）\n</ParamField>\n\n## 回應 (Response)\n\n<ResponseField name=\"id\" type=\"string\">\n  補全的唯一識別碼。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  始終為 `chat.completion`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  補全建立時的 Unix 時間戳記。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  用於補全的模型。\n</ResponseField>\n\n<ResponseField name=\"choices\" type=\"array\">\n  補全選項列表。\n\n  每個選項包含：\n  - `index` (integer)：選項索引\n  - `message` (object)：生成的訊息\n  - `finish_reason` (string)：模型停止的原因 (`stop`、`length`、`tool_calls`)\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token 使用統計。\n\n  - `prompt_tokens` (integer)：提示詞中的 token 數量\n  - `completion_tokens` (integer)：補全中的 token 數量\n  - `total_tokens` (integer)：使用的總 token 數量\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/chat/completions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Hello!' }\n  ],\n  temperature: 0.7,\n  max_tokens: 1000\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n            Temperature: 0.7,\n            MaxTokens:   1000,\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'Hello!']\n        ],\n        'temperature' => 0.7,\n        'max_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 20,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 29\n  }\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"チャット補完の作成\"\napi: \"POST /v1/chat/completions\"\ndescription: \"チャットメッセージに対する補完を作成します\"\n---\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" required>\n  使用するモデルの ID。利用可能なオプションについては [Models](https://lemondata.cc/ja/models) を参照してください。\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  会話を構成するメッセージのリスト。\n\n  各メッセージオブジェクトには以下が含まれます：\n  - `role` (string): `system`、`user`、または `assistant`\n  - `content` (string | array): メッセージの内容\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  0 から 2 の間のサンプリング温度。値が高いほど出力がランダムになります。\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  生成するトークンの最大数。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  true の場合、メッセージの部分的な差分が SSE イベントとして送信されます。\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  ストリーミングのオプション。ストリームチャンクでトークン使用量を受け取るには `include_usage: true` を設定します。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  核サンプリング（Nucleus sampling）パラメータ。これまたは temperature のいずれか一方のみを変更することをお勧めします。\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  -2.0 から 2.0 の間の数値。正の値は、繰り返されるトークンにペナルティを課します。\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  -2.0 から 2.0 の間の数値。正の値は、既にテキスト内に存在するトークンにペナルティを課します。\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  API がトークンの生成を停止する最大 4 つのシーケンス。\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  モデルが呼び出す可能性のあるツールのリスト（function calling）。\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  モデルがツールをどのように使用するかを制御します。オプション：`auto`、`none`、`required`、または特定のツールオブジェクト。\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  並列関数呼び出しを有効にするかどうか。関数を順次呼び出す場合は false に設定します。\n</ParamField>\n\n<ParamField body=\"max_completion_tokens\" type=\"integer\">\n  補完の最大トークン数。max_tokens の代替であり、o1/o3 などの新しいモデルで推奨されます。\n</ParamField>\n\n<ParamField body=\"reasoning_effort\" type=\"string\">\n  o1/o3 モデルの推論の取り組み（Reasoning effort）。オプション：`low`、`medium`、`high`。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  決定論的なサンプリングのためのランダムシード。\n</ParamField>\n\n<ParamField body=\"n\" type=\"integer\" default=\"1\">\n  生成する補完の数（1-128）。\n</ParamField>\n\n<ParamField body=\"logprobs\" type=\"boolean\">\n  対数確率を返すかどうか。\n</ParamField>\n\n<ParamField body=\"top_logprobs\" type=\"integer\">\n  返される上位の対数確率の数（0-20）。`logprobs: true` が必要です。\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Top-K サンプリングパラメータ（Anthropic/Gemini モデル用）。\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"object\">\n  レスポンス形式の指定。JSON モードの場合は `{\"type\": \"json_object\"}` を、構造化出力の場合は `{\"type\": \"json_schema\", \"json_schema\": {...}}` を使用します。\n</ParamField>\n\n<ParamField body=\"logit_bias\" type=\"object\">\n  特定のトークンが出現する可能性を変更します。トークン ID（文字列として）を -100 から 100 のバイアス値にマッピングします。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  不正利用監視のためにエンドユーザーを表す一意の識別子。\n</ParamField>\n\n<ParamField body=\"cache_control\" type=\"object\">\n  LemonData のキャッシュ制御オプション。\n\n  - `type` (string): キャッシュ戦略 - `default`、`no_cache`、`no_store`、`response_only`、`semantic_only`\n  - `max_age` (integer): 秒単位のキャッシュ TTL（最大 86400）\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  補完の一意の識別子。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  常に `chat.completion`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  補完が作成された時の Unix タイムスタンプ。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  補完に使用されたモデル。\n</ResponseField>\n\n<ResponseField name=\"choices\" type=\"array\">\n  補完の選択肢のリスト。\n\n  各選択肢には以下が含まれます：\n  - `index` (integer): 選択肢のインデックス\n  - `message` (object): 生成されたメッセージ\n  - `finish_reason` (string): モデルが停止した理由（`stop`、`length`、`tool_calls`）\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  トークン使用統計。\n\n  - `prompt_tokens` (integer): プロンプト内のトークン数\n  - `completion_tokens` (integer): 補完内のトークン数\n  - `total_tokens` (integer): 使用された合計トークン数\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/chat/completions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Hello!' }\n  ],\n  temperature: 0.7,\n  max_tokens: 1000\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n            Temperature: 0.7,\n            MaxTokens:   1000,\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'Hello!']\n        ],\n        'temperature' => 0.7,\n        'max_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 20,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 29\n  }\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"Chat Completion 생성\"\napi: \"POST /v1/chat/completions\"\ndescription: \"채팅 메시지에 대한 응답(completion)을 생성합니다.\"\n---\n\n## 요청 본문 (Request Body)\n\n<ParamField body=\"model\" type=\"string\" required>\n  사용할 모델의 ID입니다. 사용 가능한 옵션은 [Models](https://lemondata.cc/ko/models)를 참조하세요.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  대화를 구성하는 메시지 목록입니다.\n\n  각 메시지 객체는 다음을 포함합니다:\n  - `role` (string): `system`, `user` 또는 `assistant`\n  - `content` (string | array): 메시지 내용\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  0에서 2 사이의 샘플링 온도입니다. 값이 높을수록 출력이 더 무작위해집니다.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  생성할 최대 토큰 수입니다.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  true인 경우, 메시지의 일부 증분(deltas)이 SSE 이벤트로 전송됩니다.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  스트리밍 옵션입니다. 스트림 청크에서 토큰 사용량을 받으려면 `include_usage: true`로 설정하세요.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Nucleus 샘플링 파라미터입니다. 이 값 또는 temperature 중 하나만 변경하는 것을 권장합니다.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  -2.0에서 2.0 사이의 숫자입니다. 양수 값은 반복되는 토큰에 페널티를 부여합니다.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  -2.0에서 2.0 사이의 숫자입니다. 양수 값은 이미 텍스트에 존재하는 토큰에 페널티를 부여합니다.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  API가 토큰 생성을 중단할 최대 4개의 시퀀스입니다.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  모델이 호출할 수 있는 도구 목록입니다 (function calling).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  모델의 도구 사용 방식을 제어합니다. 옵션: `auto`, `none`, `required` 또는 특정 도구 객체.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\"",
      "de": "---\ntitle: \"Chat-Completion erstellen\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Erstellt eine Vervollständigung für die Chat-Nachricht\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID des zu verwendenden Modells. Siehe [Models](https://lemondata.cc/de/models) für verfügbare Optionen.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Eine Liste von Nachrichten, die die Konversation bilden.\n\n  Jedes Nachrichtenobjekt enthält:\n  - `role` (string): `system`, `user` oder `assistant`\n  - `content` (string | array): Der Inhalt der Nachricht\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Sampling-Temperatur zwischen 0 und 2. Höhere Werte machen die Ausgabe zufälliger.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Maximale Anzahl der zu generierenden Tokens.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Wenn true, werden Teilnachrichten-Deltas als SSE-Events gesendet.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Optionen für das Streaming. Setzen Sie `include_usage: true`, um die Token-Nutzung in Stream-Chunks zu erhalten.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Nucleus-Sampling-Parameter. Wir empfehlen, entweder diesen oder die Temperatur zu ändern, aber nicht beide.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  Zahl zwischen -2,0 und 2,0. Positive Werte bestrafen wiederholte Tokens.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  Zahl zwischen -2,0 und 2,0. Positive Werte bestrafen Tokens, die bereits im Text vorhanden sind.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  Bis zu 4 Sequenzen, bei denen die API die Generierung von Tokens stoppt.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Eine Liste von Tools, die das Modell aufrufen kann (Function Calling).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Steuert, wie das Modell Tools verwendet. Optionen: `auto`, `none`, `required` oder ein spezifisches Tool-Objekt.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Gibt an, ob paralleles Function Calling aktiviert werden soll. Auf false setzen,",
      "fr": "---\ntitle: \"Créer une complétion de chat\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Crée une complétion pour le message de chat\"\n---\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID du modèle à utiliser. Voir [Modèles](https://lemondata.cc/fr/models) pour les options disponibles.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Une liste de messages composant la conversation.\n\n  Chaque objet message contient :\n  - `role` (string) : `system`, `user`, ou `assistant`\n  - `content` (string | array) : Le contenu du message\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Température d'échantillonnage entre 0 et 2. Des valeurs plus élevées rendent la sortie plus aléatoire.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Nombre maximum de tokens à générer.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Si vrai, des deltas de messages partiels seront envoyés sous forme d'événements SSE.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Options pour le streaming. Définissez `include_usage: true` pour recevoir l'utilisation des tokens dans les morceaux du flux.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Paramètre d'échantillonnage nucleus. Nous recommandons de modifier ceci ou la température, mais pas les deux.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  Nombre entre -2,0 et 2,0. Les valeurs positives pénalisent les tokens répétés.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  Nombre entre -2,0 et 2,0. Les valeurs positives pénalisent les tokens déjà présents dans le texte.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  Jusqu'à 4 séquences où l'API arrêtera de générer des tokens.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Une liste d'outils que le modèle peut appeler (appel de fonction).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Contrôle la manière dont le modèle utilise les outils. Options : `auto`, `none`, `required`, ou un objet outil spécifique.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Indique s'il faut activer l'",
      "es": "---\ntitle: \"Crear Chat Completion\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Crea una completación para el mensaje de chat\"\n---\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID del modelo a utilizar. Consulta [Models](https://lemondata.cc/es/models) para ver las opciones disponibles.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Una lista de mensajes que componen la conversación.\n\n  Cada objeto de mensaje contiene:\n  - `role` (string): `system`, `user`, o `assistant`\n  - `content` (string | array): El contenido del mensaje\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Temperatura de muestreo entre 0 y 2. Los valores más altos hacen que la salida sea más aleatoria.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Número máximo de tokens a generar.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Si es true, se enviarán deltas de mensajes parciales como eventos SSE.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Opciones para streaming. Establece `include_usage: true` para recibir el uso de tokens en fragmentos de stream.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Parámetro de muestreo de núcleo. Recomendamos alterar este o la temperatura, pero no ambos.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  Número entre -2.0 y 2.0. Los valores positivos penalizan los tokens repetidos.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  Número entre -2.0 y 2.0. Los valores positivos penalizan los tokens que ya están en el texto.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  Hasta 4 secuencias donde la API dejará de generar tokens.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Una lista de herramientas que el modelo puede llamar (llamada a funciones).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Controla cómo el modelo utiliza las herramientas. Opciones: `auto`, `none`, `required`, o un objeto de herramienta específico.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Indica si se debe habilitar la llamada a funciones en paralelo. Establécelo en false para llamar a las funciones de forma secuencial.\n</ParamField>\n\n<Param",
      "pt": "---\ntitle: \"Criar Chat Completion\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Cria uma conclusão para a mensagem de chat\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID do modelo a ser usado. Veja [Models](https://lemondata.cc/pt/models) para as opções disponíveis.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Uma lista de mensagens que compõem a conversa.\n\n  Cada objeto de mensagem contém:\n  - `role` (string): `system`, `user`, ou `assistant`\n  - `content` (string | array): O conteúdo da mensagem\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Temperatura de amostragem entre 0 e 2. Valores mais altos tornam a saída mais aleatória.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Número máximo de tokens a serem gerados.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Se verdadeiro, deltas parciais de mensagens serão enviados como eventos SSE.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Opções para streaming. Defina `include_usage: true` para receber o uso de tokens em partes (chunks) do stream.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Parâmetro de amostragem de núcleo (nucleus sampling). Recomendamos alterar este ou a temperatura, mas não ambos.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  Número entre -2.0 e 2.0. Valores positivos penalizam tokens repetidos.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  Número entre -2.0 e 2.0. Valores positivos penalizam tokens que já aparecem no texto.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  Até 4 sequências onde a API interromperá a geração de tokens.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Uma lista de ferramentas que o modelo pode chamar (chamada de função).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Controla como o modelo utiliza as ferramentas. Opções: `auto`, `none`, `required`, ou um objeto de ferramenta específico.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Se deve habilitar chamadas de função paralelas. Defina como falso para chamar funções sequencialmente.\n</ParamField>\n\n<ParamField body=\"max_completion_tokens\" type=\"integer\">\n  Máximo de tokens para a conclusão. Alternativa ao max_tokens, preferido para modelos mais recentes como o1/o3.\n</ParamField>\n\n<ParamField body=\"reasoning_effort\" type=\"string\">\n  Esforço de raciocínio para modelos o1/o3. Opções: `low`, `medium`, `high`.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Semente aleatória para amostragem determinística.\n</ParamField>\n\n<ParamField body=\"n\" type=\"integer\" default=\"1\">\n  Número de conclusões a serem geradas (1-128).\n</ParamField>\n\n<ParamField body=\"logprobs\" type=\"boolean\">\n  Se deve retornar probabilidades de log (log probabilities).\n</ParamField>\n\n<ParamField body=\"top_logprobs\" type=\"integer\">\n  Número de probabilidades de log principais a serem retornadas (0-20). Requer `logprobs: true`.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Parâmetro de amostragem Top-K (para modelos Anthropic/Gemini).\n</ParamField>\n\n<ParamField body=\"response_format\" type=\"object\">\n  Especificação do formato de resposta. Use `{\"type\": \"json_object\"}` para o modo JSON, ou `{\"type\": \"json_schema\", \"json_schema\": {...}}` para saídas estruturadas.\n</ParamField>\n\n<ParamField body=\"logit_bias\" type=\"object\">\n  Modifica a probabilidade de tokens específicos aparecerem. Mapeia IDs de tokens (como strings) para valores de viés de -100 a 100.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Um identificador único que representa seu usuário final para monitoramento de abusos.\n</ParamField>\n\n<ParamField body=\"cache_control\" type=\"object\">\n  Opções de controle de cache da LemonData.\n\n  - `type` (string): Estratégia de cache - `default`, `no_cache`, `no_store`, `response_only`, `semantic_only`\n  - `max_age` (integer): TTL do cache em segundos (máximo 86400)\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  Identificador único para a conclusão.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Sempre `chat.completion`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Timestamp Unix de quando a conclusão foi criada.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  O modelo usado para a conclusão.\n</ResponseField>\n\n<ResponseField name=\"choices\" type=\"array\">\n  Lista de opções de conclusão.\n\n  Cada opção contém:\n  - `index` (integer): Índice da opção\n  - `message` (object): A mensagem gerada\n  - `finish_reason` (string): O motivo pelo qual o modelo parou (`stop`, `length`, `tool_calls`)\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Estatísticas de uso de tokens.\n\n  - `prompt_tokens` (integer): Tokens no prompt\n  - `completion_tokens` (integer): Tokens na conclusão\n  - `total_tokens` (integer): Total de tokens usados\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/chat/completions\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'Hello!' }\n  ],\n  temperature: 0.7,\n  max_tokens: 1000\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n            Temperature: 0.7,\n            MaxTokens:   1000,\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'Hello!']\n        ],\n        'temperature' => 0.7,\n        'max_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 20,\n    \"completion_tokens\": 9,\n    \"total_tokens\": 29\n  }\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"إنشاء إكمال للدردشة\"\napi: \"POST /v1/chat/completions\"\ndescription: \"يقوم بإنشاء إكمال لرسالة الدردشة\"\n---\n\n## جسم الطلب (Request Body)\n\n<ParamField body=\"model\" type=\"string\" required>\n  معرف (ID) النموذج المراد استخدامه. راجع [Models](https://lemondata.cc/ar/models) للخيارات المتاحة.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  قائمة من الرسائل التي تتكون منها المحادثة.\n\n  يحتوي كل كائن رسالة على:\n  - `role` (string): `system` أو `user` أو `assistant`\n  - `content` (string | array): محتوى الرسالة\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  درجة حرارة أخذ العينات (Sampling temperature) بين 0 و 2. القيم الأعلى تجعل المخرجات أكثر عشوائية.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  الحد الأقصى لعدد الـ tokens المراد إنشاؤها.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  إذا كانت true، فسيتم إرسال أجزاء الرسالة الجزئية كأحداث SSE.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  خيارات البث (Streaming). قم بتعيين `include_usage: true` لتلقي استخدام الـ tokens في أجزاء البث.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  معلمة أخذ عينات النواة (Nucleus sampling). نوصي بتعديل هذا أو temperature، وليس كليهما.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  رقم بين -2.0 و 2.0. القيم الإيجابية تعاقب الـ tokens المتكررة.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  رقم بين -2.0 و 2.0. القيم الإيجابية تعاقب الـ tokens الموجودة بالفعل في النص.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  ما يصل إلى 4 تسلسلات حيث سيتوقف الـ API عن إنشاء الـ tokens.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  قائمة بالأدوات التي قد يستدعيها النموذج (استدعاء الوظائف - function calling).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  يتحكم في كيفية",
      "vi": "---\ntitle: \"Tạo Chat Completion\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Tạo một phản hồi (completion) cho tin nhắn trò chuyện\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID của model cần sử dụng. Xem [Models](https://lemondata.cc/vi/models) để biết các tùy chọn có sẵn.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Danh sách các tin nhắn tạo nên cuộc hội thoại.\n\n  Mỗi đối tượng tin nhắn bao gồm:\n  - `role` (string): `system`, `user`, hoặc `assistant`\n  - `content` (string | array): Nội dung tin nhắn\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Nhiệt độ lấy mẫu (sampling temperature) trong khoảng từ 0 đến 2. Giá trị cao hơn sẽ làm cho kết quả đầu ra ngẫu nhiên hơn.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Số lượng token tối đa để tạo.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Nếu là true, các phần thay đổi tin nhắn (partial message deltas) sẽ được gửi dưới dạng các sự kiện SSE.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Các tùy chọn cho streaming. Đặt `include_usage: true` để nhận thông tin sử dụng token trong các stream chunks.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Tham số Nucleus sampling. Chúng tôi khuyên bạn nên thay đổi tham số này hoặc temperature, chứ không phải cả hai.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  Số trong khoảng từ -2.0 đến 2.0. Các giá trị dương sẽ phạt các token lặp lại.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  Số trong khoảng từ -2.0 đến 2.0. Các giá trị dương sẽ phạt các token đã xuất hiện trong văn bản.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  Tối đa 4 chuỗi ký tự mà tại đó API sẽ ngừng tạo token.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Danh sách các công cụ mà model có thể gọi (function calling).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Kiểm soát cách model sử dụng các công cụ. Các tùy chọn: `auto`, `none`, `required`, hoặc một đối tượng công cụ cụ thể.\n</ParamField>\n\n<ParamField body=\"parallel_",
      "id": "---\ntitle: \"Buat Chat Completion\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Membuat completion untuk pesan chat\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID model yang akan digunakan. Lihat [Models](https://lemondata.cc/id/models) untuk opsi yang tersedia.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Daftar pesan yang membentuk percakapan.\n\n  Setiap objek pesan berisi:\n  - `role` (string): `system`, `user`, atau `assistant`\n  - `content` (string | array): Konten pesan\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Suhu sampling antara 0 dan 2. Nilai yang lebih tinggi membuat output lebih acak.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Jumlah maksimum token yang akan dihasilkan.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Jika true, delta pesan parsial akan dikirim sebagai event SSE.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Opsi untuk streaming. Atur `include_usage: true` untuk menerima penggunaan token dalam potongan stream.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Parameter nucleus sampling. Kami menyarankan untuk mengubah ini atau temperature, bukan keduanya.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  Angka antara -2.0 dan 2.0. Nilai positif memberikan penalti pada token yang berulang.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  Angka antara -2.0 dan 2.0. Nilai positif memberikan penalti pada token yang sudah ada dalam teks.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  Hingga 4 sekuens di mana API akan berhenti menghasilkan token.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Daftar tool yang mungkin dipanggil oleh model (function calling).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Mengontrol bagaimana model menggunakan tool. Opsi: `auto`, `none`, `required`, atau objek tool tertentu.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Apakah akan mengaktifkan pemanggilan fungsi secara paralel. Atur ke false untuk memanggil fungsi secara berurutan.\n</ParamField>\n\n<ParamField body=\"max_completion_tokens\" type=\"integer\">\n  Token maksimum untuk completion. Alternatif untuk max_tokens",
      "tr": "---\ntitle: \"Sohbet Tamamlama Oluştur\"\napi: \"POST /v1/chat/completions\"\ndescription: \"Sohbet mesajı için bir tamamlama oluşturur\"\n---\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" required>\n  Kullanılacak modelin kimliği. Mevcut seçenekler için [Models](https://lemondata.cc/tr/models) sayfasına bakın.\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Konuşmayı oluşturan mesajların listesi.\n\n  Her mesaj nesnesi şunları içerir:\n  - `role` (string): `system`, `user` veya `assistant`\n  - `content` (string | array): Mesaj içeriği\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  0 ile 2 arasında örnekleme sıcaklığı. Daha yüksek değerler çıktıyı daha rastgele hale getirir.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\">\n  Oluşturulacak maksimum token sayısı.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Eğer true ise, kısmi mesaj farkları SSE olayları olarak gönderilir.\n</ParamField>\n\n<ParamField body=\"stream_options\" type=\"object\">\n  Akış seçenekleri. Akış parçalarında token kullanımını almak için `include_usage: true` olarak ayarlayın.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\" default=\"1\">\n  Nucleus örnekleme parametresi. Bunu veya temperature değerini değiştirmenizi öneririz, her ikisini birden değil.\n</ParamField>\n\n<ParamField body=\"frequency_penalty\" type=\"number\" default=\"0\">\n  -2.0 ile 2.0 arasında bir sayı. Pozitif değerler tekrarlanan token'ları cezalandırır.\n</ParamField>\n\n<ParamField body=\"presence_penalty\" type=\"number\" default=\"0\">\n  -2.0 ile 2.0 arasında bir sayı. Pozitif değerler metinde zaten bulunan token'ları cezalandırır.\n</ParamField>\n\n<ParamField body=\"stop\" type=\"string | array\">\n  API'nin token oluşturmayı durduracağı 4 adede kadar dizi.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Modelin çağırabileceği araçların listesi (fonksiyon çağırma).\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"string | object\">\n  Modelin araçları nasıl kullanacağını kontrol eder. Seçenekler: `auto`, `none`, `required` veya belirli bir araç nesnesi.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  "
    },
    "updatedAt": "2026-01-26T05:22:54.031Z"
  },
  "api-reference/embeddings/create-embedding.mdx": {
    "sourceHash": "a26979bd55faf724",
    "translations": {
      "zh": "---\ntitle: \"创建 Embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"创建一个代表输入文本的 embedding 向量\"\n---\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的 embedding 模型 ID（例如：`text-embedding-3-small`）。\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  要生成 embedding 的输入文本。可以是字符串或字符串数组。\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  embedding 的格式：`float` 或 `base64`。\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  输出的维度数量（取决于具体模型）。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  代表终端用户的唯一标识符，用于滥用监控。\n</ParamField>\n\n## 可用模型\n\n| 模型 | 维度 | 描述 |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | 最佳质量 |\n| `text-embedding-3-small` | 1536 | 平衡 |\n| `text-embedding-ada-002` | 1536 | 旧版 |\n\n## 响应\n\n<ResponseField name=\"object\" type=\"string\">\n  始终为 `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  embedding 对象数组。\n\n  每个对象包含：\n  - `object` (string): `embedding`\n  - `index` (integer): 在输入数组中的索引\n  - `embedding` (array): embedding 向量\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  所使用的模型。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  包含 `prompt_tokens` 和 `total_tokens` 的 Token 使用情况。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## 批量 Embedding\n\n```python\n# 一次性为多个文本生成 embedding\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "zh-TW": "---\ntitle: \"建立嵌入\"\napi: \"POST /v1/embeddings\"\ndescription: \"建立代表輸入文字的嵌入向量\"\n---\n\n## 請求主體\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的嵌入模型 ID（例如：`text-embedding-3-small`）。\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  要進行嵌入的輸入文字。可以是字串或字串陣列。\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  嵌入的格式：`float` 或 `base64`。\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  輸出的維度數量（取決於特定模型）。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  代表您終端用戶的唯一識別碼，用於濫用監控。\n</ParamField>\n\n## 可用模型\n\n| 模型 | 維度 | 描述 |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | 最佳品質 |\n| `text-embedding-3-small` | 1536 | 平衡 |\n| `text-embedding-ada-002` | 1536 | 舊版 |\n\n## 回應\n\n<ResponseField name=\"object\" type=\"string\">\n  固定為 `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  嵌入物件陣列。\n\n  每個物件包含：\n  - `object` (string): `embedding`\n  - `index` (integer): 輸入陣列中的索引\n  - `embedding` (array): 嵌入向量\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用的模型。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  包含 `prompt_tokens` 與 `total_tokens` 的 Token 使用量。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## 批次嵌入\n\n```python\n# Embed multiple texts at once\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "ja": "---\ntitle: \"Embeddingの作成\"\napi: \"POST /v1/embeddings\"\ndescription: \"入力テキストを表現するエンベディングベクトルを作成します\"\n---\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" required>\n  使用するエンベディングモデルのID（例：`text-embedding-3-small`）。\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  エンベディングする入力テキスト。文字列または文字列の配列を指定できます。\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  エンベディングの形式：`float` または `base64`。\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  出力の次元数（モデル固有）。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  不正利用監視のためにエンドユーザーを識別する一意の識別子。\n</ParamField>\n\n## 利用可能なモデル\n\n| モデル | 次元数 | 説明 |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | 最高品質 |\n| `text-embedding-3-small` | 1536 | バランス重視 |\n| `text-embedding-ada-002` | 1536 | レガシー |\n\n## レスポンス\n\n<ResponseField name=\"object\" type=\"string\">\n  常に `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  エンベディングオブジェクトの配列。\n\n  各オブジェクトの構成：\n  - `object` (string): `embedding`\n  - `index` (integer): 入力配列内のインデックス\n  - `embedding` (array): エンベディングベクトル\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用されたモデル。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  `prompt_tokens` および `total_tokens` を含むトークン使用量。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## バッチエンベディング\n\n```python\n# Embed multiple texts at once\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "ko": "---\ntitle: \"임베딩 생성\"\napi: \"POST /v1/embeddings\"\ndescription: \"입력 텍스트를 나타내는 임베딩 벡터를 생성합니다.\"\n---\n\n## 요청 본문\n\n<ParamField body=\"model\" type=\"string\" required>\n  사용할 임베딩 모델의 ID (예: `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  임베딩할 입력 텍스트입니다. 문자열 또는 문자열 배열일 수 있습니다.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  임베딩 형식: `float` 또는 `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  출력 차원 수 (모델별로 다름).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  오남용 모니터링을 위해 최종 사용자를 나타내는 고유 식별자.\n</ParamField>\n\n## 사용 가능한 모델\n\n| 모델 | 차원 | 설명 |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | 최고 품질 |\n| `text-embedding-3-small` | 1536 | 균형 잡힌 성능 |\n| `text-embedding-ada-002` | 1536 | 레거시 |\n\n## 응답\n\n<ResponseField name=\"object\" type=\"string\">\n  항상 `list`입니다.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  임베딩 객체의 배열.\n\n  각 객체는 다음을 포함합니다:\n  - `object` (string): `embedding`\n  - `index` (integer): 입력 배열에서의 인덱스\n  - `embedding` (array): 임베딩 벡터\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  사용된 모델.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  `prompt_tokens` 및 `total_tokens`를 포함한 토큰 사용량.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## 배치 임베딩\n\n```python\n# 여러 텍스트를 한 번에 임베딩\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "de": "---\ntitle: \"Embedding erstellen\"\napi: \"POST /v1/embeddings\"\ndescription: \"Erstellt einen Embedding-Vektor, der den Eingabetext repräsentiert\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID des zu verwendenden Embedding-Modells (z. B. `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Eingabetext für das Embedding. Kann ein String oder ein Array von Strings sein.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Format für die Embeddings: `float` oder `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Anzahl der Dimensionen für die Ausgabe (modellspezifisch).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Eine eindeutige Kennung, die Ihren Endbenutzer für die Missbrauchsüberwachung repräsentiert.\n</ParamField>\n\n## Verfügbare Modelle\n\n| Modell | Dimensionen | Beschreibung |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | Beste Qualität |\n| `text-embedding-3-small` | 1536 | Ausgewogen |\n| `text-embedding-ada-002` | 1536 | Legacy |\n\n## Response\n\n<ResponseField name=\"object\" type=\"string\">\n  Immer `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array von Embedding-Objekten.\n\n  Jedes Objekt enthält:\n  - `object` (string): `embedding`\n  - `index` (integer): Index im Eingabe-Array\n  - `embedding` (array): Der Embedding-Vektor\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Verwendetes Modell.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token-Nutzung mit `prompt_tokens` und `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings",
      "fr": "---\ntitle: \"Créer un embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"Crée un vecteur d'embedding représentant le texte d'entrée\"\n---\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID du modèle d'embedding à utiliser (par ex., `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Texte d'entrée à transformer en embedding. Peut être une chaîne de caractères ou un tableau de chaînes.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Format pour les embeddings : `float` ou `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Nombre de dimensions pour la sortie (spécifique au modèle).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identifiant unique représentant votre utilisateur final pour la surveillance des abus.\n</ParamField>\n\n## Modèles disponibles\n\n| Modèle | Dimensions | Description |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | Meilleure qualité |\n| `text-embedding-3-small` | 1536 | Équilibré |\n| `text-embedding-ada-002` | 1536 | Legacy |\n\n## Réponse\n\n<ResponseField name=\"object\" type=\"string\">\n  Toujours `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Tableau d'objets d'embedding.\n\n  Chaque objet contient :\n  - `object` (string) : `embedding`\n  - `index` (integer) : Index dans le tableau d'entrée\n  - `embedding` (array) : Le vecteur d'embedding\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modèle utilisé.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Utilisation des tokens avec `prompt_tokens` et `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## Embeddings par lots\n\n```python\n# Transformer plusieurs textes en embeddings simultanément\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "es": "---\ntitle: \"Crear Embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"Crea un vector de embedding que representa el texto de entrada\"\n---\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID del modelo de embedding a utilizar (ej., `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Texto de entrada para generar el embedding. Puede ser una cadena o un array de cadenas.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Formato para los embeddings: `float` o `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Número de dimensiones para la salida (específico del modelo).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identificador único que representa a su usuario final para el monitoreo de abusos.\n</ParamField>\n\n## Modelos disponibles\n\n| Modelo | Dimensiones | Descripción |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | Mejor calidad |\n| `text-embedding-3-small` | 1536 | Equilibrado |\n| `text-embedding-ada-002` | 1536 | Legacy |\n\n## Respuesta\n\n<ResponseField name=\"object\" type=\"string\">\n  Siempre `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array de objetos de embedding.\n\n  Cada objeto contiene:\n  - `object` (string): `embedding`\n  - `index` (integer): Índice en el array de entrada\n  - `embedding` (array): El vector de embedding\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modelo utilizado.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Uso de tokens con `prompt_tokens` y `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## Embeddings por lotes\n\n```python\n# Embed multiple texts at once\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "pt": "---\ntitle: \"Criar Embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"Cria um vetor de embedding representando o texto de entrada\"\n---\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID do modelo de embedding a ser usado (ex: `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Texto de entrada para gerar o embedding. Pode ser uma string ou um array de strings.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Formato para os embeddings: `float` ou `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Número de dimensões para a saída (específico do modelo).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Um identificador único representando seu usuário final para monitoramento de abusos.\n</ParamField>\n\n## Modelos Disponíveis\n\n| Modelo | Dimensões | Descrição |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | Melhor qualidade |\n| `text-embedding-3-small` | 1536 | Equilibrado |\n| `text-embedding-ada-002` | 1536 | Legado |\n\n## Resposta\n\n<ResponseField name=\"object\" type=\"string\">\n  Sempre `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array de objetos de embedding.\n\n  Cada objeto contém:\n  - `object` (string): `embedding`\n  - `index` (integer): Índice no array de entrada\n  - `embedding` (array): O vetor de embedding\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modelo utilizado.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Uso de tokens com `prompt_tokens` e `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## Embeddings em Lote\n\n```python\n# Embed multiple texts at once\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "ar": "---\ntitle: \"إنشاء Embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"يقوم بإنشاء متجه (vector) لـ embedding يمثل النص المدخل\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  معرف (ID) نموذج الـ embedding المراد استخدامه (على سبيل المثال، `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  النص المدخل لعمل embedding. يمكن أن يكون سلسلة نصية (string) أو مصفوفة من السلاسل النصية.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  تنسيق الـ embeddings: `float` أو `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  عدد الأبعاد للمخرجات (خاص بالنموذج).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  معرف فريد يمثل المستخدم النهائي الخاص بك لمراقبة سوء الاستخدام.\n</ParamField>\n\n## النماذج المتاحة (Available Models)\n\n| Model | الأبعاد (Dimensions) | الوصف |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | أفضل جودة |\n| `text-embedding-3-small` | 1536 | متوازن |\n| `text-embedding-ada-002` | 1536 | قديم (Legacy) |\n\n## الاستجابة (Response)\n\n<ResponseField name=\"object\" type=\"string\">\n  دائماً `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  مصفوفة من كائنات (objects) الـ embedding.\n\n  يحتوي كل كائن على:\n  - `object` (string): `embedding`\n  - `index` (integer): الفهرس (Index) في مصفوفة المدخلات\n  - `embedding` (array): متجه الـ embedding\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  النموذج المستخدم.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  استخدام الـ token مع `prompt_tokens` و `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## الـ Embeddings المجمعة (Batch Embeddings)\n\n```python\n# Embed multiple texts at once\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "vi": "---\ntitle: \"Tạo Embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"Tạo một vector embedding đại diện cho văn bản đầu vào\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID của model embedding cần sử dụng (ví dụ: `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Văn bản đầu vào để tạo embedding. Có thể là một chuỗi hoặc một mảng các chuỗi.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Định dạng cho các embedding: `float` hoặc `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Số lượng chiều (dimensions) cho đầu ra (tùy thuộc vào model).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Một mã định danh duy nhất đại diện cho người dùng cuối của bạn để theo dõi lạm dụng.\n</ParamField>\n\n## Các Model Hiện Có\n\n| Model | Số chiều | Mô tả |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | Chất lượng tốt nhất |\n| `text-embedding-3-small` | 1536 | Cân bằng |\n| `text-embedding-ada-002` | 1536 | Cũ (Legacy) |\n\n## Phản hồi\n\n<ResponseField name=\"object\" type=\"string\">\n  Luôn là `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Mảng các đối tượng embedding.\n\n  Mỗi đối tượng bao gồm:\n  - `object` (string): `embedding`\n  - `index` (integer): Chỉ mục trong mảng đầu vào\n  - `embedding` (array): Vector embedding\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Model đã sử dụng.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Mức sử dụng token với `prompt_tokens` và `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## Tạo Embedding theo lô (Batch)\n\n```python\n# Embed multiple texts at once\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "id": "---\ntitle: \"Buat Embedding\"\napi: \"POST /v1/embeddings\"\ndescription: \"Membuat vektor embedding yang merepresentasikan teks input\"\n---\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID model embedding yang akan digunakan (misalnya, `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Teks input untuk di-embed. Dapat berupa string atau array string.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Format untuk embedding: `float` atau `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Jumlah dimensi untuk output (spesifik untuk model).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Pengidentifikasi unik yang merepresentasikan pengguna akhir Anda untuk pemantauan penyalahgunaan.\n</ParamField>\n\n## Model yang Tersedia\n\n| Model | Dimensi | Deskripsi |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | Kualitas terbaik |\n| `text-embedding-3-small` | 1536 | Seimbang |\n| `text-embedding-ada-002` | 1536 | Legacy |\n\n## Respons\n\n<ResponseField name=\"object\" type=\"string\">\n  Selalu `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array dari objek embedding.\n\n  Setiap objek berisi:\n  - `object` (string): `embedding`\n  - `index` (integer): Indeks dalam array input\n  - `embedding` (array): Vektor embedding\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Model yang digunakan.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Penggunaan token dengan `prompt_tokens` dan `total_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## Batch Embedding\n\n```python\n# Embed beberapa teks sekaligus\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```",
      "tr": "---\ntitle: \"Embedding Oluştur\"\napi: \"POST /v1/embeddings\"\ndescription: \"Girdi metnini temsil eden bir embedding vektörü oluşturur\"\n---\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" required>\n  Kullanılacak embedding modelinin ID'si (örneğin, `text-embedding-3-small`).\n</ParamField>\n\n<ParamField body=\"input\" type=\"string | array\" required>\n  Embed edilecek girdi metni. Bir string veya string dizisi (array) olabilir.\n</ParamField>\n\n<ParamField body=\"encoding_format\" type=\"string\" default=\"float\">\n  Embedding'ler için format: `float` veya `base64`.\n</ParamField>\n\n<ParamField body=\"dimensions\" type=\"integer\">\n  Çıktı için boyut sayısı (modele özgü).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Kötüye kullanım izleme için son kullanıcınızı temsil eden benzersiz bir tanımlayıcı.\n</ParamField>\n\n## Mevcut Modeller\n\n| Model | Boyutlar | Açıklama |\n|-------|------------|-------------|\n| `text-embedding-3-large` | 3072 | En iyi kalite |\n| `text-embedding-3-small` | 1536 | Dengeli |\n| `text-embedding-ada-002` | 1536 | Eski sürüm |\n\n## Yanıt\n\n<ResponseField name=\"object\" type=\"string\">\n  Her zaman `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Embedding nesneleri dizisi.\n\n  Her nesne şunları içerir:\n  - `object` (string): `embedding`\n  - `index` (integer): Girdi dizisindeki dizin\n  - `embedding` (array): Embedding vektörü\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Kullanılan model.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  `prompt_tokens` ve `total_tokens` ile token kullanımı.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/embeddings\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"text-embedding-3-small\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"The quick brown fox jumps over the lazy dog\"\n)\n\nembedding = response.data[0].embedding\nprint(f\"Embedding dimension: {len(embedding)}\")\nprint(f\"First 5 values: {embedding[:5]}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.embeddings.create({\n  model: 'text-embedding-3-small',\n  input: 'The quick brown fox jumps over the lazy dog'\n});\n\nconsole.log(response.data[0].embedding.slice(0, 5));\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateEmbeddings(\n        context.Background(),\n        openai.EmbeddingRequest{\n            Model: openai.SmallEmbedding3,\n            Input: []string{\"The quick brown fox jumps over the lazy dog\"},\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Embedding dimension: %d\\n\", len(resp.Data[0].Embedding))\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/embeddings');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'text-embedding-3-small',\n        'input' => 'The quick brown fox jumps over the lazy dog'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r(array_slice($data['data'][0]['embedding'], 0, 5));\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023, -0.0194, 0.0081, ...]\n    }\n  ],\n  \"model\": \"text-embedding-3-small\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n```\n</ResponseExample>\n\n## Toplu Embedding'ler\n\n```python\n# Birden fazla metni aynı anda embed edin\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=[\n        \"First document text\",\n        \"Second document text\",\n        \"Third document text\"\n    ]\n)\n\nfor i, data in enumerate(response.data):\n    print(f\"Document {i}: {len(data.embedding)} dimensions\")\n```"
    },
    "updatedAt": "2026-01-26T05:23:14.560Z"
  },
  "api-reference/gemini/generate-content.mdx": {
    "sourceHash": "ce7fcfb9ffcfe978",
    "translations": {
      "zh": "---\ntitle: \"生成内容\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"使用 Google Gemini API 格式生成内容\"\n---\n\nLemonData 支持 Gemini 模型的原生 Google Gemini API 格式。这使得它能直接兼容 Google AI SDK。\n\n## 路径参数\n\n<ParamField path=\"model\" type=\"string\" required>\n  模型名称（例如：`gemini-2.5-pro`，`gemini-2.5-flash`）。\n</ParamField>\n\n## 查询参数\n\n<ParamField query=\"key\" type=\"string\">\n  API 密钥（Header 身份验证的替代方案）。\n</ParamField>\n\n## 身份验证\n\nGemini 端点支持多种身份验证方法：\n- `?key=YOUR_API_KEY` 查询参数\n- `x-goog-api-key: YOUR_API_KEY` Header\n- `Authorization: Bearer YOUR_API_KEY` Header\n\n## 请求体\n\n<ParamField body=\"contents\" type=\"array\" required>\n  对话内容。\n\n  每个内容对象包含：\n  - `role` (string): `user` 或 `model`\n  - `parts` (array): 内容部分（文本或内联数据）\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  模型的系统指令。\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  生成配置：\n  - `temperature` (number): 采样温度\n  - `topP` (number): 核采样概率\n  - `topK` (integer): Top-K 采样\n  - `maxOutputTokens` (integer): 最大输出 token 数\n  - `stopSequences` (array): 停止序列\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  安全过滤设置。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"candidates\" type=\"array\">\n  生成的候选内容。\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Token 使用信息。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"生成內容\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"使用 Google Gemini API 格式生成內容\"\n---\n\nLemonData 支援 Gemini 模型的原生 Google Gemini API 格式。這使得與 Google AI SDK 具有直接的相容性。\n\n## 路徑參數\n\n<ParamField path=\"model\" type=\"string\" required>\n  模型名稱（例如：`gemini-2.5-pro`、`gemini-2.5-flash`）。\n</ParamField>\n\n## 查詢參數\n\n<ParamField query=\"key\" type=\"string\">\n  API key（標頭驗證的替代方案）。\n</ParamField>\n\n## 驗證\n\nGemini 端點支援多種驗證方式：\n- `?key=YOUR_API_KEY` 查詢參數\n- `x-goog-api-key: YOUR_API_KEY` 標頭\n- `Authorization: Bearer YOUR_API_KEY` 標頭\n\n## 請求主體\n\n<ParamField body=\"contents\" type=\"array\" required>\n  對話內容。\n\n  每個內容物件包含：\n  - `role` (string)：`user` 或 `model`\n  - `parts` (array)：內容部分（文字或內嵌數據）\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  模型的系統指令。\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  生成配置：\n  - `temperature` (number)：取樣溫度\n  - `topP` (number)：核取樣機率\n  - `topK` (integer)：Top-K 取樣\n  - `maxOutputTokens` (integer)：最大輸出 token 數\n  - `stopSequences` (array)：停止序列\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  安全過濾器設定。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"candidates\" type=\"array\">\n  生成的內容候選。\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Token 使用資訊。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"コンテンツの生成\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Google Gemini API形式を使用してコンテンツを生成します\"\n---\n\nLemonDataは、Geminiモデル向けのネイティブなGoogle Gemini API形式をサポートしています。これにより、Google AI SDKとの直接的な互換性が確保されます。\n\n## パスパラメータ\n\n<ParamField path=\"model\" type=\"string\" required>\n  モデル名 (例: `gemini-2.5-pro`、`gemini-2.5-flash`)。\n</ParamField>\n\n## クエリパラメータ\n\n<ParamField query=\"key\" type=\"string\">\n  APIキー（ヘッダー認証の代替）。\n</ParamField>\n\n## 認証\n\nGeminiエンドポイントは、複数の認証方法をサポートしています：\n- `?key=YOUR_API_KEY` クエリパラメータ\n- `x-goog-api-key: YOUR_API_KEY` ヘッダー\n- `Authorization: Bearer YOUR_API_KEY` ヘッダー\n\n## リクエストボディ\n\n<ParamField body=\"contents\" type=\"array\" required>\n  会話の内容。\n\n  各コンテンツオブジェクトには以下が含まれます：\n  - `role` (string): `user` または `model`\n  - `parts` (array): コンテンツパーツ（`text` または `inline data`）\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  モデルへのシステム指示。\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  生成設定：\n  - `temperature` (number): サンプリング温度\n  - `topP` (number): 核サンプリング（Nucleus sampling）確率\n  - `topK` (integer): Top-Kサンプリング\n  - `maxOutputTokens` (integer): 最大出力トークン数\n  - `stopSequences` (array): 停止シーケンス\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  セーフティフィルター設定。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"candidates\" type=\"array\">\n  生成されたコンテンツの候補。\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  トークン使用状況の情報。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"콘텐츠 생성\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Google Gemini API 형식을 사용하여 콘텐츠를 생성합니다\"\n---\n\nLemonData는 Gemini 모델에 대해 네이티브 Google Gemini API 형식을 지원합니다. 이를 통해 Google AI SDK와 직접적인 호환이 가능합니다.\n\n## 경로 파라미터\n\n<ParamField path=\"model\" type=\"string\" required>\n  모델 이름 (예: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## 쿼리 파라미터\n\n<ParamField query=\"key\" type=\"string\">\n  API 키 (헤더 인증의 대안).\n</ParamField>\n\n## 인증\n\nGemini 엔드포인트는 여러 인증 방법을 지원합니다:\n- `?key=YOUR_API_KEY` 쿼리 파라미터\n- `x-goog-api-key: YOUR_API_KEY` 헤더\n- `Authorization: Bearer YOUR_API_KEY` 헤더\n\n## 요청 본문\n\n<ParamField body=\"contents\" type=\"array\" required>\n  대화 내용.\n\n  각 콘텐츠 객체는 다음을 포함합니다:\n  - `role` (string): `user` 또는 `model`\n  - `parts` (array): 콘텐츠 파트 (`text` 또는 `inline data`)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  모델을 위한 시스템 지침.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  생성 설정:\n  - `temperature` (number): 샘플링 온도\n  - `topP` (number): Nucleus 샘플링 확률\n  - `topK` (integer): Top-K 샘플링\n  - `maxOutputTokens` (integer): 최대 출력 토큰 수\n  - `stopSequences` (array): 중단 시퀀스\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  안전 필터 설정.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"candidates\" type=\"array\">\n  생성된 콘텐츠 후보.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  토큰 사용 정보.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"Inhalte generieren\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Generiert Inhalte im Google Gemini API-Format\"\n---\n\nLemonData unterstützt das native Google Gemini API-Format für Gemini-Modelle. Dies ermöglicht eine direkte Kompatibilität mit Google AI SDKs.\n\n## Pfadparameter\n\n<ParamField path=\"model\" type=\"string\" required>\n  Modellname (z. B. `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Abfrageparameter\n\n<ParamField query=\"key\" type=\"string\">\n  API-Key (Alternative zur Header-Authentifizierung).\n</ParamField>\n\n## Authentifizierung\n\nGemini-Endpunkte unterstützen mehrere Authentifizierungsmethoden:\n- `?key=YOUR_API_KEY` Abfrageparameter\n- `x-goog-api-key: YOUR_API_KEY` Header\n- `Authorization: Bearer YOUR_API_KEY` Header\n\n## Request-Body\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Inhalte der Konversation.\n\n  Jedes Inhaltsobjekt enthält:\n  - `role` (string): `user` oder `model`\n  - `parts` (array): Inhaltsteile (Text oder Inline-Daten)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Systemanweisung für das Modell.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Generierungskonfiguration:\n  - `temperature` (number): Sampling-Temperatur\n  - `topP` (number): Nucleus-Sampling-Wahrscheinlichkeit\n  - `topK` (integer): Top-K-Sampling\n  - `maxOutputTokens` (integer): Maximale Anzahl an Output-Tokens\n  - `stopSequences` (array): Stoppsequenzen\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Einstellungen für Sicherheitsfilter.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Generierte Inhaltskandidaten.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Informationen zur Token-Nutzung.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Générer du contenu\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Génère du contenu en utilisant le format de l'API Google Gemini\"\n---\n\nLemonData prend en charge le format natif de l'API Google Gemini pour les modèles Gemini. Cela permet une compatibilité directe avec les SDK Google AI.\n\n## Paramètres de chemin\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nom du modèle (par ex., `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Paramètres de requête\n\n<ParamField query=\"key\" type=\"string\">\n  Clé API (alternative à l'authentification par en-tête).\n</ParamField>\n\n## Authentification\n\nLes points de terminaison Gemini prennent en charge plusieurs méthodes d'authentification :\n- Paramètre de requête `?key=YOUR_API_KEY`\n- En-tête `x-goog-api-key: YOUR_API_KEY`\n- En-tête `Authorization: Bearer YOUR_API_KEY`\n\n## Corps de la requête\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Contenu de la conversation.\n\n  Chaque objet de contenu contient :\n  - `role` (string) : `user` ou `model`\n  - `parts` (array) : Parties du contenu (texte ou données inline)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Instruction système pour le modèle.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Configuration de génération :\n  - `temperature` (number) : Température d'échantillonnage\n  - `topP` (number) : Probabilité d'échantillonnage nucleus\n  - `topK` (integer) : Échantillonnage Top-K\n  - `maxOutputTokens` (integer) : Nombre maximum de tokens de sortie\n  - `stopSequences` (array) : Séquences d'arrêt\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Paramètres des filtres de sécurité.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Candidats de contenu générés.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Informations sur l'utilisation des tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Generar Contenido\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Genera contenido utilizando el formato de la API de Google Gemini\"\n---\n\nLemonData es compatible con el formato nativo de la API de Google Gemini para los modelos Gemini. Esto permite una compatibilidad directa con los SDK de Google AI.\n\n## Parámetros de Ruta\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nombre del modelo (por ejemplo, `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Parámetros de Consulta\n\n<ParamField query=\"key\" type=\"string\">\n  Clave API (alternativa a la autenticación por encabezado).\n</ParamField>\n\n## Autenticación\n\nLos endpoints de Gemini admiten múltiples métodos de autenticación:\n- Parámetro de consulta `?key=YOUR_API_KEY`\n- Encabezado `x-goog-api-key: YOUR_API_KEY`\n- Encabezado `Authorization: Bearer YOUR_API_KEY`\n\n## Cuerpo de la Solicitud\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Contenidos de la conversación.\n\n  Cada objeto de contenido contiene:\n  - `role` (string): `user` o `model`\n  - `parts` (array): Partes del contenido (texto o datos en línea)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Instrucción del sistema para el modelo.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Configuración de generación:\n  - `temperature` (number): Temperatura de muestreo\n  - `topP` (number): Probabilidad de muestreo de núcleo (nucleus sampling)\n  - `topK` (integer): Muestreo Top-K\n  - `maxOutputTokens` (integer): Máximo de tokens de salida\n  - `stopSequences` (array): Secuencias de parada\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Configuración de filtros de seguridad.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Candidatos de contenido generado.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Información de uso de tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Gerar Conteúdo\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Gera conteúdo usando o formato da API Google Gemini\"\n---\n\nA LemonData suporta o formato nativo da API Google Gemini para modelos Gemini. Isso permite compatibilidade direta com os SDKs de IA do Google.\n\n## Parâmetros de Caminho\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nome do modelo (ex: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Parâmetros de Consulta\n\n<ParamField query=\"key\" type=\"string\">\n  Chave de API (alternativa à autenticação por cabeçalho).\n</ParamField>\n\n## Autenticação\n\nOs endpoints do Gemini suportam múltiplos métodos de autenticação:\n- `?key=YOUR_API_KEY` parâmetro de consulta\n- `x-goog-api-key: YOUR_API_KEY` cabeçalho\n- `Authorization: Bearer YOUR_API_KEY` cabeçalho\n\n## Corpo da Requisição\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Conteúdos da conversa.\n\n  Cada objeto de conteúdo contém:\n  - `role` (string): `user` ou `model`\n  - `parts` (array): Partes do conteúdo (texto ou dados inline)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Instrução de sistema para o modelo.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Configuração de geração:\n  - `temperature` (number): Temperatura de amostragem\n  - `topP` (number): Probabilidade de amostragem de núcleo\n  - `topK` (integer): Amostragem Top-K\n  - `maxOutputTokens` (integer): Máximo de tokens de saída\n  - `stopSequences` (array): Sequências de parada\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Configurações de filtro de segurança.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Candidatos de conteúdo gerado.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Informações de uso de tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0",
      "ar": "---\ntitle: \"إنشاء المحتوى\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"إنشاء المحتوى باستخدام تنسيق Google Gemini API\"\n---\n\nيدعم LemonData تنسيق Google Gemini API الأصلي لنماذج Gemini. يتيح ذلك التوافق المباشر مع Google AI SDKs.\n\n## معلمات المسار (Path Parameters)\n\n<ParamField path=\"model\" type=\"string\" required>\n  اسم النموذج (على سبيل المثال، `gemini-2.5-pro`، `gemini-2.5-flash`).\n</ParamField>\n\n## معلمات الاستعلام (Query Parameters)\n\n<ParamField query=\"key\" type=\"string\">\n  مفتاح API (بديل للمصادقة عبر الترويسة).\n</ParamField>\n\n## المصادقة\n\nتدعم نقاط نهاية Gemini طرق مصادقة متعددة:\n- `?key=YOUR_API_KEY` معلمة استعلام\n- `x-goog-api-key: YOUR_API_KEY` ترويسة\n- `Authorization: Bearer YOUR_API_KEY` ترويسة\n\n## جسم الطلب (Request Body)\n\n<ParamField body=\"contents\" type=\"array\" required>\n  محتويات المحادثة.\n\n  يحتوي كل كائن محتوى على:\n  - `role` (string): `user` أو `model`\n  - `parts` (array): أجزاء المحتوى (نص أو بيانات مضمنة)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  تعليمات النظام للنموذج.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  تكوين الإنشاء:\n  - `temperature` (number): درجة حرارة أخذ العينات\n  - `topP` (number): احتمالية أخذ عينات النواة (Nucleus sampling)\n  - `topK` (integer): أخذ عينات Top-K\n  - `maxOutputTokens` (integer): الحد الأقصى لـ tokens المخرجات\n  - `stopSequences` (array): تسلسلات التوقف\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  إعدادات فلاتر الأمان.\n</ParamField>\n\n## الاستجابة\n\n<ResponseField name=\"candidates\" type=\"array\">\n  مرشحو المحتوى الذي تم إنشاؤه.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  معلومات استخدام الـ token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Tạo nội dung\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Tạo nội dung bằng định dạng Google Gemini API\"\n---\n\nLemonData hỗ trợ định dạng Google Gemini API gốc cho các mô hình Gemini. Điều này cho phép khả năng tương thích trực tiếp với các Google AI SDK.\n\n## Tham số đường dẫn\n\n<ParamField path=\"model\" type=\"string\" required>\n  Tên mô hình (ví dụ: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Tham số truy vấn\n\n<ParamField query=\"key\" type=\"string\">\n  API key (phương thức thay thế cho xác thực qua header).\n</ParamField>\n\n## Xác thực\n\nCác endpoint của Gemini hỗ trợ nhiều phương thức xác thực:\n- `?key=YOUR_API_KEY` tham số truy vấn\n- `x-goog-api-key: YOUR_API_KEY` header\n- `Authorization: Bearer YOUR_API_KEY` header\n\n## Thân yêu cầu\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Nội dung cuộc hội thoại.\n\n  Mỗi đối tượng nội dung bao gồm:\n  - `role` (string): `user` hoặc `model`\n  - `parts` (array): Các phần nội dung (văn bản hoặc dữ liệu inline)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Chỉ dẫn hệ thống cho mô hình.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Cấu hình tạo nội dung:\n  - `temperature` (number): Nhiệt độ lấy mẫu (sampling temperature)\n  - `topP` (number): Xác suất lấy mẫu hạt (nucleus sampling)\n  - `topK` (integer): Lấy mẫu Top-K\n  - `maxOutputTokens` (integer): Số lượng token đầu ra tối đa\n  - `stopSequences` (array): Các chuỗi dừng (stop sequences)\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Cài đặt bộ lọc an toàn.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Các ứng viên nội dung được tạo.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Thông tin sử dụng token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Hasilkan Konten\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Menghasilkan konten menggunakan format Google Gemini API\"\n---\n\nLemonData mendukung format native Google Gemini API untuk model Gemini. Hal ini memungkinkan kompatibilitas langsung dengan Google AI SDK.\n\n## Parameter Path\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nama model (contoh: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Parameter Query\n\n<ParamField query=\"key\" type=\"string\">\n  API key (alternatif untuk autentikasi header).\n</ParamField>\n\n## Autentikasi\n\nEndpoint Gemini mendukung beberapa metode autentikasi:\n- `?key=YOUR_API_KEY` parameter query\n- `x-goog-api-key: YOUR_API_KEY` header\n- `Authorization: Bearer YOUR_API_KEY` header\n\n## Request Body\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Isi percakapan.\n\n  Setiap objek konten berisi:\n  - `role` (string): `user` atau `model`\n  - `parts` (array): Bagian konten (teks atau data inline)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Instruksi sistem untuk model.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Konfigurasi generasi:\n  - `temperature` (number): Suhu sampling\n  - `topP` (number): Probabilitas nucleus sampling\n  - `topK` (integer): Top-K sampling\n  - `maxOutputTokens` (integer): Token output maksimum\n  - `stopSequences` (array): Stop sequences\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Pengaturan filter keamanan.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Kandidat konten yang dihasilkan.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Informasi penggunaan token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-",
      "tr": "---\ntitle: \"İçerik Oluştur\"\napi: \"POST /v1beta/models/{model}:generateContent\"\ndescription: \"Google Gemini API formatını kullanarak içerik oluşturur\"\n---\n\nLemonData, Gemini modelleri için yerel Google Gemini API formatını destekler. Bu, Google AI SDK'ları ile doğrudan uyumluluk sağlar.\n\n## Yol Parametreleri\n\n<ParamField path=\"model\" type=\"string\" required>\n  Model adı (örneğin, `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Sorgu Parametreleri\n\n<ParamField query=\"key\" type=\"string\">\n  API anahtarı (header kimlik doğrulamasına alternatif).\n</ParamField>\n\n## Kimlik Doğrulama\n\nGemini uç noktaları birden fazla kimlik doğrulama yöntemini destekler:\n- `?key=YOUR_API_KEY` sorgu parametresi\n- `x-goog-api-key: YOUR_API_KEY` header\n- `Authorization: Bearer YOUR_API_KEY` header\n\n## İstek Gövdesi\n\n<ParamField body=\"contents\" type=\"array\" required>\n  Sohbet içerikleri.\n\n  Her içerik nesnesi şunları içerir:\n  - `role` (string): `user` veya `model`\n  - `parts` (array): İçerik parçaları (metin veya satır içi veri)\n</ParamField>\n\n<ParamField body=\"systemInstruction\" type=\"object\">\n  Model için sistem talimatı.\n</ParamField>\n\n<ParamField body=\"generationConfig\" type=\"object\">\n  Oluşturma yapılandırması:\n  - `temperature` (number): Örnekleme sıcaklığı\n  - `topP` (number): Çekirdek (nucleus) örnekleme olasılığı\n  - `topK` (integer): Top-K örnekleme\n  - `maxOutputTokens` (integer): Maksimum çıktı token'ları\n  - `stopSequences` (array): Durdurma dizileri\n</ParamField>\n\n<ParamField body=\"safetySettings\" type=\"array\">\n  Güvenlik filtresi ayarları.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"candidates\" type=\"array\">\n  Oluşturulan içerik adayları.\n</ResponseField>\n\n<ResponseField name=\"usageMetadata\" type=\"object\">\n  Token kullanım bilgisi.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Hello, Gemini!\"}]\n      }\n    ],\n    \"generationConfig\": {\n      \"temperature\": 0.7,\n      \"maxOutputTokens\": 1024\n    }\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Hello, Gemini!\")\n\nprint(response.text)\n```\n\n```javascript JavaScript\nimport { GoogleGenerativeAI } from \"@google/generative-ai\";\n\nconst genAI = new GoogleGenerativeAI(\"sk-your-api-key\", {\n  baseUrl: \"https://api.lemondata.cc\"\n});\n\nconst model = genAI.getGenerativeModel({ model: \"gemini-2.5-pro\" });\nconst result = await model.generateContent(\"Hello, Gemini!\");\n\nconsole.log(result.response.text());\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\n                \"parts\": []map[string]string{\n                    {\"text\": \"Hello, Gemini!\"},\n                },\n            },\n        },\n        \"generationConfig\": map[string]interface{}{\n            \"temperature\":    0.7,\n            \"maxOutputTokens\": 1024,\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        [\n            'parts' => [\n                ['text' => 'Hello, Gemini!']\n            ]\n        ]\n    ],\n    'generationConfig' => [\n        'temperature' => 0.7,\n        'maxOutputTokens' => 1024\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:generateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['candidates'][0]['content']['parts'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Hello! How can I assist you today?\"}\n        ]\n      },\n      \"finishReason\": \"STOP\",\n      \"safetyRatings\": [\n        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"probability\": \"NEGLIGIBLE\"}\n      ]\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 5,\n    \"candidatesTokenCount\": 10,\n    \"totalTokenCount\": 15\n  }\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:23:34.778Z"
  },
  "api-reference/gemini/stream-generate-content.mdx": {
    "sourceHash": "c9d5980e149e048a",
    "translations": {
      "zh": "---\ntitle: \"流式生成内容\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"使用 Google Gemini API 格式流式生成内容\"\n---\n\nGemini `generateContent` 接口的流式版本。返回 Server-Sent Events。\n\n## 路径参数\n\n<ParamField path=\"model\" type=\"string\" required>\n  模型名称 (例如 `gemini-2.5-pro`, `gemini-2.5-flash`)。\n</ParamField>\n\n## 查询参数\n\n<ParamField query=\"key\" type=\"string\">\n  API 密钥 (Header 认证的替代方案)。\n</ParamField>\n\n## 请求体\n\n与 [生成内容](/api-reference/gemini/generate-content) 相同。\n\n## 响应\n\n返回 JSON 对象流，每个对象包含部分响应。\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json 流式数据块\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"串流生成內容\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"使用 Google Gemini API 格式串流生成內容\"\n---\n\nGemini `generateContent` 端點的串流版本。返回 Server-Sent Events。\n\n## 路徑參數\n\n<ParamField path=\"model\" type=\"string\" required>\n  模型名稱（例如：`gemini-2.5-pro`、`gemini-2.5-flash`）。\n</ParamField>\n\n## 查詢參數\n\n<ParamField query=\"key\" type=\"string\">\n  API 金鑰（標頭驗證的替代方案）。\n</ParamField>\n\n## 請求主體\n\n與 [生成內容](/api-reference/gemini/generate-content) 相同。\n\n## 回應\n\n返回 JSON 物件串流，每個物件包含部分回應。\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Stream Chunk\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"コンテンツのストリーム生成\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Google Gemini API形式を使用してコンテンツ生成をストリーミングします\"\n---\n\nGeminiの `generateContent` エンドポイントのストリーミング版です。Server-Sent Events を返します。\n\n## パスパラメータ\n\n<ParamField path=\"model\" type=\"string\" required>\n  モデル名 (例: `gemini-2.5-pro`, `gemini-2.5-flash`)。\n</ParamField>\n\n## クエリパラメータ\n\n<ParamField query=\"key\" type=\"string\">\n  APIキー (ヘッダー認証の代替)。\n</ParamField>\n\n## リクエストボディ\n\n[コンテンツ生成](/api-reference/gemini/generate-content)と同じです。\n\n## レスポンス\n\n各オブジェクトに部分的なレスポンスが含まれる、JSONオブジェクトのストリームを返します。\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json ストリームチャンク\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"콘텐츠 생성 스트리밍\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Google Gemini API 형식을 사용하여 콘텐츠 생성을 스트리밍합니다.\"\n---\n\nGemini `generateContent` 엔드포인트의 스트리밍 버전입니다. Server-Sent Events를 반환합니다.\n\n## 경로 파라미터\n\n<ParamField path=\"model\" type=\"string\" required>\n  모델 이름 (예: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## 쿼리 파라미터\n\n<ParamField query=\"key\" type=\"string\">\n  API 키 (헤더 인증의 대안).\n</ParamField>\n\n## 요청 본문\n\n[콘텐츠 생성](/api-reference/gemini/generate-content)과 동일합니다.\n\n## 응답\n\n각각 부분적인 응답을 포함하는 JSON 객체의 스트림을 반환합니다.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Stream Chunk\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"Inhalte streamen\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Streamt die Inhaltsgenerierung im Google Gemini API-Format\"\n---\n\nStreaming-Version des Gemini generateContent-Endpunkts. Gibt Server-Sent Events zurück.\n\n## Pfadparameter\n\n<ParamField path=\"model\" type=\"string\" required>\n  Modellname (z. B. `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Abfrageparameter\n\n<ParamField query=\"key\" type=\"string\">\n  API-Key (Alternative zur Header-Authentifizierung).\n</ParamField>\n\n## Anfrage-Body\n\nGleich wie [Inhalte generieren](/api-reference/gemini/generate-content).\n\n## Antwort\n\nGibt einen Stream von JSON-Objekten zurück, die jeweils eine Teilantwort enthalten.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Stream Chunk\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Génération de contenu en streaming\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Génère du contenu en streaming en utilisant le format de l'API Google Gemini\"\n---\n\nVersion streaming du point de terminaison Gemini `generateContent`. Retourne des Server-Sent Events.\n\n## Paramètres de chemin\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nom du modèle (par ex., `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Paramètres de requête\n\n<ParamField query=\"key\" type=\"string\">\n  Clé API (alternative à l'authentification par en-tête).\n</ParamField>\n\n## Corps de la requête\n\nIdentique à [Générer du contenu](/api-reference/gemini/generate-content).\n\n## Réponse\n\nRetourne un flux d'objets JSON, chacun contenant une réponse partielle.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Fragment de flux\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Generación de contenido en streaming\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Transmite la generación de contenido utilizando el formato de la API de Google Gemini\"\n---\n\nVersión en streaming del endpoint `generateContent` de Gemini. Devuelve Server-Sent Events.\n\n## Parámetros de ruta\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nombre del modelo (por ejemplo, `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Parámetros de consulta\n\n<ParamField query=\"key\" type=\"string\">\n  Clave API (alternativa a la autenticación por encabezado).\n</ParamField>\n\n## Cuerpo de la solicitud\n\nIgual que [Generar contenido](/api-reference/gemini/generate-content).\n\n## Respuesta\n\nDevuelve un flujo de objetos JSON, cada uno de los cuales contiene una respuesta parcial.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Fragmento de stream\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Gerar Conteúdo via Stream\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Realiza o stream da geração de conteúdo usando o formato da API Google Gemini\"\n---\n\nVersão em streaming do endpoint `generateContent` do Gemini. Retorna Server-Sent Events.\n\n## Parâmetros de Caminho\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nome do modelo (ex: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Parâmetros de Consulta\n\n<ParamField query=\"key\" type=\"string\">\n  Chave de API (alternativa à autenticação via header).\n</ParamField>\n\n## Corpo da Requisição\n\nIgual a [Gerar Conteúdo](/api-reference/gemini/generate-content).\n\n## Resposta\n\nRetorna um stream de objetos JSON, cada um contendo uma resposta parcial.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Chunk do Stream\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"بث توليد المحتوى\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"يقوم ببث توليد المحتوى باستخدام تنسيق Google Gemini API\"\n---\n\nنسخة البث لنقطة نهاية Gemini `generateContent`. تُرجع Server-Sent Events.\n\n## معلمات المسار\n\n<ParamField path=\"model\" type=\"string\" required>\n  اسم النموذج (على سبيل المثال، `gemini-2.5-pro`، `gemini-2.5-flash`).\n</ParamField>\n\n## معلمات الاستعلام\n\n<ParamField query=\"key\" type=\"string\">\n  مفتاح API (بديل للمصادقة عبر الترويسة).\n</ParamField>\n\n## جسم الطلب\n\nنفس [توليد المحتوى](/api-reference/gemini/generate-content).\n\n## الاستجابة\n\nيُرجع دفقاً من كائنات JSON، يحتوي كل منها على استجابة جزئية.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Stream Chunk\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Tạo nội dung dạng Stream\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Truyền phát nội dung được tạo bằng định dạng Google Gemini API\"\n---\n\nPhiên bản streaming của endpoint Gemini `generateContent`. Trả về Server-Sent Events.\n\n## Tham số đường dẫn (Path Parameters)\n\n<ParamField path=\"model\" type=\"string\" required>\n  Tên model (ví dụ: `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Tham số truy vấn (Query Parameters)\n\n<ParamField query=\"key\" type=\"string\">\n  API key (phương thức thay thế cho xác thực qua header).\n</ParamField>\n\n## Request Body\n\nTương tự như [Generate Content](/api-reference/gemini/generate-content).\n\n## Phản hồi (Response)\n\nTrả về một luồng (stream) các đối tượng JSON, mỗi đối tượng chứa một phần của phản hồi.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Stream Chunk\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Stream Generate Content\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Melakukan streaming pembuatan konten menggunakan format Google Gemini API\"\n---\n\nVersi streaming dari endpoint Gemini generateContent. Mengembalikan Server-Sent Events.\n\n## Path Parameters\n\n<ParamField path=\"model\" type=\"string\" required>\n  Nama model (misalnya, `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Query Parameters\n\n<ParamField query=\"key\" type=\"string\">\n  API key (alternatif untuk autentikasi header).\n</ParamField>\n\n## Request Body\n\nSama dengan [Generate Content](/api-reference/gemini/generate-content).\n\n## Respons\n\nMengembalikan stream objek JSON, yang masing-masing berisi respons parsial.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Stream Chunk\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>",
      "tr": "---\ntitle: \"İçerik Oluşturma Akışı (Stream)\"\napi: \"POST /v1beta/models/{model}:streamGenerateContent\"\ndescription: \"Google Gemini API formatını kullanarak içerik oluşturma akışı sağlar\"\n---\n\nGemini generateContent uç noktasının akış (streaming) sürümüdür. Server-Sent Events döndürür.\n\n## Yol Parametreleri\n\n<ParamField path=\"model\" type=\"string\" required>\n  Model adı (örneğin, `gemini-2.5-pro`, `gemini-2.5-flash`).\n</ParamField>\n\n## Sorgu Parametreleri\n\n<ParamField query=\"key\" type=\"string\">\n  API anahtarı (header kimlik doğrulamasına alternatif).\n</ParamField>\n\n## İstek Gövdesi\n\n[İçerik Oluşturma](/api-reference/gemini/generate-content) ile aynıdır.\n\n## Yanıt\n\nHer biri kısmi bir yanıt içeren bir JSON nesneleri akışı döndürür.\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [\n      {\n        \"parts\": [{\"text\": \"Tell me a story\"}]\n      }\n    ]\n  }'\n```\n\n```python Python\nimport google.generativeai as genai\n\ngenai.configure(\n    api_key=\"sk-your-api-key\",\n    transport=\"rest\",\n    client_options={\"api_endpoint\": \"api.lemondata.cc\"}\n)\n\nmodel = genai.GenerativeModel(\"gemini-2.5-pro\")\nresponse = model.generate_content(\"Tell me a story\", stream=True)\n\nfor chunk in response:\n    print(chunk.text, end=\"\")\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key',\n  {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      contents: [{ parts: [{ text: 'Tell me a story' }] }]\n    })\n  }\n);\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  console.log(decoder.decode(value));\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"bufio\"\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"contents\": []map[string]interface{}{\n            {\"parts\": []map[string]string{{\"text\": \"Tell me a story\"}}},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\",\n        \"https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key\",\n        bytes.NewBuffer(jsonData))\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    scanner := bufio.NewScanner(resp.Body)\n    for scanner.Scan() {\n        fmt.Println(scanner.Text())\n    }\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'contents' => [\n        ['parts' => [['text' => 'Tell me a story']]]\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1beta/models/gemini-2.5-pro:streamGenerateContent?key=sk-your-api-key');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => ['Content-Type: application/json'],\n    CURLOPT_POSTFIELDS => json_encode($payload),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Akış Parçası\n{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"role\": \"model\",\n        \"parts\": [\n          {\"text\": \"Once upon a time\"}\n        ]\n      }\n    }\n  ]\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:23:53.322Z"
  },
  "api-reference/introduction.mdx": {
    "sourceHash": "26e432eb82f23b31",
    "translations": {
      "zh": "---\ntitle: 'API 参考'\ndescription: 'LemonData API 的完整参考文档'\n---\n\n## 概览\n\nLemonData API **兼容 OpenAI**，这意味着您只需更改 Base URL 即可使用官方 OpenAI SDK。我们还支持原生的 **Anthropic** 和 **Gemini** 请求格式。\n\n## Base URL\n\n```\nhttps://api.lemondata.cc\n```\n\n## 身份验证\n\n所有 API 端点都需要使用 Bearer token 进行身份验证：\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n从 [控制面板](https://lemondata.cc/dashboard) 获取您的 API 密钥。\n\n## 支持的端点\n\n### 聊天与文本生成\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | 兼容 OpenAI 的聊天补全 |\n| `/v1/messages` | POST | 兼容 Anthropic 的消息 API |\n| `/v1/responses` | POST | OpenAI Responses API |\n\n### 嵌入与重排序\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | 创建文本嵌入 |\n| `/v1/rerank` | POST | 重排序文档 |\n\n### 图像\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | 根据文本生成图像 |\n| `/v1/images/edits` | POST | 编辑图像 |\n\n### 音频\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | 文本转语音 (TTS) |\n| `/v1/audio/transcriptions` | POST | 语音转文本 (STT) |\n\n### 视频\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | 创建视频生成任务 |\n| `/v1/videos/generations/{id}` | GET | 获取视频任务状态 |\n\n### 音乐\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | 创建音乐生成任务 |\n| `/v1/music/generations/{id}` | GET | 获取音乐任务状态 |\n\n### 3D 生成\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | 创建 3D 模型生成任务 |\n| `/v1/3d/generations/{id}` | GET | 获取 3D 任务状态 |\n\n### 模型\n\n| 端点 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/models` | GET | 列出所有可用模型 |\n| `/v1/models/{model}` | GET | 获取特定模型信息 |\n\n## 响应格式\n\n所有响应都遵循一致的格式：\n\n### 成功响应\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### 路由透明度\n\n所有响应都包含一个带有渠道信息的 `_routing` 字段：\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| 字段 | 描述 |\n|-------|-------------|\n| `channel.id` | 使用的渠道标识符 |\n| `channel.provider` | 上游提供商 (openai, anthropic 等) |\n| `channel.channelType` | `PLATFORM` (LemonData) 或 `PRIVATE` (BYOK) |\n| `cached` | 响应是否来自缓存 |\n| `retryCount` | 重试次数（如果有） |\n\n### 错误响应\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## 速率限制\n\n速率限制基于角色，并可由管理员配置。默认值：\n\n| 角色 | 请求数/分钟 |\n|------|-------------|\n| 用户 | 60 |\n| 合作伙伴 | 300 |\n| VIP | 1,000 |\n| 管理员 | 无限制 |\n\n<Note>\n联系支持团队以获取自定义速率限制。具体数值可能因账户配置而异。\n</Note>\n\n所有响应中都包含速率限制标头：\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## OpenAPI 规范\n\n<Card\n  title=\"OpenAPI 规范\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  下载完整的 OpenAPI 3.0 规范\n</Card>",
      "zh-TW": "---\ntitle: 'API 參考'\ndescription: 'LemonData API 的完整參考指南'\n---\n\n## 概覽\n\nLemonData API **相容於 OpenAI**，這意味著您只需更改 Base URL 即可使用官方的 OpenAI SDK。我們也支援原生的 **Anthropic** 和 **Gemini** 請求格式。\n\n## Base URL\n\n```\nhttps://api.lemondata.cc\n```\n\n## 身份驗證\n\n所有 API 端點都需要使用 Bearer token 進行身份驗證：\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n從 [控制台](https://lemondata.cc/dashboard) 獲取您的 API 金鑰。\n\n## 支援的端點\n\n### 對話與文字生成\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | 相容於 OpenAI 的對話補全 |\n| `/v1/messages` | POST | 相容於 Anthropic 的訊息 API |\n| `/v1/responses` | POST | OpenAI Responses API |\n\n### 嵌入與重排序\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | 建立文字嵌入 |\n| `/v1/rerank` | POST | 重排序文件 |\n\n### 圖片\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | 從文字生成圖片 |\n| `/v1/images/edits` | POST | 編輯圖片 |\n\n### 音訊\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | 文字轉語音 (TTS) |\n| `/v1/audio/transcriptions` | POST | 語音轉文字 (STT) |\n\n### 影片\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | 建立影片生成任務 |\n| `/v1/videos/generations/{id}` | GET | 獲取影片任務狀態 |\n\n### 音樂\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | 建立音樂生成任務 |\n| `/v1/music/generations/{id}` | GET | 獲取音樂任務狀態 |\n\n### 3D 生成\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | 建立 3D 模型生成任務 |\n| `/v1/3d/generations/{id}` | GET | 獲取 3D 任務狀態 |\n\n### 模型\n\n| 端點 | 方法 | 描述 |\n|----------|--------|-------------|\n| `/v1/models` | GET | 列出所有可用模型 |\n| `/v1/models/{model}` | GET | 獲取特定模型資訊 |\n\n## 回應格式\n\n所有回應均遵循一致的格式：\n\n### 成功回應\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### 路由透明度\n\n所有回應都包含一個帶有渠道資訊的 `_routing` 欄位：\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| 欄位 | 描述 |\n|-------|-------------|\n| `channel.id` | 使用的渠道識別碼 |\n| `channel.provider` | 上游供應商 (openai, anthropic 等) |\n| `channel.channelType` | `PLATFORM` (LemonData) 或 `PRIVATE` (BYOK) |\n| `cached` | 回應是否來自快取 |\n| `retryCount` | 重試次數（如有） |\n\n### 錯誤回應\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## 速率限制\n\n速率限制基於角色，並可由管理員配置。預設值：\n\n| 角色 | 每分鐘請求數 |\n|------|-------------|\n| 使用者 | 60 |\n| 合作夥伴 | 300 |\n| VIP | 1,000 |\n| 管理員 | 無限制 |\n\n<Note>\n聯絡支援團隊以獲取自定義速率限制。確切數值可能因帳戶配置而異。\n</Note>\n\n所有回應中都包含速率限制標頭：\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## OpenAPI 規範\n\n<Card\n  title=\"OpenAPI 規範\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  下載完整的 OpenAPI 3.0 規範\n</Card>",
      "ja": "---\ntitle: 'APIリファレンス'\ndescription: 'LemonData APIの完全なリファレンス'\n---\n\n## 概要\n\nLemonData APIは**OpenAI互換**です。つまり、ベースURLを変更するだけで公式のOpenAI SDKを使用できます。また、ネイティブの**Anthropic**および**Gemini**のリクエスト形式もサポートしています。\n\n## ベースURL\n\n```\nhttps://api.lemondata.cc\n```\n\n## 認証\n\nすべてのAPIエンドポイントは、Bearerトークンを使用した認証が必要です。\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n[ダッシュボード](https://lemondata.cc/dashboard)からAPIキーを取得してください。\n\n## サポートされているエンドポイント\n\n### チャットとテキスト生成\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | OpenAI互換のチャット完了 |\n| `/v1/messages` | POST | Anthropic互換のメッセージAPI |\n| `/v1/responses` | POST | OpenAI Responses API |\n\n### 埋め込みとリランク\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | テキスト埋め込みの作成 |\n| `/v1/rerank` | POST | ドキュメントのリランク |\n\n### 画像\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | テキストからの画像生成 |\n| `/v1/images/edits` | POST | 画像の編集 |\n\n### オーディオ\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | テキスト読み上げ (TTS) |\n| `/v1/audio/transcriptions` | POST | 音声文字変換 (STT) |\n\n### ビデオ\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | ビデオ生成タスクの作成 |\n| `/v1/videos/generations/{id}` | GET | ビデオタスクのステータス取得 |\n\n### 音楽\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | 音楽生成タスクの作成 |\n| `/v1/music/generations/{id}` | GET | 音楽タスクのステータス取得 |\n\n### 3D生成\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | 3Dモデル生成タスクの作成 |\n| `/v1/3d/generations/{id}` | GET | 3Dタスクのステータス取得 |\n\n### モデル\n\n| エンドポイント | メソッド | 説明 |\n|----------|--------|-------------|\n| `/v1/models` | GET | 利用可能なすべてのモデルをリスト |\n| `/v1/models/{model}` | GET | 特定のモデル情報の取得 |\n\n## レスポンス形式\n\nすべてのレスポンスは一貫した形式に従います。\n\n### 成功レスポンス\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### ルーティングの透明性\n\nすべてのレスポンスには、チャネル情報を含む `_routing` フィールドが含まれます。\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| フィールド | 説明 |\n|-------|-------------|\n| `channel.id` | 使用されたチャネル識別子 |\n| `channel.provider` | アップストリームプロバイダー (openai, anthropicなど) |\n| `channel.channelType` | `PLATFORM` (LemonData) または `PRIVATE` (BYOK) |\n| `cached` | レスポンスがキャッシュから提供されたかどうか |\n| `retryCount` | リトライ試行回数（ある場合） |\n\n### エラーレスポンス\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## レート制限\n\nレート制限はロールベースであり、管理者によって設定可能です。デフォルト値は以下の通りです。\n\n| ロール | リクエスト数/分 |\n|------|-------------|\n| User | 60 |\n| Partner | 300 |\n| VIP | 1,000 |\n| Admin | 無制限 |\n\n<Note>\nカスタムレート制限についてはサポートにお問い合わせください。正確な値はアカウント設定によって異なる場合があります。\n</Note>\n\nレート制限ヘッダーはすべてのレスポンスに含まれます。\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## OpenAPI仕様\n\n<Card\n  title=\"OpenAPI仕様\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  完全なOpenAPI 3.0仕様をダウンロード\n</Card>",
      "ko": "---\ntitle: 'API 레퍼런스'\ndescription: 'LemonData API에 대한 전체 레퍼런스'\n---\n\n## 개요\n\nLemonData API는 **OpenAI와 호환**되므로, 베이스 URL 변경만으로 공식 OpenAI SDK를 사용할 수 있습니다. 또한 네이티브 **Anthropic** 및 **Gemini** 요청 형식도 지원합니다.\n\n## 베이스 URL\n\n```\nhttps://api.lemondata.cc\n```\n\n## 인증\n\n모든 API 엔드포인트는 Bearer 토큰을 사용한 인증이 필요합니다:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n[대시보드](https://lemondata.cc/dashboard)에서 API 키를 확인하세요.\n\n## 지원되는 엔드포인트\n\n### 채팅 및 텍스트 생성\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | OpenAI 호환 채팅 완성 |\n| `/v1/messages` | POST | Anthropic 호환 메시지 API |\n| `/v1/responses` | POST | OpenAI Responses API |\n\n### 임베딩 및 리랭크(Rerank)\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | 텍스트 임베딩 생성 |\n| `/v1/rerank` | POST | 문서 리랭크 |\n\n### 이미지\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | 텍스트로 이미지 생성 |\n| `/v1/images/edits` | POST | 이미지 편집 |\n\n### 오디오\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | 텍스트 음성 변환 (TTS) |\n| `/v1/audio/transcriptions` | POST | 음성 텍스트 변환 (STT) |\n\n### 비디오\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | 비디오 생성 작업 생성 |\n| `/v1/videos/generations/{id}` | GET | 비디오 작업 상태 조회 |\n\n### 음악\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | 음악 생성 작업 생성 |\n| `/v1/music/generations/{id}` | GET | 음악 작업 상태 조회 |\n\n### 3D 생성\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | 3D 모델 생성 작업 생성 |\n| `/v1/3d/generations/{id}` | GET | 3D 작업 상태 조회 |\n\n### 모델\n\n| 엔드포인트 | 메서드 | 설명 |\n|----------|--------|-------------|\n| `/v1/models` | GET | 사용 가능한 모든 모델 목록 조회 |\n| `/v1/models/{model}` | GET | 특정 모델 정보 조회 |\n\n## 응답 형식\n\n모든 응답은 일관된 형식을 따릅니다:\n\n### 성공 응답\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### 라우팅 투명성\n\n모든 응답에는 채널 정보가 포함된 `_routing` 필드가 포함됩니다:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| 필드 | 설명 |\n|-------|-------------|\n| `channel.id` | 사용된 채널 식별자 |\n| `channel.provider` | 업스트림 제공자 (openai, anthropic 등) |\n| `channel.channelType` | `PLATFORM` (LemonData) 또는 `PRIVATE` (BYOK) |\n| `cached` | 응답이 캐시에서 제공되었는지 여부 |\n| `retryCount` | 재시도 횟수 (있는 경우) |\n\n### 오류 응답\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## 속도 제한 (Rate Limits)\n\n속도 제한은 역할 기반이며 관리자가 설정할 수 있습니다. 기본값은 다음과 같습니다:\n\n| 역할 | 분당 요청 수 (Requests/min) |\n|------|-------------|\n| 사용자 | 60 |\n| 파트너 | 300 |\n| VIP | 1,000 |\n| 관리자 | 무제한 |\n\n<Note>\n커스텀 속도 제한은 고객 지원에 문의하세요. 정확한 값은 계정 설정에 따라 다를 수 있습니다.\n</Note>\n\n모든 응답에는 속도 제한 헤더가 포함됩니다:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## OpenAPI 사양\n\n<Card\n  title=\"OpenAPI 사양\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  전체 OpenAPI 3.0 사양 다운로드\n</Card>",
      "de": "---\ntitle: 'API-Referenz'\ndescription: 'Vollständige Referenz für die LemonData-API'\n---\n\n## Übersicht\n\nDie LemonData-API ist **OpenAI-kompatibel**, was bedeutet, dass Sie das offizielle OpenAI-SDK mit einer einfachen Änderung der Basis-URL verwenden können. Wir unterstützen auch native **Anthropic**- und **Gemini**-Anfrageformate.\n\n## Basis-URL\n\n```\nhttps://api.lemondata.cc\n```\n\n## Authentifizierung\n\nAlle API-Endpunkte erfordern eine Authentifizierung mittels Bearer-Token:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nHolen Sie sich Ihren API-Key im [Dashboard](https://lemondata.cc/dashboard).\n\n## Unterstützte Endpunkte\n\n### Chat & Textgenerierung\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | OpenAI-kompatible Chat-Vervollständigungen |\n| `/v1/messages` | POST | Anthropic-kompatible Messages-API |\n| `/v1/responses` | POST | OpenAI Responses-API |\n\n### Embeddings & Rerank\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Text-Embeddings erstellen |\n| `/v1/rerank` | POST | Dokumente reranken |\n\n### Bilder\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Bilder aus Text generieren |\n| `/v1/images/edits` | POST | Bilder bearbeiten |\n\n### Audio\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Text-to-Speech (TTS) |\n| `/v1/audio/transcriptions` | POST | Speech-to-Text (STT) |\n\n### Video\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Video-Generierungs-Task erstellen |\n| `/v1/videos/generations/{id}` | GET | Status des Video-Tasks abrufen |\n\n### Musik\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Musik-Generierungs-Task erstellen |\n| `/v1/music/generations/{id}` | GET | Status des Musik-Tasks abrufen |\n\n### 3D-Generierung\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | 3D-Modell-Generierungs-Task erstellen |\n| `/v1/3d/generations/{id}` | GET | Status des 3D-Tasks abrufen |\n\n### Modelle\n\n| Endpunkt | Methode | Beschreibung |\n|----------|--------|-------------|\n| `/v1/models` | GET | Alle verfügbaren Modelle auflisten |\n| `/v1/models/{model}` | GET | Spezifische Modellinformationen abrufen |\n\n## Antwortformat\n\nAlle Antworten folgen einem einheitlichen Format:\n\n### Erfolgreiche Antwort\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Routing-Transparenz\n\nAlle Antworten enthalten ein `_routing`-Feld mit Kanalinformationen:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Feld | Beschreibung |\n|-------|-------------|\n| `channel.id` | Verwendete Kanal-ID |\n| `channel.provider` | Upstream-Provider (openai, anthropic, etc.) |\n| `channel.channelType` | `PLATFORM` (LemonData) oder `PRIVATE` (BYOK) |\n| `cached` | Gibt an, ob die Antwort aus dem Cache geladen wurde |\n| `retryCount` | Anzahl der Wiederholungsversuche (falls vorhanden) |\n\n### Fehlerantwort\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## Rate Limits\n\nRate Limits sind rollenbasiert und von Administratoren konfigurierbar. Standardwerte:\n\n| Rolle | Anfragen/Min. |\n|------|-------------|\n| Benutzer | 60 |\n| Partner | 300 |\n| VIP | 1.000 |\n| Admin | Unbegrenzt |\n\n<Note>\nKontaktieren Sie den Support für individuelle Rate Limits. Die genauen Werte können je nach Kontokonfiguration variieren.\n</Note>\n\nRate-Limit-Header sind in allen Antworten enthalten:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## OpenAPI-Spezifikation\n\n<Card\n  title=\"OpenAPI-Spezifikation\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Laden Sie die vollständige OpenAPI 3.0-Spezifikation herunter\n</Card>",
      "fr": "---\ntitle: 'Référence API'\ndescription: 'Référence complète pour l\\'API LemonData'\n---\n\n## Aperçu\n\nL'API LemonData est **compatible avec OpenAI**, ce qui signifie que vous pouvez utiliser le SDK OpenAI officiel en changeant simplement l'URL de base. Nous prenons également en charge les formats de requête natifs **Anthropic** et **Gemini**.\n\n## URL de base\n\n```\nhttps://api.lemondata.cc\n```\n\n## Authentification\n\nTous les points de terminaison de l'API nécessitent une authentification à l'aide d'un token Bearer :\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nObtenez votre clé API depuis le [Tableau de bord](https://lemondata.cc/dashboard).\n\n## Points de terminaison pris en charge\n\n### Chat et génération de texte\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | Complétions de chat compatibles OpenAI |\n| `/v1/messages` | POST | API de messages compatible Anthropic |\n| `/v1/responses` | POST | API de réponses OpenAI |\n\n### Embeddings et Rerank\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Créer des embeddings de texte |\n| `/v1/rerank` | POST | Réorganiser (Rerank) des documents |\n\n### Images\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Générer des images à partir de texte |\n| `/v1/images/edits` | POST | Modifier des images |\n\n### Audio\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Synthèse vocale (TTS) |\n| `/v1/audio/transcriptions` | POST | Reconnaissance vocale (STT) |\n\n### Vidéo\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Créer une tâche de génération de vidéo |\n| `/v1/videos/generations/{id}` | GET | Obtenir le statut de la tâche vidéo |\n\n### Musique\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Créer une tâche de génération de musique |\n| `/v1/music/generations/{id}` | GET | Obtenir le statut de la tâche musicale |\n\n### Génération 3D\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | Créer une tâche de génération de modèle 3D |\n| `/v1/3d/generations/{id}` | GET | Obtenir le statut de la tâche 3D |\n\n### Modèles\n\n| Point de terminaison | Méthode | Description |\n|----------|--------|-------------|\n| `/v1/models` | GET | Lister tous les modèles disponibles |\n| `/v1/models/{model}` | GET | Obtenir les informations d'un modèle spécifique |\n\n## Format de réponse\n\nToutes les réponses suivent un format cohérent :\n\n### Réponse de succès\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Transparence du routage\n\nToutes les réponses incluent un champ `_routing` avec les informations sur le canal :\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Champ | Description |\n|-------|-------------|\n| `channel.id` | Identifiant du canal utilisé |\n| `channel.provider` | Fournisseur en amont (openai, anthropic, etc.) |\n| `channel.channelType` | `PLATFORM` (LemonData) ou `PRIVATE` (BYOK) |\n| `cached` | Indique si la réponse a été servie depuis le cache |\n| `retryCount` | Nombre de tentatives de réessai (le cas échéant) |\n\n### Réponse d'erreur\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## Limites de débit\n\nLes limites de débit sont basées sur les rôles et configurables par les administrateurs. Valeurs par défaut :\n\n| Rôle | Requêtes/min |\n|------|-------------|\n| Utilisateur | 60 |\n| Partenaire | 300 |\n| VIP | 1,000 |\n| Admin | Illimité |\n\n<Note>\nContactez le support pour des limites de débit personnalisées. Les valeurs exactes peuvent varier selon la configuration du compte.\n</Note>\n\nLes en-têtes de limite de débit sont inclus dans toutes les réponses :\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## Spécification OpenAPI\n\n<Card\n  title=\"Spécification OpenAPI\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Téléchargez la spécification OpenAPI 3.0 complète\n</Card>",
      "es": "---\ntitle: 'Referencia de la API'\ndescription: 'Referencia completa para la API de LemonData'\n---\n\n## Descripción general\n\nLa API de LemonData es **compatible con OpenAI**, lo que significa que puedes usar el SDK oficial de OpenAI con solo cambiar la URL base. También admitimos formatos de solicitud nativos de **Anthropic** y **Gemini**.\n\n## URL base\n\n```\nhttps://api.lemondata.cc\n```\n\n## Autenticación\n\nTodos los endpoints de la API requieren autenticación mediante un token Bearer:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nObtén tu clave de API desde el [Dashboard](https://lemondata.cc/dashboard).\n\n## Endpoints compatibles\n\n### Chat y generación de texto\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | Completions de chat compatibles con OpenAI |\n| `/v1/messages` | POST | API de mensajes compatible con Anthropic |\n| `/v1/responses` | POST | API de respuestas de OpenAI |\n\n### Embeddings y Rerank\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Crear embeddings de texto |\n| `/v1/rerank` | POST | Reclasificar (Rerank) documentos |\n\n### Imágenes\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Generar imágenes a partir de texto |\n| `/v1/images/edits` | POST | Editar imágenes |\n\n### Audio\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Texto a voz (TTS) |\n| `/v1/audio/transcriptions` | POST | Voz a texto (STT) |\n\n### Video\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Crear tarea de generación de video |\n| `/v1/videos/generations/{id}` | GET | Obtener el estado de la tarea de video |\n\n### Música\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Crear tarea de generación de música |\n| `/v1/music/generations/{id}` | GET | Obtener el estado de la tarea de música |\n\n### Generación 3D\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | Crear tarea de generación de modelos 3D |\n| `/v1/3d/generations/{id}` | GET | Obtener el estado de la tarea 3D |\n\n### Modelos\n\n| Endpoint | Método | Descripción |\n|----------|--------|-------------|\n| `/v1/models` | GET | Listar todos los modelos disponibles |\n| `/v1/models/{model}` | GET | Obtener información de un modelo específico |\n\n## Formato de respuesta\n\nTodas las respuestas siguen un formato consistente:\n\n### Respuesta exitosa\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Transparencia de enrutamiento\n\nTodas las respuestas incluyen un campo `_routing` con información del canal:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Campo | Descripción |\n|-------|-------------|\n| `channel.id` | Identificador de canal utilizado |\n| `channel.provider` | Proveedor ascendente (openai, anthropic, etc.) |\n| `channel.channelType` | `PLATFORM` (LemonData) o `PRIVATE` (BYOK) |\n| `cached` | Si la respuesta se sirvió desde la caché |\n| `retryCount` | Número de intentos de reintento (si los hay) |\n\n### Respuesta de error\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## Límites de tasa (Rate Limits)\n\nLos límites de tasa se basan en roles y son configurables por los administradores. Valores por defecto:\n\n| Rol | Solicitudes/min |\n|------|-------------|\n| Usuario | 60 |\n| Socio | 300 |\n| VIP | 1,000 |\n| Admin | Ilimitado |\n\n<Note>\nContacta con soporte para límites de tasa personalizados. Los valores exactos pueden variar según la configuración de la cuenta.\n</Note>\n\nLos encabezados de límite de tasa se incluyen en todas las respuestas:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## Especificación OpenAPI\n\n<Card\n  title=\"Especificación OpenAPI\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Descarga la especificación completa de OpenAPI 3.0\n</Card>",
      "pt": "---\ntitle: 'Referência da API'\ndescription: 'Referência completa para a API LemonData'\n---\n\n## Visão Geral\n\nA API LemonData é **compatível com OpenAI**, o que significa que você pode usar o SDK oficial da OpenAI apenas alterando a URL base. Também oferecemos suporte aos formatos de requisição nativos da **Anthropic** e **Gemini**.\n\n## URL Base\n\n```\nhttps://api.lemondata.cc\n```\n\n## Autenticação\n\nTodos os endpoints da API exigem autenticação usando um token Bearer:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nObtenha sua chave de API no [Dashboard](https://lemondata.cc/dashboard).\n\n## Endpoints Suportados\n\n### Chat e Geração de Texto\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | Completions de chat compatíveis com OpenAI |\n| `/v1/messages` | POST | API de mensagens compatível com Anthropic |\n| `/v1/responses` | POST | API de Respostas da OpenAI |\n\n### Embeddings e Rerank\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Criar embeddings de texto |\n| `/v1/rerank` | POST | Reordenar (Rerank) documentos |\n\n### Imagens\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Gerar imagens a partir de texto |\n| `/v1/images/edits` | POST | Editar imagens |\n\n### Áudio\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Texto para fala (TTS) |\n| `/v1/audio/transcriptions` | POST | Fala para texto (STT) |\n\n### Vídeo\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Criar tarefa de geração de vídeo |\n| `/v1/videos/generations/{id}` | GET | Obter status da tarefa de vídeo |\n\n### Música\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Criar tarefa de geração de música |\n| `/v1/music/generations/{id}` | GET | Obter status da tarefa de música |\n\n### Geração 3D\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | Criar tarefa de geração de modelo 3D |\n| `/v1/3d/generations/{id}` | GET | Obter status da tarefa 3D |\n\n### Modelos\n\n| Endpoint | Método | Descrição |\n|----------|--------|-------------|\n| `/v1/models` | GET | Listar todos os modelos disponíveis |\n| `/v1/models/{model}` | GET | Obter informações de um modelo específico |\n\n## Formato de Resposta\n\nTodas as respostas seguem um formato consistente:\n\n### Resposta de Sucesso\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Transparência de Roteamento\n\nTodas as respostas incluem um campo `_routing` com informações do canal:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Campo | Descrição |\n|-------|-------------|\n| `channel.id` | Identificador do canal utilizado |\n| `channel.provider` | Provedor upstream (openai, anthropic, etc.) |\n| `channel.channelType` | `PLATFORM` (LemonData) ou `PRIVATE` (BYOK) |\n| `cached` | Se a resposta foi servida a partir do cache |\n| `retryCount` | Número de tentativas de reenvio (se houver) |\n\n### Resposta de Erro\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## Limites de Taxa (Rate Limits)\n\nOs limites de taxa são baseados em funções e configuráveis por administradores. Valores padrão:\n\n| Função | Requisições/min |\n|------|-------------|\n| Usuário | 60 |\n| Parceiro | 300 |\n| VIP | 1,000 |\n| Admin | Ilimitado |\n\n<Note>\nEntre em contato com o suporte para limites de taxa personalizados. Os valores exatos podem variar conforme a configuração da conta.\n</Note>\n\nOs cabeçalhos de limite de taxa estão incluídos em todas as respostas:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## Especificação OpenAPI\n\n<Card\n  title=\"Especificação OpenAPI\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Baixe a especificação completa do OpenAPI 3.0\n</Card>",
      "ar": "---\ntitle: 'مرجع API'\ndescription: 'مرجع كامل لـ LemonData API'\n---\n\n## نظرة عامة\n\nإن LemonData API **متوافق مع OpenAI**، مما يعني أنه يمكنك استخدام OpenAI SDK الرسمي بمجرد تغيير URL الأساسي. كما ندعم أيضاً تنسيقات طلبات **Anthropic** و **Gemini** الأصلية.\n\n## URL الأساسي\n\n```\nhttps://api.lemondata.cc\n```\n\n## المصادقة\n\nتتطلب جميع نقاط نهاية API المصادقة باستخدام Bearer token:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nاحصل على مفتاح API الخاص بك من [لوحة التحكم](https://lemondata.cc/dashboard).\n\n## نقاط النهاية المدعومة\n\n### الدردشة وتوليد النصوص\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | إكمال الدردشة المتوافق مع OpenAI |\n| `/v1/messages` | POST | API الرسائل المتوافق مع Anthropic |\n| `/v1/responses` | POST | OpenAI Responses API |\n\n### التضمينات (Embeddings) وإعادة التصنيف (Rerank)\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | إنشاء تضمينات النصوص |\n| `/v1/rerank` | POST | إعادة تصنيف المستندات |\n\n### الصور\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | توليد الصور من النصوص |\n| `/v1/images/edits` | POST | تعديل الصور |\n\n### الصوت\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | تحويل النص إلى كلام (TTS) |\n| `/v1/audio/transcriptions` | POST | تحويل الكلام إلى نص (STT) |\n\n### الفيديو\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | إنشاء مهمة توليد فيديو |\n| `/v1/videos/generations/{id}` | GET | الحصول على حالة مهمة الفيديو |\n\n### الموسيقى\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | إنشاء مهمة توليد موسيقى |\n| `/v1/music/generations/{id}` | GET | الحصول على حالة مهمة الموسيقى |\n\n### توليد ثلاثي الأبعاد (3D)\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | إنشاء مهمة توليد نموذج ثلاثي الأبعاد |\n| `/v1/3d/generations/{id}` | GET | الحصول على حالة مهمة 3D |\n\n### النماذج\n\n| نقطة النهاية | الطريقة | الوصف |\n|----------|--------|-------------|\n| `/v1/models` | GET | قائمة بجميع النماذج المتاحة |\n| `/v1/models/{model}` | GET | الحصول على معلومات نموذج محدد |\n\n## تنسيق الاستجابة\n\nتتبع جميع الاستجابات تنسيقاً ثابتاً:\n\n### استجابة النجاح\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### شفافية التوجيه (Routing)\n\nتتضمن جميع الاستجابات حقل `_routing` مع معلومات القناة:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| الحقل | الوصف |\n|-------|-------------|\n| `channel.id` | معرف القناة المستخدم |\n| `channel.provider` | المزود الرئيسي (openai، anthropic، إلخ.) |\n| `channel.channelType` | `PLATFORM` (LemonData) أو `PRIVATE` (BYOK) |\n| `cached` | ما إذا كانت الاستجابة قد تمت خدمتها من التخزين المؤقت (cache) |\n| `retryCount` | عدد محاولات إعادة المحاولة (إن وجدت) |\n\n### استجابة الخطأ\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## حدود المعدل (Rate Limits)\n\nتعتمد حدود المعدل على الأدوار وهي قابلة للتكوين من قبل المسؤولين. القيم الافتراضية:\n\n| الدور | الطلبات/الدقيقة |\n|------|-------------|\n| مستخدم | 60 |\n| شريك | 300 |\n| VIP | 1,000 |\n| مسؤول | غير محدود |\n\n<Note>\nاتصل بالدعم للحصول على حدود معدل مخصصة. قد تختلف القيم الدقيقة حسب تكوين الحساب.\n</Note>\n\nيتم تضمين رؤوس (headers) حدود المعدل في جميع الاستجابات:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## مواصفات OpenAPI\n\n<Card\n  title=\"مواصفات OpenAPI\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  قم بتنزيل مواصفات OpenAPI 3.0 الكاملة\n</Card>",
      "vi": "---\ntitle: 'Tài liệu tham khảo API'\ndescription: 'Tài liệu tham khảo đầy đủ cho LemonData API'\n---\n\n## Tổng quan\n\nLemonData API **tương thích với OpenAI**, điều đó có nghĩa là bạn có thể sử dụng OpenAI SDK chính thức chỉ với việc thay đổi URL cơ sở. Chúng tôi cũng hỗ trợ các định dạng yêu cầu gốc của **Anthropic** và **Gemini**.\n\n## URL cơ sở\n\n```\nhttps://api.lemondata.cc\n```\n\n## Xác thực\n\nTất cả các endpoint API đều yêu cầu xác thực bằng Bearer token:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nLấy API key của bạn từ [Dashboard](https://lemondata.cc/dashboard).\n\n## Các Endpoint được hỗ trợ\n\n### Chat & Tạo văn bản\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | Hoàn thiện chat tương thích với OpenAI |\n| `/v1/messages` | POST | API tin nhắn tương thích với Anthropic |\n| `/v1/responses` | POST | API phản hồi của OpenAI |\n\n### Embeddings & Rerank\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Tạo text embeddings |\n| `/v1/rerank` | POST | Xếp hạng lại tài liệu |\n\n### Hình ảnh\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Tạo hình ảnh từ văn bản |\n| `/v1/images/edits` | POST | Chỉnh sửa hình ảnh |\n\n### Âm thanh\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Chuyển đổi văn bản thành giọng nói (TTS) |\n| `/v1/audio/transcriptions` | POST | Chuyển đổi giọng nói thành văn bản (STT) |\n\n### Video\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Tạo tác vụ tạo video |\n| `/v1/videos/generations/{id}` | GET | Lấy trạng thái tác vụ video |\n\n### Âm nhạc\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Tạo tác vụ tạo nhạc |\n| `/v1/music/generations/{id}` | GET | Lấy trạng thái tác vụ nhạc |\n\n### Tạo mô hình 3D\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | Tạo tác vụ tạo mô hình 3D |\n| `/v1/3d/generations/{id}` | GET | Lấy trạng thái tác vụ 3D |\n\n### Mô hình\n\n| Endpoint | Phương thức | Mô tả |\n|----------|--------|-------------|\n| `/v1/models` | GET | Liệt kê tất cả các mô hình có sẵn |\n| `/v1/models/{model}` | GET | Lấy thông tin mô hình cụ thể |\n\n## Định dạng phản hồi\n\nTất cả các phản hồi đều tuân theo một định dạng nhất quán:\n\n### Phản hồi thành công\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Tính minh bạch trong điều hướng (Routing)\n\nTất cả các phản hồi đều bao gồm trường `_routing` với thông tin kênh:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Trường | Mô tả |\n|-------|-------------|\n| `channel.id` | Định danh kênh được sử dụng |\n| `channel.provider` | Nhà cung cấp thượng nguồn (openai, anthropic, v.v.) |\n| `channel.channelType` | `PLATFORM` (LemonData) hoặc `PRIVATE` (BYOK) |\n| `cached` | Liệu phản hồi có được cung cấp từ bộ nhớ đệm (cache) hay không |\n| `retryCount` | Số lần thử lại (nếu có) |\n\n### Phản hồi lỗi\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## Giới hạn tốc độ (Rate Limits)\n\nGiới hạn tốc độ dựa trên vai trò và có thể cấu hình bởi quản trị viên. Các giá trị mặc định:\n\n| Vai trò | Yêu cầu/phút |\n|------|-------------|\n| Người dùng | 60 |\n| Đối tác | 300 |\n| VIP | 1,000 |\n| Quản trị viên | Không giới hạn |\n\n<Note>\nLiên hệ bộ phận hỗ trợ để biết giới hạn tốc độ tùy chỉnh. Các giá trị chính xác có thể thay đổi tùy theo cấu hình tài khoản.\n</Note>\n\nCác header giới hạn tốc độ được bao gồm trong tất cả các phản hồi:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## Đặc tả OpenAPI\n\n<Card\n  title=\"Đặc tả OpenAPI\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Tải xuống bản đặc tả OpenAPI 3.0 đầy đủ\n</Card>",
      "id": "---\ntitle: 'Referensi API'\ndescription: 'Referensi lengkap untuk API LemonData'\n---\n\n## Ringkasan\n\nAPI LemonData bersifat **OpenAI-compatible**, yang berarti Anda dapat menggunakan SDK resmi OpenAI hanya dengan mengubah base URL. Kami juga mendukung format permintaan asli **Anthropic** dan **Gemini**.\n\n## Base URL\n\n```\nhttps://api.lemondata.cc\n```\n\n## Autentikasi\n\nSemua endpoint API memerlukan autentikasi menggunakan token Bearer:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nDapatkan API key Anda dari [Dashboard](https://lemondata.cc/dashboard).\n\n## Endpoint yang Didukung\n\n### Chat & Generasi Teks\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | Chat completion yang kompatibel dengan OpenAI |\n| `/v1/messages` | POST | API pesan yang kompatibel dengan Anthropic |\n| `/v1/responses` | POST | API Respons OpenAI |\n\n### Embeddings & Rerank\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Buat embedding teks |\n| `/v1/rerank` | POST | Urutkan ulang (rerank) dokumen |\n\n### Gambar\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Hasilkan gambar dari teks |\n| `/v1/images/edits` | POST | Edit gambar |\n\n### Audio\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Text-to-speech (TTS) |\n| `/v1/audio/transcriptions` | POST | Speech-to-text (STT) |\n\n### Video\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Buat tugas pembuatan video |\n| `/v1/videos/generations/{id}` | GET | Dapatkan status tugas video |\n\n### Musik\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Buat tugas pembuatan musik |\n| `/v1/music/generations/{id}` | GET | Dapatkan status tugas musik |\n\n### Generasi 3D\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | Buat tugas pembuatan model 3D |\n| `/v1/3d/generations/{id}` | GET | Dapatkan status tugas 3D |\n\n### Model\n\n| Endpoint | Metode | Deskripsi |\n|----------|--------|-------------|\n| `/v1/models` | GET | Daftar semua model yang tersedia |\n| `/v1/models/{model}` | GET | Dapatkan info model spesifik |\n\n## Format Respons\n\nSemua respons mengikuti format yang konsisten:\n\n### Respons Berhasil\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Transparansi Routing\n\nSemua respons menyertakan field `_routing` dengan informasi saluran (channel):\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Field | Deskripsi |\n|-------|-------------|\n| `channel.id` | Pengidentifikasi saluran yang digunakan |\n| `channel.provider` | Penyedia upstream (openai, anthropic, dll.) |\n| `channel.channelType` | `PLATFORM` (LemonData) atau `PRIVATE` (BYOK) |\n| `cached` | Apakah respons dilayani dari cache |\n| `retryCount` | Jumlah upaya percobaan ulang (jika ada) |\n\n### Respons Kesalahan (Error)\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## Batasan Laju (Rate Limits)\n\nBatasan laju (rate limits) berbasis peran dan dapat dikonfigurasi oleh administrator. Nilai default:\n\n| Peran | Permintaan/menit |\n|------|-------------|\n| User | 60 |\n| Partner | 300 |\n| VIP | 1,000 |\n| Admin | Tanpa Batas |\n\n<Note>\nHubungi dukungan untuk batasan laju khusus. Nilai tepatnya dapat bervariasi tergantung konfigurasi akun.\n</Note>\n\nHeader batasan laju disertakan dalam semua respons:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## Spesifikasi OpenAPI\n\n<Card\n  title=\"Spesifikasi OpenAPI\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Unduh spesifikasi lengkap OpenAPI 3.0\n</Card>",
      "tr": "---\ntitle: 'API Referansı'\ndescription: 'LemonData API için tam referans'\n---\n\n## Genel Bakış\n\nLemonData API, **OpenAI uyumludur**; bu, resmi OpenAI SDK'sını yalnızca bir temel URL değişikliği ile kullanabileceğiniz anlamına gelir. Ayrıca yerel **Anthropic** ve **Gemini** istek formatlarını da destekliyoruz.\n\n## Base URL\n\n```\nhttps://api.lemondata.cc\n```\n\n## Kimlik Doğrulama\n\nTüm API uç noktaları, bir Bearer token kullanarak kimlik doğrulaması gerektirir:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\nAPI anahtarınızı [Dashboard](https://lemondata.cc/dashboard) üzerinden alabilirsiniz.\n\n## Desteklenen Uç Noktalar\n\n### Sohbet ve Metin Oluşturma\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/chat/completions` | POST | OpenAI uyumlu sohbet tamamlama |\n| `/v1/messages` | POST | Anthropic uyumlu mesajlar API'si |\n| `/v1/responses` | POST | OpenAI Yanıtlar API'si |\n\n### Embedding'ler ve Rerank\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/embeddings` | POST | Metin embedding'leri oluşturun |\n| `/v1/rerank` | POST | Belgeleri yeniden sıralayın (Rerank) |\n\n### Görseller\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/images/generations` | POST | Metinden görsel oluşturun |\n| `/v1/images/edits` | POST | Görselleri düzenleyin |\n\n### Ses\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/audio/speech` | POST | Metinden sese (TTS) |\n| `/v1/audio/transcriptions` | POST | Sesten metne (STT) |\n\n### Video\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/videos/generations` | POST | Video oluşturma görevi oluşturun |\n| `/v1/videos/generations/{id}` | GET | Video görevi durumunu alın |\n\n### Müzik\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/music/generations` | POST | Müzik oluşturma görevi oluşturun |\n| `/v1/music/generations/{id}` | GET | Müzik görevi durumunu alın |\n\n### 3D Oluşturma\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/3d/generations` | POST | 3D model oluşturma görevi oluşturun |\n| `/v1/3d/generations/{id}` | GET | 3D görevi durumunu alın |\n\n### Modeller\n\n| Uç Nokta | Metot | Açıklama |\n|----------|--------|-------------|\n| `/v1/models` | GET | Tüm mevcut modelleri listeleyin |\n| `/v1/models/{model}` | GET | Belirli model bilgilerini alın |\n\n## Yanıt Formatı\n\nTüm yanıtlar tutarlı bir format izler:\n\n### Başarılı Yanıt\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gpt-4o\",\n  \"choices\": [...],\n  \"usage\": {\n    \"prompt_tokens\": 10,\n    \"completion_tokens\": 20,\n    \"total_tokens\": 30\n  }\n}\n```\n\n### Yönlendirme Şeffaflığı\n\nTüm yanıtlar, kanal bilgilerini içeren bir `_routing` alanı içerir:\n\n```json\n{\n  \"id\": \"chatcmpl-abc123\",\n  ...,\n  \"_routing\": {\n    \"channel\": {\n      \"id\": \"ch_xxx\",\n      \"name\": \"channel-name\",\n      \"provider\": \"openai\",\n      \"channelType\": \"PLATFORM\"\n    },\n    \"cached\": false,\n    \"retryCount\": 0\n  }\n}\n```\n\n| Alan | Açıklama |\n|-------|-------------|\n| `channel.id` | Kullanılan kanal tanımlayıcısı |\n| `channel.provider` | Üst sağlayıcı (openai, anthropic, vb.) |\n| `channel.channelType` | `PLATFORM` (LemonData) veya `PRIVATE` (BYOK) |\n| `cached` | Yanıtın önbellekten sunulup sunulmadığı |\n| `retryCount` | Yeniden deneme sayısı (varsa) |\n\n### Hata Yanıtı\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```\n\n## İstek Limitleri\n\nİstek limitleri (rate limits) rol tabanlıdır ve yöneticiler tarafından yapılandırılabilir. Varsayılan değerler:\n\n| Rol | İstek/dak |\n|------|-------------|\n| Kullanıcı | 60 |\n| İş Ortağı | 300 |\n| VIP | 1,000 |\n| Yönetici | Sınırsız |\n\n<Note>\nÖzel istek limitleri için destek ekibiyle iletişime geçin. Kesin değerler hesap yapılandırmasına göre değişiklik gösterebilir.\n</Note>\n\nİstek limiti başlıkları tüm yanıtlara dahil edilir:\n\n```\nX-RateLimit-Limit: 60\nX-RateLimit-Remaining: 55\nX-RateLimit-Reset: 1234567890\n```\n\n## OpenAPI Spesifikasyonu\n\n<Card\n  title=\"OpenAPI Spesifikasyonu\"\n  icon=\"file-code\"\n  href=\"/openapi.json\"\n>\n  Tam OpenAPI 3.0 spesifikasyonunu indirin\n</Card>"
    },
    "updatedAt": "2026-01-26T05:24:14.083Z"
  },
  "api-reference/messages/create-message.mdx": {
    "sourceHash": "c91563b735990633",
    "translations": {
      "zh": "---\ntitle: \"创建消息\"\napi: \"POST /v1/messages\"\ndescription: \"使用 Anthropic Messages API 格式创建消息\"\n---\n\n## 概览\n\n此端点提供原生的 Anthropic Messages API 兼容性。适用于具备扩展思维（extended thinking）等特性的 Claude 模型。\n\n<Note>\nAnthropic SDK 的基础 URL：`https://api.lemondata.cc`（无 `/v1` 后缀）\n</Note>\n\n## 请求头\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  您的 LemonData API 密钥。Bearer token 的替代方案。\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Anthropic API 版本。请使用 `2023-06-01`。\n</ParamField>\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" required>\n  Claude 模型 ID（例如 `claude-sonnet-4-5`）。\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  包含 `role` 和 `content` 的消息对象数组。\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  生成的最大 token 数量。\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  系统提示词（与消息数组分开）。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  采样温度 (0-1)。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  启用流式响应。\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  扩展思维配置（Claude Opus 4.5）。\n\n  - `type` (string): `\"enabled\"` 表示启用\n  - `budget_tokens` (integer): 用于思维的 token 预算\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  模型可用的工具。\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  模型应如何使用工具。选项：`auto`、`any`、`tool`（特定工具）。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  核采样参数。请使用 temperature 或 top_p，不要同时使用两者。\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  仅从每个 token 的前 K 个选项中进行采样。\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  自定义停止序列，将导致模型停止生成。\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  附加到请求的元数据，用于跟踪目的。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  唯一消息标识符。\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  始终为 `message`。\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  始终为 `assistant`。\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  内容块数组（text、thinking、tool_use）。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用的模型。\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  生成停止的原因（`end_turn`、`max_tokens`、`tool_use`）。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  包含 `input_tokens` 和 `output_tokens` 的 token 使用情况。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## 扩展思维示例\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "zh-TW": "---\ntitle: \"建立訊息\"\napi: \"POST /v1/messages\"\ndescription: \"使用 Anthropic Messages API 格式建立訊息\"\n---\n\n## 總覽\n\n此端點提供原生的 Anthropic Messages API 相容性。適用於具備擴展思考（extended thinking）等功能的 Claude 模型。\n\n<Note>\nAnthropic SDK 的 Base URL：`https://api.lemondata.cc`（無 `/v1` 後綴）\n</Note>\n\n## 請求標頭\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  您的 LemonData API 金鑰。Bearer token 的替代方案。\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Anthropic API 版本。請使用 `2023-06-01`。\n</ParamField>\n\n## 請求主體\n\n<ParamField body=\"model\" type=\"string\" required>\n  Claude 模型 ID（例如 `claude-sonnet-4-5`）。\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  包含 `role` 與 `content` 的訊息物件陣列。\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  生成的最大 token 數量。\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  系統提示詞（與 messages 陣列分開）。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  取樣溫度（0-1）。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  啟用串流回應。\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  擴展思考配置（Claude Opus 4.5）。\n\n  - `type` (string)：設為 `\"enabled\"` 以啟用\n  - `budget_tokens` (integer)：思考的 token 預算\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  模型可用的工具。\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  模型應如何使用工具。選項：`auto`、`any`、`tool`（特定工具）。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  核取樣（Nucleus sampling）參數。請使用 temperature 或 top_p 其中之一，不要同時使用。\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  僅從每個 token 的前 K 個選項中進行取樣。\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  自定義停止序列，將導致模型停止生成。\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  附加到請求的元",
      "ja": "---\ntitle: \"メッセージの作成\"\napi: \"POST /v1/messages\"\ndescription: \"Anthropic Messages API 形式を使用してメッセージを作成します\"\n---\n\n## 概要\n\nこのエンドポイントは、ネイティブな Anthropic Messages API 互換性を提供します。拡張思考（extended thinking）などの機能を備えた Claude モデルに使用します。\n\n<Note>\nAnthropic SDK のベース URL: `https://api.lemondata.cc` （`/v1` サフィックスなし）\n</Note>\n\n## リクエストヘッダー\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  LemonData API キー。Bearer トークンの代わりに使用します。\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Anthropic API バージョン。`2023-06-01` を使用してください。\n</ParamField>\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" required>\n  Claude モデル ID（例：`claude-sonnet-4-5`）。\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  `role` と `content` を持つメッセージオブジェクトの配列。\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  生成する最大トークン数。\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  システムプロンプト（`messages` 配列とは別）。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  サンプリング温度（0-1）。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  ストリーミングレスポンスを有効にします。\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  拡張思考の設定（Claude Opus 4.5）。\n\n  - `type` (string): 有効にするには `\"enabled\"` を指定\n  - `budget_tokens` (integer): 思考用のトークン予算\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  モデルが利用可能なツール。\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  モデルがツールをどのように使用するか。オプション：`auto`、`any`、`tool`（特定のツール）。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  核サンプリング（Nucleus sampling）パラメータ。`temperature` または `top_p` のいずれか一方を使用してください。\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  各トークンについて、上位 K 個の候補からのみサンプリングします。\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  モデルの生成を停止させるカスタム停止シーケンス。\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  トラッキング目的でリクエストに付加するメタデータ。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  一意のメッセージ識別子。\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  常に `message`。\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  常に `assistant`。\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  コンテンツブロック（`text`、`thinking`、`tool_use`）の配列。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用されたモデル。\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  生成が停止した理由（`end_turn`、`max_tokens`、`tool_use`）。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  `input_tokens` と `output_tokens` を含むトークン使用量。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## 拡張思考の例\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "ko": "---\ntitle: \"메시지 생성\"\napi: \"POST /v1/messages\"\ndescription: \"Anthropic Messages API 형식을 사용하여 메시지를 생성합니다\"\n---\n\n## 개요\n\n이 엔드포인트는 네이티브 Anthropic Messages API 호환성을 제공합니다. 확장된 사고(extended thinking)와 같은 기능을 갖춘 Claude 모델에 이 엔드포인트를 사용하세요.\n\n<Note>\nAnthropic SDK용 Base URL: `https://api.lemondata.cc` (`/v1` 접미사 없음)\n</Note>\n\n## 요청 헤더\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  LemonData API 키입니다. Bearer 토큰의 대안입니다.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Anthropic API 버전입니다. `2023-06-01`을 사용하세요.\n</ParamField>\n\n## 요청 본문\n\n<ParamField body=\"model\" type=\"string\" required>\n  Claude 모델 ID (예: `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  `role`과 `content`를 포함하는 메시지 객체 배열입니다.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  생성할 최대 토큰 수입니다.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  시스템 프롬프트 (messages 배열과 별도).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  샘플링 온도 (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  스트리밍 응답을 활성화합니다.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  확장된 사고(Extended thinking) 설정 (Claude Opus 4.5).\n\n  - `type` (string): 활성화하려면 `\"enabled\"` 사용\n  - `budget_tokens` (integer): 사고를 위한 토큰 예산\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  모델이 사용할 수 있는 도구들입니다.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  모델이 도구를 사용하는 방식입니다. 옵션: `auto`, `any`, `tool` (특정 도구).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Nucleus 샘플링 파라미터입니다. `temperature` 또는 `top_p` 중 하나만 사용하세요.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  각 토큰에 대해 상위 K개의 옵션에서만 샘플링합니다.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  모델 생성을 중단시키는 사용자 정의 중단 시퀀스입니다.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  추적 목적으로 요청에 첨부할 메타데이터입니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"id\" type=\"string\">\n  고유 메시지 식별자입니다.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  항상 `message`입니다.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  항상 `assistant`입니다.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  콘텐츠 블록 배열 (`text`, `thinking`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  사용된 모델입니다.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  생성이 중단된 이유 (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  `input_tokens` 및 `output_tokens`를 포함한 토큰 사용량입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## 확장된 사고(Extended Thinking) 예시\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "de": "---\ntitle: \"Nachricht erstellen\"\napi: \"POST /v1/messages\"\ndescription: \"Erstellt eine Nachricht im Format der Anthropic Messages API\"\n---\n\n## Übersicht\n\nDieser Endpunkt bietet native Kompatibilität mit der Anthropic Messages API. Verwenden Sie diesen für Claude-Modelle mit Funktionen wie Extended Thinking.\n\n<Note>\nBasis-URL für das Anthropic SDK: `https://api.lemondata.cc` (kein `/v1` Suffix)\n</Note>\n\n## Request-Header\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  Ihr LemonData API-Key. Alternative zum Bearer-Token.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Anthropic API-Version. Verwenden Sie `2023-06-01`.\n</ParamField>\n\n## Request-Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  Claude-Modell-ID (z. B. `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Array von Nachrichtenobjekten mit `role` und `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Maximale Anzahl an zu generierenden Tokens.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  System-Prompt (getrennt vom messages-Array).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Sampling-Temperatur (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Streaming-Antworten aktivieren.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Konfiguration für Extended Thinking (Claude Opus 4.5).\n\n  - `type` (string): `\"enabled\"` zum Aktivieren\n  - `budget_tokens` (integer): Token-Budget für das Thinking\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Verfügbare Tools für das Modell.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Wie das Modell Tools verwenden soll. Optionen: `auto`, `any`, `tool` (spezifisches Tool).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Nucleus-Sampling-Parameter. Verwenden Sie entweder temperature oder top_p, nicht beides.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Nur aus den Top-K-Optionen für jedes Token sampeln.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Benutzerdefinierte Stop-Sequenzen, die das Modell veranlassen, die Generierung zu beenden.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadaten, die der Anfrage zu Tracking-Zwecken beigefügt werden.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  Eindeutige Nachrichten-ID.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  Immer `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  Immer `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  Array von Inhaltsblöcken (text, thinking, tool_use).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Verwendetes Modell.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  Grund, warum die Generierung gestoppt wurde (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token-Verbrauch mit `input_tokens` und `output_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## Beispiel für Extended Thinking\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "fr": "---\ntitle: \"Créer un message\"\napi: \"POST /v1/messages\"\ndescription: \"Crée un message en utilisant le format de l'API Anthropic Messages\"\n---\n\n## Aperçu\n\nCet endpoint offre une compatibilité native avec l'API Anthropic Messages. Utilisez-le pour les modèles Claude avec des fonctionnalités telles que la réflexion étendue (extended thinking).\n\n<Note>\nURL de base pour le SDK Anthropic : `https://api.lemondata.cc` (pas de suffixe `/v1`)\n</Note>\n\n## En-têtes de requête\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  Votre clé API LemonData. Alternative au token Bearer.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Version de l'API Anthropic. Utilisez `2023-06-01`.\n</ParamField>\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID du modèle Claude (ex: `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Tableau d'objets de message avec `role` et `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Nombre maximum de tokens à générer.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  Prompt système (séparé du tableau `messages`).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Température d'échantillonnage (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Activer les réponses en streaming.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Configuration de la réflexion étendue (Claude Opus 4.5).\n\n  - `type` (string) : `\"enabled\"` pour activer\n  - `budget_tokens` (integer) : Budget de tokens pour la réflexion\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Outils disponibles pour le modèle.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Comment le modèle doit utiliser les outils. Options : `auto`, `any`, `tool` (outil spécifique).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Paramètre d'échantillonnage nucleus. Utilisez soit `temperature` soit `top_p`, mais pas les deux.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Échantillonner uniquement parmi les `K` meilleures options pour chaque token.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Séquences d'arrêt personnalisées qui forceront le modèle à arrêter la génération.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Métadonnées à joindre à la requête à des fins de suivi.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  Identifiant unique du message.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  Toujours `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  Toujours `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  Tableau de blocs de contenu (`text`, `thinking`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modèle utilisé.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  Raison pour laquelle la génération s'est arrêtée (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Utilisation des tokens avec `input_tokens` et `output_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## Exemple de réflexion étendue (Extended Thinking)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "es": "---\ntitle: \"Crear mensaje\"\napi: \"POST /v1/messages\"\ndescription: \"Crea un mensaje utilizando el formato de la API de Anthropic Messages\"\n---\n\n## Descripción general\n\nEste endpoint proporciona compatibilidad nativa con la API de Anthropic Messages. Utilícelo para modelos Claude con funciones como el pensamiento extendido (extended thinking).\n\n<Note>\nURL base para el SDK de Anthropic: `https://api.lemondata.cc` (sin el sufijo `/v1`)\n</Note>\n\n## Encabezados de la solicitud\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  Su clave de API de LemonData. Alternativa al token Bearer.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Versión de la API de Anthropic. Use `2023-06-01`.\n</ParamField>\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID del modelo Claude (ej. `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Array de objetos de mensaje con `role` y `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Cantidad máxima de tokens a generar.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  Prompt de sistema (separado del array de mensajes).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Temperatura de muestreo (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Habilitar respuestas en streaming.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Configuración de pensamiento extendido (Claude Opus 4.5).\n\n  - `type` (string): `\"enabled\"` para habilitar\n  - `budget_tokens` (integer): Presupuesto de tokens para el pensamiento\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Herramientas disponibles para el modelo.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Cómo debe el modelo utilizar las herramientas. Opciones: `auto`, `any`, `tool` (herramienta específica).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Parámetro de muestreo nucleus. Use temperature o top_p, pero no ambos.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Solo realiza el muestreo de las mejores K opciones para cada token.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Secuencias de parada personalizadas que harán que el modelo deje de",
      "pt": "---\ntitle: \"Criar Mensagem\"\napi: \"POST /v1/messages\"\ndescription: \"Cria uma mensagem usando o formato da Anthropic Messages API\"\n---\n\n## Visão Geral\n\nEste endpoint fornece compatibilidade nativa com a Anthropic Messages API. Use-o para modelos Claude com recursos como pensamento estendido (extended thinking).\n\n<Note>\nURL base para o Anthropic SDK: `https://api.lemondata.cc` (sem o sufixo `/v1`)\n</Note>\n\n## Cabeçalhos da Requisição\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  Sua chave de API LemonData. Alternativa ao token Bearer.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Versão da API Anthropic. Use `2023-06-01`.\n</ParamField>\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID do modelo Claude (ex: `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Array de objetos de mensagem com `role` e `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Máximo de tokens a serem gerados.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  Prompt de sistema (separado do array de mensagens).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Temperatura de amostragem (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Habilitar respostas via streaming.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Configuração de pensamento estendido (Claude Opus 4.5).\n\n  - `type` (string): `\"enabled\"` para habilitar\n  - `budget_tokens` (integer): Orçamento de tokens para o pensamento\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Ferramentas disponíveis para o modelo.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Como o modelo deve usar as ferramentas. Opções: `auto`, `any`, `tool` (ferramenta específica).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Parâmetro de amostragem de núcleo (nucleus sampling). Use `temperature` ou `top_p`, não ambos.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Amostra apenas das principais K opções para cada token.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Sequências de parada personalizadas que farão o modelo interromper a geração.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadados para anexar à requisição para fins de rastreamento.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  Identificador único da mensagem.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  Sempre `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  Sempre `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  Array de blocos de conteúdo (`text`, `thinking`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modelo utilizado.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  Por que a geração foi interrompida (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Uso de tokens com `input_tokens` e `output_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## Exemplo de Pensamento Estendido\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "ar": "---\ntitle: \"إنشاء رسالة\"\napi: \"POST /v1/messages\"\ndescription: \"يقوم بإنشاء رسالة باستخدام تنسيق Anthropic Messages API\"\n---\n\n## نظرة عامة\n\nيوفر هذا المسار توافقاً أصلياً مع Anthropic Messages API. استخدم هذا لموديلات Claude مع ميزات مثل التفكير الموسع (extended thinking).\n\n<Note>\nBase URL لـ Anthropic SDK: `https://api.lemondata.cc` (بدون لاحقة `/v1`)\n</Note>\n\n## ترويسات الطلب (Request Headers)\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  مفتاح LemonData API الخاص بك. بديل لـ Bearer token.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  إصدار Anthropic API. استخدم `2023-06-01`.\n</ParamField>\n\n## جسم الطلب (Request Body)\n\n<ParamField body=\"model\" type=\"string\" required>\n  معرف موديل Claude (على سبيل المثال، `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  مصفوفة من كائنات الرسائل تحتوي على `role` و `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  الحد الأقصى لـ tokens المراد إنشاؤها.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  موجه النظام (System prompt) (منفصل عن مصفوفة الرسائل).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  درجة حرارة العينات (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  تفعيل استجابات البث (streaming).\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  إعدادات التفكير الموسع (Claude Opus 4.5).\n\n  - `type` (string): `\"enabled\"` للتفعيل\n  - `budget_tokens` (integer): ميزانية الـ tokens للتفكير\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  الأدوات المتاحة للموديل.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  كيفية استخدام الموديل للأدوات. الخيارات: `auto` ، `any` ، `tool` (أداة محددة).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  معامل Nucleus sampling. استخدم إما temperature أو top_p، وليس كليهما.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  أخذ عينات فقط من أفضل K خيارات لكل token.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  تسلسلات إيقاف مخصصة ستؤدي إلى توقف الموديل عن الإنشاء.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  بيانات وصفية (Metadata) لإرفاقها بالطلب لأغراض التتبع.\n</ParamField>\n\n## الاستجابة\n\n<ResponseField name=\"id\" type=\"string\">\n  معرف فريد للرسالة.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  دائماً `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  دائماً `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  مصفوفة من كتل المحتوى (`text` ، `thinking` ، `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  الموديل المستخدم.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  سبب توقف الإنشاء (`end_turn` ، `max_tokens` ، `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  استخدام الـ tokens مع `input_tokens` و `output_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## مثال على التفكير الموسع (Extended Thinking)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "vi": "---\ntitle: \"Tạo Tin nhắn\"\napi: \"POST /v1/messages\"\ndescription: \"Tạo một tin nhắn bằng định dạng Anthropic Messages API\"\n---\n\n## Tổng quan\n\nEndpoint này cung cấp khả năng tương thích gốc với Anthropic Messages API. Sử dụng endpoint này cho các mô hình Claude với các tính năng như suy nghĩ mở rộng (extended thinking).\n\n<Note>\nBase URL cho Anthropic SDK: `https://api.lemondata.cc` (không có hậu tố `/v1`)\n</Note>\n\n## Request Headers\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  API key LemonData của bạn. Giải pháp thay thế cho Bearer token.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Phiên bản Anthropic API. Sử dụng `2023-06-01`.\n</ParamField>\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID mô hình Claude (ví dụ: `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Mảng các đối tượng tin nhắn với `role` và `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Số lượng token tối đa để tạo.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  System prompt (tách biệt với mảng messages).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Nhiệt độ lấy mẫu (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Bật phản hồi dạng luồng (streaming).\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Cấu hình suy nghĩ mở rộng (Claude Opus 4.5).\n\n  - `type` (string): `\"enabled\"` để kích hoạt\n  - `budget_tokens` (integer): Ngân sách token cho việc suy nghĩ\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Các công cụ có sẵn cho mô hình.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Cách mô hình nên sử dụng các công cụ. Các tùy chọn: `auto`, `any`, `tool` (công cụ cụ thể).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Tham số lấy mẫu hạt nhân (nucleus sampling). Sử dụng temperature hoặc top_p, không sử dụng cả hai.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Chỉ lấy mẫu từ K tùy chọn hàng đầu cho mỗi token.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Các chuỗi dừng tùy chỉnh sẽ khiến mô hình ngừng tạo nội dung.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadata để đính kèm vào yêu cầu cho mục đích theo dõi.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"id\" type=\"string\">\n  Định danh tin nhắn duy nhất.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  Luôn là `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  Luôn là `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  Mảng các khối nội dung (text, thinking, tool_use).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Mô hình được sử dụng.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  Lý do việc tạo nội dung dừng lại (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Mức sử dụng token với `input_tokens` và `output_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## Ví dụ về Suy nghĩ Mở rộng\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "id": "---\ntitle: \"Buat Pesan\"\napi: \"POST /v1/messages\"\ndescription: \"Membuat pesan menggunakan format Anthropic Messages API\"\n---\n\n## Ringkasan\n\nEndpoint ini menyediakan kompatibilitas native Anthropic Messages API. Gunakan ini untuk model Claude dengan fitur seperti pemikiran mendalam (extended thinking).\n\n<Note>\nBase URL untuk Anthropic SDK: `https://api.lemondata.cc` (tanpa akhiran `/v1`)\n</Note>\n\n## Header Permintaan\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  Kunci API LemonData Anda. Alternatif untuk token Bearer.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Versi Anthropic API. Gunakan `2023-06-01`.\n</ParamField>\n\n## Body Permintaan\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID model Claude (misalnya, `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  Array objek pesan dengan `role` dan `content`.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Jumlah token maksimum untuk dihasilkan.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  Prompt sistem (terpisah dari array messages).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Suhu sampling (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Aktifkan respons streaming.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Konfigurasi pemikiran mendalam (extended thinking) (Claude Opus 4.5).\n\n  - `type` (string): `\"enabled\"` untuk mengaktifkan\n  - `budget_tokens` (integer): Anggaran token untuk pemikiran\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Alat (tools) yang tersedia untuk model.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Bagaimana model harus menggunakan alat. Opsi: `auto`, `any`, `tool` (alat spesifik).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Parameter nucleus sampling. Gunakan salah satu dari temperature atau top_p, jangan keduanya.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Hanya ambil sampel dari K opsi teratas untuk setiap token.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Urutan penghentian (stop sequences) khusus yang akan menyebabkan model berhenti menghasilkan.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadata untuk dilampirkan pada permintaan untuk tujuan pelacakan.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"id\" type=\"string\">\n  Pengidentifikasi pesan unik.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  Selalu `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  Selalu `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  Array blok konten (text, thinking, tool_use).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Model yang digunakan.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  Alasan pembuatan berhenti (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Penggunaan token dengan `input_tokens` and `output_tokens`.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## Contoh Pemikiran Mendalam (Extended Thinking)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```",
      "tr": "---\ntitle: \"Mesaj Oluştur\"\napi: \"POST /v1/messages\"\ndescription: \"Anthropic Messages API formatını kullanarak bir mesaj oluşturur\"\n---\n\n## Genel Bakış\n\nBu uç nokta, yerel Anthropic Messages API uyumluluğu sağlar. Bunu, genişletilmiş düşünme (extended thinking) gibi özelliklere sahip Claude modelleri için kullanın.\n\n<Note>\nAnthropic SDK için temel URL: `https://api.lemondata.cc` (`/v1` son eki yok)\n</Note>\n\n## İstek Başlıkları\n\n<ParamField header=\"x-api-key\" type=\"string\" required>\n  LemonData API anahtarınız. Bearer token'a alternatiftir.\n</ParamField>\n\n<ParamField header=\"anthropic-version\" type=\"string\" required>\n  Anthropic API sürümü. `2023-06-01` kullanın.\n</ParamField>\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" required>\n  Claude model kimliği (ör. `claude-sonnet-4-5`).\n</ParamField>\n\n<ParamField body=\"messages\" type=\"array\" required>\n  `role` ve `content` içeren mesaj nesneleri dizisi.\n</ParamField>\n\n<ParamField body=\"max_tokens\" type=\"integer\" required>\n  Oluşturulacak maksimum token sayısı.\n</ParamField>\n\n<ParamField body=\"system\" type=\"string\">\n  Sistem istemi (mesajlar dizisinden ayrı).\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Örnekleme sıcaklığı (0-1).\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Akış (streaming) yanıtlarını etkinleştirin.\n</ParamField>\n\n<ParamField body=\"thinking\" type=\"object\">\n  Genişletilmiş düşünme (extended thinking) yapılandırması (Claude Opus 4.5).\n\n  - `type` (string): Etkinleştirmek için `\"enabled\"`\n  - `budget_tokens` (integer): Düşünme için token bütçesi\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Model için kullanılabilir araçlar.\n</ParamField>\n\n<ParamField body=\"tool_choice\" type=\"object\">\n  Modelin araçları nasıl kullanması gerektiği. Seçenekler: `auto`, `any`, `tool` (belirli bir araç).\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Nucleus örnekleme parametresi. `temperature` veya `top_p` seçeneklerinden birini kullanın, ikisini birden kullanmayın.\n</ParamField>\n\n<ParamField body=\"top_k\" type=\"integer\">\n  Her token için yalnızca en iyi K seçenek arasından örnekleme yapın.\n</ParamField>\n\n<ParamField body=\"stop_sequences\" type=\"array\">\n  Modelin oluşturmayı durdurmasına neden olacak özel durdurma dizileri.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  İzleme amacıyla isteğe eklenecek meta veriler.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Benzersiz mesaj tanımlayıcı.\n</ResponseField>\n\n<ResponseField name=\"type\" type=\"string\">\n  Her zaman `message`.\n</ResponseField>\n\n<ResponseField name=\"role\" type=\"string\">\n  Her zaman `assistant`.\n</ResponseField>\n\n<ResponseField name=\"content\" type=\"array\">\n  İçerik blokları dizisi (`text`, `thinking`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Kullanılan model.\n</ResponseField>\n\n<ResponseField name=\"stop_reason\" type=\"string\">\n  Oluşturmanın neden durduğu (`end_turn`, `max_tokens`, `tool_use`).\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  `input_tokens` ve `output_tokens` ile token kullanımı.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/messages\" \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": \"You are a helpful assistant.\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n  }'\n```\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n\nconst message = await client.messages.create({\n  model: 'claude-sonnet-4-5',\n  max_tokens: 1024,\n  system: 'You are a helpful assistant.',\n  messages: [\n    { role: 'user', content: 'Hello, Claude!' }\n  ]\n});\n\nconsole.log(message.content[0].text);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"io\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":      \"claude-sonnet-4-5\",\n        \"max_tokens\": 1024,\n        \"system\":     \"You are a helpful assistant.\",\n        \"messages\": []map[string]string{\n            {\"role\": \"user\", \"content\": \"Hello, Claude!\"},\n        },\n    }\n\n    jsonData, _ := json.Marshal(payload)\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/messages\", bytes.NewBuffer(jsonData))\n    req.Header.Set(\"x-api-key\", \"sk-your-api-key\")\n    req.Header.Set(\"anthropic-version\", \"2023-06-01\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    body, _ := io.ReadAll(resp.Body)\n    fmt.Println(string(body))\n}\n```\n\n```php PHP\n<?php\n$payload = [\n    'model' => 'claude-sonnet-4-5',\n    'max_tokens' => 1024,\n    'system' => 'You are a helpful assistant.',\n    'messages' => [\n        ['role' => 'user', 'content' => 'Hello, Claude!']\n    ]\n];\n\n$ch = curl_init('https://api.lemondata.cc/v1/messages');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'x-api-key: sk-your-api-key',\n        'anthropic-version: 2023-06-01',\n        'Content-Type: application/json'\n    ],\n    CURLOPT_POSTFIELDS => json_encode($payload)\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['content'][0]['text'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"msg_abc123\",\n  \"type\": \"message\",\n  \"role\": \"assistant\",\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Hello! How can I help you today?\"\n    }\n  ],\n  \"model\": \"claude-sonnet-4-5\",\n  \"stop_reason\": \"end_turn\",\n  \"usage\": {\n    \"input_tokens\": 15,\n    \"output_tokens\": 10\n  }\n}\n```\n</ResponseExample>\n\n## Genişletilmiş Düşünme Örneği\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this math problem...\"}]\n)\n\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```"
    },
    "updatedAt": "2026-01-26T05:26:14.616Z"
  },
  "api-reference/models/get-model.mdx": {
    "sourceHash": "754f6929f02c936c",
    "translations": {
      "zh": "---\ntitle: \"获取模型\"\napi: \"GET /v1/models/{model}\"\ndescription: \"检索模型实例\"\n---\n\n## 路径参数\n\n<ParamField path=\"model\" type=\"string\" required>\n  要检索的模型 ID（例如：`gpt-4o`，`claude-sonnet-4-5`）。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  模型标识符。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  始终为 `model`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  创建时间戳。\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  模型提供商。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## 错误处理\n\n如果模型不存在，您将收到 404 错误：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "zh-TW": "---\ntitle: \"獲取模型\"\napi: \"GET /v1/models/{model}\"\ndescription: \"檢索模型實例\"\n---\n\n## 路徑參數\n\n<ParamField path=\"model\" type=\"string\" required>\n  要檢索的模型 ID（例如：`gpt-4o`、`claude-sonnet-4-5`）。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"id\" type=\"string\">\n  模型識別碼。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  固定為 `model`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  建立時間戳記。\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  模型提供者。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## 錯誤處理\n\n如果模型不存在，您將收到 404 錯誤：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "ja": "---\ntitle: \"モデルの取得\"\napi: \"GET /v1/models/{model}\"\ndescription: \"モデルインスタンスを取得します\"\n---\n\n## パスパラメータ\n\n<ParamField path=\"model\" type=\"string\" required>\n  取得するモデルのID（例：`gpt-4o`、`claude-sonnet-4-5`）。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  モデル識別子。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  常に `model`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  作成時のタイムスタンプ。\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  モデルプロバイダー。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## エラーハンドリング\n\nモデルが存在しない場合、404エラーが返されます：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "ko": "---\ntitle: \"모델 조회\"\napi: \"GET /v1/models/{model}\"\ndescription: \"모델 인스턴스를 조회합니다\"\n---\n\n## 경로 파라미터\n\n<ParamField path=\"model\" type=\"string\" required>\n  조회할 모델의 ID입니다 (예: `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"id\" type=\"string\">\n  모델 식별자.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  항상 `model`입니다.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  생성 타임스탬프.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  모델 제공자.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## 에러 처리\n\n모델이 존재하지 않는 경우 404 에러를 받게 됩니다:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "de": "---\ntitle: \"Modell abrufen\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Ruft eine Modell-Instanz ab\"\n---\n\n## Pfadparameter\n\n<ParamField path=\"model\" type=\"string\" required>\n  Die ID des abzurufenden Modells (z. B. `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"id\" type=\"string\">\n  Modell-Identifikator.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Immer `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Erstellungs-Zeitstempel.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Modellanbieter.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Fehlerbehandlung\n\nWenn das Modell nicht existiert, erhalten Sie einen 404-Fehler:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "fr": "---\ntitle: \"Récupérer un modèle\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Récupère une instance de modèle\"\n---\n\n## Paramètres de chemin\n\n<ParamField path=\"model\" type=\"string\" required>\n  L'ID du modèle à récupérer (par ex., `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  Identifiant du modèle.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Toujours `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Horodatage de création.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Fournisseur du modèle.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Gestion des erreurs\n\nSi le modèle n'existe pas, vous recevrez une erreur 404 :\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "es": "---\ntitle: \"Obtener modelo\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Recupera una instancia de modelo\"\n---\n\n## Parámetros de ruta\n\n<ParamField path=\"model\" type=\"string\" required>\n  El ID del modelo a recuperar (por ejemplo, `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"id\" type=\"string\">\n  Identificador del modelo.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Siempre `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Marca de tiempo de creación.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Proveedor del modelo.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Manejo de errores\n\nSi el modelo no existe, recibirá un error 404:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "pt": "---\ntitle: \"Obter Modelo\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Recupera uma instância de modelo\"\n---\n\n## Parâmetros de Caminho\n\n<ParamField path=\"model\" type=\"string\" required>\n  O ID do modelo a ser recuperado (ex: `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  Identificador do modelo.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Sempre `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Timestamp de criação.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Provedor do modelo.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Tratamento de Erros\n\nSe o modelo não existir, você receberá um erro 404:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "ar": "---\ntitle: \"الحصول على النموذج\"\napi: \"GET /v1/models/{model}\"\ndescription: \"يسترجع نسخة من النموذج\"\n---\n\n## معلمات المسار\n\n<ParamField path=\"model\" type=\"string\" required>\n  معرف (ID) النموذج المراد استرجاعه (على سبيل المثال، `gpt-4o`، `claude-sonnet-4-5`).\n</ParamField>\n\n## الاستجابة\n\n<ResponseField name=\"id\" type=\"string\">\n  معرف النموذج.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  دائماً `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  الطابع الزمني للإنشاء.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  مزود النموذج.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## معالجة الأخطاء\n\nإذا كان النموذج غير موجود، ستتلقى خطأ 404:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "vi": "---\ntitle: \"Lấy thông tin Model\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Truy xuất một phiên bản model\"\n---\n\n## Tham số đường dẫn\n\n<ParamField path=\"model\" type=\"string\" required>\n  ID của model cần truy xuất (ví dụ: `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"id\" type=\"string\">\n  Định danh của model.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Luôn là `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Dấu thời gian tạo (timestamp).\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Nhà cung cấp model.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Xử lý lỗi\n\nNếu model không tồn tại, bạn sẽ nhận được lỗi 404:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "id": "---\ntitle: \"Dapatkan Model\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Mengambil instansi model\"\n---\n\n## Parameter Path\n\n<ParamField path=\"model\" type=\"string\" required>\n  ID model yang akan diambil (misalnya, `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"id\" type=\"string\">\n  Pengidentifikasi model.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Selalu `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Timestamp pembuatan.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Penyedia model.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Penanganan Error\n\nJika model tidak ada, Anda akan menerima error 404:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```",
      "tr": "---\ntitle: \"Modeli Getir\"\napi: \"GET /v1/models/{model}\"\ndescription: \"Bir model örneğini getirir\"\n---\n\n## Yol Parametreleri\n\n<ParamField path=\"model\" type=\"string\" required>\n  Getirilecek modelin ID'si (örneğin, `gpt-4o`, `claude-sonnet-4-5`).\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Model tanımlayıcı.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Her zaman `model`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Oluşturulma zaman damgası.\n</ResponseField>\n\n<ResponseField name=\"owned_by\" type=\"string\">\n  Model sağlayıcısı.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models/gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodel = client.models.retrieve(\"gpt-4o\")\nprint(f\"Model: {model.id}, Provider: {model.owned_by}\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst model = await client.models.retrieve('gpt-4o');\nconsole.log(`Model: ${model.id}, Provider: ${model.owned_by}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    model, err := client.GetModel(context.Background(), \"gpt-4o\")\n    if err != nil {\n        panic(err)\n    }\n    fmt.Printf(\"Model: %s, Provider: %s\\n\", model.ID, model.OwnedBy)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models/gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$model = json_decode($response, true);\necho \"Model: {$model['id']}, Provider: {$model['owned_by']}\\n\";\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"gpt-4o\",\n  \"object\": \"model\",\n  \"created\": 1706000000,\n  \"owned_by\": \"openai\"\n}\n```\n</ResponseExample>\n\n## Hata Yönetimi\n\nModel mevcut değilse, 404 hatası alırsınız:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Model 'invalid-model' not found\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"model_not_found\"\n  }\n}\n```"
    },
    "updatedAt": "2026-01-26T05:26:33.467Z"
  },
  "api-reference/models/list-models.mdx": {
    "sourceHash": "716c6f9d987486b6",
    "translations": {
      "zh": "---\ntitle: \"列出模型\"\napi: \"GET /v1/models\"\ndescription: \"列出所有可用模型\"\n---\n\n## 响应\n\n<ResponseField name=\"object\" type=\"string\">\n  始终为 `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  模型对象数组。\n\n  每个模型包含：\n  - `id` (string): 模型标识符\n  - `object` (string): `model`\n  - `created` (integer): 创建时间戳\n  - `owned_by` (string): 模型提供商\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 按提供商过滤\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## 模型类别\n\n| 提供商 | 示例模型 |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "zh-TW": "---\ntitle: \"列出模型\"\napi: \"GET /v1/models\"\ndescription: \"列出所有可用的模型\"\n---\n\n## 回應\n\n<ResponseField name=\"object\" type=\"string\">\n  固定為 `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  模型物件陣列。\n\n  每個模型包含：\n  - `id` (string): 模型識別碼\n  - `object` (string): `model`\n  - `created` (integer): 建立時間戳記\n  - `owned_by` (string): 模型提供者\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 依提供者篩選\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## 模型類別\n\n| 提供者 | 範例模型 |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "ja": "---\ntitle: \"モデル一覧\"\napi: \"GET /v1/models\"\ndescription: \"利用可能なすべてのモデルを一覧表示します\"\n---\n\n## レスポンス\n\n<ResponseField name=\"object\" type=\"string\">\n  常に `list` です。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  モデルオブジェクトの配列。\n\n  各モデルには以下が含まれます：\n  - `id` (string): モデル識別子\n  - `object` (string): `model`\n  - `created` (integer): 作成時のタイムスタンプ\n  - `owned_by` (string): モデルプロバイダー\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## プロバイダーによるフィルタリング\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## モデルカテゴリー\n\n| プロバイダー | モデルの例 |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "ko": "---\ntitle: \"모델 목록 조회\"\napi: \"GET /v1/models\"\ndescription: \"사용 가능한 모든 모델의 목록을 조회합니다.\"\n---\n\n## 응답\n\n<ResponseField name=\"object\" type=\"string\">\n  항상 `list`입니다.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  모델 객체의 배열입니다.\n\n  각 모델은 다음을 포함합니다:\n  - `id` (string): 모델 식별자\n  - `object` (string): `model`\n  - `created` (integer): 생성 타임스탬프\n  - `owned_by` (string): 모델 제공자\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 제공자별 필터링\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## 모델 카테고리\n\n| 제공자 | 예시 모델 |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "de": "---\ntitle: \"Modelle auflisten\"\napi: \"GET /v1/models\"\ndescription: \"Listet alle verfügbaren Modelle auf\"\n---\n\n## Antwort\n\n<ResponseField name=\"object\" type=\"string\">\n  Immer `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array von Modell-Objekten.\n\n  Jedes Modell enthält:\n  - `id` (string): Modell-Identifikator\n  - `object` (string): `model`\n  - `created` (integer): Erstellungs-Zeitstempel\n  - `owned_by` (string): Modell-Anbieter\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Filtern nach Anbieter\n\n```python\n# Alle OpenAI-Modelle abrufen\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Alle Anthropic-Modelle abrufen\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## Modell-Kategorien\n\n| Anbieter | Beispiel-Modelle |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "fr": "---\ntitle: \"Lister les modèles\"\napi: \"GET /v1/models\"\ndescription: \"Liste tous les modèles disponibles\"\n---\n\n## Réponse\n\n<ResponseField name=\"object\" type=\"string\">\n  Toujours `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Tableau d'objets de modèle.\n\n  Chaque modèle contient :\n  - `id` (string) : Identifiant du modèle\n  - `object` (string) : `model`\n  - `created` (integer) : Horodatage de création\n  - `owned_by` (string) : Fournisseur du modèle\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Filtrage par fournisseur\n\n```python\n# Obtenir tous les modèles OpenAI\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Obtenir tous les modèles Anthropic\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## Catégories de modèles\n\n| Fournisseur | Exemples de modèles |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "es": "---\ntitle: \"Listar Modelos\"\napi: \"GET /v1/models\"\ndescription: \"Enumera todos los modelos disponibles\"\n---\n\n## Respuesta\n\n<ResponseField name=\"object\" type=\"string\">\n  Siempre `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array de objetos de modelo.\n\n  Cada modelo contiene:\n  - `id` (string): Identificador del modelo\n  - `object` (string): `model`\n  - `created` (integer): Marca de tiempo de creación\n  - `owned_by` (string): Proveedor del modelo\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Filtrado por Proveedor\n\n```python\n# Obtener todos los modelos de OpenAI\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Obtener todos los modelos de Anthropic\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## Categorías de Modelos\n\n| Proveedor | Modelos de ejemplo |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "pt": "---\ntitle: \"Listar Modelos\"\napi: \"GET /v1/models\"\ndescription: \"Lista todos os modelos disponíveis\"\n---\n\n## Resposta\n\n<ResponseField name=\"object\" type=\"string\">\n  Sempre `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array de objetos de modelo.\n\n  Cada modelo contém:\n  - `id` (string): Identificador do modelo\n  - `object` (string): `model`\n  - `created` (integer): Timestamp de criação\n  - `owned_by` (string): Provedor do modelo\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Filtragem por Provedor\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## Categorias de Modelos\n\n| Provedor | Exemplos de Modelos |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "ar": "---\ntitle: \"قائمة النماذج\"\napi: \"GET /v1/models\"\ndescription: \"يسرد جميع النماذج المتاحة\"\n---\n\n## الاستجابة\n\n<ResponseField name=\"object\" type=\"string\">\n  دائماً `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  مصفوفة من كائنات النماذج.\n\n  يحتوي كل نموذج على:\n  - `id` (string): معرف النموذج\n  - `object` (string): `model`\n  - `created` (integer): الطابع الزمني للإنشاء\n  - `owned_by` (string): مزود النموذج\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## التصفية حسب المزود\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## فئات النماذج\n\n| المزود | نماذج أمثلة |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "vi": "---\ntitle: \"Danh sách Model\"\napi: \"GET /v1/models\"\ndescription: \"Liệt kê tất cả các model hiện có\"\n---\n\n## Phản hồi\n\n<ResponseField name=\"object\" type=\"string\">\n  Luôn là `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Mảng chứa các đối tượng model.\n\n  Mỗi model bao gồm:\n  - `id` (string): Mã định danh model\n  - `object` (string): `model`\n  - `created` (integer): Dấu thời gian tạo\n  - `owned_by` (string): Nhà cung cấp model\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Lọc theo Nhà cung cấp\n\n```python\n# Lấy tất cả các model của OpenAI\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Lấy tất cả các model của Anthropic\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## Danh mục Model\n\n| Nhà cung cấp | Các model ví dụ |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "id": "---\ntitle: \"Daftar Model\"\napi: \"GET /v1/models\"\ndescription: \"Menampilkan daftar semua model yang tersedia\"\n---\n\n## Respons\n\n<ResponseField name=\"object\" type=\"string\">\n  Selalu `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array dari objek model.\n\n  Setiap model berisi:\n  - `id` (string): Pengidentifikasi model\n  - `object` (string): `model`\n  - `created` (integer): Timestamp pembuatan\n  - `owned_by` (string): Penyedia model\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/models\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nmodels = client.models.list()\n\nfor model in models.data:\n    print(f\"{model.id} ({model.owned_by})\")\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst models = await client.models.list();\n\nfor (const model of models.data) {\n  console.log(`${model.id} (${model.owned_by})`);\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    models, err := client.ListModels(context.Background())\n    if err != nil {\n        panic(err)\n    }\n\n    for _, model := range models.Models {\n        fmt.Printf(\"%s (%s)\\n\", model.ID, model.OwnedBy)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/models');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nforeach ($data['data'] as $model) {\n    echo \"{$model['id']} ({$model['owned_by']})\\n\";\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"gpt-4o\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"openai\"\n    },\n    {\n      \"id\": \"claude-sonnet-4-5\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"anthropic\"\n    },\n    {\n      \"id\": \"gemini-2.5-flash\",\n      \"object\": \"model\",\n      \"created\": 1706000000,\n      \"owned_by\": \"google\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Memfilter berdasarkan Penyedia\n\n```python\n# Get all OpenAI models\nopenai_models = [m for m in models.data if m.owned_by == \"openai\"]\n\n# Get all Anthropic models\nanthropic_models = [m for m in models.data if m.owned_by == \"anthropic\"]\n```\n\n## Kategori Model\n\n| Penyedia | Contoh Model |\n|----------|----------------|\n| `openai` | gpt-4o, gpt-4o-mini, o3, gpt-5.2, dall-e-3 |\n| `anthropic` | claude-sonnet-4-5, claude-opus-4-5, claude-haiku-4-5 |\n| `google` | gemini-2.5-flash, gemini-2.5-pro, gemini-3-pro-preview |\n| `deepseek` | deepseek-r1, deepseek-v3-2 |\n| `meta` | llama-3.3-70b, llama-3.1-405b |",
      "tr": "Example>`\n    Wait, this is a component.\n    Okay, I will keep the code blocks inside.\n\n    *Wait, I noticed one more thing:*\n    `<ResponseExample>`\n    ````json Response`\n    `...`\n    ```` `\n    `</ResponseExample>`\n    Wait, this is a component.\n    Okay, I will keep the code block inside.\n\n    *Wait, I noticed one more thing:*\n    `# Get all OpenAI models`\n    `# Get all Anthropic models`\n    Wait, these are comments in a code block.\n    Rule 2: \"DO NOT translate: Code blocks (content between ``` markers)\".\n    Okay, I will NOT translate them.\n\n    *Wait, I noticed one more thing:*\n    `| Provider | Example Models |`\n    Wait, these are table headers.\n    Okay, I will translate them.\n\n    *Wait, I noticed one more thing:*\n    `| openai | gpt-4o, ... |`\n    Wait, these are table rows.\n    Okay, I will keep the content as it is technical identifiers.\n\n    *Wait, I noticed one more thing:*\n    `| anthropic | claude-sonnet-4-5, ... |`\n    Wait, these are table rows.\n    Okay, I will keep the content as it is technical identifiers.\n\n    *Wait, I noticed one more thing:*\n    `| google | gemini-2.5-flash, ... |`\n    Wait, these are table rows.\n    Okay, I will keep the content as it is technical identifiers.\n\n    *Wait, I noticed one more thing:*\n    `| deepseek | deepseek-r1, ... |`\n    Wait, these are table rows.\n    Okay, I will keep the content as it is technical identifiers.\n\n    *Wait, I noticed one more thing:*\n    `| meta | llama-3.3-70b, ... |`\n    Wait, these are table rows.\n    Okay, I will keep the content as it is technical identifiers.\n\n    *Wait, I noticed one more thing:*\n    `title: \"List Models\"`\n    `api: \"GET /v1/models\"`\n    `description: \"Lists all available models\"`\n    Wait, these are frontmatter.\n    Okay, I will translate the values of `title` and `description`.\n\n    *Wait, I noticed one more thing:*\n    `## Response`\n    Wait, this is a heading.\n    Okay, I will translate it.\n\n    *Wait, I noticed one more thing:*\n    `## Filtering by Provider`\n    Wait, this is a heading.\n    Okay, I will translate it.\n\n    *Wait, I noticed one more thing:*\n    `## Model Categories`\n    Wait, this is a heading.\n    Okay, I will translate it.\n\n    *Wait, I noticed one more thing:*\n    `<ResponseField name=\""
    },
    "updatedAt": "2026-01-26T05:26:48.854Z"
  },
  "api-reference/music/create-music.mdx": {
    "sourceHash": "0ba0c42056dba453",
    "translations": {
      "zh": "---\ntitle: \"创建音乐\"\napi: \"POST /v1/music/generations\"\ndescription: \"使用 Suno 创建音乐生成任务\"\n---\n\n使用 AI 生成音乐和歌词。这是一个异步 API - 您将收到一个任务 ID 用于轮询完成状态。\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  使用的模型：`suno_music` 用于音乐生成，`suno_lyrics` 仅用于歌词。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  要生成的音乐描述。可以包括风格、情绪、乐器等。\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  生成的歌曲标题。\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  风格标签（例如 \"pop, upbeat, electronic\"）。\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  生成类型：`MUSIC`（默认）或 `LYRICS`。\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  要使用的模型版本。\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  要续写的上一个片段的 ID。\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  续写的起始时间戳（以秒为单位）。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  终端用户的唯一标识符。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  用于轮询状态的任务 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任务状态：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  任务创建的 Unix 时间戳。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"建立音樂\"\napi: \"POST /v1/music/generations\"\ndescription: \"使用 Suno 建立音樂生成任務\"\n---\n\n使用 AI 生成音樂和歌詞。這是一個非同步 API - 您將收到一個任務 ID 用於輪詢完成狀態。\n\n## 請求參數\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  要使用的模型：`suno_music` 用於音樂生成，`suno_lyrics` 僅用於歌詞。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  要生成的音樂描述。可以包含風格、心情、樂器等。\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  生成歌曲的標題。\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  風格標籤（例如：\"pop, upbeat, electronic\"）。\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  生成類型：`MUSIC`（預設）或 `LYRICS`。\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  要使用的模型版本。\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  要續接的前一個片段 ID。\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  續接的時間戳記（以秒為單位）。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  終端用戶的唯一識別碼。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"id\" type=\"string\">\n  用於輪詢狀態的任務 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任務狀態：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  任務建立的 Unix 時間戳記。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"音楽の作成\"\napi: \"POST /v1/music/generations\"\ndescription: \"Sunoを使用して音楽生成タスクを作成します\"\n---\n\nAIを使用して音楽と歌詞を生成します。これは非同期APIです。完了を確認するためにポーリングを行うためのタスクIDを受け取ります。\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  使用するモデル：音楽生成には `suno_music`、歌詞のみには `suno_lyrics` を使用します。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  生成する音楽の説明。スタイル、ムード、楽器などを含めることができます。\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  生成された曲のタイトル。\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  スタイルタグ（例：「pop, upbeat, electronic」）。\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  生成タイプ：`MUSIC`（デフォルト）または `LYRICS`。\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  使用するモデルのバージョン。\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  続きから開始する前のクリップのID。\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  続きから開始するタイムスタンプ（秒単位）。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  エンドユーザーの一意識別子。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  ステータスをポーリングするためのタスクID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  タスクのステータス：`pending`、`processing`、`completed`、または `failed`。\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  タスク作成時のUnixタイムスタンプ。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"음악 생성\"\napi: \"POST /v1/music/generations\"\ndescription: \"Suno를 사용하여 음악 생성 작업을 생성합니다.\"\n---\n\nAI를 사용하여 음악과 가사를 생성합니다. 이 API는 비동기 방식으로 작동하며, 완료 여부를 확인하기 위해 폴링(polling)할 수 있는 작업 ID를 받게 됩니다.\n\n## 요청 본문\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  사용할 모델: 음악 생성의 경우 `suno_music`, 가사 전용의 경우 `suno_lyrics`입니다.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  생성할 음악에 대한 설명입니다. 스타일, 분위기, 악기 등을 포함할 수 있습니다.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  생성된 곡의 제목입니다.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  스타일 태그 (예: \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  생성 유형: `MUSIC` (기본값) 또는 `LYRICS`입니다.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  사용할 모델 버전입니다.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  이어 나갈 이전 클립의 ID입니다.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  이어 나갈 시작 지점의 타임스탬프(초 단위)입니다.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  최종 사용자를 위한 고유 식별자입니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"id\" type=\"string\">\n  상태 폴링을 위한 작업 ID입니다.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  작업 상태: `pending`, `processing`, `completed` 또는 `failed`입니다.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  작업이 생성된 Unix 타임스탬프입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"Musik erstellen\"\napi: \"POST /v1/music/generations\"\ndescription: \"Erstellt eine Musikgenerierungsaufgabe mit Suno\"\n---\n\nGenerieren Sie Musik und Songtexte mithilfe von KI. Dies ist eine asynchrone API – Sie erhalten eine Task-ID, um den Status abzufragen.\n\n## Anfrage-Body\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Zu verwendendes Modell: `suno_music` für die Musikgenerierung, `suno_lyrics` nur für Songtexte.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Beschreibung der zu generierenden Musik. Kann Stil, Stimmung, Instrumente usw. enthalten.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Titel für den generierten Song.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Stil-Tags (z. B. „pop, upbeat, electronic“).\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Generierungstyp: `MUSIC` (Standard) oder `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Zu verwendende Modellversion.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  ID eines vorherigen Clips, an dem fortgesetzt werden soll.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Zeitstempel (in Sekunden), ab dem fortgesetzt werden soll.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Eine eindeutige Kennung für den Endbenutzer.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"id\" type=\"string\">\n  Task-ID für die Statusabfrage.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Task-Status: `pending`, `processing`, `completed` oder `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Unix-Zeitstempel der Task-Erstellung.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Créer de la musique\"\napi: \"POST /v1/music/generations\"\ndescription: \"Crée une tâche de génération de musique à l'aide de Suno\"\n---\n\nGénérez de la musique et des paroles à l'aide de l'IA. Il s'agit d'une API asynchrone - vous recevrez un ID de tâche pour interroger l'état d'avancement.\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Modèle à utiliser : `suno_music` pour la génération de musique, `suno_lyrics` pour les paroles uniquement.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Description de la musique à générer. Peut inclure le style, l'ambiance, les instruments, etc.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Titre de la chanson générée.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Tags de style (ex : \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Type de génération : `MUSIC` (par défaut) ou `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Version du modèle à utiliser.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  ID d'un clip précédent à partir duquel continuer.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Horodatage (en secondes) à partir duquel continuer.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identifiant unique pour l'utilisateur final.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tâche pour interroger le statut.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Statut de la tâche : `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Horodatage Unix de la création de la tâche.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Crear música\"\napi: \"POST /v1/music/generations\"\ndescription: \"Crea una tarea de generación de música utilizando Suno\"\n---\n\nGenera música y letras utilizando IA. Esta es una API asíncrona: recibirás un ID de tarea para consultar el estado de finalización.\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Modelo a utilizar: `suno_music` para generación de música, `suno_lyrics` solo para letras.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Descripción de la música a generar. Puede incluir estilo, estado de ánimo, instrumentos, etc.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Título para la canción generada.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Etiquetas de estilo (por ejemplo, \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Tipo de generación: `MUSIC` (predeterminado) o `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Versión del modelo a utilizar.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  ID de un clip anterior desde el cual continuar.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Marca de tiempo (en segundos) desde la cual continuar.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identificador único para el usuario final.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tarea para consultar el estado.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Estado de la tarea: `pending`, `processing`, `completed` o `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Marca de tiempo Unix de la creación de la tarea.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Criar Música\"\napi: \"POST /v1/music/generations\"\ndescription: \"Cria uma tarefa de geração de música usando Suno\"\n---\n\nGere música e letras usando IA. Esta é uma API assíncrona - você receberá um ID de tarefa para consultar o status de conclusão.\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Modelo a ser usado: `suno_music` para geração de música, `suno_lyrics` apenas para letras.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Descrição da música a ser gerada. Pode incluir estilo, clima, instrumentos, etc.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Título para a música gerada.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Tags de estilo (ex: \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Tipo de geração: `MUSIC` (padrão) ou `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Versão do modelo a ser usada.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  ID de um clipe anterior para continuar a partir dele.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Timestamp (em segundos) para continuar a partir dele.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Um identificador exclusivo para o usuário final.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID da tarefa para consultar o status.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status da tarefa: `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Timestamp Unix da criação da tarefa.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"إنشاء الموسيقى\"\napi: \"POST /v1/music/generations\"\ndescription: \"إنشاء مهمة توليد موسيقى باستخدام Suno\"\n---\n\nقم بتوليد الموسيقى والكلمات باستخدام الذكاء الاصطناعي. هذه API غير متزامنة (asynchronous) - ستتلقى معرف مهمة (task ID) للاستعلام عن حالة الاكتمال.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  النموذج المستخدم: `suno_music` لتوليد الموسيقى، و `suno_lyrics` للكلمات فقط.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  وصف للموسيقى المراد توليدها. يمكن أن يتضمن النمط، والمزاج، والآلات الموسيقية، وما إلى ذلك.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  عنوان الأغنية التي تم توليدها.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  علامات النمط (مثل: \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  نوع التوليد: `MUSIC` (افتراضي) أو `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  إصدار النموذج المستخدم.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  معرف (ID) لمقطع سابق للمتابعة منه.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  الطابع الزمني (بالثواني) للمتابعة منه.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  معرف فريد للمستخدم النهائي.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  معرف المهمة (Task ID) للاستعلام عن الحالة.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  حالة المهمة: `pending` أو `processing` أو `completed` أو `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  طابع Unix الزمني لوقت إنشاء المهمة.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Tạo Nhạc\"\napi: \"POST /v1/music/generations\"\ndescription: \"Tạo một tác vụ tạo nhạc bằng Suno\"\n---\n\nTạo nhạc và lời bài hát bằng AI. Đây là một API bất đồng bộ - bạn sẽ nhận được một task ID để kiểm tra trạng thái hoàn thành.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Model để sử dụng: `suno_music` để tạo nhạc, `suno_lyrics` chỉ để tạo lời bài hát.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Mô tả về bản nhạc cần tạo. Có thể bao gồm phong cách, tâm trạng, nhạc cụ, v.v.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Tiêu đề cho bài hát được tạo.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Các thẻ phong cách (ví dụ: \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Loại tạo: `MUSIC` (mặc định) hoặc `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Phiên bản model để sử dụng.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  ID của một clip trước đó để tiếp tục.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Dấu thời gian (tính bằng giây) để tiếp tục.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Một mã định danh duy nhất cho người dùng cuối.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tác vụ để kiểm tra trạng thái.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Trạng thái tác vụ: `pending`, `processing`, `completed`, hoặc `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Dấu thời gian Unix khi tạo tác vụ.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Buat Musik\"\napi: \"POST /v1/music/generations\"\ndescription: \"Membuat tugas pembuatan musik menggunakan Suno\"\n---\n\nHasilkan musik dan lirik menggunakan AI. Ini adalah API asinkron - Anda akan menerima ID tugas untuk melakukan polling penyelesaian.\n\n## Body Permintaan\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Model yang digunakan: `suno_music` untuk pembuatan musik, `suno_lyrics` hanya untuk lirik.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Deskripsi musik yang akan dihasilkan. Dapat mencakup gaya, suasana hati, instrumen, dll.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Judul untuk lagu yang dihasilkan.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Tag gaya (misalnya, \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Tipe pembuatan: `MUSIC` (default) atau `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Versi model yang digunakan.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  ID klip sebelumnya untuk dilanjutkan.\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Timestamp (dalam detik) untuk dilanjutkan.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Pengidentifikasi unik untuk pengguna akhir.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tugas untuk polling status.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status tugas: `pending`, `processing`, `completed`, atau `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Unix timestamp pembuatan tugas.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance",
      "tr": "---\ntitle: \"Müzik Oluştur\"\napi: \"POST /v1/music/generations\"\ndescription: \"Suno kullanarak bir müzik oluşturma görevi oluşturur\"\n---\n\nYapay zeka kullanarak müzik ve şarkı sözü oluşturun. Bu asenkron bir API'dir - tamamlanma durumunu sorgulamak için bir görev kimliği (task ID) alacaksınız.\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" default=\"suno_music\">\n  Kullanılacak model: müzik oluşturma için `suno_music`, yalnızca şarkı sözleri için `suno_lyrics`.\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Oluşturulacak müziğin açıklaması. Tarz, ruh hali, enstrümanlar vb. içerebilir.\n</ParamField>\n\n<ParamField body=\"title\" type=\"string\">\n  Oluşturulan şarkı için başlık.\n</ParamField>\n\n<ParamField body=\"tags\" type=\"string\">\n  Tarz etiketleri (örneğin, \"pop, upbeat, electronic\").\n</ParamField>\n\n<ParamField body=\"action\" type=\"string\">\n  Oluşturma türü: `MUSIC` (varsayılan) veya `LYRICS`.\n</ParamField>\n\n<ParamField body=\"mv\" type=\"string\">\n  Kullanılacak model sürümü.\n</ParamField>\n\n<ParamField body=\"continue_clip_id\" type=\"string\">\n  Devam edilecek önceki klibin kimliği (ID).\n</ParamField>\n\n<ParamField body=\"continue_at\" type=\"number\">\n  Devam edilecek zaman damgası (saniye cinsinden).\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Son kullanıcı için benzersiz bir tanımlayıcı.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Durum sorgulama için görev kimliği (task ID).\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Görev durumu: `pending`, `processing`, `completed` veya `failed`.\n</ResponseField>\n\n<ResponseField name=\"created\" type=\"integer\">\n  Görevin oluşturulma zamanına ait Unix zaman damgası.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/music/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"suno_music\",\n    \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n    \"title\": \"Night Drive\",\n    \"tags\": \"electronic, EDM, energetic\"\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/music/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\": \"Night Drive\",\n        \"tags\": \"electronic, EDM, energetic\"\n    }\n)\n\ntask_id = response.json()[\"id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/music/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'suno_music',\n    prompt: 'An upbeat electronic dance track with heavy bass',\n    title: 'Night Drive',\n    tags: 'electronic, EDM, energetic'\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":  \"suno_music\",\n        \"prompt\": \"An upbeat electronic dance track with heavy bass\",\n        \"title\":  \"Night Drive\",\n        \"tags\":   \"electronic, EDM, energetic\",\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/music/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/music/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'suno_music',\n        'prompt' => 'An upbeat electronic dance track with heavy bass',\n        'title' => 'Night Drive',\n        'tags' => 'electronic, EDM, energetic'\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"music_abc123\",\n  \"status\": \"pending\",\n  \"created\": 1706000000,\n  \"model\": \"suno_music\"\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:27:05.148Z"
  },
  "api-reference/music/get-music-status.mdx": {
    "sourceHash": "04ba63ef61336f0c",
    "translations": {
      "zh": "---\ntitle: \"获取音乐状态\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"获取音乐生成任务的状态和结果\"\n---\n\n轮询此端点以检查音乐生成任务的状态。\n\n## 路径参数\n\n<ParamField path=\"id\" type=\"string\" required>\n  音乐生成任务 ID。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  任务 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任务状态：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  进度百分比 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  音频文件下载 URL（完成后）。\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  带有可视化效果的视频版本 URL（完成后）。\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  生成的歌曲标题。\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  生成或提供的歌词。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  错误信息（失败时）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json 响应 (已完成)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"獲取音樂狀態\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"獲取音樂生成任務的狀態與結果\"\n---\n\n輪詢此端點以檢查音樂生成任務的狀態。\n\n## 路徑參數\n\n<ParamField path=\"id\" type=\"string\" required>\n  音樂生成任務 ID。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"id\" type=\"string\">\n  任務 ID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任務狀態：`pending`、`processing`、`completed` 或 `failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  進度百分比 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  音訊檔案下載連結（完成時）。\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  帶有視覺化效果的影片版本連結（完成時）。\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  生成的歌曲標題。\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  生成或提供的歌詞。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  錯誤訊息（失敗時）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"音楽生成ステータスの取得\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"音楽生成タスクのステータスと結果を取得します\"\n---\n\nこのエンドポイントをポーリングして、音楽生成タスクのステータスを確認します。\n\n## パスパラメータ\n\n<ParamField path=\"id\" type=\"string\" required>\n  音楽生成タスクのID。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  タスクID。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  タスクのステータス: `pending`、`processing`、`completed`、または `failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  進捗率 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  オーディオファイルをダウンロードするためのURL（完了時）。\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  ビジュアライザー付きビデオバージョンのURL（完了時）。\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  生成された曲のタイトル。\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  生成された、または提供された歌詞。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  エラーメッセージ（失敗時）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"음악 생성 상태 조회\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"음악 생성 작업의 상태와 결과를 조회합니다.\"\n---\n\n이 엔드포인트를 폴링하여 음악 생성 작업의 상태를 확인하세요.\n\n## 경로 파라미터\n\n<ParamField path=\"id\" type=\"string\" required>\n  음악 생성 작업 ID입니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"id\" type=\"string\">\n  작업 ID.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  작업 상태: `pending`, `processing`, `completed` 또는 `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  진행률 백분율 (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  오디오 파일을 다운로드할 URL (완료 시).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  비주얼라이저가 포함된 비디오 버전의 URL (완료 시).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  생성된 곡 제목.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  생성되거나 제공된 가사.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  에러 메시지 (실패 시).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json 응답 (완료)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"Musik-Status abrufen\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Ruft den Status und das Ergebnis einer Musik-Generierungsaufgabe ab\"\n---\n\nFragen Sie diesen Endpunkt ab, um den Status Ihrer Musik-Generierungsaufgabe zu überprüfen.\n\n## Pfad-Parameter\n\n<ParamField path=\"id\" type=\"string\" required>\n  Die ID der Musik-Generierungsaufgabe.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"id\" type=\"string\">\n  Aufgaben-ID.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Aufgabenstatus: `pending`, `processing`, `completed` oder `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Fortschritt in Prozent (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  URL zum Herunterladen der Audiodatei (wenn abgeschlossen).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL zur Videoversion mit Visualizer (wenn abgeschlossen).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Generierter Songtitel.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Generierter oder bereitgestellter Songtext.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Fehlermeldung (wenn fehlgeschlagen).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Antwort (Abgeschlossen)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Obtenir le statut de la musique\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Récupère le statut et le résultat d'une tâche de génération de musique\"\n---\n\nInterrogez cet endpoint pour vérifier le statut de votre tâche de génération de musique.\n\n## Paramètres de chemin\n\n<ParamField path=\"id\" type=\"string\" required>\n  L'ID de la tâche de génération de musique.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tâche.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Statut de la tâche : `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Pourcentage de progression (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  URL pour télécharger le fichier audio (une fois terminé).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL vers la version vidéo avec visualiseur (une fois terminé).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Titre de la chanson générée.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Paroles générées ou fournies.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Message d'erreur (en cas d'échec).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Obtener estado de la música\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Recupera el estado y el resultado de una tarea de generación de música\"\n---\n\nRealice polling a este endpoint para verificar el estado de su tarea de generación de música.\n\n## Parámetros de ruta\n\n<ParamField path=\"id\" type=\"string\" required>\n  El ID de la tarea de generación de música.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID de la tarea.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Estado de la tarea: `pending`, `processing`, `completed` o `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Porcentaje de progreso (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  URL para descargar el archivo de audio (cuando se haya completado).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL de la versión en video con visualizador (cuando se haya completado).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Título de la canción generada.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Letras generadas o proporcionadas.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Mensaje de error (cuando falle).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Obter Status da Música\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Recupera o status e o resultado de uma tarefa de geração de música\"\n---\n\nConsulte este endpoint para verificar o status da sua tarefa de geração de música.\n\n## Parâmetros de Caminho\n\n<ParamField path=\"id\" type=\"string\" required>\n  O ID da tarefa de geração de música.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  ID da tarefa.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status da tarefa: `pending`, `processing`, `completed` ou `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Porcentagem de progresso (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  URL para baixar o arquivo de áudio (quando concluído).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL para a versão em vídeo com visualizador (quando concluído).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Título da música gerada.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Letras geradas ou fornecidas.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Mensagem de erro (quando houver falha).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Resposta (Concluída)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "ar": "",
      "vi": "---\ntitle: \"Lấy Trạng thái Nhạc\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Lấy trạng thái và kết quả của một tác vụ tạo nhạc\"\n---\n\nTruy vấn (poll) endpoint này để kiểm tra trạng thái của tác vụ tạo nhạc của bạn.\n\n## Tham số Đường dẫn\n\n<ParamField path=\"id\" type=\"string\" required>\n  ID của tác vụ tạo nhạc.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"id\" type=\"string\">\n  ID của tác vụ.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Trạng thái tác vụ: `pending`, `processing`, `completed`, hoặc `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Phần trăm tiến độ (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  URL để tải xuống tệp âm thanh (khi hoàn tất).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL dẫn đến phiên bản video với visualizer (khi hoàn tất).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Tiêu đề bài hát được tạo.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Lời bài hát được tạo hoặc được cung cấp.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Thông báo lỗi (khi thất bại).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Phản hồi (Hoàn tất)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Dapatkan Status Musik\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Mengambil status dan hasil dari tugas pembuatan musik\"\n---\n\nLakukan polling pada endpoint ini untuk memeriksa status tugas pembuatan musik Anda.\n\n## Parameter Path\n\n<ParamField path=\"id\" type=\"string\" required>\n  ID tugas pembuatan musik.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"id\" type=\"string\">\n  ID tugas.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status tugas: `pending`, `processing`, `completed`, atau `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  Persentase progres (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  URL untuk mengunduh file audio (saat selesai).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL ke versi video dengan visualizer (saat selesai).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Judul lagu yang dihasilkan.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Lirik yang dihasilkan atau disediakan.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Pesan kesalahan (saat gagal).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Polling setiap 5 detik\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Respons (Selesai)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>",
      "tr": "---\ntitle: \"Müzik Durumunu Al\"\napi: \"GET /v1/music/generations/{id}\"\ndescription: \"Bir müzik oluşturma görevinin durumunu ve sonucunu getirir\"\n---\n\nMüzik oluşturma görevinizin durumunu kontrol etmek için bu endpoint'i sorgulayın.\n\n## Yol Parametreleri\n\n<ParamField path=\"id\" type=\"string\" required>\n  Müzik oluşturma görevi ID'si.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Görev ID'si.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Görev durumu: `pending`, `processing`, `completed` veya `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"integer\">\n  İlerleme yüzdesi (0-100).\n</ResponseField>\n\n<ResponseField name=\"audio_url\" type=\"string\">\n  Ses dosyasını indirmek için URL (tamamlandığında).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  Görselleştirici içeren video sürümünün URL'si (tamamlandığında).\n</ResponseField>\n\n<ResponseField name=\"title\" type=\"string\">\n  Oluşturulan şarkı başlığı.\n</ResponseField>\n\n<ResponseField name=\"lyrics\" type=\"string\">\n  Oluşturulan veya sağlanan şarkı sözleri.\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Hata mesajı (başarısız olduğunda).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/music/generations/music_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"music_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/music/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n    result = response.json()\n\n    if result[\"status\"] == \"completed\":\n        print(f\"Audio URL: {result['audio_url']}\")\n        break\n    elif result[\"status\"] == \"failed\":\n        print(f\"Error: {result['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'music_abc123';\n\nasync function pollForMusic() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/music/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const result = await response.json();\n\n    if (result.status === 'completed') {\n      console.log(`Audio URL: ${result.audio_url}`);\n      return result.audio_url;\n    } else if (result.status === 'failed') {\n      throw new Error(result.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"music_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/music/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Audio URL: %s\\n\", result[\"audio_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'music_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/music/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $result = json_decode($response, true);\n\n    if ($result['status'] === 'completed') {\n        echo \"Audio URL: {$result['audio_url']}\\n\";\n        break;\n    } elseif ($result['status'] === 'failed') {\n        echo \"Error: {$result['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (Completed)\n{\n  \"id\": \"suno:abc123\",\n  \"status\": \"completed\",\n  \"audio_url\": \"https://cdn.suno.ai/abc123.mp3\",\n  \"video_url\": \"https://cdn.suno.ai/abc123.mp4\",\n  \"title\": \"Night Drive\",\n  \"lyrics\": \"[Verse 1]\\nDriving through the night...\"\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:27:27.211Z"
  },
  "api-reference/pricing/get-pricing.mdx": {
    "sourceHash": "eaf0951af2533116",
    "translations": {
      "zh": "---\ntitle: \"获取定价\"\napi: \"GET /v1/pricing\"\ndescription: \"获取模型的定价信息\"\n---\n\n## 查询参数\n\n<ParamField query=\"model\" type=\"string\">\n  要获取定价的模型 ID。如果未指定，则返回所有模型的定价。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"object\" type=\"string\">\n  始终为 `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  模型定价对象数组。\n\n  每个对象包含：\n  - `model` (string): 模型标识符\n  - `pricing` (object): 定价信息\n    - `input_per_1m` (string | null): 每 1M 输入 token 的美元价格\n    - `output_per_1m` (string | null): 每 1M 输出 token 的美元价格\n    - `per_request` (string | null): 每次请求的美元价格（适用于固定价格模型）\n    - `is_lock_price` (boolean): 定价是否按每次请求固定\n    - `currency` (string): 始终为 `USD`\n  - `provider` (string): 模型提供商\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 定价类型\n\n| 类型 | 描述 | 示例模型 |\n|------|-------------|----------------|\n| **基于 Token** | 按输入/输出 token 计费 | GPT-4o, Claude, Gemini |\n| **按次计费** | 每次请求固定价格 | DALL-E 3, Sora, Suno |",
      "zh-TW": "---\ntitle: \"獲取定價\"\napi: \"GET /v1/pricing\"\ndescription: \"獲取模型的定價資訊\"\n---\n\n## 查詢參數\n\n<ParamField query=\"model\" type=\"string\">\n  要獲取定價的模型 ID。如果未指定，則返回所有模型的定價。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"object\" type=\"string\">\n  始終為 `list`。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  模型定價物件的陣列。\n\n  每個物件包含：\n  - `model` (string): 模型識別碼\n  - `pricing` (object): 定價資訊\n    - `input_per_1m` (string | null): 每 1M input tokens 的美元價格\n    - `output_per_1m` (string | null): 每 1M output tokens 的美元價格\n    - `per_request` (string | null): 每次請求的美元價格（適用於固定價格模型）\n    - `is_lock_price` (boolean): 定價是否為每次請求固定價格\n    - `currency` (string): 始終為 `USD`\n  - `provider` (string): 模型供應商\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 定價類型\n\n| 類型 | 描述 | 範例模型 |\n|------|-------------|----------------|\n| **基於 Token** | 按輸入/輸出 tokens 計費 | GPT-4o, Claude, Gemini |\n| **按次計費** | 每次請求固定價格 | DALL-E 3, Sora, Suno |",
      "ja": "---\ntitle: \"料金情報の取得\"\napi: \"GET /v1/pricing\"\ndescription: \"モデルの料金情報を取得します\"\n---\n\n## クエリパラメータ\n\n<ParamField query=\"model\" type=\"string\">\n  料金を取得するモデルのID。指定しない場合、すべてのモデルの料金を返します。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"object\" type=\"string\">\n  常に `list` です。\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  モデル料金オブジェクトの配列。\n\n  各オブジェクトには以下が含まれます：\n  - `model` (string): モデル識別子\n  - `pricing` (object): 料金情報\n    - `input_per_1m` (string | null): 100万入力トークンあたりのUSD\n    - `output_per_1m` (string | null): 100万出力トークンあたりのUSD\n    - `per_request` (string | null): リクエストあたりのUSD（固定料金モデルの場合）\n    - `is_lock_price` (boolean): リクエストごとに料金が固定されているかどうか\n    - `currency` (string): 常に `USD`\n  - `provider` (string): モデルプロバイダー\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```",
      "ko": "---\ntitle: \"가격 정보 조회\"\napi: \"GET /v1/pricing\"\ndescription: \"모델의 가격 정보를 조회합니다.\"\n---\n\n## 쿼리 파라미터\n\n<ParamField query=\"model\" type=\"string\">\n  가격을 조회할 모델 ID입니다. 지정하지 않으면 모든 모델의 가격을 반환합니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"object\" type=\"string\">\n  항상 `list`입니다.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  모델 가격 객체의 배열입니다.\n\n  각 객체는 다음을 포함합니다:\n  - `model` (string): 모델 식별자\n  - `pricing` (object): 가격 정보\n    - `input_per_1m` (string | null): 100만 입력 토큰당 USD\n    - `output_per_1m` (string | null): 100만 출력 토큰당 USD\n    - `per_request` (string | null): 요청당 USD (고정 가격 모델의 경우)\n    - `is_lock_price` (boolean): 요청당 가격이 고정되어 있는지 여부\n    - `currency` (string): 항상 `USD`\n  - `provider` (string): 모델 제공자\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## 가격 유형\n\n| 유형 | 설명 | 예시 모델 |\n|------|-------------|----------------|\n| **토큰 기반** | 입력/출력 토큰당 과금 | GPT-4o, Claude, Gemini |\n| **요청당 과금** | 요청당 고정 가격 | DALL-E 3, Sora, Suno |",
      "de": "---\ntitle: \"Preise abrufen\"\napi: \"GET /v1/pricing\"\ndescription: \"Preisinformationen für Modelle abrufen\"\n---\n\n## Abfrageparameter\n\n<ParamField query=\"model\" type=\"string\">\n  Modell-ID, für die Preise abgerufen werden sollen. Falls nicht angegeben, werden die Preise für alle Modelle zurückgegeben.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"object\" type=\"string\">\n  Immer `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array von Modell-Preisobjekten.\n\n  Jedes Objekt enthält:\n  - `model` (string): Modell-Identifikator\n  - `pricing` (object): Preisinformationen\n    - `input_per_1m` (string | null): USD pro 1 Mio. Input-Tokens\n    - `output_per_1m` (string | null): USD pro 1 Mio. Output-Tokens\n    - `per_request` (string | null): USD pro Anfrage (für Modelle mit Festpreis)\n    - `is_lock_price` (boolean): Gibt an, ob der Preis pro Anfrage fest ist\n    - `currency` (string): Immer `USD`\n  - `provider` (string): Modellanbieter\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Preistypen\n\n| Typ | Beschreibung | Beispielmodelle |\n|------|-------------|----------------|\n| **Token-basiert** | Abrechnung pro Input/Output-Tokens | GPT-4o, Claude, Gemini |\n| **Pro Anfrage** | Festpreis pro Anfrage | DALL-E 3, Sora, Suno |",
      "fr": "---\ntitle: \"Obtenir les tarifs\"\napi: \"GET /v1/pricing\"\ndescription: \"Obtenir les informations de tarification pour les modèles\"\n---\n\n## Paramètres de requête\n\n<ParamField query=\"model\" type=\"string\">\n  ID du modèle pour lequel obtenir les tarifs. Si non spécifié, retourne les tarifs pour tous les modèles.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"object\" type=\"string\">\n  Toujours `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Tableau d'objets de tarification de modèle.\n\n  Chaque objet contient :\n  - `model` (string) : Identifiant du modèle\n  - `pricing` (object) : Informations de tarification\n    - `input_per_1m` (string | null) : USD pour 1M de tokens d'entrée\n    - `output_per_1m` (string | null) : USD pour 1M de tokens de sortie\n    - `per_request` (string | null) : USD par requête (pour les modèles à prix fixe)\n    - `is_lock_price` (boolean) : Indique si le tarif est fixe par requête\n    - `currency` (string) : Toujours `USD`\n  - `provider` (string) : Fournisseur du modèle\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Types de tarification\n\n| Type | Description | Exemples de modèles |\n|------|-------------|----------------|\n| **Basé sur les tokens** | Facturé par tokens d'entrée/sortie | GPT-4o, Claude, Gemini |\n| **Par requête** | Prix fixe par requête | DALL-E 3, Sora, Suno |",
      "es": "---\ntitle: \"Obtener Precios\"\napi: \"GET /v1/pricing\"\ndescription: \"Obtener información de precios para los modelos\"\n---\n\n## Parámetros de Consulta\n\n<ParamField query=\"model\" type=\"string\">\n  ID del modelo para el cual obtener los precios. Si no se especifica, devuelve los precios de todos los modelos.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"object\" type=\"string\">\n  Siempre `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array de objetos de precios de modelos.\n\n  Cada objeto contiene:\n  - `model` (string): Identificador del modelo\n  - `pricing` (object): Información de precios\n    - `input_per_1m` (string | null): USD por cada 1M de tokens de entrada\n    - `output_per_1m` (string | null): USD por cada 1M de tokens de salida\n    - `per_request` (string | null): USD por solicitud (para modelos de precio fijo)\n    - `is_lock_price` (boolean): Indica si el precio es fijo por solicitud\n    - `currency` (string): Siempre `USD`\n  - `provider` (string): Proveedor del modelo\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Tipos de Precios\n\n| Tipo | Descripción | Modelos de Ejemplo |\n|------|-------------|----------------|\n| **Basado en tokens** | Cobrado por tokens de entrada/salida | GPT-4o, Claude, Gemini |\n| **Por solicitud** | Precio fijo por solicitud | DALL-E 3, Sora, Suno |",
      "pt": "---\ntitle: \"Obter Preços\"\napi: \"GET /v1/pricing\"\ndescription: \"Obtenha informações de preços para os modelos\"\n---\n\n## Parâmetros de Consulta\n\n<ParamField query=\"model\" type=\"string\">\n  ID do modelo para obter o preço. Se não for especificado, retorna os preços de todos os modelos.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"object\" type=\"string\">\n  Sempre `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array de objetos de preços de modelos.\n\n  Cada objeto contém:\n  - `model` (string): Identificador do modelo\n  - `pricing` (object): Informações de preços\n    - `input_per_1m` (string | null): USD por 1M de tokens de entrada\n    - `output_per_1m` (string | null): USD por 1M de tokens de saída\n    - `per_request` (string | null): USD por requisição (para modelos de preço fixo)\n    - `is_lock_price` (boolean): Se o preço é fixo por requisição\n    - `currency` (string): Sempre `USD`\n  - `provider` (string): Provedor do modelo\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Tipos de Preços\n\n| Tipo | Descrição | Exemplos de Modelos |\n|------|-------------|----------------|\n| **Baseado em tokens** | Cobrado por tokens de entrada/saída | GPT-4o, Claude, Gemini |\n| **Por requisição** | Preço fixo por requisição | DALL-E 3, Sora, Suno |",
      "ar": "---\ntitle: \"الحصول على الأسعار\"\napi: \"GET /v1/pricing\"\ndescription: \"الحصول على معلومات الأسعار للنماذج\"\n---\n\n## معلمات الاستعلام (Query Parameters)\n\n<ParamField query=\"model\" type=\"string\">\n  معرف النموذج (`model ID`) للحصول على أسعاره. إذا لم يتم تحديده، فسيتم إرجاع الأسعار لجميع النماذج.\n</ParamField>\n\n## الاستجابة (Response)\n\n<ResponseField name=\"object\" type=\"string\">\n  دائماً `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  مصفوفة من كائنات أسعار النماذج.\n\n  يحتوي كل كائن على:\n  - `model` (string): معرف النموذج\n  - `pricing` (object): معلومات الأسعار\n    - `input_per_1m` (string | null): دولار أمريكي لكل 1 مليون `input tokens`\n    - `output_per_1m` (string | null): دولار أمريكي لكل 1 مليون `output tokens`\n    - `per_request` (string | null): دولار أمريكي لكل طلب (للنماذج ذات السعر الثابت)\n    - `is_lock_price` (boolean): ما إذا كان السعر ثابتاً لكل طلب\n    - `currency` (string): دائماً `USD`\n  - `provider` (string): مزود النموذج\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## أنواع التسعير\n\n| النوع | الوصف | نماذج أمثلة |\n|------|-------------|----------------|\n| **قائم على الـ Token** | يتم المحاسبة بناءً على `input/output tokens` | GPT-4o, Claude, Gemini |\n| **لكل طلب** | سعر ثابت لكل طلب | DALL-E 3, Sora, Suno |",
      "vi": "---\ntitle: \"Lấy thông tin giá cả\"\napi: \"GET /v1/pricing\"\ndescription: \"Lấy thông tin giá cả cho các mô hình\"\n---\n\n## Tham số Query\n\n<ParamField query=\"model\" type=\"string\">\n  ID của mô hình để lấy giá. Nếu không được chỉ định, sẽ trả về giá cho tất cả các mô hình.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"object\" type=\"string\">\n  Luôn là `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Mảng các đối tượng giá của mô hình.\n\n  Mỗi đối tượng bao gồm:\n  - `model` (string): Mã định danh mô hình\n  - `pricing` (object): Thông tin giá cả\n    - `input_per_1m` (string | null): USD trên 1 triệu input tokens\n    - `output_per_1m` (string | null): USD trên 1 triệu output tokens\n    - `per_request` (string | null): USD trên mỗi yêu cầu (đối với các mô hình giá cố định)\n    - `is_lock_price` (boolean): Liệu giá có được cố định trên mỗi yêu cầu hay không\n    - `currency` (string): Luôn là `USD`\n  - `provider` (string): Nhà cung cấp mô hình\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Các loại hình tính giá\n\n| Loại | Mô tả | Mô hình ví dụ |\n|------|-------------|----------------|\n| **Dựa trên token** | Tính phí trên mỗi input/output tokens | GPT-4o, Claude, Gemini |\n| **Trên mỗi yêu cầu** | Giá cố định cho mỗi yêu cầu | DALL-E 3, Sora, Suno |",
      "id": "---\ntitle: \"Dapatkan Harga\"\napi: \"GET /v1/pricing\"\ndescription: \"Dapatkan informasi harga untuk model\"\n---\n\n## Parameter Query\n\n<ParamField query=\"model\" type=\"string\">\n  ID Model untuk mendapatkan harga. Jika tidak ditentukan, mengembalikan harga untuk semua model.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"object\" type=\"string\">\n  Selalu `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Array dari objek harga model.\n\n  Setiap objek berisi:\n  - `model` (string): Identifikasi model\n  - `pricing` (object): Informasi harga\n    - `input_per_1m` (string | null): USD per 1 juta token input\n    - `output_per_1m` (string | null): USD per 1 juta token output\n    - `per_request` (string | null): USD per permintaan (untuk model dengan harga tetap)\n    - `is_lock_price` (boolean): Apakah harga tetap per permintaan\n    - `currency` (string): Selalu `USD`\n  - `provider` (string): Penyedia model\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Jenis Harga\n\n| Jenis | Deskripsi | Contoh Model |\n|------|-------------|----------------|\n| **Berbasis token** | Dikenakan biaya per token input/output | GPT-4o, Claude, Gemini |\n| **Per permintaan** | Harga tetap per permintaan | DALL-E 3, Sora, Suno |",
      "tr": "---\ntitle: \"Fiyatlandırmayı Al\"\napi: \"GET /v1/pricing\"\ndescription: \"Modeller için fiyatlandırma bilgilerini alın\"\n---\n\n## Sorgu Parametreleri\n\n<ParamField query=\"model\" type=\"string\">\n  Fiyatlandırması alınacak model ID'si. Belirtilmezse tüm modeller için fiyatlandırmayı döndürür.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"object\" type=\"string\">\n  Her zaman `list`.\n</ResponseField>\n\n<ResponseField name=\"data\" type=\"array\">\n  Model fiyatlandırma nesneleri dizisi.\n\n  Her nesne şunları içerir:\n  - `model` (string): Model tanımlayıcı\n  - `pricing` (object): Fiyatlandırma bilgisi\n    - `input_per_1m` (string | null): 1M giriş token başına USD\n    - `output_per_1m` (string | null): 1M çıkış token başına USD\n    - `per_request` (string | null): İstek başına USD (sabit fiyatlı modeller için)\n    - `is_lock_price` (boolean): Fiyatlandırmanın istek başına sabit olup olmadığı\n    - `currency` (string): Her zaman `USD`\n  - `provider` (string): Model sağlayıcısı\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\n\nresponse = requests.get(\n    \"https://api.lemondata.cc/v1/pricing\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    params={\"model\": \"gpt-4o\"}\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch(\n  'https://api.lemondata.cc/v1/pricing?model=gpt-4o',\n  {\n    headers: { 'Authorization': 'Bearer sk-your-api-key' }\n  }\n);\n\nconst data = await response.json();\nconsole.log(data);\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    req, _ := http.NewRequest(\"GET\", \"https://api.lemondata.cc/v1/pricing?model=gpt-4o\", nil)\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/pricing?model=gpt-4o');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer sk-your-api-key'\n    ]\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"model\": \"gpt-4o\",\n      \"pricing\": {\n        \"input_per_1m\": \"2.50\",\n        \"output_per_1m\": \"10.00\",\n        \"per_request\": null,\n        \"is_lock_price\": false,\n        \"currency\": \"USD\"\n      },\n      \"provider\": \"openai\"\n    }\n  ]\n}\n```\n</ResponseExample>\n\n## Fiyatlandırma Türleri\n\n| Tür | Açıklama | Örnek Modeller |\n|------|-------------|----------------|\n| **Token tabanlı** | Giriş/çıkış token'ları başına ücretlendirilir | GPT-4o, Claude, Gemini |\n| **İstek başına** | İstek başına sabit fiyat | DALL-E 3, Sora, Suno |"
    },
    "updatedAt": "2026-01-26T05:27:48.079Z"
  },
  "api-reference/rerank/create-rerank.mdx": {
    "sourceHash": "8a4847dd5aa4ab5f",
    "translations": {
      "zh": "---\ntitle: \"重排序文档\"\napi: \"POST /v1/rerank\"\ndescription: \"根据与查询的相关性对文档进行重排序\"\n---\n\n使用语义相似度模型对文档进行重排序。适用于优化搜索结果和 RAG 应用。\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的重排序模型 ID（例如 `BAAI/bge-reranker-v2-m3`，`qwen3-rerank`）。\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  用于对文档进行排序的查询语句。\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  待重排序的文档列表（字符串）。\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  返回的前几个结果的数量。默认为所有文档。\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  是否在响应中包含原始文档文本。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"results\" type=\"array\">\n  带有评分的已排序文档列表。\n\n  每个结果包含：\n  - `index` (integer): 原始文档索引\n  - `relevance_score` (number): 相关性评分 (0-1)\n  - `document` (string): 原始文本（如果 `return_documents=true`）\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  用于重排序的模型。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token 使用统计。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"重排文件\"\napi: \"POST /v1/rerank\"\ndescription: \"根據與查詢的相關性對文件進行重排\"\n---\n\n使用語義相似度模型對文件進行重排。適用於優化搜尋結果和 RAG 應用程式。\n\n## 請求主體\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的重排模型 ID（例如：`BAAI/bge-reranker-v2-m3`、`qwen3-rerank`）。\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  用於對文件進行排名的查詢語句。\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  要進行重排的文件列表（字串）。\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  要返回的前幾個結果數量。預設為所有文件。\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  是否在回應中包含原始文件文本。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"results\" type=\"array\">\n  帶有評分的已排序文件列表。\n\n  每個結果包含：\n  - `index` (integer)：原始文件索引\n  - `relevance_score` (number)：相關性評分 (0-1)\n  - `document` (string)：原始文本（若 `return_documents=true`）\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  用於重排的模型。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token 使用量統計。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is",
      "ja": "---\ntitle: \"ドキュメントの再ランク付け\"\napi: \"POST /v1/rerank\"\ndescription: \"クエリに対する関連性に基づいてドキュメントを再ランク付けします\"\n---\n\n意味的類似性モデルを使用してドキュメントを再ランク付けします。検索結果の改善や RAG アプリケーションに役立ちます。\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" required>\n  使用するリランカーモデルの ID（例：`BAAI/bge-reranker-v2-m3`、`qwen3-rerank`）。\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  ドキュメントをランク付けするためのクエリ。\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  再ランク付けするドキュメント（文字列）のリスト。\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  返却する上位結果の数。デフォルトはすべてのドキュメントです。\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  レスポンスに元のドキュメントテキストを含めるかどうか。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"results\" type=\"array\">\n  スコア順にランク付けされたドキュメントのリスト。\n\n  各結果には以下が含まれます：\n  - `index` (integer): 元のドキュメントのインデックス\n  - `relevance_score` (number): 関連性スコア (0-1)\n  - `document` (string): 元のテキスト（`return_documents=true` の場合）\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  再ランク付けに使用されたモデル。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  トークン使用統計。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"문서 재순위화 (Rerank Documents)\"\napi: \"POST /v1/rerank\"\ndescription: \"쿼리와의 관련성에 따라 문서의 순위를 재조정합니다\"\n---\n\n시맨틱 유사도 모델을 사용하여 문서의 순위를 재조정합니다. 검색 결과 개선 및 RAG 애플리케이션에 유용합니다.\n\n## 요청 본문 (Request Body)\n\n<ParamField body=\"model\" type=\"string\" required>\n  사용할 reranker 모델의 ID입니다 (예: `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  문서 순위를 매길 기준이 되는 쿼리입니다.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  재순위화할 문서(문자열) 목록입니다.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  반환할 상위 결과의 개수입니다. 기본값은 모든 문서입니다.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  응답에 원본 문서 텍스트를 포함할지 여부입니다.\n</ParamField>\n\n## 응답 (Response)\n\n<ResponseField name=\"results\" type=\"array\">\n  점수가 포함된 문서의 순위 목록입니다.\n\n  각 결과에는 다음이 포함됩니다:\n  - `index` (integer): 원본 문서 인덱스\n  - `relevance_score` (number): 관련성 점수 (0-1)\n  - `document` (string): 원본 텍스트 (`return_documents=true`인 경우)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  재순위화에 사용된 모델입니다.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  토큰 사용량 통계입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "de": "---\ntitle: \"Dokumente neu ordnen (Rerank)\"\napi: \"POST /v1/rerank\"\ndescription: \"Ordnet Dokumente nach Relevanz für eine Suchanfrage neu\"\n---\n\nOrdnen Sie Dokumente mithilfe von semantischen Ähnlichkeitsmodellen neu. Nützlich zur Verbesserung von Suchergebnissen und RAG-Anwendungen.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID des zu verwendenden Reranker-Modells (z. B. `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  Die Suchanfrage, gegen die die Dokumente bewertet werden sollen.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Liste der neu zu ordnenden Dokumente (Strings).\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Anzahl der zurückzugebenden Top-Ergebnisse. Standardmäßig werden alle Dokumente zurückgegeben.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Gibt an, ob der ursprüngliche Dokumenttext in der Antwort enthalten sein soll.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"results\" type=\"array\">\n  Rangliste der Dokumente mit Scores.\n\n  Jedes Ergebnis enthält:\n  - `index` (integer): Ursprünglicher Dokument-Index\n  - `relevance_score` (number): Relevanz-Score (0-1)\n  - `document` (string): Ursprünglicher Text (wenn `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Das für das Reranking verwendete Modell.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Statistiken zur Token-Nutzung.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"",
      "fr": "---\ntitle: \"Réorganiser les documents\"\napi: \"POST /v1/rerank\"\ndescription: \"Réorganise les documents par pertinence par rapport à une requête\"\n---\n\nRéorganisez les documents à l'aide de modèles de similitude sémantique. Utile pour améliorer les résultats de recherche et les applications RAG.\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID du modèle de reranker à utiliser (par ex., `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  La requête par rapport à laquelle classer les documents.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Liste de documents (chaînes de caractères) à réorganiser.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Nombre de meilleurs résultats à retourner. Par défaut, tous les documents.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Indique s'il faut inclure le texte original du document dans la réponse.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"results\" type=\"array\">\n  Liste classée des documents avec scores.\n\n  Chaque résultat contient :\n  - `index` (integer) : Index du document original\n  - `relevance_score` (number) : Score de pertinence (0-1)\n  - `document` (string) : Texte original (si `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Le modèle utilisé pour la réorganisation.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Statistiques d'utilisation des tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "es": "---\ntitle: \"Reordenar documentos\"\napi: \"POST /v1/rerank\"\ndescription: \"Reordena documentos por relevancia respecto a una consulta\"\n---\n\nReordena documentos utilizando modelos de similitud semántica. Útil para mejorar los resultados de búsqueda y las aplicaciones RAG.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID del modelo de reranker a utilizar (por ejemplo, `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  La consulta con la que se clasificarán los documentos.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Lista de documentos (cadenas de texto) a reordenar.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Número de mejores resultados a devolver. Por defecto se devuelven todos los documentos.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Indica si se debe incluir el texto original del documento en la respuesta.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"results\" type=\"array\">\n  Lista clasificada de documentos con puntuaciones.\n\n  Cada resultado contiene:\n  - `index` (integer): Índice original del documento\n  - `relevance_score` (number): Puntuación de relevancia (0-1)\n  - `document` (string): Texto original (si `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  El modelo utilizado para el reordenamiento.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Estadísticas de uso de tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "pt": "---\ntitle: \"Reordenar Documentos\"\napi: \"POST /v1/rerank\"\ndescription: \"Reordena documentos por relevância para uma consulta\"\n---\n\nReordene documentos usando modelos de similaridade semântica. Útil para melhorar resultados de busca e aplicações de RAG.\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID do modelo de reranker a ser usado (ex: `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  A consulta para classificar os documentos.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Lista de documentos (strings) para reordenar.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Número de resultados principais a retornar. O padrão é retornar todos os documentos.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Se deve incluir o texto original do documento na resposta.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"results\" type=\"array\">\n  Lista classificada de documentos com pontuações.\n\n  Cada resultado contém:\n  - `index` (integer): Índice original do documento\n  - `relevance_score` (number): Pontuação de relevância (0-1)\n  - `document` (string): Texto original (se `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  O modelo usado para a reordenação.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Estatísticas de uso de tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"إعادة ترتيب المستندات (Rerank Documents)\"\napi: \"POST /v1/rerank\"\ndescription: \"إعادة ترتيب المستندات بناءً على مدى صلتها بالاستعلام\"\n---\n\nإعادة ترتيب المستندات باستخدام نماذج التشابه الدلالي (semantic similarity). مفيد لتحسين نتائج البحث وتطبيقات RAG.\n\n## جسم الطلب (Request Body)\n\n<ParamField body=\"model\" type=\"string\" required>\n  معرف نموذج إعادة الترتيب (reranker) المراد استخدامه (على سبيل المثال، `BAAI/bge-reranker-v2-m3` أو `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  الاستعلام الذي سيتم ترتيب المستندات بناءً عليه.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  قائمة المستندات (نصوص) المراد إعادة ترتيبها.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  عدد أفضل النتائج التي سيتم إرجاعها. القيمة الافتراضية هي جميع المستندات.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  ما إذا كان سيتم تضمين نص المستند الأصلي في الاستجابة.\n</ParamField>\n\n## الاستجابة (Response)\n\n<ResponseField name=\"results\" type=\"array\">\n  قائمة مرتبة من المستندات مع الدرجات.\n\n  تحتوي كل نتيجة على:\n  - `index` (integer): فهرس المستند الأصلي\n  - `relevance_score` (number): درجة الصلة (0-1)\n  - `document` (string): النص الأصلي (إذا كان `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  النموذج المستخدم لإعادة الترتيب.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  إحصائيات استخدام الـ token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Xếp hạng lại tài liệu\"\napi: \"POST /v1/rerank\"\ndescription: \"Xếp hạng lại các tài liệu theo mức độ liên quan với một truy vấn\"\n---\n\nXếp hạng lại các tài liệu bằng cách sử dụng các mô hình tương đồng ngữ nghĩa. Hữu ích cho việc cải thiện kết quả tìm kiếm và các ứng dụng RAG.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID của mô hình reranker cần sử dụng (ví dụ: `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  Truy vấn để xếp hạng các tài liệu dựa trên đó.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Danh sách các tài liệu (chuỗi) cần xếp hạng lại.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Số lượng kết quả hàng đầu cần trả về. Mặc định là tất cả tài liệu.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Có bao gồm văn bản tài liệu gốc trong phản hồi hay không.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"results\" type=\"array\">\n  Danh sách các tài liệu đã được xếp hạng kèm theo điểm số.\n\n  Mỗi kết quả bao gồm:\n  - `index` (integer): Chỉ mục tài liệu gốc\n  - `relevance_score` (number): Điểm số liên quan (0-1)\n  - `document` (string): Văn bản gốc (nếu `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Mô hình được sử dụng để xếp hạng lại.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Thống kê mức sử dụng token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Rerank Dokumen\"\napi: \"POST /v1/rerank\"\ndescription: \"Mengurutkan ulang dokumen berdasarkan relevansi terhadap kueri\"\n---\n\nUrutkan ulang dokumen menggunakan model kesamaan semantik. Berguna untuk meningkatkan hasil pencarian dan aplikasi RAG.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID model reranker yang akan digunakan (misalnya, `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  Kueri untuk membandingkan peringkat dokumen.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Daftar dokumen (string) untuk diurutkan ulang.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Jumlah hasil teratas yang akan dikembalikan. Default ke semua dokumen.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Apakah akan menyertakan teks dokumen asli dalam respons.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"results\" type=\"array\">\n  Daftar dokumen yang telah diurutkan beserta skornya.\n\n  Setiap hasil berisi:\n  - `index` (integer): Indeks dokumen asli\n  - `relevance_score` (number): Skor relevansi (0-1)\n  - `document` (string): Teks asli (jika `return_documents=true`)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Model yang digunakan untuk reranking.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Statistik penggunaan token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>",
      "tr": "---\ntitle: \"Dokümanları Yeniden Sırala\"\napi: \"POST /v1/rerank\"\ndescription: \"Dokümanları bir sorguyla olan ilgi düzeyine göre yeniden sıralar\"\n---\n\nSemantik benzerlik modellerini kullanarak dokümanları yeniden sıralayın. Arama sonuçlarını ve RAG uygulamalarını iyileştirmek için kullanışlıdır.\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" required>\n  Kullanılacak reranker modelinin kimliği (örneğin, `BAAI/bge-reranker-v2-m3`, `qwen3-rerank`).\n</ParamField>\n\n<ParamField body=\"query\" type=\"string\" required>\n  Dokümanların sıralanacağı sorgu.\n</ParamField>\n\n<ParamField body=\"documents\" type=\"array\" required>\n  Yeniden sıralanacak dokümanların (dizelerin) listesi.\n</ParamField>\n\n<ParamField body=\"top_n\" type=\"integer\">\n  Döndürülecek en iyi sonuçların sayısı. Varsayılan olarak tüm dokümanlar döndürülür.\n</ParamField>\n\n<ParamField body=\"return_documents\" type=\"boolean\" default=\"false\">\n  Orijinal doküman metninin yanıtta yer alıp almayacağı.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"results\" type=\"array\">\n  Puanlarıyla birlikte sıralanmış doküman listesi.\n\n  Her sonuç şunları içerir:\n  - `index` (integer): Orijinal doküman dizini\n  - `relevance_score` (number): İlgi düzeyi puanı (0-1)\n  - `document` (string): Orijinal metin (eğer `return_documents=true` ise)\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Yeniden sıralama için kullanılan model.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token kullanım istatistikleri.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/rerank\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"BAAI/bge-reranker-v2-m3\",\n    \"query\": \"What is machine learning?\",\n    \"documents\": [\n      \"Machine learning is a subset of AI\",\n      \"The weather is nice today\",\n      \"Deep learning uses neural networks\"\n    ],\n    \"top_n\": 2,\n    \"return_documents\": true\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/rerank\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": [\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\"\n        ],\n        \"top_n\": 2\n    }\n)\n\nprint(response.json())\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/rerank', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'BAAI/bge-reranker-v2-m3',\n    query: 'What is machine learning?',\n    documents: [\n      'Machine learning is a subset of AI',\n      'The weather is nice today',\n      'Deep learning uses neural networks'\n    ],\n    top_n: 2\n  })\n});\n\nconst data = await response.json();\nconsole.log(data.results);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"BAAI/bge-reranker-v2-m3\",\n        \"query\": \"What is machine learning?\",\n        \"documents\": []string{\n            \"Machine learning is a subset of AI\",\n            \"The weather is nice today\",\n            \"Deep learning uses neural networks\",\n        },\n        \"top_n\": 2,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/rerank\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"results\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/rerank');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'BAAI/bge-reranker-v2-m3',\n        'query' => 'What is machine learning?',\n        'documents' => [\n            'Machine learning is a subset of AI',\n            'The weather is nice today',\n            'Deep learning uses neural networks'\n        ],\n        'top_n' => 2\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['results']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.95,\n      \"document\": \"Machine learning is a subset of AI\"\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.82,\n      \"document\": \"Deep learning uses neural networks\"\n    }\n  ],\n  \"model\": \"BAAI/bge-reranker-v2-m3\",\n  \"usage\": {\n    \"prompt_tokens\": 45,\n    \"total_tokens\": 45\n  }\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:28:11.539Z"
  },
  "api-reference/responses/create-response.mdx": {
    "sourceHash": "409ad62c8334f226",
    "translations": {
      "zh": "---\ntitle: \"创建响应\"\napi: \"POST /v1/responses\"\ndescription: \"使用 OpenAI Responses API 格式创建响应\"\n---\n\nResponses API 是 OpenAI 较新的有状态对话 API。LemonData 为兼容模型支持此格式。\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的模型 ID。请参阅 [模型](https://lemondata.cc/zh/models) 以获取可用选项。\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  包含对话的输入项列表。\n\n  每个项目可以是：\n  - `message`: 包含角色和内容的对话消息\n  - `function_call`: 函数调用请求\n  - `function_call_output`: 函数调用输出\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  模型的系统指令（相当于系统消息）。\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  生成的最大 token 数量。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  介于 0 到 2 之间的采样温度。\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  模型可能调用的工具列表。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  如果为 true，则返回事件流。\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  用于继续对话的上一个响应的 ID。\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  是否存储响应以便稍后检索。\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  附加到响应的元数据，用于跟踪目的。\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  文本生成配置选项。\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  是否允许并行进行多个工具调用。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  核采样参数 (0-1)。\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  o1/o3 模型的推理配置。\n\n  - `effort` (string): 推理努力程度 (`low`, `medium`, `high`)\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"id\" type=\"string\">\n  响应的唯一标识符。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  始终为 `response`。\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  响应创建时的 Unix 时间戳。\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  模型生成的输出项列表。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token 使用统计数据。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "zh-TW": "---\ntitle: \"建立回應\"\napi: \"POST /v1/responses\"\ndescription: \"使用 OpenAI Responses API 格式建立回應\"\n---\n\nResponses API 是 OpenAI 較新的有狀態對話 API。LemonData 為相容模型提供此格式的支援。\n\n## 請求主體\n\n<ParamField body=\"model\" type=\"string\" required>\n  要使用的模型 ID。請參閱 [Models](https://lemondata.cc/zh-TW/models) 以查看可用選項。\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  組成對話的輸入項目列表。\n\n  每個項目可以是：\n  - `message`：包含角色和內容的對話訊息\n  - `function_call`：函式呼叫請求\n  - `function_call_output`：函式呼叫的輸出\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  模型的系統指令（相當於系統訊息）。\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  要生成的最大 token 數量。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  介於 0 到 2 之間的取樣溫度。\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  模型可能呼叫的工具列表。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  若為 true，則回傳事件串流。\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  先前回應的 ID，用於接續對話。\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  是否儲存回應以便稍後檢索。\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  附加到回應的元數據，用於追蹤目的。\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  文字生成配置選項。\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  是否允許同時進行多個工具呼叫。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  核取樣參數 (0-1)。\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  o1/o3 模型的推理配置。\n\n  - `effort` (string)：推理強度等級 (`low`, `medium`, `high`)\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"id\" type=\"string\">\n  回應的唯一識別碼。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  固定為 `response`。\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  回應建立時的 Unix 時間戳記。\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  模型生成的輸出項目列表。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token 使用量統計。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "ja": "---\ntitle: \"レスポンスの作成\"\napi: \"POST /v1/responses\"\ndescription: \"OpenAI Responses API形式を使用してレスポンスを作成します\"\n---\n\nResponses APIは、OpenAIの新しいステートフルな会話用APIです。LemonDataは、互換性のあるモデルでこの形式をサポートしています。\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" required>\n  使用するモデルのID。利用可能なオプションについては[Models](https://lemondata.cc/ja/models)を参照してください。\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  会話を構成する入力項目のリスト。\n\n  各項目は以下のいずれかです：\n  - `message`: ロールとコンテンツを含む会話メッセージ\n  - `function_call`: 関数呼び出しリクエスト\n  - `function_call_output`: 関数呼び出しからの出力\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  モデルへのシステム指示（システムメッセージに相当）。\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  生成する最大トークン数。\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  0から2の間のサンプリング温度。\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  モデルが呼び出す可能性のあるツールのリスト。\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  trueの場合、イベントのストリームを返します。\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  会話を継続するための、以前のレスポンスのID。\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  後で取得するためにレスポンスを保存するかどうか。\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  トラッキング目的でレスポンスに付加するメタデータ。\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  テキスト生成の設定オプション。\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  複数のツール呼び出しを並列で許可するかどうか。\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  核サンプリング（Nucleus sampling）パラメータ (0-1)。\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  o1/o3モデルの推論設定。\n\n  - `effort` (string): 推論の努力レベル (`low`, `medium`, `high`)\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"id\" type=\"string\">\n  レスポンスの一意識別子。\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  常に `response`。\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  レスポンスが作成された時のUnixタイムスタンプ。\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  モデルによって生成された出力項目のリスト。\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  トークン使用統計。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "ko": "---\ntitle: \"응답 생성\"\napi: \"POST /v1/responses\"\ndescription: \"OpenAI Responses API 형식을 사용하여 응답을 생성합니다.\"\n---\n\nResponses API는 OpenAI의 새로운 상태 유지형(stateful) 대화 API입니다. LemonData는 호환되는 모델에 대해 이 형식을 지원합니다.\n\n## 요청 본문\n\n<ParamField body=\"model\" type=\"string\" required>\n  사용할 모델의 ID입니다. 사용 가능한 옵션은 [모델](https://lemondata.cc/ko/models)을 참조하세요.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  대화를 구성하는 입력 항목 목록입니다.\n\n  각 항목은 다음과 같을 수 있습니다:\n  - `message`: 역할(role)과 내용(content)이 포함된 대화 메시지\n  - `function_call`: 함수 호출 요청\n  - `function_call_output`: 함수 호출 결과물\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  모델을 위한 시스템 지침입니다 (시스템 메시지와 동일).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  생성할 최대 토큰 수입니다.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  0에서 2 사이의 샘플링 온도입니다.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  모델이 호출할 수 있는 도구 목록입니다.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  true인 경우, 이벤트 스트림을 반환합니다.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  대화를 이어가기 위한 이전 응답의 ID입니다.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  나중에 조회할 수 있도록 응답을 저장할지 여부입니다.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  추적 목적으로 응답에 첨부할 메타데이터입니다.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  텍스트 생성 구성 옵션입니다.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  여러 도구 호출을 병렬로 허용할지 여부입니다.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Nucleus 샘플링 파라미터 (0-1)입니다.\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  o1/o3 모델을 위한 추론(Reasoning) 구성입니다.\n\n  - `effort` (string): 추론 노력 수준 (`low`, `medium`, `high`)\n</ParamField",
      "de": "---\ntitle: \"Response erstellen\"\napi: \"POST /v1/responses\"\ndescription: \"Erstellt eine Response unter Verwendung des OpenAI Responses API-Formats\"\n---\n\nDie Responses API ist die neuere zustandsbehaftete Konversations-API von OpenAI. LemonData unterstützt dieses Format für kompatible Modelle.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID des zu verwendenden Modells. Siehe [Models](https://lemondata.cc/de/models) für verfügbare Optionen.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  Eine Liste von Eingabeelementen, aus denen die Konversation besteht.\n\n  Jedes Element kann sein:\n  - `message`: Eine Konversationsnachricht mit Rolle und Inhalt\n  - `function_call`: Eine Funktionsaufrufanfrage\n  - `function_call_output`: Ausgabe eines Funktionsaufrufs\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  Systemanweisungen für das Modell (entspricht der Systemnachricht).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  Maximale Anzahl der zu generierenden Token.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Sampling-Temperatur zwischen 0 und 2.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Eine Liste von Tools, die das Modell aufrufen kann.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Wenn true, wird ein Stream von Ereignissen zurückgegeben.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  ID einer vorherigen Response, um die Konversation fortzusetzen.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  Gibt an, ob die Response für einen späteren Abruf gespeichert werden soll.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadaten, die der Response zu Tracking-Zwecken beigefügt werden.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  Konfigurationsoptionen für die Textgenerierung.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Gibt an, ob mehrere Tool-Aufrufe parallel zulässig sind.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Nucleus-Sampling-Parameter (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  Reasoning-Konfiguration für o1/o3-Modelle.\n\n  - `effort` (string): Reasoning-Aufwandsebene (`low`, `medium`, `high`)\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  Eindeutiger Identifikator für die Response.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Immer `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Unix-Zeitstempel des Zeitpunkts, an dem die Response erstellt wurde.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  Liste der vom Modell generierten Ausgabeelemente.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token-Nutzungsstatistiken.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "fr": "---\ntitle: \"Créer une réponse\"\napi: \"POST /v1/responses\"\ndescription: \"Crée une réponse en utilisant le format de l'API OpenAI Responses\"\n---\n\nL'API Responses est la nouvelle API de conversation avec état d'OpenAI. LemonData prend en charge ce format pour les modèles compatibles.\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID du modèle à utiliser. Voir [Models](https://lemondata.cc/fr/models) pour les options disponibles.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  Une liste d'éléments d'entrée composant la conversation.\n\n  Chaque élément peut être :\n  - `message` : Un message de conversation avec un rôle et un contenu\n  - `function_call` : Une requête d'appel de fonction\n  - `function_call_output` : Le résultat d'un appel de fonction\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  Instructions système pour le modèle (équivalent au message système).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  Nombre maximum de tokens à générer.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Température d'échantillonnage entre 0 et 2.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Une liste d'outils que le modèle peut appeler.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Si vrai, renvoie un flux d'événements.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  ID d'une réponse précédente pour continuer la conversation à partir de celle-ci.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  Indique s'il faut stocker la réponse pour une récupération ultérieure.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Métadonnées à joindre à la réponse à des fins de suivi.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  Options de configuration de la génération de texte.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Indique s'il faut autoriser plusieurs appels d'outils en parallèle.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Paramètre d'échantillonnage nucléaire (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  Configuration du raisonnement pour les modèles o1/o3.\n\n  - `effort` (string) : Niveau d'effort de raisonnement (`low`, `medium`, `high`)\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"id\" type=\"string\">\n  Identifiant unique de la réponse.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Toujours `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Horodatage Unix du moment où la réponse a été créée.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  Liste des éléments de sortie générés par le modèle.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Statistiques d'utilisation des tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "es": "",
      "pt": "---\ntitle: \"Criar Resposta\"\napi: \"POST /v1/responses\"\ndescription: \"Cria uma resposta usando o formato da API de Responses da OpenAI\"\n---\n\nA API de Responses é a nova API de conversação com estado da OpenAI. O LemonData suporta este formato para modelos compatíveis.\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID do modelo a ser utilizado. Veja [Models](https://lemondata.cc/pt/models) para as opções disponíveis.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  Uma lista de itens de entrada que compõem a conversa.\n\n  Cada item pode ser:\n  - `message`: Uma mensagem de conversa com `role` e `content`\n  - `function_call`: Uma solicitação de chamada de função\n  - `function_call_output`: Saída de uma chamada de função\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  Instruções de sistema para o modelo (equivalente à mensagem de sistema).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  Número máximo de tokens a serem gerados.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Temperatura de amostragem entre 0 e 2.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Uma lista de ferramentas que o modelo pode chamar.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Se verdadeiro, retorna um fluxo (stream) de eventos.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  ID de uma resposta anterior para continuar a conversa a partir dela.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  Se deve armazenar a resposta para recuperação posterior.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadados para anexar à resposta para fins de rastreamento.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  Opções de configuração de geração de texto.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Se deve permitir múltiplas chamadas de ferramentas em paralelo.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Parâmetro de amostragem de núcleo (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  Configuração de raciocínio para modelos o1/o3.\n\n  - `effort` (string): Nível de esforço de raciocínio (`low`, `medium`, `high`)\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"id\" type=\"string\">\n  Identificador único para a resposta.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Sempre `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Timestamp Unix de quando a resposta foi criada.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  Lista de itens de saída gerados pelo modelo.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Estatísticas de uso de tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "ar": "---\ntitle: \"إنشاء استجابة\"\napi: \"POST /v1/responses\"\ndescription: \"يقوم بإنشاء استجابة باستخدام تنسيق OpenAI Responses API\"\n---\n\nتعد واجهة برمجة تطبيقات الاستجابات (Responses API) هي واجهة برمجة تطبيقات المحادثة الأحدث من OpenAI التي تحافظ على الحالة (stateful). يدعم LemonData هذا التنسيق للنماذج المتوافقة.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  معرف النموذج المراد استخدامه. راجع [Models](https://lemondata.cc/ar/models) للخيارات المتاحة.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  قائمة من عناصر المدخلات التي تتكون منها المحادثة.\n\n  يمكن أن يكون كل عنصر:\n  - `message`: رسالة محادثة تحتوي على الدور (role) والمحتوى (content)\n  - `function_call`: طلب استدعاء وظيفة (function call)\n  - `function_call_output`: مخرجات من استدعاء وظيفة\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  تعليمات النظام للنموذج (تعادل رسالة النظام).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  الحد الأقصى لعدد الـ tokens المراد إنشاؤها.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  درجة حرارة أخذ العينات (Sampling temperature) بين 0 و 2.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  قائمة بالأدوات التي قد يستدعيها النموذج.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  إذا كانت القيمة true، فسيتم إرجاع تدفق (stream) من الأحداث.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  معرف استجابة سابقة لمواصلة المحادثة منها.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  ما إذا كان سيتم تخزين الاستجابة لاستردادها لاحقاً.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  بيانات وصفية (Metadata) لإرفاقها بالاستجابة لأغراض التتبع.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  خيارات تكوين إنشاء النصوص.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  ما إذا كان سيتم السماح باستدعاءات أدوات متعددة بشكل متوازٍ.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  معلمة أخذ عينات النواة (Nucleus sampling) بين (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  تكوين التفكير (Reasoning) لنماذج o1/o3.\n\n  - `effort` (string): مستوى جهد التفكير (`low`, `medium`, `high`)\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  معرف فريد للاستجابة.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  دائماً `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  طابع زمني Unix لوقت إنشاء الاستجابة.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  قائمة بعناصر المخرجات التي أنشأها النموذج.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  إحصائيات استخدام الـ tokens.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "vi": "---\ntitle: \"Tạo Response\"\napi: \"POST /v1/responses\"\ndescription: \"Tạo một response bằng định dạng OpenAI Responses API\"\n---\n\nResponses API là API hội thoại có trạng thái (stateful) mới hơn của OpenAI. LemonData hỗ trợ định dạng này cho các mô hình tương thích.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID của mô hình cần sử dụng. Xem [Models](https://lemondata.cc/vi/models) để biết các tùy chọn có sẵn.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  Một danh sách các mục đầu vào bao gồm cuộc hội thoại.\n\n  Mỗi mục có thể là:\n  - `message`: Một tin nhắn hội thoại với vai trò (role) và nội dung (content)\n  - `function_call`: Một yêu cầu gọi hàm (function call)\n  - `function_call_output`: Kết quả đầu ra từ một cuộc gọi hàm\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  Các chỉ dẫn hệ thống cho mô hình (tương đương với tin nhắn hệ thống).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  Số lượng token tối đa được tạo ra.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Nhiệt độ lấy mẫu (sampling temperature) trong khoảng từ 0 đến 2.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Danh sách các công cụ mà mô hình có thể gọi.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Nếu là true, trả về một luồng (stream) các sự kiện.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  ID của một response trước đó để tiếp tục cuộc hội thoại.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  Có lưu trữ response để truy xuất sau này hay không.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadata để đính kèm vào response cho mục đích theo dõi.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  Các tùy chọn cấu hình tạo văn bản.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Có cho phép nhiều cuộc gọi công cụ song song hay không.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Tham số lấy mẫu hạt nhân (nucleus sampling) (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  Cấu hình suy luận (reasoning) cho các mô hình o1/o3.\n\n  - `effort` (string): Mức độ nỗ lực suy luận (`low`, `medium`, `high`)\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  Định danh duy nhất cho response.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Luôn là `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Dấu thời gian Unix (Unix timestamp) khi response được tạo.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  Danh sách các mục đầu ra được tạo bởi mô hình.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Thống kê sử dụng token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "id": "---\ntitle: \"Buat Response\"\napi: \"POST /v1/responses\"\ndescription: \"Membuat response menggunakan format OpenAI Responses API\"\n---\n\nResponses API adalah API percakapan stateful terbaru dari OpenAI. LemonData mendukung format ini untuk model yang kompatibel.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" required>\n  ID model yang akan digunakan. Lihat [Models](https://lemondata.cc/id/models) untuk opsi yang tersedia.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  Daftar item input yang menyusun percakapan.\n\n  Setiap item dapat berupa:\n  - `message`: Pesan percakapan dengan role dan content\n  - `function_call`: Permintaan panggilan fungsi\n  - `function_call_output`: Output dari panggilan fungsi\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  Instruksi sistem untuk model (setara dengan system message).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  Jumlah maksimum token yang akan dihasilkan.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  Temperatur sampling antara 0 dan 2.\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Daftar tool yang mungkin dipanggil oleh model.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Jika true, mengembalikan stream event.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  ID dari response sebelumnya untuk melanjutkan percakapan.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  Apakah akan menyimpan response untuk pengambilan di lain waktu.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Metadata untuk dilampirkan pada response untuk tujuan pelacakan.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  Opsi konfigurasi pembuatan teks.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Apakah akan mengizinkan beberapa panggilan tool secara paralel.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Parameter nucleus sampling (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  Konfigurasi reasoning untuk model o1/o3.\n\n  - `effort` (string): Tingkat upaya reasoning (`low`, `medium`, `high`)\n</ParamField>\n\n## Response\n\n<ResponseField name=\"id\" type=\"string\">\n  Identifier unik untuk response.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Selalu `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Timestamp Unix saat response dibuat.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  Daftar item output yang dihasilkan oleh model.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Statistik penggunaan token.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>",
      "tr": "---\ntitle: \"Yanıt Oluştur\"\napi: \"POST /v1/responses\"\ndescription: \"OpenAI Responses API formatını kullanarak bir yanıt oluşturur\"\n---\n\nResponses API, OpenAI'ın daha yeni durum bilgisi koruyan (stateful) sohbet API'ıdır. LemonData, uyumlu modeller için bu formatı destekler.\n\n## İstek Gövdesi\n\n<ParamField body=\"model\" type=\"string\" required>\n  Kullanılacak modelin ID'si. Mevcut seçenekler için [Models](https://lemondata.cc/tr/models) sayfasına bakın.\n</ParamField>\n\n<ParamField body=\"input\" type=\"array\" required>\n  Sohbeti oluşturan girdi öğelerinin listesi.\n\n  Her bir öğe şunlar olabilir:\n  - `message`: Rol ve içerik barındıran bir sohbet mesajı\n  - `function_call`: Bir fonksiyon çağırma isteği\n  - `function_call_output`: Bir fonksiyon çağrısından gelen çıktı\n</ParamField>\n\n<ParamField body=\"instructions\" type=\"string\">\n  Model için sistem talimatları (sistem mesajına eşdeğerdir).\n</ParamField>\n\n<ParamField body=\"max_output_tokens\" type=\"integer\">\n  Oluşturulacak maksimum token sayısı.\n</ParamField>\n\n<ParamField body=\"temperature\" type=\"number\" default=\"1\">\n  0 ile 2 arasında örnekleme sıcaklığı (temperature).\n</ParamField>\n\n<ParamField body=\"tools\" type=\"array\">\n  Modelin çağırabileceği araçların listesi.\n</ParamField>\n\n<ParamField body=\"stream\" type=\"boolean\" default=\"false\">\n  Eğer true ise, bir olay akışı (stream) döndürür.\n</ParamField>\n\n<ParamField body=\"previous_response_id\" type=\"string\">\n  Sohbete devam etmek için önceki bir yanıtın ID'si.\n</ParamField>\n\n<ParamField body=\"store\" type=\"boolean\" default=\"true\">\n  Yanıtın daha sonra geri çağrılmak üzere saklanıp saklanmayacağı.\n</ParamField>\n\n<ParamField body=\"metadata\" type=\"object\">\n  Takip amaçlı olarak yanıta eklenecek meta veriler.\n</ParamField>\n\n<ParamField body=\"text\" type=\"object\">\n  Metin oluşturma yapılandırma seçenekleri.\n</ParamField>\n\n<ParamField body=\"parallel_tool_calls\" type=\"boolean\" default=\"true\">\n  Paralel olarak birden fazla araç çağrısına izin verilip verilmeyeceği.\n</ParamField>\n\n<ParamField body=\"top_p\" type=\"number\">\n  Nucleus örnekleme parametresi (0-1).\n</ParamField>\n\n<ParamField body=\"reasoning\" type=\"object\">\n  o1/o3 modelleri için akıl yürütme (reasoning) yapılandırması.\n\n  - `effort` (string): Akıl yürütme çaba seviyesi (`low`, `medium`, `high`)\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"id\" type=\"string\">\n  Yanıt için benzersiz tanımlayıcı.\n</ResponseField>\n\n<ResponseField name=\"object\" type=\"string\">\n  Her zaman `response`.\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Yanıtın oluşturulduğu zamanın Unix zaman damgası.\n</ResponseField>\n\n<ResponseField name=\"output\" type=\"array\">\n  Model tarafından oluşturulan çıktı öğelerinin listesi.\n</ResponseField>\n\n<ResponseField name=\"usage\" type=\"object\">\n  Token kullanım istatistikleri.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/responses\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"input\": [\n      {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    \"max_output_tokens\": 1000\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.responses.create(\n    model=\"gpt-4o\",\n    input=[\n        {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    max_output_tokens=1000\n)\n\nprint(response.output)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.responses.create({\n  model: 'gpt-4o',\n  input: [\n    { type: 'message', role: 'user', content: 'Hello!' }\n  ],\n  max_output_tokens: 1000\n});\n\nconsole.log(response.output);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\": \"gpt-4o\",\n        \"input\": []map[string]interface{}{\n            {\"type\": \"message\", \"role\": \"user\", \"content\": \"Hello!\"},\n        },\n        \"max_output_tokens\": 1000,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/responses\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Println(result[\"output\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/responses');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'input' => [\n            ['type' => 'message', 'role' => 'user', 'content' => 'Hello!']\n        ],\n        'max_output_tokens' => 1000\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\nprint_r($data['output']);\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"id\": \"resp_abc123\",\n  \"object\": \"response\",\n  \"created_at\": 1706000000,\n  \"model\": \"gpt-4o\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\"type\": \"text\", \"text\": \"Hello! How can I help you today?\"}\n      ]\n    }\n  ],\n  \"usage\": {\n    \"input_tokens\": 10,\n    \"output_tokens\": 12,\n    \"total_tokens\": 22\n  }\n}\n```\n</ResponseExample>"
    },
    "updatedAt": "2026-01-26T05:28:42.132Z"
  },
  "api-reference/video/create-video.mdx": {
    "sourceHash": "33b3f16c1d5a84bc",
    "translations": {
      "zh": "---\ntitle: \"创建视频\"\napi: \"POST /v1/videos/generations\"\ndescription: \"创建一个视频生成任务\"\n---\n\n## 概览\n\n视频生成是异步的。您提交请求并接收一个任务 ID，然后轮询结果。\n\n## 请求体\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  视频模型（例如：`kling-video`，`veo3.1`，`wan2.6-i2v`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  要生成的视频的文本描述。\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  用于图生视频生成的可选起始图片。\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  视频时长，以秒为单位（取决于模型）。\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  宽高比（例如：`16:9`，`9:16`，`1:1`）。\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  视频分辨率（例如：`1080p`，`720p`）。\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  每秒帧数 (1-120)。\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  视频生成中需要避免的内容。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  用于可复现生成的随机种子。\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  无分类器指导比例 (0-20)。\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  运动强度 (0-1)。\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  起始帧图片的 URL。\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  结束帧图片的 URL。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  终端用户的唯一标识符。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"task_id\" type=\"string\">\n  用于轮询的唯一任务标识符。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  初始状态：`pending`。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用的模型。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 图生视频\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## 可用模型\n\n| 模型 | 类型 | 描述 |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | 逼真的动态效果，多个版本 (v1.0-v2.6) |\n| `veo3.1` | T2V | Google 最新模型 |\n| `veo3.1-pro` | T2V | 专业级质量 |\n| `minimax/video-01` | T2V | 高节奏感与稳定性 |\n| `wan2.6-i2v` | I2V | 图生视频 |",
      "zh-TW": "---\ntitle: \"建立影片\"\napi: \"POST /v1/videos/generations\"\ndescription: \"建立影片生成任務\"\n---\n\n## 概覽\n\n影片生成是非同步的。您提交請求並收到一個 task ID，然後輪詢結果。\n\n## 請求主體\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  影片模型（例如：`kling-video`、`veo3.1`、`wan2.6-i2v`）。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  要生成的影片文字描述。\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  圖生影片（image-to-video）生成的選填起始圖片。\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  影片時長，以秒為單位（取決於模型）。\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  長寬比（例如：`16:9`、`9:16`、`1:1`）。\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  影片解析度（例如：`1080p`、`720p`）。\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  每秒幀數 (1-120)。\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  影片生成中要避免的內容。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  用於可重複生成的隨機種子。\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  無分類器指導比例 (0-20)。\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  動作強度 (0-1)。\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  起始幀圖片的 URL。\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  結束幀圖片的 URL。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  終端用戶的唯一識別碼。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"task_id\" type=\"string\">\n  用於輪詢的唯一任務識別碼。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  初始狀態：`pending`。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用的模型。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 圖生影片\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## 可用模型\n\n| 模型 | 類型 | 描述 |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | 逼真的動作，多個版本 (v1.0-v2.6) |\n| `veo3.1` | T2V | Google 最新模型 |\n| `veo3.1-pro` | T2V | 專業級品質 |\n| `minimax/video-01` | T2V | 高節奏感與穩定性 |\n| `wan2.6-i2v` | I2V | 圖生影片 |",
      "ja": "---\ntitle: \"ビデオ生成\"\napi: \"POST /v1/videos/generations\"\ndescription: \"ビデオ生成タスクを作成します\"\n---\n\n## 概要\n\nビデオ生成は非同期で行われます。リクエストを送信してタスク ID を受け取り、その後結果をポーリングして取得します。\n\n## リクエストボディ\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  ビデオモデル (例: `kling-video`, `veo3.1`, `wan2.6-i2v`)。\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  生成するビデオのテキスト説明。\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  image-to-video 生成用のオプションの開始画像。\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  ビデオの長さ（秒単位、モデルに依存）。\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  アスペクト比 (例: `16:9`, `9:16`, `1:1`)。\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  ビデオ解像度 (例: `1080p`, `720p`)。\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  フレームレート (1-120)。\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  ビデオ生成で避けるべき内容。\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  再現可能な生成のためのランダムシード。\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Classifier-free guidance スケール (0-20)。\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  モーションの強さ (0-1)。\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  開始フレーム画像の URL。\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  終了フレーム画像の URL。\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  エンドユーザーの一意の識別子。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"task_id\" type=\"string\">\n  ポーリング用の一意のタスク識別子。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  初期ステータス: `pending`。\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  使用されたモデル。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 画像からビデオ生成\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## 利用可能なモデル\n\n| モデル | タイプ | 説明 |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | リアルな動き、複数のバージョン (v1.0-v2.6) |\n| `veo3.1` | T2V | Google の最新モデル |\n| `veo3.1-pro` | T2V | プロフェッショナル品質 |\n| `minimax/video-01` | T2V | 高いリズム感と安定性 |\n| `wan2.6-i2v` | I2V | 画像からビデオ生成 |",
      "ko": "---\ntitle: \"비디오 생성\"\napi: \"POST /v1/videos/generations\"\ndescription: \"비디오 생성 작업을 생성합니다\"\n---\n\n## 개요\n\n비디오 생성은 비동기 방식으로 이루어집니다. 요청을 제출하고 작업 ID(`task_id`)를 받은 후, 결과를 확인하기 위해 폴링(poll)을 수행합니다.\n\n## 요청 본문 (Request Body)\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  비디오 모델 (예: `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  생성할 비디오에 대한 텍스트 설명입니다.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  이미지 투 비디오(image-to-video) 생성을 위한 선택적 시작 이미지입니다.\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  비디오 재생 시간(초 단위, 모델에 따라 다름).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  종횡비 (예: `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  비디오 해상도 (예: `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  초당 프레임 수 (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  비디오 생성 시 제외할 요소입니다.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  재현 가능한 생성을 위한 랜덤 시드(seed)입니다.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Classifier-free guidance 스케일 (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  모션 강도 (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  시작 프레임 이미지의 URL입니다.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  종료 프레임 이미지의 URL입니다.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  최종 사용자를 위한 고유 식별자입니다.\n</ParamField>\n\n## 응답 (Response)\n\n<ResponseField name=\"task_id\" type=\"string\">\n  폴링을 위한 고유 작업 식별자입니다.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  초기 상태: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  사용된 모델입니다.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 이미지 투 비디오 (Image to Video)\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## 사용 가능한 모델\n\n| 모델 | 유형 | 설명 |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | 사실적인 움직임, 다양한 버전 (v1.0-v2.6) |\n| `veo3.1` | T2V | Google의 최신 모델 |\n| `veo3.1-pro` | T2V | 전문가급 품질 |\n| `minimax/video-01` | T2V | 높은 리듬감 및 안정성 |\n| `wan2.6-i2v` | I2V | 이미지 투 비디오 |",
      "de": "---\ntitle: \"Video erstellen\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Erstellt eine Video-Generierungsaufgabe\"\n---\n\n## Übersicht\n\nDie Video-Generierung erfolgt asynchron. Sie senden eine Anfrage und erhalten eine Task-ID, anschließend fragen Sie das Ergebnis ab (Polling).\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Video-Modell (z. B. `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Textbeschreibung des zu generierenden Videos.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Optionales Startbild für die Image-to-Video-Generierung.\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Videodauer in Sekunden (modellabhängig).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  Seitenverhältnis (z. B. `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Videoauflösung (z. B. `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Bilder pro Sekunde (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  Was bei der Video-Generierung vermieden werden soll.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Zufallswert (Seed) für reproduzierbare Generierung.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Classifier-free Guidance Scale (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Bewegungsintensität (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  URL des Startbildes.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  URL des Endbildes.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Eine eindeutige Kennung für den Endnutzer.\n</ParamField>\n\n## Response\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Eindeutige Task-Kennung für das Polling.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Initialer Status: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Verwendetes Modell.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Image to Video\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## Verfügbare Modelle\n\n| Modell | Typ | Beschreibung |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | Realistische Bewegung, mehrere Versionen (v1.0-v2.6) |\n| `veo3.1` | T2V | Googles neuestes Modell |\n| `veo3.1-pro` | T2V | Professionelle Qualität |\n| `minimax/video-01` | T2V | Hoher Rhythmus und Stabilität |\n| `wan2.6-i2v` | I2V | Image to Video |",
      "fr": "---\ntitle: \"Créer une vidéo\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Crée une tâche de génération de vidéo\"\n---\n\n## Aperçu\n\nLa génération de vidéo est asynchrone. Vous soumettez une requête et recevez un ID de tâche, puis vous effectuez un polling pour obtenir le résultat.\n\n## Corps de la requête\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Modèle vidéo (ex. : `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Description textuelle de la vidéo à générer.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Image de départ optionnelle pour la génération image-vers-vidéo.\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Durée de la vidéo en secondes (dépend du modèle).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  Format d'image (ex. : `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Résolution vidéo (ex. : `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Images par seconde (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  Ce qu'il faut éviter dans la génération de la vidéo.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Graine aléatoire (`seed`) pour une génération reproductible.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Échelle de guidage sans classificateur (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Intensité du mouvement (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  URL de l'image de la frame de départ.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  URL de l'image de la frame de fin.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identifiant unique pour l'utilisateur final.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Identifiant de tâche unique pour le polling.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Statut initial : `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modèle",
      "es": "---\ntitle: \"Crear Video\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Crea una tarea de generación de video\"\n---\n\n## Descripción general\n\nLa generación de video es asíncrona. Usted envía una solicitud y recibe un ID de tarea, luego realiza consultas (polling) para obtener el resultado.\n\n## Cuerpo de la solicitud\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Modelo de video (ej., `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Descripción de texto del video a generar.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Imagen inicial opcional para la generación de imagen a video.\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Duración del video en segundos (depende del modelo).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  Relación de aspecto (ej., `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Resolución de video (ej., `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Cuadros por segundo (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  Qué evitar en la generación del video.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Semilla aleatoria para una generación reproducible.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Escala de guía libre de clasificador (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Intensidad del movimiento (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  URL de la imagen del cuadro inicial.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  URL de la imagen del cuadro final.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Un identificador único para el usuario final.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Identificador único de la tarea para realizar consultas (polling).\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Estado inicial: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modelo utilizado.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Imagen a Video\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## Modelos disponibles\n\n| Modelo | Tipo | Descripción |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | Movimiento realista, múltiples versiones (v1.0-v2.6) |\n| `veo3.1` | T2V | Lo último de Google |\n| `veo3.1-pro` | T2V | Calidad profesional |\n| `minimax/video-01` | T2V | Alto ritmo y estabilidad |\n| `wan2.6-i2v` | I2V | Imagen a video |",
      "pt": "---\ntitle: \"Criar Vídeo\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Cria uma tarefa de geração de vídeo\"\n---\n\n## Visão Geral\n\nA geração de vídeo é assíncrona. Você envia uma solicitação e recebe um `task_id`, depois realiza o polling para obter o resultado.\n\n## Corpo da Requisição\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Modelo de vídeo (ex: `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Descrição em texto do vídeo a ser gerado.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Imagem inicial opcional para geração de imagem para vídeo (image-to-video).\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Duração do vídeo em segundos (dependente do modelo).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  Proporção de tela (ex: `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Resolução do vídeo (ex: `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Quadros por segundo (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  O que evitar na geração do vídeo.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Semente aleatória para geração reproduzível.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Escala de orientação livre de classificador (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Intensidade do movimento (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  URL da imagem do quadro inicial.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  URL da imagem do quadro final.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Um identificador exclusivo para o usuário final.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Identificador exclusivo da tarefa para polling.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status inicial: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Modelo utilizado.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Imagem para Vídeo\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## Modelos Disponíveis\n\n| Modelo | Tipo | Descrição |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | Movimento realista, múltiplas versões (v1.0-v2.6) |\n| `veo3.1` | T2V | O mais recente do Google |\n| `veo3.1-pro` | T2V | Qualidade profissional |\n| `minimax/video-01` | T2V | Alto ritmo e estabilidade |\n| `wan2.6-i2v` | I2V | Imagem para vídeo |",
      "ar": "---\ntitle: \"إنشاء فيديو\"\napi: \"POST /v1/videos/generations\"\ndescription: \"إنشاء مهمة توليد فيديو\"\n---\n\n## نظرة عامة\n\nعملية توليد الفيديو غير متزامنة. تقوم بإرسال طلب وتتلقى معرف مهمة `task_id`، ثم تقوم بالاستعلام (polling) للحصول على النتيجة.\n\n## جسم الطلب (Request Body)\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  نموذج الفيديو (على سبيل المثال، `kling-video`، `veo3.1`، `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  وصف نصي للفيديو المراد توليده.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  صورة بداية اختيارية لتوليد فيديو من صورة (image-to-video).\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  مدة الفيديو بالثواني (تعتمد على النموذج).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  نسبة العرض إلى الارتفاع (على سبيل المثال، `16:9`، `9:16`، `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  دقة الفيديو (على سبيل المثال، `1080p`، `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  عدد الإطارات في الثانية (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  ما يجب تجنبه في توليد الفيديو.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  بذرة عشوائية (`seed`) لإنتاج نتائج قابلة للتكرار.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  مقياس التوجيه الخالي من المصنف (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  كثافة الحركة (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  رابط URL لصورة الإطار الأول.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  رابط URL لصورة الإطار الأخير.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  معرف فريد للمستخدم النهائي.\n</ParamField>\n\n## الاستجابة\n\n<ResponseField name=\"task_id\" type=\"string\">\n  معرف مهمة فريد للاستعلام.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  الحالة الأولية: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  النموذج المستخدم.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## من صورة إلى فيديو (Image to Video)\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## النماذج المتاحة\n\n| النموذج | النوع | الوصف |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | حركة واقعية، إصدارات متعددة (v1.0-v2.6) |\n| `veo3.1` | T2V | الأحدث من Google |\n| `veo3.1-pro` | T2V | جودة احترافية |\n| `minimax/video-01` | T2V | إيقاع واستقرار عالٍ |\n| `wan2.6-i2v` | I2V | من صورة إلى فيديو |",
      "vi": "---\ntitle: \"Tạo Video\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Tạo một tác vụ tạo video\"\n---\n\n## Tổng quan\n\nQuá trình tạo video là bất đồng bộ. Bạn gửi một yêu cầu và nhận được một task ID, sau đó thực hiện polling để lấy kết quả.\n\n## Request Body\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Model video (ví dụ: `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Mô tả văn bản của video cần tạo.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Hình ảnh bắt đầu tùy chọn cho việc tạo video từ hình ảnh (image-to-video).\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Thời lượng video tính bằng giây (tùy thuộc vào model).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  Tỷ lệ khung hình (ví dụ: `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Độ phân giải video (ví dụ: `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Số khung hình trên giây (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  Những gì cần tránh trong quá trình tạo video.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed ngẫu nhiên để tái tạo kết quả.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Thang đo Classifier-free guidance (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Cường độ chuyển động (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  URL của hình ảnh khung hình bắt đầu.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  URL của hình ảnh khung hình kết thúc.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Mã định danh duy nhất cho người dùng cuối.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Mã định danh tác vụ duy nhất để polling.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Trạng thái ban đầu: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Model được sử dụng.",
      "id": "---\ntitle: \"Buat Video\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Membuat tugas pembuatan video\"\n---\n\n## Ringkasan\n\nPembuatan video bersifat asinkron. Anda mengirimkan permintaan dan menerima ID tugas, lalu melakukan polling untuk hasilnya.\n\n## Body Permintaan\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Model video (misalnya, `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Deskripsi teks dari video yang akan dibuat.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Gambar awal opsional untuk pembuatan image-to-video.\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Durasi video dalam detik (tergantung model).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  Rasio aspek (misalnya, `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Resolusi video (misalnya, `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Frame per detik (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  Apa yang harus dihindari dalam pembuatan video.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Seed acak untuk pembuatan yang dapat direproduksi.\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Skala classifier-free guidance (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Intensitas gerakan (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  URL gambar frame awal.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  URL gambar frame akhir.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Pengidentifikasi unik untuk pengguna akhir.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Pengidentifikasi tugas unik untuk polling.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status awal: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Model yang digunakan.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Image to Video\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## Model yang Tersedia\n\n| Model | Tipe | Deskripsi |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | Gerakan realistis, beberapa versi (v1.0-v2.6) |\n| `veo3.1` | T2V | Terbaru dari Google |\n| `veo3.1-pro` | T2V | Kualitas profesional |\n| `minimax/video-01` | T2V | Ritme dan stabilitas tinggi |\n| `wan2.6-i2v` | I2V | Gambar ke video |",
      "tr": "---\ntitle: \"Video Oluştur\"\napi: \"POST /v1/videos/generations\"\ndescription: \"Bir video oluşturma görevi oluşturur\"\n---\n\n## Genel Bakış\n\nVideo oluşturma işlemi asenkrondur. Bir istek gönderirsiniz ve bir görev kimliği (`task_id`) alırsınız, ardından sonuç için sorgulama (polling) yaparsınız.\n\n## İstek Gövdesi (Request Body)\n\n<ParamField body=\"model\" type=\"string\" default=\"kling-video\">\n  Video modeli (örneğin, `kling-video`, `veo3.1`, `wan2.6-i2v`).\n</ParamField>\n\n<ParamField body=\"prompt\" type=\"string\" required>\n  Oluşturulacak videonun metin açıklaması.\n</ParamField>\n\n<ParamField body=\"image_url\" type=\"string\">\n  Görselden videoya (image-to-video) oluşturma için isteğe bağlı başlangıç görseli.\n</ParamField>\n\n<ParamField body=\"duration\" type=\"integer\">\n  Saniye cinsinden video süresi (modele bağlıdır).\n</ParamField>\n\n<ParamField body=\"aspect_ratio\" type=\"string\">\n  En boy oranı (örneğin, `16:9`, `9:16`, `1:1`).\n</ParamField>\n\n<ParamField body=\"resolution\" type=\"string\">\n  Video çözünürlüğü (örneğin, `1080p`, `720p`).\n</ParamField>\n\n<ParamField body=\"fps\" type=\"integer\">\n  Saniye başına kare sayısı (1-120).\n</ParamField>\n\n<ParamField body=\"negative_prompt\" type=\"string\">\n  Video oluşturma sırasında nelerin önleneceği.\n</ParamField>\n\n<ParamField body=\"seed\" type=\"integer\">\n  Tekrarlanabilir oluşturma için rastgele tohum (seed).\n</ParamField>\n\n<ParamField body=\"cfg_scale\" type=\"number\">\n  Classifier-free guidance ölçeği (0-20).\n</ParamField>\n\n<ParamField body=\"motion_strength\" type=\"number\">\n  Hareket yoğunluğu (0-1).\n</ParamField>\n\n<ParamField body=\"start_image\" type=\"string\">\n  Başlangıç karesi görselinin URL'si.\n</ParamField>\n\n<ParamField body=\"end_image\" type=\"string\">\n  Bitiş karesi görselinin URL'si.\n</ParamField>\n\n<ParamField body=\"user\" type=\"string\">\n  Son kullanıcı için benzersiz bir tanımlayıcı.\n</ParamField>\n\n## Yanıt (Response)\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Sorgulama için benzersiz görev tanımlayıcı.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Başlangıç durumu: `pending`.\n</ResponseField>\n\n<ResponseField name=\"model\" type=\"string\">\n  Kullanılan model.\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl -X POST \"https://api.lemondata.cc/v1/videos/generations\" \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"kling-video\",\n    \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n    \"duration\": 5\n  }'\n```\n\n```python Python\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\n        \"Authorization\": \"Bearer sk-your-api-key\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"kling-video\",\n        \"prompt\": \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5\n    }\n)\n\ndata = response.json()\ntask_id = data[\"task_id\"]\nprint(f\"Task ID: {task_id}\")\n```\n\n```javascript JavaScript\nconst response = await fetch('https://api.lemondata.cc/v1/videos/generations', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer sk-your-api-key',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    model: 'kling-video',\n    prompt: 'A cat walking through a garden, cinematic lighting',\n    duration: 5\n  })\n});\n\nconst data = await response.json();\nconsole.log(`Task ID: ${data.task_id}`);\n```\n\n```go Go\npackage main\n\nimport (\n    \"bytes\"\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    payload := map[string]interface{}{\n        \"model\":    \"kling-v2.6-pro\",\n        \"prompt\":   \"A cat walking through a garden, cinematic lighting\",\n        \"duration\": 5,\n    }\n    body, _ := json.Marshal(payload)\n\n    req, _ := http.NewRequest(\"POST\", \"https://api.lemondata.cc/v1/videos/generations\", bytes.NewBuffer(body))\n    req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n    req.Header.Set(\"Content-Type\", \"application/json\")\n\n    client := &http.Client{}\n    resp, _ := client.Do(req)\n    defer resp.Body.Close()\n\n    var result map[string]interface{}\n    json.NewDecoder(resp.Body).Decode(&result)\n    fmt.Printf(\"Task ID: %s\\n\", result[\"task_id\"])\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/videos/generations');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'kling-v2.6-pro',\n        'prompt' => 'A cat walking through a garden, cinematic lighting',\n        'duration' => 5\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho \"Task ID: \" . $data['task_id'];\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"model\": \"kling-video\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Görselden Videoya (Image to Video)\n\n```python\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1/videos/generations\",\n    headers={\"Authorization\": \"Bearer sk-your-api-key\"},\n    json={\n        \"model\": \"wan2.6-i2v\",\n        \"prompt\": \"The person starts walking forward\",\n        \"image_url\": \"https://example.com/image.jpg\"\n    }\n)\n```\n\n## Mevcut Modeller\n\n| Model | Tür | Açıklama |\n|-------|------|-------------|\n| `kling-video` | T2V/I2V | Gerçekçi hareket, birden fazla sürüm (v1.0-v2.6) |\n| `veo3.1` | T2V | Google'ın en yenisi |\n| `veo3.1-pro` | T2V | Profesyonel kalite |\n| `minimax/video-01` | T2V | Yüksek ritim ve kararlılık |\n| `wan2.6-i2v` | I2V | Görselden videoya |"
    },
    "updatedAt": "2026-01-26T05:29:01.087Z"
  },
  "api-reference/video/get-video-status.mdx": {
    "sourceHash": "9a5238cf989722a4",
    "translations": {
      "zh": "---\ntitle: \"获取视频状态\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"获取视频生成任务的状态和结果\"\n---\n\n## 路径参数\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  创建视频请求返回的任务 ID。\n</ParamField>\n\n## 响应\n\n<ResponseField name=\"task_id\" type=\"string\">\n  任务标识符。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任务状态：`pending`、`processing`、`completed`、`failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  进度百分比 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  生成的视频 URL（完成后）。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  错误信息（如果失败）。\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  创建时间戳。\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  完成时间戳（完成后）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 轮询最佳实践\n\n- 每 5-10 秒轮询一次\n- 针对耗时较长的任务实现指数退避\n- 设置最大超时时间（例如 10 分钟）\n- 妥善处理 `failed` 状态\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "zh-TW": "---\ntitle: \"獲取影片狀態\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"獲取影片生成任務的狀態與結果\"\n---\n\n## 路徑參數\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  從建立影片請求中返回的任務 ID。\n</ParamField>\n\n## 回應\n\n<ResponseField name=\"task_id\" type=\"string\">\n  任務識別碼。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  任務狀態：`pending`、`processing`、`completed`、`failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  進度百分比 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  生成的影片 URL（完成時）。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  錯誤訊息（如果失敗）。\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  建立時間戳記。\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  完成時間戳記（完成時）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 輪詢最佳實踐\n\n- 每 5-10 秒進行一次輪詢\n- 針對耗時較長的任務實施指數退避 (exponential backoff)\n- 設定最大逾時時間（例如 10 分鐘）\n- 妥善處理 `failed` 狀態\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "ja": "---\ntitle: \"ビデオステータスの取得\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"ビデオ生成タスクのステータスと結果を取得します\"\n---\n\n## パスパラメータ\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  ビデオ作成リクエストから返されたタスクID。\n</ParamField>\n\n## レスポンス\n\n<ResponseField name=\"task_id\" type=\"string\">\n  タスク識別子。\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  タスクのステータス: `pending`、`processing`、`completed`、`failed`。\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  進捗率 (0-100)。\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  生成されたビデオのURL（完了時）。\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  エラーメッセージ（失敗時）。\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  作成日時のタイムスタンプ。\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  完了日時のタイムスタンプ（完了時）。\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## ポーリングのベストプラクティス\n\n- 5〜10秒ごとにポーリングを行う\n- 長時間のタスクにはエクスポネンシャルバックオフを実装する\n- 最大タイムアウト（例：10分）を設定する\n- `failed` ステータスを適切に処理する\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "ko": "---\ntitle: \"비디오 상태 조회\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"비디오 생성 작업의 상태와 결과를 조회합니다.\"\n---\n\n## 경로 파라미터\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  비디오 생성 요청에서 반환된 작업 ID입니다.\n</ParamField>\n\n## 응답\n\n<ResponseField name=\"task_id\" type=\"string\">\n  작업 식별자.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  작업 상태: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  진행률 백분율 (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  생성된 비디오의 URL (완료 시).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  에러 메시지 (실패 시).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  생성 타임스탬프.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  완료 타임스탬프 (완료 시).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## 폴링 권장 사항\n\n- 5-10초마다 폴링을 수행하세요.\n- 시간이 오래 걸리는 작업의 경우 지수 백오프(exponential backoff)를 구현하세요.\n- 최대 타임아웃을 설정하세요 (예: 10분).\n- `failed` 상태를 적절하게 처리하세요.\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "de": "---\ntitle: \"Videostatus abrufen\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Ruft den Status und das Ergebnis einer Videogenerierungsaufgabe ab\"\n---\n\n## Pfadparameter\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  Die Task-ID, die von der Anfrage zur Videoerstellung zurückgegeben wurde.\n</ParamField>\n\n## Antwort\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Task-Identifikator.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Task-Status: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  Fortschritt in Prozent (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL des generierten Videos (nach Abschluss).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Fehlermeldung (falls fehlgeschlagen).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Zeitstempel der Erstellung.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Zeitstempel der Fertigstellung (nach Abschluss).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`",
      "fr": "---\ntitle: \"Obtenir le statut de la vidéo\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Récupère le statut et le résultat d'une tâche de génération de vidéo\"\n---\n\n## Paramètres de chemin\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  L'ID de tâche retourné par la requête de création de vidéo.\n</ParamField>\n\n## Réponse\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Identifiant de la tâche.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Statut de la tâche : `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  Pourcentage de progression (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL de la vidéo générée (une fois terminée).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Message d'erreur (en cas d'échec).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Horodatage de création.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Horodatage de fin (une fois terminée).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Bonnes pratiques de polling\n\n- Effectuez un polling toutes les 5 à 10 secondes\n- Implémentez un backoff exponentiel pour les tâches longues\n- Définissez un délai d'attente maximum (ex : 10 minutes)\n- Gérez le statut `failed` de manière appropriée\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "es": "---\ntitle: \"Obtener estado del video\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Recupera el estado y el resultado de una tarea de generación de video\"\n---\n\n## Parámetros de ruta\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  El ID de la tarea devuelto por la solicitud de creación de video.\n</ParamField>\n\n## Respuesta\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Identificador de la tarea.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Estado de la tarea: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  Porcentaje de progreso (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL del video generado (cuando se completa).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Mensaje de error (si falla).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Marca de tiempo de creación.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Marca de tiempo de finalización (cuando termina).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Respuesta (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Respuesta (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Respuesta (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Respuesta (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Mejores prácticas de polling\n\n- Realizar polling cada 5-10 segundos\n- Implementar exponential backoff para tareas largas\n- Establecer un tiempo de espera máximo (ej. 10 minutos)\n- Gestionar el estado `failed` de forma adecuada\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "pt": "---\ntitle: \"Obter Status do Vídeo\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Recupera o status e o resultado de uma tarefa de geração de vídeo\"\n---\n\n## Parâmetros de Caminho\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  O ID da tarefa retornado da requisição de criação de vídeo.\n</ParamField>\n\n## Resposta\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Identificador da tarefa.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status da tarefa: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  Porcentagem de progresso (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL do vídeo gerado (quando concluído).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Mensagem de erro (se houver falha).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Timestamp de criação.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Timestamp de conclusão (quando finalizado).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: {data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Melhores Práticas de Polling\n\n- Faça polling a cada 5-10 segundos\n- Implemente exponential backoff para tarefas longas\n- Defina um tempo limite máximo (ex: 10 minutos)\n- Trate o status `failed` de forma adequada\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "ar": "---\ntitle: \"الحصول على حالة الفيديو\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"يسترجع حالة ونتيجة مهمة إنشاء الفيديو\"\n---\n\n## معلمات المسار (Path Parameters)\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  معرف المهمة (task ID) الذي تم إرجاعه من طلب إنشاء الفيديو.\n</ParamField>\n\n## الاستجابة (Response)\n\n<ResponseField name=\"task_id\" type=\"string\">\n  معرف المهمة.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  حالة المهمة: `pending`، `processing`، `completed`، `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  نسبة التقدم (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  رابط (URL) الفيديو الذي تم إنشاؤه (عند الاكتمال).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  رسالة الخطأ (في حال الفشل).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  الطابع الزمني للإنشاء.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  الطابع الزمني للاكتمال (عند الانتهاء).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## أفضل ممارسات الاستقصاء (Polling)\n\n- قم بالاستقصاء (Poll) كل 5-10 ثوانٍ\n- قم بتنفيذ التراجع الأسي (exponential backoff) للمهام الطويلة\n- قم بتعيين حد أقصى للمهلة (على سبيل المثال، 10 دقائق)\n- تعامل مع حالة `failed` بشكل مناسب\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "vi": "---\ntitle: \"Lấy Trạng thái Video\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Lấy trạng thái và kết quả của một tác vụ tạo video\"\n---\n\n## Tham số Đường dẫn\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  ID tác vụ được trả về từ yêu cầu tạo video.\n</ParamField>\n\n## Phản hồi\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Định danh tác vụ.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Trạng thái tác vụ: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  Phần trăm tiến độ (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL của video đã tạo (khi hoàn tất).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Thông báo lỗi (nếu thất bại).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Dấu thời gian tạo.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Dấu thời gian hoàn tất (khi xong).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Các phương pháp Polling tốt nhất\n\n- Thực hiện poll mỗi 5-10 giây\n- Triển khai exponential backoff cho các tác vụ dài\n- Thiết lập thời gian chờ tối đa (ví dụ: 10 phút)\n- Xử lý trạng thái `failed` một cách khéo léo\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "id": "---\ntitle: \"Dapatkan Status Video\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Mengambil status dan hasil dari tugas pembuatan video\"\n---\n\n## Parameter Path\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  ID tugas yang dikembalikan dari permintaan pembuatan video.\n</ParamField>\n\n## Respons\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Pengidentifikasi tugas.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Status tugas: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  Persentase progres (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  URL dari video yang dihasilkan (saat selesai).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Pesan kesalahan (jika gagal).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Timestamp pembuatan.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Timestamp penyelesaian (saat selesai).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: {data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Praktik Terbaik Polling\n\n- Lakukan polling setiap 5-10 detik\n- Terapkan exponential backoff untuk tugas yang lama\n- Atur batas waktu maksimum (misalnya, 10 menit)\n- Tangani status `failed` dengan baik\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```",
      "tr": "---\ntitle: \"Video Durumunu Al\"\napi: \"GET /v1/videos/generations/{id}\"\ndescription: \"Bir video oluşturma görevinin durumunu ve sonucunu getirir\"\n---\n\n## Yol Parametreleri\n\n<ParamField path=\"task_id\" type=\"string\" required>\n  Video oluşturma isteğinden dönen görev ID'si.\n</ParamField>\n\n## Yanıt\n\n<ResponseField name=\"task_id\" type=\"string\">\n  Görev tanımlayıcı.\n</ResponseField>\n\n<ResponseField name=\"status\" type=\"string\">\n  Görev durumu: `pending`, `processing`, `completed`, `failed`.\n</ResponseField>\n\n<ResponseField name=\"progress\" type=\"number\">\n  İlerleme yüzdesi (0-100).\n</ResponseField>\n\n<ResponseField name=\"video_url\" type=\"string\">\n  Oluşturulan videonun URL'i (tamamlandığında).\n</ResponseField>\n\n<ResponseField name=\"error\" type=\"string\">\n  Hata mesajı (başarısız olursa).\n</ResponseField>\n\n<ResponseField name=\"created_at\" type=\"integer\">\n  Oluşturulma zaman damgası.\n</ResponseField>\n\n<ResponseField name=\"completed_at\" type=\"integer\">\n  Tamamlanma zaman damgası (tamamlandığında).\n</ResponseField>\n\n<RequestExample>\n```bash cURL\ncurl \"https://api.lemondata.cc/v1/videos/generations/video_abc123\" \\\n  -H \"Authorization: Bearer sk-your-api-key\"\n```\n\n```python Python\nimport requests\nimport time\n\ntask_id = \"video_abc123\"\n\nwhile True:\n    response = requests.get(\n        f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n        headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n    )\n\n    data = response.json()\n    print(f\"Status: {data['status']}, Progress: {data.get('progress', 0)}%\")\n\n    if data[\"status\"] == \"completed\":\n        print(f\"Video URL: {data['video_url']}\")\n        break\n    elif data[\"status\"] == \"failed\":\n        print(f\"Error: {data['error']}\")\n        break\n\n    time.sleep(5)  # Poll every 5 seconds\n```\n\n```javascript JavaScript\nconst taskId = 'video_abc123';\n\nasync function pollForVideo() {\n  while (true) {\n    const response = await fetch(\n      `https://api.lemondata.cc/v1/videos/generations/${taskId}`,\n      { headers: { 'Authorization': 'Bearer sk-your-api-key' } }\n    );\n\n    const data = await response.json();\n    console.log(`Status: ${data.status}, Progress: ${data.progress || 0}%`);\n\n    if (data.status === 'completed') {\n      console.log(`Video URL: ${data.video_url}`);\n      return data.video_url;\n    } else if (data.status === 'failed') {\n      throw new Error(data.error);\n    }\n\n    await new Promise(r => setTimeout(r, 5000));\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"net/http\"\n    \"time\"\n)\n\nfunc main() {\n    taskID := \"video_abc123\"\n\n    for {\n        req, _ := http.NewRequest(\"GET\",\n            fmt.Sprintf(\"https://api.lemondata.cc/v1/videos/generations/%s\", taskID), nil)\n        req.Header.Set(\"Authorization\", \"Bearer sk-your-api-key\")\n\n        client := &http.Client{}\n        resp, _ := client.Do(req)\n\n        var result map[string]interface{}\n        json.NewDecoder(resp.Body).Decode(&result)\n        resp.Body.Close()\n\n        fmt.Printf(\"Status: %s, Progress: %.0f%%\\n\", result[\"status\"], result[\"progress\"])\n\n        if result[\"status\"] == \"completed\" {\n            fmt.Printf(\"Video URL: %s\\n\", result[\"video_url\"])\n            break\n        } else if result[\"status\"] == \"failed\" {\n            fmt.Printf(\"Error: %s\\n\", result[\"error\"])\n            break\n        }\n\n        time.Sleep(5 * time.Second)\n    }\n}\n```\n\n```php PHP\n<?php\n$taskId = 'video_abc123';\n\nwhile (true) {\n    $ch = curl_init(\"https://api.lemondata.cc/v1/videos/generations/{$taskId}\");\n\n    curl_setopt_array($ch, [\n        CURLOPT_RETURNTRANSFER => true,\n        CURLOPT_HTTPHEADER => [\n            'Authorization: Bearer sk-your-api-key'\n        ]\n    ]);\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $data = json_decode($response, true);\n    echo \"Status: {$data['status']}, Progress: \" . ($data['progress'] ?? 0) . \"%\\n\";\n\n    if ($data['status'] === 'completed') {\n        echo \"Video URL: {$data['video_url']}\\n\";\n        break;\n    } elseif ($data['status'] === 'failed') {\n        echo \"Error: {$data['error']}\\n\";\n        break;\n    }\n\n    sleep(5);\n}\n```\n</RequestExample>\n\n<ResponseExample>\n```json Response (pending)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"pending\",\n  \"progress\": 0,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (processing)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n\n```json Response (completed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"video_url\": \"https://cdn.lemondata.cc/videos/abc123.mp4\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000,\n  \"completed_at\": 1706000060\n}\n```\n\n```json Response (failed)\n{\n  \"task_id\": \"video_abc123\",\n  \"status\": \"failed\",\n  \"error\": \"Content policy violation\",\n  \"model\": \"kling-v2.6-pro\",\n  \"created_at\": 1706000000\n}\n```\n</ResponseExample>\n\n## Polling En İyi Uygulamaları\n\n- Her 5-10 saniyede bir sorgulama yapın\n- Uzun süreli görevler için üstel geri çekilme (exponential backoff) uygulayın\n- Maksimum bir zaman aşımı süresi belirleyin (örneğin 10 dakika)\n- `failed` durumunu uygun şekilde ele alın\n\n```python\nimport time\n\ndef wait_for_video(task_id, max_wait=600, interval=5):\n    \"\"\"Wait for video with timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < max_wait:\n        response = requests.get(\n            f\"https://api.lemondata.cc/v1/videos/generations/{task_id}\",\n            headers={\"Authorization\": \"Bearer sk-your-api-key\"}\n        )\n        data = response.json()\n\n        if data[\"status\"] == \"completed\":\n            return data[\"video_url\"]\n        elif data[\"status\"] == \"failed\":\n            raise Exception(data.get(\"error\", \"Video generation failed\"))\n\n        time.sleep(interval)\n\n    raise TimeoutError(\"Video generation timed out\")\n```"
    },
    "updatedAt": "2026-01-26T05:29:29.536Z"
  },
  "authentication.mdx": {
    "sourceHash": "4b41c5eac7129a1a",
    "translations": {
      "zh": "---\ntitle: \"身份验证\"\ndescription: \"使用 API 密钥保护您的 API 请求\"\n---\n\n## API 密钥\n\n所有 API 请求都需要使用 API 密钥进行身份验证。请在 `Authorization` 请求头中包含您的密钥：\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## 获取您的 API 密钥\n\n1. 登录您的 [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. 导航至 **API Keys** 栏目\n3. 点击 **Create New Key**\n4. 为您的密钥起一个描述性的名称\n5. 立即复制该密钥 - 它仅会显示一次\n\n<Warning>\n  **安全最佳实践：**\n  - 切勿在客户端代码中暴露 API 密钥\n  - 不要将密钥提交到版本控制系统\n  - 使用环境变量存储密钥\n  - 定期轮换密钥\n  - 删除未使用的密钥\n</Warning>\n\n## 使用 API 密钥\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## API 密钥特性\n\n### 使用限制\n\n您可以为每个 API 密钥设置使用限制，以控制支出：\n\n| 设置 | 描述 |\n|---------|-------------|\n| **无限制** | 密钥可以无限制地使用您的账户余额 |\n| **固定限制** | 达到指定金额后，密钥将停止工作 |\n\n### 密钥前缀\n\n所有 LemonData API 密钥都以 `sk-` 前缀开头。密钥格式为：\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Anthropic API 兼容性\n\n对于 `/v1/messages` 接口，您可以使用 `x-api-key` 请求头（兼容 Anthropic SDK）：\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  `x-api-key` 请求头仅在 `/v1/messages` 和 `/v1/messages/count_tokens` 接口中受支持。其他接口需要使用 `Authorization: Bearer` 请求头。\n</Note>\n\n## 错误响应\n\n| 状态码 | 类型 | 代码 | 描述 |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | API 密钥缺失或无效 |\n| 401 | `invalid_request_error` | `expired_api_key` | API 密钥已被撤销 |\n| 402 | `insufficient_quota` | `insufficient_quota` | 账户余额不足 |\n| 402 | `insufficient_quota` | `quota_exceeded` | 已达到 API 密钥使用限制 |\n\n错误响应示例：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "zh-TW": "---\ntitle: \"身份驗證\"\ndescription: \"使用 API 金鑰保護您的 API 請求\"\n---\n\n## API 金鑰\n\n所有 API 請求都需要使用 API 金鑰進行身份驗證。請在 `Authorization` 標頭中包含您的金鑰：\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## 獲取您的 API 金鑰\n\n1. 登入您的 [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. 導覽至 **API Keys** 區塊\n3. 點擊 **Create New Key**\n4. 為您的金鑰提供一個具描述性的名稱\n5. 立即複製金鑰 — 它僅會顯示一次\n\n<Warning>\n  **安全性最佳實踐：**\n  - 切勿在用戶端程式碼中洩露 API 金鑰\n  - 不要將金鑰提交至版本控制系統\n  - 使用環境變數來儲存金鑰\n  - 定期輪換金鑰\n  - 刪除未使用的金鑰\n</Warning>\n\n## 使用 API 金鑰\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## API 金鑰功能\n\n### 使用限制\n\n您可以為每個 API 金鑰設置使用限制，以控制支出：\n\n| 設定 | 描述 |\n|---------|-------------|\n| **無限制** | 金鑰將無限制地使用您的帳戶餘額 |\n| **固定限制** | 金鑰在達到指定金額後將停止運作 |\n\n### 金鑰前綴\n\n所有 LemonData API 金鑰都以 `sk-` 前綴開頭。金鑰格式為：\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Anthropic API 相容性\n\n對於 `/v1/messages` 端點，您可以使用 `x-api-key` 標頭（相容 Anthropic SDK）：\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  `x-api-key` 標頭僅支援 `/v1/messages` 和 `/v1/messages/count_tokens` 端點。其他端點需要 `Authorization: Bearer` 標頭。\n</Note>\n\n## 錯誤回應\n\n| 狀態碼 | 類型 | 代碼 | 描述 |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | 遺失或無效的 API 金鑰 |\n| 401 | `invalid_request_error` | `expired_api_key` | API 金鑰已被撤銷 |\n| 402 | `insufficient_quota` | `insufficient_quota` | 帳戶餘額不足 |\n| 402 | `insufficient_quota` | `quota_exceeded` | 已達到 API 金鑰使用限制 |\n\n錯誤回應範例：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "ja": "---\ntitle: \"認証\"\ndescription: \"APIキーを使用してAPIリクエストを保護します\"\n---\n\n## APIキー\n\nすべてのAPIリクエストには、APIキーを使用した認証が必要です。`Authorization`ヘッダーにキーを含めてください：\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## APIキーの取得方法\n\n1. [LemonDataダッシュボード](https://lemondata.cc/dashboard)にログインします\n2. **API Keys**セクションに移動します\n3. **Create New Key**をクリックします\n4. キーに分かりやすい名前を付けます\n5. すぐにキーをコピーしてください。一度しか表示されません\n\n<Warning>\n  **セキュリティのベストプラクティス：**\n  - クライアント側のコードでAPIキーを公開しないでください\n  - バージョン管理システムにキーをコミットしないでください\n  - キーの保存には環境変数を使用してください\n  - 定期的にキーをローテーションしてください\n  - 未使用のキーは削除してください\n</Warning>\n\n## APIキーの使用方法\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## APIキーの機能\n\n### 使用制限\n\n支出を管理するために、各APIキーに使用制限を設定できます：\n\n| 設定 | 説明 |\n|---------|-------------|\n| **制限なし** | キーは制限なくアカウント残高を使用します |\n| **固定制限** | 指定された金額に達するとキーは機能しなくなります |\n\n### キーのプレフィックス\n\nすべてのLemonData APIキーは`sk-`プレフィックスで始まります。キーの形式は以下の通りです：\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Anthropic APIの互換性\n\n`/v1/messages`エンドポイントでは、`x-api-key`ヘッダー（Anthropic SDK互換）を使用できます：\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  `x-api-key`ヘッダーは、`/v1/messages`および`/v1/messages/count_tokens`エンドポイントでのみサポートされています。その他のエンドポイントでは`Authorization: Bearer`ヘッダーが必要です。\n</Note>\n\n## エラーレスポンス\n\n| ステータスコード | タイプ | コード | 説明 |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | APIキーが不足しているか無効です |\n| 401 | `invalid_request_error` | `expired_api_key` | APIキーが失効しています |\n| 402 | `insufficient_quota` | `insufficient_quota` | アカウント残高が不足しています |\n| 402 | `insufficient_quota` | `quota_exceeded` | APIキーの使用制限に達しました |\n\nエラーレスポンスの例：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "ko": "---\ntitle: \"인증\"\ndescription: \"API 키를 사용하여 API 요청을 안전하게 보호하세요\"\n---\n\n## API 키\n\n모든 API 요청은 API 키를 사용한 인증이 필요합니다. `Authorization` 헤더에 키를 포함하세요:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## API 키 발급받기\n\n1. [LemonData 대시보드](https://lemondata.cc/dashboard)에 로그인합니다\n2. **API Keys** 섹션으로 이동합니다\n3. **Create New Key**를 클릭합니다\n4. 키에 식별 가능한 이름을 지정합니다\n5. 키를 즉시 복사하세요 - 한 번만 표시됩니다\n\n<Warning>\n  **보안 권장 사항:**\n  - 클라이언트 측 코드에 API 키를 노출하지 마세요\n  - 버전 관리 시스템에 키를 커밋하지 마세요\n  - 환경 변수를 사용하여 키를 저장하세요\n  - 주기적으로 키를 교체(Rotate)하세요\n  - 사용하지 않는 키는 삭제하세요\n</Warning>\n\n## API 키 사용하기\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## API 키 기능\n\n### 사용량 제한\n\n지출을 제어하기 위해 각 API 키에 사용량 제한을 설정할 수 있습니다:\n\n| 설정 | 설명 |\n|---------|-------------|\n| **제한 없음** | 키가 제한 없이 계정 잔액을 사용합니다 |\n| **고정 제한** | 지정된 금액에 도달하면 키 작동이 중지됩니다 |\n\n### 키 접두사\n\n모든 LemonData API 키는 `sk-` 접두사로 시작합니다. 키 형식은 다음과 같습니다:\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Anthropic API 호환성\n\n`/v1/messages` 엔드포인트의 경우, `x-api-key` 헤더(Anthropic SDK 호환)를 사용할 수 있습니다:\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  `x-api-key` 헤더는 `/v1/messages` 및 `/v1/messages/count_tokens` 엔드포인트에서만 지원됩니다. 다른 엔드포인트는 `Authorization: Bearer` 헤더가 필요합니다.\n</Note>\n\n## 오류 응답\n\n| 상태 코드 | 유형 | 코드 | 설명 |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | API 키가 누락되었거나 유효하지 않음 |\n| 401 | `invalid_request_error` | `expired_api_key` | API 키가 취소됨 |\n| 402 | `insufficient_quota` | `insufficient_quota` | 계정 잔액이 부족함 |\n| 402 | `insufficient_quota` | `quota_exceeded` | API 키 사용량 제한에 도달함 |\n\n오류 응답 예시:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "de": "---\ntitle: \"Authentifizierung\"\ndescription: \"Sichern Sie Ihre API-Anfragen mit API-Keys\"\n---\n\n## API-Keys\n\nAlle API-Anfragen erfordern eine Authentifizierung mittels eines API-Keys. Geben Sie Ihren Key im `Authorization`-Header an:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## So erhalten Sie Ihren API-Key\n\n1. Melden Sie sich in Ihrem [LemonData Dashboard](https://lemondata.cc/dashboard) an\n2. Navigieren Sie zum Bereich **API Keys**\n3. Klicken Sie auf **Create New Key**\n4. Geben Sie Ihrem Key einen aussagekräftigen Namen\n5. Kopieren Sie den Key sofort – er wird nur einmal angezeigt\n\n<Warning>\n  **Best Practices für die Sicherheit:**\n  - Exponieren Sie API-Keys niemals in clientseitigem Code\n  - Übertragen (Commit) Sie Keys nicht in die Versionsverwaltung\n  - Verwenden Sie Umgebungsvariablen zum Speichern von Keys\n  - Rotieren Sie Keys regelmäßig\n  - Löschen Sie nicht verwendete Keys\n</Warning>\n\n## Verwendung von API-Keys\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>",
      "fr": "---\ntitle: \"Authentification\"\ndescription: \"Sécurisez vos requêtes API avec des clés API\"\n---\n\n## Clés API\n\nToutes les requêtes API nécessitent une authentification à l'aide d'une clé API. Incluez votre clé dans l'en-tête `Authorization` :\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## Obtenir votre clé API\n\n1. Connectez-vous à votre [Tableau de bord LemonData](https://lemondata.cc/dashboard)\n2. Accédez à la section **API Keys**\n3. Cliquez sur **Create New Key**\n4. Donnez un nom descriptif à votre clé\n5. Copiez la clé immédiatement - elle ne s'affiche qu'une seule fois\n\n<Warning>\n  **Meilleures pratiques de sécurité :**\n  - N'exposez jamais les clés API dans le code côté client\n  - Ne validez pas les clés dans le contrôle de version\n  - Utilisez des variables d'environnement pour stocker les clés\n  - Effectuez une rotation périodique des clés\n  - Supprimez les clés inutilisées\n</Warning>\n\n## Utilisation des clés API\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## Fonctionnalités des clés API\n\n### Limites d'utilisation\n\nVous pouvez définir une limite d'utilisation sur chaque clé API pour contrôler les dépenses :\n\n| Paramètre | Description |\n|---------|-------------|\n| **No Limit** | La clé utilise le solde de votre compte sans restrictions |\n| **Fixed Limit** | La clé cesse de fonctionner après avoir atteint le montant spécifié |\n\n### Préfixe de clé\n\nToutes les clés API LemonData commencent par le préfixe `sk-`. Le format de la clé est :\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Compatibilité avec l'API Anthropic\n\nPour le point de terminaison `/v1/messages`, vous pouvez utiliser l'en-tête `x-api-key` (compatible avec le SDK Anthropic) :\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  L'en-tête `x-api-key` est uniquement pris en charge sur les points de terminaison `/v1/messages` et `/v1/messages/count_tokens`. Les autres points de terminaison nécessitent l'en-tête `Authorization: Bearer`.\n</Note>\n\n## Réponses d'erreur\n\n| Code d'état | Type | Code | Description |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | Clé API manquante ou invalide |\n| 401 | `invalid_request_error` | `expired_api_key` | La clé API a été révoquée |\n| 402 | `insufficient_quota` | `insufficient_quota` | Le solde du compte est insuffisant |\n| 402 | `insufficient_quota` | `quota_exceeded` | Limite d'utilisation de la clé API atteinte |\n\nExemple de réponse d'erreur :\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "es": "---\ntitle: \"Autenticación\"\ndescription: \"Proteja sus solicitudes de API con claves de API\"\n---\n\n## Claves de API\n\nTodas las solicitudes de API requieren autenticación mediante una clave de API. Incluya su clave en el encabezado `Authorization`:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## Cómo obtener su clave de API\n\n1. Inicie sesión en su [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navegue a la sección **API Keys**\n3. Haga clic en **Create New Key**\n4. Asigne un nombre descriptivo a su clave\n5. Copie la clave de inmediato; solo se muestra una vez\n\n<Warning>\n  **Mejores prácticas de seguridad:**\n  - Nunca exponga las claves de API en el código del lado del cliente\n  - No suba las claves al control de versiones\n  - Utilice variables de entorno para almacenar las claves\n  - Rote las claves periódicamente\n  - Elimine las claves que no utilice\n</Warning>\n\n## Uso de claves de API\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## Características de las claves de API\n\n### Límites de uso\n\nPuede establecer un límite de uso en cada clave de API para controlar el gasto:\n\n| Configuración | Descripción |\n|---------|-------------|\n| **Sin límite** | La clave utiliza el saldo de su cuenta sin restricciones |\n| **Límite fijo** | La clave deja de funcionar tras alcanzar el importe especificado |\n\n### Prefijo de la clave\n\nTodas las claves de API de LemonData comienzan con el prefijo `sk-`. El formato de la clave es:\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Compatibilidad con la API de Anthropic\n\nPara el endpoint `/v1/messages`, puede utilizar el encabezado `x-api-key` (compatible con el SDK de Anthropic):\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  El encabezado `x-api-key` solo es compatible con los endpoints `/v1/messages` y `/v1/messages/count_tokens`. Otros endpoints requieren el encabezado `Authorization: Bearer`.\n</Note>\n\n## Respuestas de error\n\n| Código de estado | Tipo | Código | Descripción |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | Clave de API faltante o no válida |\n| 401 | `invalid_request_error` | `expired_api_key` | La clave de API ha sido revocada |\n| 402 | `insufficient_quota` | `insufficient_quota` | El saldo de la cuenta es insuficiente |\n| 402 | `insufficient_quota` | `quota_exceeded` | Se ha alcanzado el límite de uso de la clave de API |\n\nEjemplo de respuesta de error:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "pt": "---\ntitle: \"Autenticação\"\ndescription: \"Proteja suas requisições de API com chaves de API\"\n---\n\n## Chaves de API\n\nTodas as requisições de API exigem autenticação usando uma chave de API. Inclua sua chave no cabeçalho `Authorization`:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## Obtendo sua Chave de API\n\n1. Faça login no seu [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navegue até a seção **API Keys**\n3. Clique em **Create New Key**\n4. Dê um nome descritivo para sua chave\n5. Copie a chave imediatamente - ela é exibida apenas uma vez\n\n<Warning>\n  **Melhores Práticas de Segurança:**\n  - Nunca exponha chaves de API em código do lado do cliente (client-side)\n  - Não envie chaves para sistemas de controle de versão\n  - Use variáveis de ambiente para armazenar chaves\n  - Rotacione as chaves periodicamente\n  - Exclua chaves não utilizadas\n</Warning>\n\n## Usando Chaves de API\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## Recursos da Chave de API\n\n### Limites de Uso\n\nVocê pode definir um limite de uso em cada chave de API para controlar os gastos:\n\n| Configuração | Descrição |\n|---------|-------------|\n| **No Limit** | A chave usa o saldo da sua conta sem restrições |\n| **Fixed Limit** | A chave para de funcionar após atingir o valor especificado |\n\n### Prefixo da Chave\n\nTodas as chaves de API da LemonData começam com o prefixo `sk-`. O formato da chave é:\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Compatibilidade com a API da Anthropic\n\nPara o endpoint `/v1/messages`, você pode usar o cabeçalho `x-api-key` (compatível com o SDK da Anthropic):\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  O cabeçalho `x-api-key` é suportado apenas nos endpoints `/v1/messages` e `/v1/messages/count_tokens`. Outros endpoints exigem o cabeçalho `Authorization: Bearer`.\n</Note>\n\n## Respostas de Erro\n\n| Código de Status | Tipo | Código | Descrição |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | Chave de API ausente ou inválida |\n| 401 | `invalid_request_error` | `expired_api_key` | A chave de API foi revogada |\n| 402 | `insufficient_quota` | `insufficient_quota` | O saldo da conta é insuficiente |\n| 402 | `insufficient_quota` | `quota_exceeded` | Limite de uso da chave de API atingido |\n\nExemplo de resposta de erro:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "ar": "---\ntitle: \"المصادقة\"\ndescription: \"قم بتأمين طلبات API الخاصة بك باستخدام مفاتيح API\"\n---\n\n## مفاتيح API\n\nتتطلب جميع طلبات API المصادقة باستخدام مفتاح API. قم بتضمين مفتاحك في ترويسة `Authorization`:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## الحصول على مفتاح API الخاص بك\n\n1. قم بتسجيل الدخول إلى [لوحة تحكم LemonData](https://lemondata.cc/dashboard)\n2. انتقل إلى قسم **مفاتيح API**\n3. انقر فوق **إنشاء مفتاح جديد**\n4. قم بتسمية مفتاحك باسم وصفي\n5. انسخ المفتاح فوراً - يتم عرضه مرة واحدة فقط\n\n<Warning>\n  **أفضل ممارسات الأمان:**\n  - لا تقم أبداً بالكشف عن مفاتيح API في كود جانب العميل\n  - لا تقم بإرسال المفاتيح إلى أنظمة التحكم في الإصدار (version control)\n  - استخدم متغيرات البيئة لتخزين المفاتيح\n  - قم بتدوير المفاتيح بشكل دوري\n  - احذف المفاتيح غير المستخدمة\n</Warning>\n\n## استخدام مفاتيح API\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bear",
      "vi": "---\ntitle: \"Xác thực\"\ndescription: \"Bảo mật các yêu cầu API của bạn bằng API key\"\n---\n\n## API Key\n\nTất cả các yêu cầu API đều yêu cầu xác thực bằng API key. Hãy bao gồm key của bạn trong header `Authorization`:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## Cách lấy API Key của bạn\n\n1. Đăng nhập vào [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Đi tới phần **API Keys**\n3. Nhấp vào **Create New Key**\n4. Đặt tên mô tả cho key của bạn\n5. Sao chép key ngay lập tức - nó chỉ được hiển thị một lần duy nhất\n\n<Warning>\n  **Thực hành Bảo mật Tốt nhất:**\n  - Không bao giờ để lộ API key trong mã nguồn phía client\n  - Không commit key lên hệ thống quản lý phiên bản (version control)\n  - Sử dụng biến môi trường để lưu trữ key\n  - Thay đổi (rotate) key định kỳ\n  - Xóa các key không sử dụng\n</Warning>\n\n## Sử dụng API Key\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## Các tính năng của API Key\n\n### Giới hạn sử dụng\n\nBạn có thể thiết lập giới hạn sử dụng cho mỗi API key để kiểm soát chi tiêu:\n\n| Cài đặt | Mô tả |\n|---------|-------------|\n| **No Limit** | Key sử dụng số dư tài khoản của bạn mà không có hạn chế |\n| **Fixed Limit** | Key sẽ ngừng hoạt động sau khi đạt đến số tiền được chỉ định |\n\n### Tiền tố Key\n\nTất cả các API key của LemonData đều bắt đầu bằng tiền tố `sk-`. Định dạng key là:\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Khả năng tương thích với Anthropic API\n\nĐối với endpoint `/v1/messages`, bạn có thể sử dụng header `x-api-key` (tương thích với Anthropic SDK):\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  Header `x-api-key` chỉ được hỗ trợ trên các endpoint `/v1/messages` và `/v1/messages/count_tokens`. Các endpoint khác yêu cầu header `Authorization: Bearer`.\n</Note>\n\n## Phản hồi lỗi\n\n| Mã trạng thái | Loại | Mã | Mô tả |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | Thiếu hoặc API key không hợp lệ |\n| 401 | `invalid_request_error` | `expired_api_key` | API key đã bị thu hồi |\n| 402 | `insufficient_quota` | `insufficient_quota` | Số dư tài khoản không đủ |\n| 402 | `insufficient_quota` | `quota_exceeded` | Đã đạt đến giới hạn sử dụng của API key |\n\nVí dụ phản hồi lỗi:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "id": "---\ntitle: \"Autentikasi\"\ndescription: \"Amankan permintaan API Anda dengan API key\"\n---\n\n## API Key\n\nSemua permintaan API memerlukan autentikasi menggunakan API key. Sertakan key Anda dalam header `Authorization`:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## Mendapatkan API Key Anda\n\n1. Masuk ke [LemonData Dashboard](https://lemondata.cc/dashboard) Anda\n2. Buka bagian **API Keys**\n3. Klik **Create New Key**\n4. Berikan nama deskriptif untuk key Anda\n5. Salin key tersebut segera - key hanya akan ditampilkan sekali\n\n<Warning>\n  **Praktik Keamanan Terbaik:**\n  - Jangan pernah mengekspos API key dalam kode sisi klien (client-side)\n  - Jangan melakukan commit key ke version control\n  - Gunakan environment variables untuk menyimpan key\n  - Rotasi key secara berkala\n  - Hapus key yang tidak digunakan\n</Warning>\n\n## Menggunakan API Key\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## Fitur API Key\n\n### Batas Penggunaan\n\nAnda dapat menetapkan batas penggunaan pada setiap API key untuk mengontrol pengeluaran:\n\n| Pengaturan | Deskripsi |\n|---------|-------------|\n| **Tanpa Batas** | Key menggunakan saldo akun Anda tanpa batasan |\n| **Batas Tetap** | Key berhenti berfungsi setelah mencapai jumlah yang ditentukan |\n\n### Prefiks Key\n\nSemua API key LemonData dimulai dengan prefiks `sk-`. Format key adalah:\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Kompatibilitas API Anthropic\n\nUntuk endpoint `/v1/messages`, Anda dapat menggunakan header `x-api-key` (kompatibel dengan Anthropic SDK):\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  Header `x-api-key` hanya didukung pada endpoint `/v1/messages` dan `/v1/messages/count_tokens`. Endpoint lainnya memerlukan header `Authorization: Bearer`.\n</Note>\n\n## Respons Error\n\n| Kode Status | Tipe | Kode | Deskripsi |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | API key hilang atau tidak valid |\n| 401 | `invalid_request_error` | `expired_api_key` | API key telah dicabut |\n| 402 | `insufficient_quota` | `insufficient_quota` | Saldo akun tidak mencukupi |\n| 402 | `insufficient_quota` | `quota_exceeded` | Batas penggunaan API key tercapai |\n\nContoh respons error:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```",
      "tr": "---\ntitle: \"Kimlik Doğrulama\"\ndescription: \"API anahtarları ile API isteklerinizi güvence altına alın\"\n---\n\n## API Anahtarları\n\nTüm API istekleri, bir API anahtarı kullanılarak kimlik doğrulaması gerektirir. Anahtarınızı `Authorization` başlığına (header) ekleyin:\n\n```bash\nAuthorization: Bearer sk-your-api-key\n```\n\n## API Anahtarınızı Alma\n\n1. [LemonData Dashboard](https://lemondata.cc/dashboard) panelinizde oturum açın\n2. **API Keys** bölümüne gidin\n3. **Create New Key** düğmesine tıklayın\n4. Anahtarınıza açıklayıcı bir ad verin\n5. Anahtarı hemen kopyalayın - yalnızca bir kez gösterilir\n\n<Warning>\n  **Güvenlik En İyi Uygulamaları:**\n  - API anahtarlarını asla istemci tarafı (client-side) kodunda açıkta bırakmayın\n  - Anahtarları sürüm kontrol sistemlerine göndermeyin\n  - Anahtarları saklamak için ortam değişkenlerini (environment variables) kullanın\n  - Anahtarları periyodik olarak yenileyin\n  - Kullanılmayan anahtarları silin\n</Warning>\n\n## API Anahtarlarını Kullanma\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer $LEMONDATA_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(os.Getenv(\"LEMONDATA_API_KEY\"))\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$apiKey = getenv('LEMONDATA_API_KEY');\n\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_HTTPHEADER => [\n        'Authorization: Bearer ' . $apiKey,\n        'Content-Type: application/json'\n    ]\n]);\n```\n\n</CodeGroup>\n\n## API Anahtarı Özellikleri\n\n### Kullanım Limitleri\n\nHarcamaları kontrol etmek için her API anahtarı üzerinde bir kullanım limiti belirleyebilirsiniz:\n\n| Ayar | Açıklama |\n|---------|-------------|\n| **Limit Yok** | Anahtar, hesap bakiyenizi kısıtlama olmaksızın kullanır |\n| **Sabit Limit** | Anahtar, belirtilen tutara ulaştıktan sonra çalışmayı durdurur |\n\n### Anahtar Öneki\n\nTüm LemonData API anahtarları `sk-` öneki ile başlar. Anahtar formatı şöyledir:\n\n```\nsk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n```\n\n## Anthropic API Uyumluluğu\n\n`/v1/messages` uç noktası (endpoint) için `x-api-key` başlığını (Anthropic SDK uyumlu) kullanabilirsiniz:\n\n```bash\ncurl https://api.lemondata.cc/v1/messages \\\n  -H \"x-api-key: sk-your-api-key\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n<Note>\n  `x-api-key` başlığı yalnızca `/v1/messages` ve `/v1/messages/count_tokens` uç noktalarında desteklenir. Diğer uç noktalar `Authorization: Bearer` başlığını gerektirir.\n</Note>\n\n## Hata Yanıtları\n\n| Durum Kodu | Tür | Kod | Açıklama |\n|-------------|------|------|-------------|\n| 401 | `invalid_request_error` | `invalid_api_key` | Eksik veya geçersiz API anahtarı |\n| 401 | `invalid_request_error` | `expired_api_key` | API anahtarı iptal edilmiş |\n| 402 | `insufficient_quota` | `insufficient_quota` | Hesap bakiyesi yetersiz |\n| 402 | `insufficient_quota` | `quota_exceeded` | API anahtarı kullanım limitine ulaşıldı |\n\nÖrnek hata yanıtı:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Invalid API key provided\",\n    \"type\": \"invalid_api_key\",\n    \"code\": \"invalid_api_key\"\n  }\n}\n```"
    },
    "updatedAt": "2026-01-26T05:29:50.070Z"
  },
  "guides/api-formats.mdx": {
    "sourceHash": "425438e5f38b2067",
    "translations": {
      "zh": "---\ntitle: \"✨ 多格式 API\"\ndescription: \"只需一个 API 密钥，即可使用 OpenAI、Anthropic 或 Gemini 格式\"\n---\n\n## 概览\n\nLemonData 支持通过单个 API 密钥使用 **三种原生 API 格式**。选择最适合您用例的格式——无需更改配置。\n\n<CardGroup cols={3}>\n  <Card title=\"OpenAI 格式\" icon=\"plug\">\n    `/v1/chat/completions`\n    标准格式，兼容性最广\n  </Card>\n  <Card title=\"Anthropic 格式\" icon=\"message\">\n    `/v1/messages`\n    深度思考，原生 Claude 功能\n  </Card>\n  <Card title=\"Gemini 格式\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Google 生态系统集成\n  </Card>\n</CardGroup>\n\n## 为什么选择多格式？\n\n| 优势 | 描述 |\n|---------|-------------|\n| **无需切换 SDK** | 使用您偏好的 SDK 即可调用任何模型 |\n| **原生功能** | 访问特定格式的功能 |\n| **轻松迁移** | 只需更改 base URL 即可从官方 API 切换 |\n| **统一计费** | 一个账号，一个 API 密钥，支持所有格式 |\n\n## 格式对比\n\n| 功能 | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **认证请求头** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **系统提示词** | 在 messages 数组中 | 独立的 `system` 字段 | 在 `systemInstruction` 中 |\n| **深度思考 (Extended Thinking)** | ❌ | ✅ | ❌ |\n| **流式传输** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **工具调用** | ✅ | ✅ | ✅ |\n| **视觉能力** | ✅ | ✅ | ✅ |\n\n## OpenAI 格式\n\n兼容性最广泛的格式。适用于所有 LemonData 模型。\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# 适用于任何模型\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # 通过 OpenAI 格式调用 Claude\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**最适用于：**\n- 通用场景\n- 现有的 OpenAI SDK 集成\n- 最大化兼容性\n\n## Anthropic 格式\n\n原生 Anthropic Messages API。使用 Claude 特定功能（如深度思考）时必选。\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # 无需 /v1 后缀！\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # 独立的 system 字段\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### 深度思考 (Claude Opus 4.5)\n\n仅在 Anthropic 格式中可用：\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# 访问思考过程\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**最适用于：**\n- Claude 特定功能\n- 深度思考模式\n- 原生 Anthropic SDK 用户\n\n## Gemini 格式\n\n原生 Google Gemini API 格式，用于 Google 生态系统集成。\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### 流式传输\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**最适用于：**\n- Google Cloud 集成\n- 现有的 Gemini SDK 代码\n- 原生 Gemini 功能\n\n## 选择合适的格式\n\n```mermaid\ngraph TD\n    A[哪种格式？] --> B{需要 Claude 深度思考？}\n    B -->|是| C[使用 Anthropic 格式]\n    B -->|否| D{现有代码库？}\n    D -->|OpenAI SDK| E[使用 OpenAI 格式]\n    D -->|Anthropic SDK| C\n    D -->|Gemini SDK| F[使用 Gemini 格式]\n    D -->|新项目| E\n```\n\n## 迁移指南\n\n### 从 OpenAI 官方 API 迁移\n\n```python\n# 迁移前 (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# 迁移后 (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # 添加此行\n)\n# 大功告成！同样的代码即可运行\n```\n\n### 从 Anthropic 官方 API 迁移\n\n```python\n# 迁移前 (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# 迁移后 (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # 添加此行（无需 /v1！）\n)\n```\n\n### 从 Google AI Studio 迁移\n\n```python\n# 迁移前 (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# 迁移后 (LemonData) - 使用 REST API\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## 跨模型兼容性\n\nLemonData 的魅力在于：可以使用 **任何 SDK** 调用 **任何模型**。网关会自动处理格式转换。\n\n### 任意 SDK → 任意模型\n\n```python\n# 使用 Anthropic SDK 调用 GPT-4o（自动转换为 OpenAI 格式）\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ 可用！已自动转换\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# 同一个 SDK，不同的模型 - 无需更改代码\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ 可用！\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ 可用！\n```\n\n### OpenAI SDK → 所有模型\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# 以下模型均可使用同一个 SDK 调用：\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n## 行业对比\n\n| 平台 | OpenAI 格式 | Anthropic 格式 | Gemini 格式 | Responses API |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ 所有模型 | ✅ 所有模型 | ✅ 所有模型 | ✅ 所有模型 |\n| OpenRouter | ✅ 所有模型 | ❌ | ❌ | ❌ |\n| Together AI | ✅ 所有模型 | ❌ | ❌ | ❌ |\n| Fireworks | ✅ 所有模型 | ❌ | ❌ | ❌ |\n\n<Note>\n虽然跨格式支持大多数功能，但特定格式的功能（如 Anthropic 深度思考）仍需要使用原生格式。\n</Note>",
      "zh-TW": "---\ntitle: \"✨ 多格式 API\"\ndescription: \"只需一個 API key 即可使用 OpenAI、Anthropic 或 Gemini 格式\"\n---\n\n## 概覽\n\nLemonData 透過單一 API key 支援 **三種原生 API 格式**。請選擇最適合您使用場景的格式——無需更改配置。\n\n<CardGroup cols={3}>\n  <Card title=\"OpenAI 格式\" icon=\"plug\">\n    `/v1/chat/completions`\n    標準格式，相容性最廣\n  </Card>\n  <Card title=\"Anthropic 格式\" icon=\"message\">\n    `/v1/messages`\n    擴展思考，原生 Claude 功能\n  </Card>\n  <Card title=\"Gemini 格式\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Google 生態系統整合\n  </Card>\n</CardGroup>\n\n## 為什麼使用多格式？\n\n| 優勢 | 描述 |\n|---------|-------------|\n| **無需切換 SDK** | 使用您偏好的 SDK 即可調用任何模型 |\n| **原生功能** | 存取特定格式的功能 |\n| **輕鬆遷移** | 只需更改 base URL 即可從官方 API 切換 |\n| **統一計費** | 一個帳戶、一個 API key，支援所有格式 |\n\n## 格式比較\n\n| 功能 | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **認證標頭** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **System Prompt** | 在 messages 陣列中 | 獨立的 `system` 欄位 | 在 `systemInstruction` 中 |\n| **擴展思考** | ❌ | ✅ | ❌ |\n| **串流 (Streaming)** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **工具調用** | ✅ | ✅ | ✅ |\n| **視覺 (Vision)** | ✅ | ✅ | ✅ |\n\n## OpenAI 格式\n\n相容性最廣泛的格式。適用於所有 LemonData 模型。\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# 適用於任何模型\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # 透過 OpenAI 格式調用 Claude\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**最適用於：**\n- 一般用途\n- 現有的 OpenAI SDK 整合\n- 最大相容性\n\n## Anthropic 格式\n\n原生 Anthropic Messages API。使用 Claude 特定功能（如擴展思考）時必備。\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # 無需 /v1 後綴！\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # 獨立的 system 欄位\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### 擴展思考 (Claude Opus 4.5)\n\n僅在 Anthropic 格式中提供：\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# 存取思考過程\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**最適用於：**\n- Claude 特定功能\n- 擴展思考模式\n- 原生 Anthropic SDK 使用者\n\n## Gemini 格式\n\n原生 Google Gemini API 格式，用於 Google 生態系統整合。\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### 串流 (Streaming)\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**最適用於：**\n- Google Cloud 整合\n- 現有的 Gemini SDK 程式碼\n- 原生 Gemini 功能\n\n## 選擇正確的格式\n\n```mermaid\ngraph TD\n    A[該選擇哪種格式？] --> B{需要 Claude 擴展思考嗎？}\n    B -->|是| C[使用 Anthropic 格式]\n    B -->|否| D{現有程式碼庫？}\n    D -->|OpenAI SDK| E[使用 OpenAI 格式]\n    D -->|Anthropic SDK| C\n    D -->|Gemini SDK| F[使用 Gemini 格式]\n    D -->|新專案| E\n```\n\n## 遷移指南\n\n### 從 OpenAI 官方 API 遷移\n\n```python\n# 遷移前 (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# 遷移後 (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # 加入此行\n)\n# 就這樣！同樣的程式碼即可運作\n```\n\n### 從 Anthropic 官方 API 遷移\n\n```python\n# 遷移前 (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# 遷移後 (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # 加入此行 (無需 /v1！)\n)\n```\n\n### 從 Google AI Studio 遷移\n\n```python\n# 遷移前 (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# 遷移後 (LemonData) - 使用 REST API\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## 跨模型相容性\n\nLemonData 的魔力：使用 **任何 SDK** 搭配 **任何模型**。閘道器會自動處理格式轉換。\n\n### 任何 SDK → 任何模型\n\n```python\n# 使用 Anthropic SDK 調用 GPT-4o（自動轉換為 OpenAI 格式）\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ 運作正常！已自動轉換\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# 同一個 SDK，不同的模型——無需更改程式碼\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ 運作正常！\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ 運作正常！\n```\n\n### OpenAI SDK → 所有模型\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# 以下模型皆可使用同一個 SDK 運作：\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### 業界比較\n\n| 平台 | OpenAI 格式 | Anthropic 格式 | Gemini 格式 | Responses API |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ 所有模型 | ✅ 所有模型 | ✅ 所有模型 | ✅ 所有模型 |\n| OpenRouter | ✅ 所有模型 | ❌ | ❌ | ❌ |\n| Together AI | ✅ 所有模型 | ❌ | ❌ | ❌ |\n| Fireworks | ✅ 所有模型 | ❌ | ❌ | ❌ |\n\n<Note>\n雖然跨格式支援大多數功能，但特定格式的功能（如 Anthropic 擴展思考）仍需要使用原生格式。\n</Note>",
      "ja": "---\ntitle: \"✨ マルチフォーマット API\"\ndescription: \"単一の API key で OpenAI、Anthropic、または Gemini フォーマットを使用\"\n---\n\n## 概要\n\nLemonData は、単一の API key で **3 つのネイティブ API フォーマット**をサポートしています。設定を変更することなく、ユースケースに最適なフォーマットを選択できます。\n\n<CardGroup cols={3}>\n  <Card title=\"OpenAI フォーマット\" icon=\"plug\">\n    `/v1/chat/completions`\n    標準的なフォーマット、最高の互換性\n  </Card>\n  <Card title=\"Anthropic フォーマット\" icon=\"message\">\n    `/v1/messages`\n    Extended thinking、ネイティブな Claude 機能\n  </Card>\n  <Card title=\"Gemini フォーマット\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Google エコシステムとの統合\n  </Card>\n</CardGroup>\n\n## なぜマルチフォーマットなのか？\n\n| メリット | 説明 |\n|---------|-------------|\n| **SDK の切り替えが不要** | 好みの SDK であらゆるモデルを使用可能 |\n| **ネイティブ機能** | フォーマット固有の機能にアクセス |\n| **簡単な移行** | base URL を変更するだけで公式 API から切り替え可能 |\n| **一括請求** | 1 つのアカウント、1 つの API key で全フォーマットに対応 |\n\n## フォーマットの比較\n\n| 機能 | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **認証ヘッダー** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **システムプロンプト** | messages 配列内 | 独立した `system` フィールド | `systemInstruction` 内 |\n| **Extended Thinking** | ❌ | ✅ | ❌ |\n| **ストリーミング** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **ツール呼び出し** | ✅ | ✅ | ✅ |\n| **ビジョン** | ✅ | ✅ | ✅ |\n\n## OpenAI フォーマット\n\n最も広く互換性のあるフォーマットです。すべての LemonData モデルで動作します。\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# あらゆるモデルで動作\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # OpenAI フォーマット経由の Claude\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**最適なケース:**\n- 一般的な利用\n- 既存の OpenAI SDK 統合\n- 最大限の互換性\n\n## Anthropic フォーマット\n\nネイティブな Anthropic Messages API です。extended thinking などの Claude 固有の機能に必要です。\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # /v1 サフィックスは不要！\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # 独立した system フィールド\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Extended Thinking (Claude Opus 4.5)\n\nAnthropic フォーマットでのみ利用可能:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# 思考プロセスへのアクセス\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**最適なケース:**\n- Claude 固有の機能\n- Extended thinking モード\n- ネイティブな Anthropic SDK ユーザー\n\n## Gemini フォーマット\n\nGoogle エコシステム統合のためのネイティブな Google Gemini API フォーマットです。\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### ストリーミング\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**最適なケース:**\n- Google Cloud 統合\n- 既存の Gemini SDK コード\n- ネイティブな Gemini 機能\n\n## 適切なフォーマットの選択\n\n```mermaid\ngraph TD\n    A[どのフォーマット？] --> B{Claude の extended thinking が必要？}\n    B -->|はい| C[Anthropic フォーマットを使用]\n    B -->|いいえ| D{既存のコードベースは？}\n    D -->|OpenAI SDK| E[OpenAI フォーマットを使用]\n    D -->|Anthropic SDK| C\n    D -->|Gemini SDK| F[Gemini フォーマットを使用]\n    D -->|新規プロジェクト| E\n```\n\n## 移行ガイド\n\n### OpenAI 公式 API からの移行\n\n```python\n# 移行前 (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# 移行後 (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # この行を追加\n)\n# これだけです！同じコードが動作します\n```\n\n### Anthropic 公式 API からの移行\n\n```python\n# 移行前 (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# 移行後 (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # この行を追加 (/v1 は不要！)\n)\n```\n\n### Google AI Studio からの移行\n\n```python\n# 移行前 (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# 移行後 (LemonData) - REST API を使用\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## モデル間の互換性\n\nLemonData の魔法：**あらゆる SDK** を **あらゆるモデル** で使用できます。ゲートウェイがフォーマット変換を自動的に処理します。\n\n### あらゆる SDK → あらゆるモデル\n\n```python\n# GPT-4o を使用した Anthropic SDK (OpenAI フォーマットに自動変換)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ 動作します！自動変換済み\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# 同じ SDK、異なるモデル - コードの変更なし\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ 動作します！\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ 動作します！\n```\n\n### OpenAI SDK → すべてのモデル\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# これらはすべて同じ SDK で動作します:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### 業界比較\n\n| プラットフォーム | OpenAI フォーマット | Anthropic フォーマット | Gemini フォーマット | Responses API |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ 全モデル | ✅ 全モデル | ✅ 全モデル | ✅ 全モデル |\n| OpenRouter | ✅ 全モデル | ❌ | ❌ | ❌ |\n| Together AI | ✅ 全モデル | ❌ | ❌ | ❌ |\n| Fireworks | ✅ 全モデル | ❌ | ❌ | ❌ |\n\n<Note>\nクロスフォーマットはほとんどの機能で動作しますが、フォーマット固有の機能（Anthropic の extended thinking など）にはネイティブフォーマットが必要です。\n</Note>",
      "ko": "---\ntitle: \"✨ 멀티 포맷 API\"\ndescription: \"단일 API 키로 OpenAI, Anthropic 또는 Gemini 포맷을 사용하세요\"\n---\n\n## 개요\n\nLemonData는 단일 API 키로 **세 가지 네이티브 API 포맷**을 지원합니다. 설정 변경 없이 사용 사례에 가장 적합한 포맷을 선택하세요.\n\n<CardGroup cols={3}>\n  <Card title=\"OpenAI 포맷\" icon=\"plug\">\n    `/v1/chat/completions`\n    표준 포맷, 가장 넓은 호환성\n  </Card>\n  <Card title=\"Anthropic 포맷\" icon=\"message\">\n    `/v1/messages`\n    확장된 사고(Extended thinking), 네이티브 Claude 기능\n  </Card>\n  <Card title=\"Gemini 포맷\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Google 에코시스템 통합\n  </Card>\n</CardGroup>\n\n## 왜 멀티 포맷인가요?\n\n| 장점 | 설명 |\n|---------|-------------|\n| **SDK 전환 불필요** | 선호하는 SDK로 모든 모델 사용 가능 |\n| **네이티브 기능** | 포맷별 특화 기능 액세스 |\n| **쉬운 마이그레이션** | 베이스 URL 변경만으로 공식 API에서 전환 가능 |\n| **단일 결제** | 하나의 계정, 하나의 API 키로 모든 포맷 이용 |\n\n## 포맷 비교\n\n| 기능 | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **엔드포인트** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **인증 헤더** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **시스템 프롬프트** | `messages` 배열 내 | 별도 `system` 필드 | `systemInstruction` 내 |\n| **확장된 사고** | ❌ | ✅ | ❌ |\n| **스트리밍** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **도구 호출** | ✅ | ✅ | ✅ |\n| **비전** | ✅ | ✅ | ✅ |\n\n## OpenAI 포맷\n\n가장 널리 호환되는 포맷입니다. 모든 LemonData 모델에서 작동합니다.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# 모든 모델과 작동합니다\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # OpenAI 포맷을 통한 Claude\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**다음의 경우에 권장:**\n- 일반적인 용도\n- 기존 OpenAI SDK 통합 환경\n- 최대 호환성\n\n## Anthropic 포맷\n\n네이티브 Anthropic Messages API입니다. 확장된 사고와 같은 Claude 전용 기능에 필요합니다.\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # /v1 접미사 없음!\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # 별도 시스템 필드\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### 확장된 사고 (Claude Opus 4.5)\n\nAnthropic 포맷에서만 사용 가능합니다:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# 사고 과정 액세스\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**다음의 경우에 권장:**\n- Claude 전용 기능\n- 확장된 사고 모드\n- 네이티브 Anthropic SDK 사용자\n\n## Gemini 포맷\n\nGoogle 에코시스템 통합을 위한 네이티브 Google Gemini API 포맷입니다.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### 스트리밍\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**다음의 경우에 권장:**\n- Google Cloud 통합\n- 기존 Gemini SDK 코드\n- 네이티브 Gemini 기능\n\n## 적절한 포맷 선택하기\n\n```mermaid\ngraph TD\n    A[어떤 포맷을 사용할까요?] --> B{Claude의 확장된 사고 기능이 필요한가요?}\n    B -->|예| C[Anthropic 포맷 사용]\n    B -->|아니요| D{기존 코드베이스가 있나요?}\n    D -->|OpenAI SDK| E[OpenAI 포맷 사용]\n    D -->|Anthropic SDK| C\n    D -->|Gemini SDK| F[Gemini 포맷 사용]\n    D -->|새 프로젝트| E\n```\n\n## 마이그레이션 가이드\n\n### OpenAI 공식 API에서 전환\n\n```python\n# 전 (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# 후 (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # 이 라인 추가\n)\n# 이게 전부입니다! 동일한 코드가 작동합니다\n```\n\n### Anthropic 공식 API에서 전환\n\n```python\n# 전 (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# 후 (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # 이 라인 추가 (/v1 없음!)\n)\n```\n\n### Google AI Studio에서 전환\n\n```python\n# 전 (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# 후 (LemonData) - REST API 사용\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## 교차 모델 호환성\n\nLemonData의 마법: **어떤 SDK**로도 **어떤 모델**이든 사용하세요. 게이트웨이가 포맷 변환을 자동으로 처리합니다.\n\n### 모든 SDK → 모든 모델\n\n```python\n# GPT-4o와 함께 사용하는 Anthropic SDK (OpenAI 포맷으로 자동 변환)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ 작동함! 자동 변환됨\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# 동일한 SDK, 다른 모델 - 코드 변경 없음\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ 작동함!\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ 작동함!\n```\n\n### OpenAI SDK → 모든 모델\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# 이 모든 것이 동일한 SDK로 작동합니다:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### 업계 비교\n\n| 플랫폼 | OpenAI 포맷 | Anthropic 포맷 | Gemini 포맷 | Responses API |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ 모든 모델 | ✅ 모든 모델 | ✅ 모든 모델 | ✅ 모든 모델 |\n| OpenRouter | ✅ 모든 모델 | ❌ | ❌ | ❌ |\n| Together AI | ✅ 모든 모델 | ❌ | ❌ | ❌ |\n| Fireworks | ✅ 모든 모델 | ❌ | ❌ | ❌ |\n\n<Note>\n대부분의 기능에서 교차 포맷이 작동하지만, 특정 포맷 전용 기능(예: Anthropic 확장된 사고)은 네이티브 포맷이 필요합니다.\n</Note>",
      "de": "---\ntitle: \"✨ Multi-Format-API\"\ndescription: \"Nutzen Sie OpenAI-, Anthropic- oder Gemini-Formate mit einem einzigen API-Key\"\n---\n\n## Übersicht\n\nLemonData unterstützt **drei native API-Formate** mit einem einzigen API-Key. Wählen Sie das Format, das am besten zu Ihrem Anwendungsfall passt – keine Konfigurationsänderungen erforderlich.\n\n<CardGroup cols={3}>\n  <Card title=\"OpenAI-Format\" icon=\"plug\">\n    `/v1/chat/completions`\n    Standardformat, breiteste Kompatibilität\n  </Card>\n  <Card title=\"Anthropic-Format\" icon=\"message\">\n    `/v1/messages`\n    Extended Thinking, native Claude-Funktionen\n  </Card>\n  <Card title=\"Gemini-Format\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Integration in das Google-Ökosystem\n  </Card>\n</CardGroup>\n\n## Warum Multi-Format?\n\n| Vorteil | Beschreibung |\n|---------|-------------|\n| **Kein SDK-Wechsel** | Nutzen Sie jedes Modell mit Ihrem bevorzugten SDK |\n| **Native Funktionen** | Zugriff auf formatspezifische Funktionen |\n| **Einfache Migration** | Wechseln Sie von offiziellen APIs mit nur einer Änderung der Basis-URL |\n| **Einheitliche Abrechnung** | Ein Konto, ein API-Key, alle Formate |\n\n## Format-Vergleich\n\n| Funktion | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpunkt** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **Auth-Header** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **System-Prompt** | Im `messages`-Array | Separates `system`-Feld | In `systemInstruction` |\n| **Extended Thinking** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Tool Calling** | ✅ | ✅ | ✅ |\n| **Vision** | ✅ | ✅ | ✅ |\n\n## OpenAI-Format\n\nDas am weitesten kompatible Format. Funktioniert mit allen LemonData-Modellen.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Works with ANY model\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # Claude via OpenAI format\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**Bestens geeignet für:**\n- Allgemeine Nutzung\n- Bestehende OpenAI SDK-Integrationen\n- Maximale Kompatibilität\n\n## Anthropic-Format\n\nNative Anthropic Messages API. Erforderlich für Claude-spezifische Funktionen wie Extended Thinking.\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # No /v1 suffix!\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # Separate system field\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Extended Thinking (Claude Opus 4.5)\n\nNur im Anthropic-Format verfügbar:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# Access thinking process\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**Bestens geeignet für:**\n- Claude-spezifische Funktionen\n- Extended Thinking-Modus\n- Nutzer des nativen Anthropic SDK\n\n## Gemini-Format\n\nNatives Google Gemini API-Format für die Integration in das Google-Ökosystem.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### Streaming\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**Bestens geeignet für:**\n- Google Cloud-Integrationen\n- Bestehender Gemini SDK-Code\n- Native Gemini-Funktionen\n\n## Das richtige Format wählen\n\n```mermaid\ngraph TD\n    A[Which format?] --> B{Need Claude extended thinking?}\n    B -->|Yes| C[Use Anthropic Format]\n    B -->|No| D{Existing codebase?}\n    D -->|OpenAI SDK| E[Use OpenAI Format]\n    D -->|Anthropic SDK| C\n    D -->|Gemini SDK| F[Use Gemini Format]\n    D -->|New project| E\n```\n\n## Migrations-Leitfäden\n\n### Von der offiziellen OpenAI API\n\n```python\n# Before (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# After (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # Add this line\n)\n# That's it! Same code works\n```\n\n### Von der offiziellen Anthropic API\n\n```python\n# Before (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# After (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Add this line (no /v1!)\n)\n```\n\n### Von Google AI Studio\n\n```python\n# Before (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# After (LemonData) - Use REST API\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## Modellübergreifende Kompatibilität\n\nDie Magie von LemonData: Nutzen Sie **jedes SDK** mit **jedem Modell**. Das Gateway übernimmt automatisch die Formatkonvertierung.\n\n### Jedes SDK → Jedes Modell\n\n```python\n# Anthropic SDK with GPT-4o (auto-converts to OpenAI format)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ Works! Auto-converted\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Same SDK, different models - no code changes\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ Works!\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ Works!\n```\n\n### OpenAI SDK → Alle Modelle\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# All these work with the same SDK:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### Branchenvergleich\n\n| Plattform | OpenAI-Format | Anthropic-Format | Gemini-Format | Responses-API |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ Alle Modelle | ✅ Alle Modelle | ✅ Alle Modelle | ✅ Alle Modelle |\n| OpenRouter | ✅ Alle Modelle | ❌ | ❌ | ❌ |\n| Together AI | ✅ Alle Modelle | ❌ | ❌ | ❌ |\n| Fireworks | ✅ Alle Modelle | ❌ | ❌ | ❌ |\n\n<Note>\nWährend die formatübergreifende Kompatibilität für die meisten Funktionen gegeben ist, erfordern formatspezifische Funktionen (wie Anthropic Extended Thinking) das native Format.\n</Note>",
      "fr": "---\ntitle: \"✨ API Multi-Format\"\ndescription: \"Utilisez les formats OpenAI, Anthropic ou Gemini avec une seule clé API\"\n---\n\n## Aperçu\n\nLemonData prend en charge **trois formats d'API natifs** avec une seule clé API. Choisissez le format qui convient le mieux à votre cas d'utilisation - aucun changement de configuration n'est nécessaire.\n\n<CardGroup cols={3}>\n  <Card title=\"Format OpenAI\" icon=\"plug\">\n    `/v1/chat/completions`\n    Format standard, compatibilité la plus large\n  </Card>\n  <Card title=\"Format Anthropic\" icon=\"message\">\n    `/v1/messages`\n    Réflexion étendue, fonctionnalités Claude natives\n  </Card>\n  <Card title=\"Format Gemini\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Intégration à l'écosystème Google\n  </Card>\n</CardGroup>\n\n## Pourquoi le Multi-Format ?\n\n| Avantage | Description |\n|---------|-------------|\n| **Pas de changement de SDK** | Utilisez n'importe quel modèle avec votre SDK préféré |\n| **Fonctionnalités natives** | Accédez aux capacités spécifiques à chaque format |\n| **Migration facile** | Passez des API officielles avec un simple changement d'URL de base |\n| **Facturation unique** | Un seul compte, une seule clé API, tous les formats |\n\n## Comparaison des formats\n\n| Fonctionnalité | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **En-tête d'authentification** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **Prompt système** | Dans le tableau `messages` | Champ `system` séparé | Dans `systemInstruction` |\n| **Réflexion étendue** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Tool Calling** | ✅ | ✅ | ✅ |\n| **Vision** | ✅ | ✅ | ✅ |\n\n## Format OpenAI\n\nLe format le plus largement compatible. Fonctionne avec tous les modèles LemonData.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Fonctionne avec N'IMPORTE QUEL modèle\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # Claude via le format OpenAI\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**Idéal pour :**\n- Utilisation générale\n- Intégrations existantes du SDK OpenAI\n- Compatibilité maximale\n\n## Format Anthropic\n\nAPI Messages native d'Anthropic. Requis pour les fonctionnalités spécifiques à Claude comme la réflexion étendue (extended thinking).\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Pas de suffixe /v1 !\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # Champ system séparé\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Réflexion étendue (Claude Opus 4.5)\n\nUniquement disponible au format Anthropic :\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# Accéder au processus de réflexion\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**Idéal pour :**\n- Fonctionnalités spécifiques à Claude\n- Mode de réflexion étendue\n- Utilisateurs natifs du SDK Anthropic\n\n## Format Gemini\n\nFormat d'API natif Google Gemini pour l'intégration à l'écosystème Google.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### Streaming\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**Idéal pour :**\n- Intégrations Google Cloud\n- Code existant du SDK Gemini\n- Fonctionnalités Gemini natives\n\n## Choisir le bon format\n\n```mermaid\ngraph TD\n    A[Quel format ?] --> B{Besoin de la réflexion étendue de Claude ?}\n    B -->|Oui| C[Utiliser le format Anthropic]\n    B -->|Non| D{Code existant ?}\n    D -->|SDK OpenAI| E[Utiliser le format OpenAI]\n    D -->|SDK Anthropic| C\n    D -->|SDK Gemini| F[Utiliser le format Gemini]\n    D -->|Nouveau projet| E\n```\n\n## Guides de migration\n\n### Depuis l'API officielle d'OpenAI\n\n```python\n# Avant (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# Après (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # Ajouter cette ligne\n)\n# C'est tout ! Le même code fonctionne\n```\n\n### Depuis l'API officielle d'Anthropic\n\n```python\n# Avant (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# Après (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Ajouter cette ligne (pas de /v1 !)\n)\n```\n\n### Depuis Google AI Studio\n\n```python\n# Avant (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# Après (LemonData) - Utiliser l'API REST\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## Compatibilité entre modèles\n\nLa magie de LemonData : utilisez **n'importe quel SDK** avec **n'importe quel modèle**. La passerelle gère automatiquement la conversion de format.\n\n### N'importe quel SDK → N'importe quel modèle\n\n```python\n# SDK Anthropic avec GPT-4o (conversion automatique au format OpenAI)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ Fonctionne ! Converti automatiquement\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Même SDK, différents modèles - aucun changement de code\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ Fonctionne !\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ Fonctionne !\n```\n\n### SDK OpenAI → Tous les modèles\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# Tous ces modèles fonctionnent avec le même SDK :\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### Comparaison de l'industrie\n\n| Plateforme | Format OpenAI | Format Anthropic | Format Gemini | API Responses |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ Tous les modèles | ✅ Tous les modèles | ✅ Tous les modèles | ✅ Tous les modèles |\n| OpenRouter | ✅ Tous les modèles | ❌ | ❌ | ❌ |\n| Together AI | ✅ Tous les modèles | ❌ | ❌ | ❌ |\n| Fireworks | ✅ Tous les modèles | ❌ | ❌ | ❌ |\n\n<Note>\nBien que le cross-format fonctionne pour la plupart des fonctionnalités, les fonctionnalités spécifiques à un format (comme la réflexion étendue d'Anthropic) nécessitent le format natif.\n</Note>",
      "es": "---\ntitle: \"✨ API Multi-Formato\"\ndescription: \"Usa los formatos de OpenAI, Anthropic o Gemini con una sola API key\"\n---\n\n## Resumen\n\nLemonData admite **tres formatos de API nativos** con una sola API key. Elige el formato que mejor se adapte a tu caso de uso; no se requieren cambios de configuración.\n\n<CardGroup cols={3}>\n  <Card title=\"Formato OpenAI\" icon=\"plug\">\n    `/v1/chat/completions`\n    Formato estándar, la compatibilidad más amplia\n  </Card>\n  <Card title=\"Formato Anthropic\" icon=\"message\">\n    `/v1/messages`\n    Pensamiento extendido, funciones nativas de Claude\n  </Card>\n  <Card title=\"Formato Gemini\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Integración con el ecosistema de Google\n  </Card>\n</CardGroup>\n\n## ¿Por qué Multi-Formato?\n\n| Beneficio | Descripción |\n|---------|-------------|\n| **Sin cambio de SDK** | Usa cualquier modelo con tu SDK preferido |\n| **Funciones nativas** | Accede a capacidades específicas del formato |\n| **Migración sencilla** | Cambia desde las API oficiales con solo un cambio de URL base |\n| **Facturación única** | Una cuenta, una API key, todos los formatos |\n\n## Comparación de Formatos\n\n| Función | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **Encabezado de autenticación** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **Prompt de sistema** | En el array de mensajes | Campo `system` separado | En `systemInstruction` |\n| **Pensamiento extendido** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Tool Calling** | ✅ | ✅ | ✅ |\n| **Visión** | ✅ | ✅ | ✅ |\n\n## Formato OpenAI\n\nEl formato con mayor compatibilidad. Funciona con todos los modelos de LemonData.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Works with ANY model\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # Claude via OpenAI format\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**Ideal para:**\n- Uso general\n- Integraciones existentes con el SDK de OpenAI\n- Máxima compatibilidad\n\n## Formato Anthropic\n\nAPI de mensajes nativa de Anthropic. Requerida para funciones específicas de Claude como el pensamiento extendido.\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # No /v1 suffix!\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # Separate system field\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Pensamiento extendido (Claude Opus 4.5)\n\nSolo disponible en el formato de Anthropic:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# Access thinking process\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**Ideal para:**\n- Funciones específicas de Claude\n- Modo de pensamiento extendido\n- Usuarios del SDK nativo de Anthropic\n\n## Formato Gemini\n\nFormato nativo de la API de Google Gemini para la integración con el ecosistema de Google.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### Streaming\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**Ideal para:**\n- Integraciones con Google Cloud\n- Código existente del SDK de Gemini\n- Funciones nativas de Gemini\n\n## Elegir el formato adecuado\n\n```mermaid\ngraph TD\n    A[¿Qué formato?] --> B{¿Necesitas pensamiento extendido de Claude?}\n    B -->|Sí| C[Usa el formato de Anthropic]\n    B -->|No| D{¿Base de código existente?}\n    D -->|SDK de OpenAI| E[Usa el formato de OpenAI]\n    D -->|SDK de Anthropic| C\n    D -->|SDK de Gemini| F[Usa el formato de Gemini]\n    D -->|Nuevo proyecto| E\n```\n\n## Guías de migración\n\n### Desde la API oficial de OpenAI\n\n```python\n# Antes (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# Después (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # Añade esta línea\n)\n# ¡Eso es todo! El mismo código funciona\n```\n\n### Desde la API oficial de Anthropic\n\n```python\n# Antes (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# Después (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Añade esta línea (¡sin /v1!)\n)\n```\n\n### Desde Google AI Studio\n\n```python\n# Antes (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# Después (LemonData) - Usa la API REST\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## Compatibilidad entre modelos\n\nLa magia de LemonData: usa **cualquier SDK** con **cualquier modelo**. El gateway gestiona automáticamente la conversión de formato.\n\n### Cualquier SDK → Cualquier modelo\n\n```python\n# Anthropic SDK with GPT-4o (auto-converts to OpenAI format)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ ¡Funciona! Convertido automáticamente\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Same SDK, different models - no code changes\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ ¡Funciona!\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ ¡Funciona!\n```\n\n### SDK de OpenAI → Todos los modelos\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# All these work with the same SDK:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n## Comparación con la industria\n\n| Plataforma | Formato OpenAI | Formato Anthropic | Formato Gemini | API de respuestas |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ Todos los modelos | ✅ Todos los modelos | ✅ Todos los modelos | ✅ Todos los modelos |\n| OpenRouter | ✅ Todos los modelos | ❌ | ❌ | ❌ |\n| Together AI | ✅ Todos los modelos | ❌ | ❌ | ❌ |\n| Fireworks | ✅ Todos los modelos | ❌ | ❌ | ❌ |\n\n<Note>\nAunque la compatibilidad entre formatos funciona para la mayoría de las funciones, las características específicas de un formato (como el pensamiento extendido de Anthropic) requieren el formato nativo.\n</Note>",
      "pt": "---\ntitle: \"✨ API Multi-Formato\"\ndescription: \"Use os formatos OpenAI, Anthropic ou Gemini com uma única chave de API\"\n---\n\n## Visão Geral\n\nO LemonData suporta **três formatos nativos de API** com uma única chave de API. Escolha o formato que melhor se adapta ao seu caso de uso - sem necessidade de alterações de configuração.\n\n<CardGroup cols={3}>\n  <Card title=\"Formato OpenAI\" icon=\"plug\">\n    `/v1/chat/completions`\n    Formato padrão, maior compatibilidade\n  </Card>\n  <Card title=\"Formato Anthropic\" icon=\"message\">\n    `/v1/messages`\n    Raciocínio estendido, recursos nativos do Claude\n  </Card>\n  <Card title=\"Formato Gemini\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Integração com o ecossistema Google\n  </Card>\n</CardGroup>\n\n## Por que Multi-Formato?\n\n| Benefício | Descrição |\n|---------|-------------|\n| **Sem troca de SDK** | Use qualquer modelo com seu SDK preferido |\n| **Recursos nativos** | Acesse funcionalidades específicas de cada formato |\n| **Migração fácil** | Mude das APIs oficiais apenas alterando a URL base |\n| **Faturamento único** | Uma conta, uma chave de API, todos os formatos |\n\n## Comparação de Formatos\n\n| Recurso | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **Cabeçalho de Autenticação** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **Prompt de Sistema** | No array de mensagens | Campo `system` separado | Em `systemInstruction` |\n| **Raciocínio Estendido** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Chamada de Ferramentas** | ✅ | ✅ | ✅ |\n| **Visão** | ✅ | ✅ | ✅ |\n\n## Formato OpenAI\n\nO formato com maior compatibilidade. Funciona com todos os modelos do LemonData.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Works with ANY model\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # Claude via OpenAI format\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**Ideal para:**\n- Uso geral\n- Integrações existentes com o SDK da OpenAI\n- Compatibilidade máxima\n\n## Formato Anthropic\n\nAPI de Mensagens nativa da Anthropic. Necessária para recursos específicos do Claude, como o raciocínio estendido.\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # No /v1 suffix!\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # Separate system field\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Raciocínio Estendido (Claude Opus 4.5)\n\nDisponível apenas no formato Anthropic:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# Access thinking process\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**Ideal para:**\n- Recursos específicos do Claude\n- Modo de raciocínio estendido\n- Usuários nativos do SDK da Anthropic\n\n## Formato Gemini\n\nFormato nativo da API Google Gemini para integração com o ecossistema Google.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### Streaming\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**Ideal para:**\n- Integrações com o Google Cloud\n- Código existente com o SDK do Gemini\n- Recursos nativos do Gemini\n\n## Escolhendo o Formato Certo\n\n```mermaid\ngraph TD\n    A[Qual formato?] --> B{Precisa de raciocínio estendido do Claude?}\n    B -->|Sim| C[Use o Formato Anthropic]\n    B -->|Não| D{Base de código existente?}\n    D -->|SDK da OpenAI| E[Use o Formato OpenAI]\n    D -->|SDK da Anthropic| C\n    D -->|SDK do Gemini| F[Use o Formato Gemini]\n    D -->|Novo projeto| E\n```\n\n## Guias de Migração\n\n### Da API Oficial da OpenAI\n\n```python\n# Antes (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# Depois (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # Adicione esta linha\n)\n# Pronto! O mesmo código funciona\n```\n\n### Da API Oficial da Anthropic\n\n```python\n# Antes (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# Depois (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Adicione esta linha (sem o /v1!)\n)\n```\n\n### Do Google AI Studio\n\n```python\n# Antes (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# Depois (LemonData) - Use a API REST\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## Compatibilidade Entre Modelos\n\nA mágica do LemonData: use **qualquer SDK** com **qualquer modelo**. O gateway lida automaticamente com a conversão de formato.\n\n### Qualquer SDK → Qualquer Modelo\n\n```python\n# Anthropic SDK with GPT-4o (auto-converts to OpenAI format)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ Funciona! Convertido automaticamente\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Mesmo SDK, modelos diferentes - sem alterações no código\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ Funciona!\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ Funciona!\n```\n\n### SDK da OpenAI → Todos os Modelos\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# Todos estes funcionam com o mesmo SDK:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n## Comparação com a Indústria\n\n| Plataforma | Formato OpenAI | Formato Anthropic | Formato Gemini | API de Respostas |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ Todos os modelos | ✅ Todos os modelos | ✅ Todos os modelos | ✅ Todos os modelos |\n| OpenRouter | ✅ Todos os modelos | ❌ | ❌ | ❌ |\n| Together AI | ✅ Todos os modelos | ❌ | ❌ | ❌ |\n| Fireworks | ✅ Todos os modelos | ❌ | ❌ | ❌ |\n\n<Note>\nEmbora a compatibilidade entre formatos funcione para a maioria dos recursos, recursos específicos de um formato (como o raciocínio estendido da Anthropic) exigem o formato nativo.\n</Note>",
      "ar": "",
      "vi": "---\ntitle: \"✨ API Đa Định Dạng\"\ndescription: \"Sử dụng các định dạng OpenAI, Anthropic hoặc Gemini chỉ với một API key duy nhất\"\n---\n\n## Tổng quan\n\nLemonData hỗ trợ **ba định dạng API gốc** với một API key duy nhất. Hãy chọn định dạng phù hợp nhất với nhu cầu sử dụng của bạn - không cần thay đổi cấu hình.\n\n<CardGroup cols={3}>\n  <Card title=\"Định dạng OpenAI\" icon=\"plug\">\n    `/v1/chat/completions`\n    Định dạng tiêu chuẩn, khả năng tương thích rộng nhất\n  </Card>\n  <Card title=\"Định dạng Anthropic\" icon=\"message\">\n    `/v1/messages`\n    Suy nghĩ mở rộng, các tính năng Claude gốc\n  </Card>\n  <Card title=\"Định dạng Gemini\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Tích hợp hệ sinh thái Google\n  </Card>\n</CardGroup>\n\n## Tại sao nên dùng Đa Định Dạng?\n\n| Lợi ích | Mô tả |\n|---------|-------------|\n| **Không cần chuyển đổi SDK** | Sử dụng bất kỳ mô hình nào với SDK ưa thích của bạn |\n| **Tính năng gốc** | Truy cập các khả năng đặc thù của từng định dạng |\n| **Di chuyển dễ dàng** | Chuyển đổi từ các API chính thức chỉ với việc thay đổi base URL |\n| **Thanh toán tập trung** | Một tài khoản, một API key, tất cả định dạng |\n\n## So sánh Định dạng\n\n| Tính năng | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **Auth Header** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **System Prompt** | Trong mảng messages | Trường `system` riêng biệt | Trong `systemInstruction` |\n| **Extended Thinking** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Tool Calling** | ✅ | ✅ | ✅ |\n| **Vision** | ✅ | ✅ | ✅ |\n\n## Định dạng OpenAI\n\nĐịnh dạng tương thích rộng rãi nhất. Hoạt động với tất cả các mô hình của LemonData.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Works with ANY model\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # Claude via OpenAI format\n    messages=[\n        {\"role\": \"system\", \"content\": \"",
      "id": "---\ntitle: \"✨ API Multi-Format\"\ndescription: \"Gunakan format OpenAI, Anthropic, atau Gemini dengan satu kunci API\"\n---\n\n## Gambaran Umum\n\nLemonData mendukung **tiga format API asli** dengan satu kunci API. Pilih format yang paling sesuai dengan kebutuhan Anda - tidak perlu perubahan konfigurasi.\n\n<CardGroup cols={3}>\n  <Card title=\"Format OpenAI\" icon=\"plug\">\n    `/v1/chat/completions`\n    Format standar, kompatibilitas terluas\n  </Card>\n  <Card title=\"Format Anthropic\" icon=\"message\">\n    `/v1/messages`\n    Pemikiran mendalam (extended thinking), fitur asli Claude\n  </Card>\n  <Card title=\"Format Gemini\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Integrasi ekosistem Google\n  </Card>\n</CardGroup>\n\n## Mengapa Multi-Format?\n\n| Manfaat | Deskripsi |\n|---------|-------------|\n| **Tanpa pergantian SDK** | Gunakan model apa pun dengan SDK pilihan Anda |\n| **Fitur asli** | Akses kemampuan khusus format |\n| **Migrasi mudah** | Beralih dari API resmi hanya dengan perubahan base URL |\n| **Penagihan tunggal** | Satu akun, satu kunci API, semua format |\n\n## Perbandingan Format\n\n| Fitur | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **Header Autentikasi** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **Prompt Sistem** | Dalam array messages | Bidang `system` terpisah | Dalam `systemInstruction` |\n| **Pemikiran Mendalam** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Tool Calling** | ✅ | ✅ | ✅ |\n| **Vision** | ✅ | ✅ | ✅ |\n\n## Format OpenAI\n\nFormat yang paling kompatibel secara luas. Berfungsi dengan semua model LemonData.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Berfungsi dengan model APA PUN\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # Claude via format OpenAI\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**Terbaik untuk:**\n- Penggunaan umum\n- Integrasi SDK OpenAI yang sudah ada\n- Kompatibilitas maksimal\n\n## Format Anthropic\n\nAPI Pesan Anthropic asli. Diperlukan untuk fitur khusus Claude seperti pemikiran mendalam (extended thinking).\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Tanpa akhiran /v1!\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # Bidang system terpisah\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Pemikiran Mendalam (Claude Opus 4.5)\n\nHanya tersedia dalam format Anthropic:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# Akses proses pemikiran\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**Terbaik untuk:**\n- Fitur khusus Claude\n- Mode pemikiran mendalam\n- Pengguna SDK Anthropic asli\n\n## Format Gemini\n\nFormat API Google Gemini asli untuk integrasi ekosistem Google.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### Streaming\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**Terbaik untuk:**\n- Integrasi Google Cloud\n- Kode SDK Gemini yang sudah ada\n- Fitur asli Gemini\n\n## Memilih Format yang Tepat\n\n```mermaid\ngraph TD\n    A[Format mana?] --> B{Butuh pemikiran mendalam Claude?}\n    B -->|Ya| C[Gunakan Format Anthropic]\n    B -->|Tidak| D{Basis kode yang sudah ada?}\n    D -->|SDK OpenAI| E[Gunakan Format OpenAI]\n    D -->|SDK Anthropic| C\n    D -->|SDK Gemini| F[Gunakan Format Gemini]\n    D -->|Proyek baru| E\n```\n\n## Panduan Migrasi\n\n### Dari API Resmi OpenAI\n\n```python\n# Sebelum (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# Sesudah (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # Tambahkan baris ini\n)\n# Selesai! Kode yang sama tetap berfungsi\n```\n\n### Dari API Resmi Anthropic\n\n```python\n# Sebelum (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# Sesudah (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Tambahkan baris ini (tanpa /v1!)\n)\n```\n\n### Dari Google AI Studio\n\n```python\n# Sebelum (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# Sesudah (LemonData) - Gunakan REST API\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## Kompatibilitas Lintas-Model\n\nKeajaiban LemonData: gunakan **SDK apa pun** dengan **model apa pun**. Gateway secara otomatis menangani konversi format.\n\n### SDK Apa Pun → Model Apa Pun\n\n```python\n# SDK Anthropic dengan GPT-4o (otomatis dikonversi ke format OpenAI)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ Berhasil! Dikonversi otomatis\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# SDK yang sama, model berbeda - tanpa perubahan kode\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ Berhasil!\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ Berhasil!\n```\n\n### SDK OpenAI → Semua Model\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# Semua ini berfungsi dengan SDK yang sama:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### Perbandingan Industri\n\n| Platform | Format OpenAI | Format Anthropic | Format Gemini | API Respons |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ Semua model | ✅ Semua model | ✅ Semua model | ✅ Semua model |\n| OpenRouter | ✅ Semua model | ❌ | ❌ | ❌ |\n| Together AI | ✅ Semua model | ❌ | ❌ | ❌ |\n| Fireworks | ✅ Semua model | ❌ | ❌ | ❌ |\n\n<Note>\nMeskipun lintas format berfungsi untuk sebagian besar fitur, fitur khusus format (seperti pemikiran mendalam Anthropic) memerlukan format aslinya.\n</Note>",
      "tr": "---\ntitle: \"✨ Çoklu Format API\"\ndescription: \"Tek bir API anahtarı ile OpenAI, Anthropic veya Gemini formatlarını kullanın\"\n---\n\n## Genel Bakış\n\nLemonData, tek bir API anahtarı ile **üç yerel API formatını** destekler. Kullanım durumunuza en uygun formatı seçin - yapılandırma değişikliği gerekmez.\n\n<CardGroup cols={3}>\n  <Card title=\"OpenAI Formatı\" icon=\"plug\">\n    `/v1/chat/completions`\n    Standart format, en geniş uyumluluk\n  </Card>\n  <Card title=\"Anthropic Formatı\" icon=\"message\">\n    `/v1/messages`\n    Extended thinking, yerel Claude özellikleri\n  </Card>\n  <Card title=\"Gemini Formatı\" icon=\"sparkles\">\n    `/v1beta/models/:model:generateContent`\n    Google ekosistemi entegrasyonu\n  </Card>\n</CardGroup>\n\n## Neden Çoklu Format?\n\n| Avantaj | Açıklama |\n|---------|-------------|\n| **SDK değiştirme yok** | Tercih ettiğiniz SDK ile herhangi bir modeli kullanın |\n| **Yerel özellikler** | Formata özgü yeteneklere erişin |\n| **Kolay geçiş** | Sadece bir base URL değişikliği ile resmi API'lerden geçiş yapın |\n| **Tek faturalandırma** | Tek hesap, tek API anahtarı, tüm formatlar |\n\n## Format Karşılaştırması\n\n| Özellik | OpenAI | Anthropic | Gemini |\n|---------|--------|-----------|--------|\n| **Endpoint** | `/v1/chat/completions` | `/v1/messages` | `/v1beta/models/:model:generateContent` |\n| **Auth Header** | `Authorization: Bearer` | `x-api-key` | `Authorization: Bearer` |\n| **System Prompt** | `messages` dizisi içinde | Ayrı `system` alanı | `systemInstruction` içinde |\n| **Extended Thinking** | ❌ | ✅ | ❌ |\n| **Streaming** | ✅ SSE | ✅ SSE | ✅ SSE |\n| **Tool Calling** | ✅ | ✅ | ✅ |\n| **Vision** | ✅ | ✅ | ✅ |\n\n## OpenAI Formatı\n\nEn yaygın uyumlu format. Tüm LemonData modelleriyle çalışır.\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# HERHANGİ bir modelle çalışır\nresponse = client.chat.completions.create(\n    model=\"claude-sonnet-4-5\",  # OpenAI formatı üzerinden Claude\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n**Şunlar için en iyisidir:**\n- Genel kullanım\n- Mevcut OpenAI SDK entegrasyonları\n- Maksimum uyumluluk\n\n## Anthropic Formatı\n\nYerel Anthropic Messages API. Extended thinking gibi Claude'a özgü özellikler için gereklidir.\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # /v1 son eki yok!\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful assistant.\",  # Ayrı system alanı\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n```\n\n### Extended Thinking (Claude Opus 4.5)\n\nYalnızca Anthropic formatında mevcuttur:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex problem...\"}]\n)\n\n# Düşünme sürecine erişin\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Answer: {block.text}\")\n```\n\n**Şunlar için en iyisidir:**\n- Claude'a özgü özellikler\n- Extended thinking modu\n- Yerel Anthropic SDK kullanıcıları\n\n## Gemini Formatı\n\nGoogle ekosistemi entegrasyonu için yerel Google Gemini API formatı.\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\n      \"parts\": [{\"text\": \"Hello!\"}]\n    }],\n    \"systemInstruction\": {\n      \"parts\": [{\"text\": \"You are a helpful assistant.\"}]\n    }\n  }'\n```\n\n### Streaming\n\n```bash\ncurl \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:streamGenerateContent?alt=sse\" \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"contents\": [{\"parts\": [{\"text\": \"Write a story\"}]}]\n  }'\n```\n\n**Şunlar için en iyisidir:**\n- Google Cloud entegrasyonları\n- Mevcut Gemini SDK kodu\n- Yerel Gemini özellikleri\n\n## Doğru Formatı Seçmek\n\n```mermaid\ngraph TD\n    A[Hangi format?] --> B{Claude extended thinking gerekiyor mu?}\n    B -->|Evet| C[Anthropic Formatını Kullan]\n    B -->|Hayır| D{Mevcut kod tabanı?}\n    D -->|OpenAI SDK| E[OpenAI Formatını Kullan]\n    D -->|Anthropic SDK| C\n    D -->|Gemini SDK| F[Gemini Formatını Kullan]\n    D -->|Yeni proje| E\n```\n\n## Geçiş Kılavuzları\n\n### Resmi OpenAI API'sinden\n\n```python\n# Önce (OpenAI)\nclient = OpenAI(api_key=\"sk-openai-key\")\n\n# Sonra (LemonData)\nclient = OpenAI(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"  # Bu satırı ekleyin\n)\n# İşte bu kadar! Aynı kod çalışır\n```\n\n### Resmi Anthropic API'sinden\n\n```python\n# Önce (Anthropic)\nclient = Anthropic(api_key=\"sk-ant-key\")\n\n# Sonra (LemonData)\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"  # Bu satırı ekleyin (/v1 yok!)\n)\n```\n\n### Google AI Studio'dan\n\n```python\n# Önce (Google)\nimport google.generativeai as genai\ngenai.configure(api_key=\"google-api-key\")\n\n# Sonra (LemonData) - REST API kullanın\nimport requests\n\nresponse = requests.post(\n    \"https://api.lemondata.cc/v1beta/models/gemini-2.5-flash:generateContent\",\n    headers={\"Authorization\": \"Bearer sk-lemondata-key\"},\n    json={\"contents\": [{\"parts\": [{\"text\": \"Hello\"}]}]}\n)\n```\n\n## Modeller Arası Uyumluluk\n\nLemonData'nın sihri: **herhangi bir SDK**'yı **herhangi bir model** ile kullanın. Gateway, format dönüşümünü otomatik olarak halleder.\n\n### Herhangi bir SDK → Herhangi bir Model\n\n```python\n# GPT-4o ile Anthropic SDK (otomatik olarak OpenAI formatına dönüştürülür)\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nresponse = client.messages.create(\n    model=\"gpt-4o\",  # ✅ Çalışıyor! Otomatik dönüştürüldü\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Aynı SDK, farklı modeller - kod değişikliği yok\nresponse = client.messages.create(model=\"gemini-2.5-flash\", ...)  # ✅ Çalışıyor!\nresponse = client.messages.create(model=\"deepseek-r1\", ...)       # ✅ Çalışıyor!\n```\n\n### OpenAI SDK → Tüm Modeller\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"https://api.lemondata.cc/v1\", api_key=\"sk-...\")\n\n# Bunların tümü aynı SDK ile çalışır:\nresponse = client.chat.completions.create(model=\"gpt-4o\", ...)\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", ...)\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", ...)\n```\n\n### Sektör Karşılaştırması\n\n| Platform | OpenAI Formatı | Anthropic Formatı | Gemini Formatı | Responses API |\n|----------|:---:|:---:|:---:|:---:|\n| **LemonData** | ✅ Tüm modeller | ✅ Tüm modeller | ✅ Tüm modeller | ✅ Tüm modeller |\n| OpenRouter | ✅ Tüm modeller | ❌ | ❌ | ❌ |\n| Together AI | ✅ Tüm modeller | ❌ | ❌ | ❌ |\n| Fireworks | ✅ Tüm modeller | ❌ | ❌ | ❌ |\n\n<Note>\nFormatlar arası geçiş çoğu özellik için çalışsa da, formata özgü özellikler (Anthropic extended thinking gibi) yerel formatı gerektirir.\n</Note>"
    },
    "updatedAt": "2026-01-26T05:30:22.504Z"
  },
  "guides/best-practices.mdx": {
    "sourceHash": "360e589073c3b648",
    "translations": {
      "zh": "---\ntitle: \"最佳实践\"\ndescription: \"优化您的 LemonData API 使用，以提升成本效益、性能和可靠性\"\n---\n\n## 模型选择\n\n选择合适的模型可以显著影响成本和质量。\n\n### 基于任务的建议\n\n| 任务 | 推荐模型 | 推荐理由 |\n|------|-------------------|-----------|\n| **简单问答** | `gpt-4o-mini`, `gemini-2.5-flash` | 快速、廉价、效果足够好 |\n| **复杂推理** | `o3`, `claude-opus-4-5`, `deepseek-r1` | 更强的逻辑和规划能力 |\n| **编程** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | 针对代码进行了优化 |\n| **创意写作** | `claude-sonnet-4-5`, `gpt-4o` | 更好的文本质量 |\n| **视觉/图像** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | 原生视觉支持 |\n| **长上下文** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 1M+ token 窗口 |\n| **成本敏感** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | 性价比最高 |\n\n### 成本层级\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## 成本优化\n\n### 1. 优先使用小型模型\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. 设置 max_tokens\n\n始终设置合理的 `max_tokens` 限制：\n\n```python\n# ❌ Bad: No limit, could generate thousands of tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Good: Limit response length\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Reasonable limit for a summary\n)\n```\n\n### 3. 优化提示词 (Prompts)\n\n```python\n# ❌ Verbose prompt (more input tokens)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Concise prompt (fewer tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. 启用缓存\n\n利用 [语义缓存](/guides/caching)：\n\n```python\n# For repeated similar queries, caching provides major savings\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministic = better cache hits\n)\n```\n\n### 5. 批量处理相似请求\n\n```python\n# ❌ Many small requests\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Fewer larger requests\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## 性能优化\n\n### 1. 使用流式传输 (Streaming) 提升用户体验\n\n流式传输可以提高感知性能：\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. 为交互式场景选择快速模型\n\n| 使用场景 | 推荐模型 | 延迟 |\n|----------|-------------|---------|\n| 聊天界面 | `gpt-4o-mini`, `gemini-2.5-flash` | 首个 token 约 200ms |\n| Tab 补全 | `claude-haiku-4-5` | 首个 token 约 150ms |\n| 后台处理 | `gpt-4o`, `claude-sonnet-4-5` | 首个 token 约 500ms |\n\n### 3. 设置超时时间\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 second timeout\n)\n```\n\n## 可靠性\n\n### 1. 实现重试机制\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. 优雅地处理错误\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Check API key\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Queue for later or use backup\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Server error, retry later\n        schedule_retry(request)\n```\n\n### 3. 使用备用模型 (Fallback Models)\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## 安全性\n\n### 1. 保护 API Key\n\n```python\n# ❌ Never hardcode keys\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Use environment variables\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. 验证用户输入\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Add other validation as needed\n    return True\n```\n\n### 3. 设置 API Key 限制\n\n为以下场景创建具有支出限制的独立 API Key：\n- 开发/测试\n- 生产环境\n- 不同的应用程序\n\n## 监控\n\n### 1. 追踪使用情况\n\n定期检查您的仪表板以了解：\n- 各模型的 token 使用量\n- 费用明细\n- 缓存命中率\n- 错误率\n\n### 2. 记录重要指标\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. 设置警报\n\n在仪表板中配置余额不足警报，以避免服务中断。\n\n## 检查清单\n\n<AccordionGroup>\n  <Accordion title=\"成本优化\">\n    - [ ] 为每个任务使用合适的模型\n    - [ ] 设置 max_tokens 限制\n    - [ ] 提示词简洁明了\n    - [ ] 在适当的情况下启用缓存\n    - [ ] 批量处理相似请求\n  </Accordion>\n\n  <Accordion title=\"性能\">\n    - [ ] 为交互式体验使用流式传输\n    - [ ] 为实时场景使用快速模型\n    - [ ] 已配置超时时间\n  </Accordion>\n\n  <Accordion title=\"可靠性\">\n    - [ ] 已实现重试逻辑\n    - [ ] 错误处理已就绪\n    - [ ] 已配置备用模型\n  </Accordion>\n\n  <Accordion title=\"安全性\">\n    - [ ] API Key 存储在环境变量中\n    - [ ] 输入验证\n    - [ ] 开发/生产环境使用独立的 Key\n    - [ ] 已设置支出限制\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"最佳實踐\"\ndescription: \"優化您的 LemonData API 使用，以提升成本效益、效能與可靠性\"\n---\n\n## 模型選擇\n\n選擇合適的模型會顯著影響成本與品質。\n\n### 基於任務的建議\n\n| 任務 | 推薦模型 | 原因 |\n|------|-------------------|-----------|\n| **簡單問答** | `gpt-4o-mini`, `gemini-2.5-flash` | 快速、便宜、足夠好用 |\n| **複雜推理** | `o3`, `claude-opus-4-5`, `deepseek-r1` | 更好的邏輯與規劃能力 |\n| **程式編寫** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | 針對程式碼優化 |\n| **創意寫作** | `claude-sonnet-4-5`, `gpt-4o` | 更好的散文品質 |\n| **視覺/圖像** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | 原生視覺支援 |\n| **長上下文** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 1M+ token 視窗 |\n| **成本敏感** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | 最佳性價比 |\n\n### 成本分級\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## 成本優化\n\n### 1. 優先使用小型模型\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. 設定 max_tokens\n\n務必設定合理的 `max_tokens` 限制：\n\n```python\n# ❌ 錯誤：未設限制，可能會產生數千個 token\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ 正確：限制回應長度\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # 摘要的合理限制\n)\n```\n\n### 3. 優化 Prompt\n\n```python\n# ❌ 冗長的 prompt (消耗更多 input token)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ 簡潔的 prompt (消耗較少 token)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. 啟用快取\n\n善用 [語義快取](/guides/caching)：\n\n```python\n# 對於重複的相似查詢，快取可大幅節省成本\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # 確定性 = 更好的快取命中率\n)\n```\n\n### 5. 批次處理相似請求\n\n```python\n# ❌ 許多小型請求\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ 較少的大型請求\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## 效能優化\n\n### 1. 使用串流 (Streaming) 提升使用者體驗\n\n串流可以提升感知的效能：\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. 為互動式用途選擇快速模型\n\n| 使用場景 | 推薦模型 | 延遲 |\n|----------|-------------|---------|\n| 聊天介面 | `gpt-4o-mini`, `gemini-2.5-flash` | 首個 token 約 200ms |\n| 自動補全 | `claude-haiku-4-5` | 首個 token 約 150ms |\n| 背景處理 | `gpt-4o`, `claude-sonnet-4-5` | 首個 token 約 500ms |\n\n### 3. 設定逾時 (Timeouts)\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 秒逾時\n)\n```\n\n## 可靠性\n\n### 1. 實作重試機制\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. 優雅地處理錯誤\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # 檢查 API key\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # 加入隊列稍後處理或使用備援\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # 伺服器錯誤，稍後重試\n        schedule_retry(request)\n```\n\n### 3. 使用備援模型 (Fallback Models)\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## 安全性\n\n### 1. 保護 API Key\n\n```python\n# ❌ 切勿將金鑰寫死在程式碼中\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ 使用環境變數\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. 驗證使用者輸入\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"在發送到 API 之前驗證使用者輸入。\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # 根據需要添加其他驗證\n    return True\n```\n\n### 3. 設定 API Key 限制\n\n為以下用途建立具備支出限制的獨立 API Key：\n- 開發/測試\n- 正式環境\n- 不同的應用程式\n\n## 監控\n\n### 1. 追蹤使用量\n\n定期檢查您的控制面板以了解：\n- 各模型的 token 使用量\n- 費用明細\n- 快取命中率\n- 錯誤率\n\n### 2. 記錄重要指標\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. 設定警示\n\n在控制面板中配置餘額不足警示，以避免服務中斷。\n\n## 檢查清單\n\n<AccordionGroup>\n  <Accordion title=\"成本優化\">\n    - [ ] 為每項任務使用合適的模型\n    - [ ] 設定 max_tokens 限制\n    - [ ] Prompt 保持簡潔\n    - [ ] 在適當的地方啟用快取\n    - [ ] 批次處理相似請求\n  </Accordion>\n\n  <Accordion title=\"效能\">\n    - [ ] 為互動式體驗使用串流\n    - [ ] 為即時用途使用快速模型\n    - [ ] 已配置逾時設定\n  </Accordion>\n\n  <Accordion title=\"可靠性\">\n    - [ ] 已實作重試邏輯\n    - [ ] 錯誤處理已就緒\n    - [ ] 已配置備援模型\n  </Accordion>\n\n  <Accordion title=\"安全性\">\n    - [ ] API Key 儲存於環境變數中\n    - [ ] 輸入驗證\n    - [ ] 開發/正式環境使用獨立金鑰\n    - [ ] 已設定支出限制\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"ベストプラクティス\"\ndescription: \"コスト、パフォーマンス、信頼性のために LemonData API の使用を最適化します\"\n---\n\n## モデルの選択\n\n適切なモデルを選択することは、コストと品質に大きな影響を与えます。\n\n### タスク別の推奨事項\n\n| タスク | 推奨モデル | 理由 |\n|------|-------------------|-----------|\n| **シンプルな Q&A** | `gpt-4o-mini`, `gemini-2.5-flash` | 高速、低コスト、十分な品質 |\n| **複雑な推論** | `o3`, `claude-opus-4-5`, `deepseek-r1` | より優れた論理と計画 |\n| **コーディング** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | コードに最適化 |\n| **クリエイティブライティング** | `claude-sonnet-4-5`, `gpt-4o` | より優れた文章品質 |\n| **ビジョン/画像** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | ネイティブのビジョンサポート |\n| **長いコンテキスト** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 100万以上のトークンウィンドウ |\n| **コスト重視** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | 最高のコスパ |\n\n### コスト階層\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## コストの最適化\n\n### 1. 最初に小規模なモデルを使用する\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. max_tokens を設定する\n\n常に適切な `max_tokens` 制限を設定してください：\n\n```python\n# ❌ Bad: No limit, could generate thousands of tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Good: Limit response length\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Reasonable limit for a summary\n)\n```\n\n### 3. プロンプトを最適化する\n\n```python\n# ❌ Verbose prompt (more input tokens)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Concise prompt (fewer tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. キャッシュを有効にする\n\n[セマンティックキャッシュ](/guides/caching)を活用してください：\n\n```python\n# For repeated similar queries, caching provides major savings\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministic = better cache hits\n)\n```\n\n### 5. 同様のリクエストをバッチ処理する\n\n```python\n# ❌ Many small requests\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Fewer larger requests\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## パフォーマンスの最適化\n\n### 1. UX のためにストリーミングを使用する\n\nストリーミングは体感パフォーマンスを向上させます：\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. インタラクティブな用途には高速なモデルを選択する\n\n| ユースケース | 推奨 | レイテンシ |\n|----------|-------------|---------|\n| チャット UI | `gpt-4o-mini`, `gemini-2.5-flash` | 最初のトークンまで約200ms |\n| タブ補完 | `claude-haiku-4-5` | 最初のトークンまで約150ms |\n| バックグラウンド処理 | `gpt-4o`, `claude-sonnet-4-5` | 最初のトークンまで約500ms |\n\n### 3. タイムアウトを設定する\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 second timeout\n)\n```\n\n## 信頼性\n\n### 1. リトライを実装する\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. エラーを適切に処理する\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Check API key\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Queue for later or use backup\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Server error, retry later\n        schedule_retry(request)\n```\n\n### 3. フォールバックモデルを使用する\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## セキュリティ\n\n### 1. API キーを保護する\n\n```python\n# ❌ Never hardcode keys\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Use environment variables\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. ユーザー入力を検証する\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Add other validation as needed\n    return True\n```\n\n### 3. API キーの制限を設定する\n\n以下の用途ごとに、支出制限を設定した個別の API キーを作成してください：\n- 開発/テスト\n- 本番\n- 異なるアプリケーション\n\n## モニタリング\n\n### 1. 使用状況を追跡する\n\nダッシュボードを定期的にチェックして、以下を確認してください：\n- モデル別のトークン使用量\n- コストの内訳\n- キャッシュヒット率\n- エラー率\n\n### 2. 重要なメトリクスをログに記録する\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. アラートを設定する\n\nサービスの停止を避けるために、ダッシュボードで残高不足のアラートを設定してください。\n\n## チェックリスト\n\n<AccordionGroup>\n  <Accordion title=\"コストの最適化\">\n    - [ ] 各タスクに適切なモデルを使用している\n    - [ ] max_tokens 制限を設定している\n    - [ ] プロンプトが簡潔である\n    - [ ] 適切な場所でキャッシュが有効になっている\n    - [ ] 同様のリクエストをバッチ処理している\n  </Accordion>\n\n  <Accordion title=\"パフォーマンス\">\n    - [ ] インタラクティブな UX のためのストリーミング\n    - [ ] リアルタイム用途の高速モデル\n    - [ ] タイムアウトが設定されている\n  </Accordion>\n\n  <Accordion title=\"信頼性\">\n    - [ ] リトライロジックが実装されている\n    - [ ] エラー処理が整っている\n    - [ ] フォールバックモデルが設定されている\n  </Accordion>\n\n  <Accordion title=\"セキュリティ\">\n    - [ ] API キーが環境変数にある\n    - [ ] 入力検証\n    - [ ] 開発/本番用の個別のキー\n    - [ ] 支出制限が設定されている\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"모범 사례\"\ndescription: \"비용, 성능 및 안정성을 위해 LemonData API 사용을 최적화하세요\"\n---\n\n## 모델 선택\n\n적절한 모델을 선택하면 비용과 품질에 큰 영향을 미칠 수 있습니다.\n\n### 작업별 권장 사항\n\n| 작업 | 권장 모델 | 이유 |\n|------|-------------------|-----------|\n| **단순 Q&A** | `gpt-4o-mini`, `gemini-2.5-flash` | 빠르고 저렴하며 충분한 성능 |\n| **복잡한 추론** | `o3`, `claude-opus-4-5`, `deepseek-r1` | 더 나은 논리 및 계획 능력 |\n| **코딩** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | 코드에 최적화됨 |\n| **창의적 글쓰기** | `claude-sonnet-4-5`, `gpt-4o` | 더 나은 문장 품질 |\n| **비전/이미지** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | 네이티브 비전 지원 |\n| **긴 컨텍스트** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 100만 개 이상의 토큰 윈도우 |\n| **비용 효율성 중시** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | 최고의 가성비 |\n\n### 비용 등급\n\n```\n$$$$ 프리미엄: o3, claude-opus-4-5, gpt-4o\n$$$  표준:    claude-sonnet-4-5, gpt-4o\n$$   예산형:  gpt-4o-mini, gemini-2.5-flash\n$    절약형:  deepseek-v3.2, deepseek-r1\n```\n\n## 비용 최적화\n\n### 1. 더 작은 모델부터 사용하기\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. max_tokens 설정하기\n\n항상 합리적인 `max_tokens` 제한을 설정하세요:\n\n```python\n# ❌ 나쁨: 제한이 없어 수천 개의 토큰이 생성될 수 있음\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ 좋음: 응답 길이 제한\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # 요약에 적절한 제한\n)\n```\n\n### 3. 프롬프트 최적화\n\n```python\n# ❌ 장황한 프롬프트 (더 많은 입력 토큰 발생)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ 간결한 프롬프트 (더 적은 토큰 발생)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. 캐싱 활성화\n\n[시맨틱 캐싱](/guides/caching)을 활용하세요:\n\n```python\n# 반복되는 유사한 쿼리의 경우, 캐싱을 통해 비용을 크게 절감할 수 있습니다.\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # 결정론적 응답 = 더 나은 캐시 히트율\n)\n```\n\n### 5. 유사한 요청 일괄 처리(Batching)\n\n```python\n# ❌ 다수의 작은 요청\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ 적은 수의 큰 요청\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## 성능 최적화\n\n### 1. 사용자 경험(UX)을 위한 스트리밍 사용\n\n스트리밍은 체감 성능을 향상시킵니다:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. 대화형 사용을 위한 빠른 모델 선택\n\n| 사용 사례 | 권장 모델 | 지연 시간(Latency) |\n|----------|-------------|---------|\n| 채팅 UI | `gpt-4o-mini`, `gemini-2.5-flash` | 첫 토큰까지 약 200ms |\n| 탭 완성 | `claude-haiku-4-5` | 첫 토큰까지 약 150ms |\n| 백그라운드 처리 | `gpt-4o`, `claude-sonnet-4-5` | 첫 토큰까지 약 500ms |\n\n### 3. 타임아웃 설정\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60초 타임아웃\n)\n```\n\n## 안정성\n\n### 1. 재시도 로직 구현\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. 우아한 에러 처리\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # API 키 확인\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # 나중에 처리하도록 큐에 추가하거나 백업 사용\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # 서버 에러, 나중에 재시도 예약\n        schedule_retry(request)\n```\n\n### 3. 폴백(Fallback) 모델 사용\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## 보안\n\n### 1. API 키 보호\n\n```python\n# ❌ 키를 코드에 직접 작성하지 마세요\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ 환경 변수를 사용하세요\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. 사용자 입력 검증\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # 필요한 다른 검증 로직 추가\n    return True\n```\n\n### 3. API 키 제한 설정\n\n다음을 위해 지출 제한이 설정된 별도의 API 키를 생성하세요:\n- 개발/테스트\n- 운영(Production)\n- 서로 다른 애플리케이션\n\n## 모니터링\n\n### 1. 사용량 추적\n\n대시보드에서 다음 사항을 정기적으로 확인하세요:\n- 모델별 토큰 사용량\n- 비용 내역\n- 캐시 히트율\n- 에러율\n\n### 2. 주요 지표 로깅\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. 알림 설정\n\n서비스 중단을 방지하기 위해 대시보드에서 잔액 부족 알림을 구성하세요.\n\n## 체크리스트\n\n<AccordionGroup>\n  <Accordion title=\"비용 최적화\">\n    - [ ] 각 작업에 적절한 모델 사용 중\n    - [ ] max_tokens 제한 설정됨\n    - [ ] 프롬프트가 간결함\n    - [ ] 적절한 경우 캐싱 활성화됨\n    - [ ] 유사한 요청을 일괄 처리함\n  </Accordion>\n\n  <Accordion title=\"성능\">\n    - [ ] 대화형 UX를 위한 스트리밍 사용\n    - [ ] 실시간 사용을 위한 빠른 모델 사용\n    - [ ] 타임아웃 구성됨\n  </Accordion>\n\n  <Accordion title=\"안정성\">\n    - [ ] 재시도 로직 구현됨\n    - [ ] 에러 처리 적용됨\n    - [ ] 폴백 모델 구성됨\n  </Accordion>\n\n  <Accordion title=\"보안\">\n    - [ ] 환경 변수에 API 키 저장\n    - [ ] 입력 검증 수행\n    - [ ] 개발/운영용 키 분리\n    - [ ] 지출 제한 설정됨\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Best Practices\"\ndescription: \"Optimieren Sie Ihre LemonData API-Nutzung hinsichtlich Kosten, Leistung und Zuverlässigkeit\"\n---\n\n## Modellauswahl\n\nDie Wahl des richtigen Modells kann erhebliche Auswirkungen auf Kosten und Qualität haben.\n\n### Aufgabenbasierte Empfehlungen\n\n| Aufgabe | Empfohlene Modelle | Begründung |\n|------|-------------------|-----------|\n| **Einfache Q&A** | `gpt-4o-mini`, `gemini-2.5-flash` | Schnell, günstig, ausreichend |\n| **Komplexes logisches Denken** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Bessere Logik und Planung |\n| **Programmierung** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimiert für Code |\n| **Kreatives Schreiben** | `claude-sonnet-4-5`, `gpt-4o` | Bessere Prosaqualität |\n| **Vision/Bilder** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Native Vision-Unterstützung |\n| **Langer Kontext** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 1M+ Token-Fenster |\n| **Kostenbewusst** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Bestes Preis-Leistungs-Verhältnis |\n\n### Kostenstufen\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## Kostenoptimierung\n\n### 1. Kleinere Modelle zuerst verwenden\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Nutzen Sie günstigere Modelle für einfache Aufgaben.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Günstig starten, bei Bedarf eskalieren\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. max_tokens festlegen\n\nLegen Sie immer ein angemessenes `max_tokens`-Limit fest:\n\n```python\n# ❌ Schlecht",
      "fr": "---\ntitle: \"Bonnes pratiques\"\ndescription: \"Optimisez votre utilisation de l'API LemonData pour le coût, la performance et la fiabilité\"\n---\n\n## Sélection du modèle\n\nChoisir le bon modèle peut avoir un impact significatif sur le coût et la qualité.\n\n### Recommandations par tâche\n\n| Tâche | Modèles recommandés | Raisonnement |\n|------|-------------------|-----------|\n| **Questions-réponses simples** | `gpt-4o-mini`, `gemini-2.5-flash` | Rapide, économique, suffisant |\n| **Raisonnement complexe** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Meilleure logique et planification |\n| **Codage** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimisé pour le code |\n| **Écriture créative** | `claude-sonnet-4-5`, `gpt-4o` | Meilleure qualité de prose |\n| **Vision/Images** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Support natif de la vision |\n| **Contexte long** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Fenêtres de plus de 1M de tokens |\n| **Sensible au coût** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Meilleur rapport qualité-prix |\n\n### Niveaux de coût\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## Optimisation des coûts\n\n### 1. Utiliser d'abord des modèles plus petits\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. Définir max_tokens\n\nDéfinissez toujours une limite `max_tokens` raisonnable :\n\n```python\n# ❌ Mauvais : Aucune limite, pourrait générer des milliers de tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Bon : Limiter la longueur de la réponse\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Limite raisonnable pour un résumé\n)\n```\n\n### 3. Optimiser les prompts\n\n```python\n# ❌ Prompt verbeux (plus de tokens d'entrée)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Prompt concis (moins de tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. Activer la mise en cache\n\nProfitez de la [mise en cache sémantique](/guides/caching) :\n\n```python\n# Pour les requêtes similaires répétées, le cache permet des économies majeures\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Déterministe = meilleurs hits de cache\n)\n```\n\n### 5. Grouper les requêtes similaires\n\n```python\n# ❌ De nombreuses petites requêtes\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Moins de requêtes plus volumineuses\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## Optimisation de la performance\n\n### 1. Utiliser le streaming pour l'UX\n\nLe streaming améliore la performance perçue :\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. Choisir des modèles rapides pour une utilisation interactive\n\n| Cas d'utilisation | Recommandé | Latence |\n|----------|-------------|---------|\n| Interface de chat | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms premier token |\n| Complétion par tabulation | `claude-haiku-4-5` | ~150ms premier token |\n| Traitement en arrière-plan | `gpt-4o`, `claude-sonnet-4-5` | ~500ms premier token |\n\n### 3. Définir des délais d'expiration (timeouts)\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # Délai d'expiration de 60 secondes\n)\n```\n\n## Fiabilité\n\n### 1. Implémenter des tentatives (retries)\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. Gérer les erreurs avec élégance\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Vérifier la clé API\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Mettre en file d'attente pour plus tard ou utiliser un secours\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Erreur serveur, réessayer plus tard\n        schedule_retry(request)\n```\n\n### 3. Utiliser des modèles de secours (fallback)\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## Sécurité\n\n### 1. Protéger les clés API\n\n```python\n# ❌ Ne jamais coder les clés en dur\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Utiliser des variables d'environnement\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. Valider les entrées utilisateur\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Ajouter d'autres validations si nécessaire\n    return True\n```\n\n### 3. Définir des limites pour les clés API\n\nCréez des clés API distinctes avec des limites de dépenses pour :\n- Le développement/test\n- La production\n- Différentes applications\n\n## Surveillance (Monitoring)\n\n### 1. Suivre l'utilisation\n\nConsultez régulièrement votre tableau de bord pour :\n- L'utilisation des tokens par modèle\n- La répartition des coûts\n- Les taux de réussite du cache\n- Les taux d'erreur\n\n### 2. Enregistrer les métriques importantes\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. Configurer des alertes\n\nConfigurez des alertes de solde bas dans votre tableau de bord pour éviter toute interruption de service.\n\n## Liste de contrôle\n\n<AccordionGroup>\n  <Accordion title=\"Optimisation des coûts\">\n    - [ ] Utilisation du modèle approprié pour chaque tâche\n    - [ ] Définition des limites max_tokens\n    - [ ] Les prompts sont concis\n    - [ ] Mise en cache activée lorsque c'est approprié\n    - [ ] Groupement des requêtes similaires\n  </Accordion>\n\n  <Accordion title=\"Performance\">\n    - [ ] Streaming pour l'UX interactive\n    - [ ] Modèles rapides pour une utilisation en temps réel\n    - [ ] Timeouts configurés\n  </Accordion>\n\n  <Accordion title=\"Fiabilité\">\n    - [ ] Logique de tentative (retry) implémentée\n    - [ ] Gestion des erreurs en place\n    - [ ] Modèles de secours configurés\n  </Accordion>\n\n  <Accordion title=\"Sécurité\">\n    - [ ] Clés API dans les variables d'environnement\n    - [ ] Validation des entrées\n    - [ ] Clés distinctes pour dev/prod\n    - [ ] Limites de dépenses définies\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Mejores Prácticas\"\ndescription: \"Optimice su uso de la API de LemonData para mejorar el costo, el rendimiento y la confiabilidad\"\n---\n\n## Selección de Modelos\n\nElegir el modelo adecuado puede impactar significativamente en el costo y la calidad.\n\n### Recomendaciones Basadas en Tareas\n\n| Tarea | Modelos Recomendados | Razonamiento |\n|------|-------------------|-----------|\n| **Preguntas y respuestas simples** | `gpt-4o-mini`, `gemini-2.5-flash` | Rápido, económico, suficientemente bueno |\n| **Razonamiento complejo** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Mejor lógica y planificación |\n| **Programación** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimizado para código |\n| **Escritura creativa** | `claude-sonnet-4-5`, `gpt-4o` | Mejor calidad de prosa |\n| **Visión/Imágenes** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Soporte nativo de visión |\n| **Contexto largo** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Ventanas de más de 1M de tokens |\n| **Sensible al costo** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Mejor relación calidad-precio |\n\n### Niveles de Costo\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## Optimización de Costos\n\n### 1. Use Modelos más Pequeños Primero\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use modelos más económicos para tareas simples.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Comenzar con lo económico, escalar si es necesario\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. Configure max_tokens\n\nConfigure siempre un límite razonable de `max_tokens`:\n\n```python\n# ❌ Mal: Sin límite, podría generar miles de tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Bien: Limitar la longitud de la respuesta\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Límite razonable para un resumen\n)\n```\n\n### 3. Optimice los Prompts\n\n```python\n# ❌ Prompt detallado (más tokens de entrada)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Prompt conciso (menos tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. Habilite el Caching\n\nAproveche el [caching semántico](/guides/caching):\n\n```python\n# Para consultas similares repetidas, el caching proporciona grandes ahorros\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Determinista = mejores aciertos de caché\n)\n```\n\n### 5. Agrupe Solicitudes Similares (Batching)\n\n```python\n# ❌ Muchas solicitudes pequeñas\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Menos solicitudes pero más grandes\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## Optimización del Rendimiento\n\n### 1. Use Streaming para la UX\n\nEl streaming mejora el rendimiento percibido:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. Elija Modelos Rápidos para Uso Interactivo\n\n| Caso de Uso | Recomendado | Latencia |\n|----------|-------------|---------|\n| Interfaz de Chat | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms primer token |\n| Autocompletado | `claude-haiku-4-5` | ~150ms primer token |\n| Procesamiento en segundo plano | `gpt-4o`, `claude-sonnet-4-5` | ~500ms primer token |\n\n### 3. Configure Timeouts\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # Timeout de 60 segundos\n)\n```\n\n## Confiabilidad\n\n### 1. Implemente Reintentos\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Límite de tasa alcanzado, esperando {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Se excedió el máximo de reintentos\")\n```\n\n### 2. Gestione los Errores con Elegancia\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Verificar la API key\n    notify_admin(\"API key inválida\")\nexcept RateLimitError:\n    # Encolar para más tarde o usar respaldo\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Saldo bajo\")\n    elif e.status_code >= 500:\n        # Error del servidor, reintentar más tarde\n        schedule_retry(request)\n```\n\n### 3. Use Modelos de Respaldo (Fallback)\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"Todos los modelos fallaron\")\n```\n\n## Seguridad\n\n### 1. Proteja sus API Keys\n\n```python\n# ❌ Nunca incluya las llaves directamente en el código\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Use variables de entorno\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. Valide la Entrada del Usuario\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Valida la entrada del usuario antes de enviarla a la API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Mensaje demasiado largo\")\n    # Agregar otras validaciones según sea necesario\n    return True\n```\n\n### 3. Establezca Límites para las API Keys\n\nCree API keys separadas con límites de gasto para:\n- Desarrollo/pruebas\n- Producción\n- Diferentes aplicaciones\n\n## Monitoreo\n\n### 1. Rastree el Uso\n\nRevise su panel de control regularmente para:\n- Uso de tokens por modelo\n- Desglose de costos\n- Tasas de acierto de caché\n- Tasas de error\n\n### 2. Registre Métricas Importantes\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. Configure Alertas\n\nConfigure alertas de saldo bajo en su panel de control para evitar interrupciones del servicio.\n\n## Lista de Verificación\n\n<AccordionGroup>\n  <Accordion title=\"Optimización de costos\">\n    - [ ] Uso del modelo apropiado para cada tarea\n    - [ ] Configuración de límites de max_tokens\n    - [ ] Los prompts son concisos\n    - [ ] Caching habilitado donde sea apropiado\n    - [ ] Agrupación de solicitudes similares\n  </Accordion>\n\n  <Accordion title=\"Rendimiento\">\n    - [ ] Streaming para UX interactiva\n    - [ ] Modelos rápidos para uso en tiempo real\n    - [ ] Timeouts configurados\n  </Accordion>\n\n  <Accordion title=\"Confiabilidad\">\n    - [ ] Lógica de reintento implementada\n    - [ ] Gestión de errores implementada\n    - [ ] Modelos de respaldo configurados\n  </Accordion>\n\n  <Accordion title=\"Seguridad\">\n    - [ ] API keys en variables de entorno\n    - [ ] Validación de entrada\n    - [ ] Llaves separadas para desarrollo/producción\n    - [ ] Límites de gasto establecidos\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Melhores Práticas\"\ndescription: \"Otimize seu uso da API LemonData para custo, desempenho e confiabilidade\"\n---\n\n## Seleção de Modelos\n\nEscolher o modelo certo pode impactar significativamente o custo e a qualidade.\n\n### Recomendações Baseadas em Tarefas\n\n| Tarefa | Modelos Recomendados | Justificativa |\n|------|-------------------|-----------|\n| **Q&A Simples** | `gpt-4o-mini`, `gemini-2.5-flash` | Rápido, barato, bom o suficiente |\n| **Raciocínio complexo** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Melhor lógica e planejamento |\n| **Programação** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Otimizado para código |\n| **Escrita criativa** | `claude-sonnet-4-5`, `gpt-4o` | Melhor qualidade de prosa |\n| **Visão/Imagens** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Suporte nativo a visão |\n| **Contexto longo** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Janelas de mais de 1M de tokens |\n| **Sensível a custos** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Melhor custo-benefício |\n\n### Níveis de Custo\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## Otimização de Custos\n\n### 1. Use Modelos Menores Primeiro\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. Configure max_tokens\n\nSempre defina um limite razoável de `max_tokens`:\n\n```python\n# ❌ Bad: No limit, could generate thousands of tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Good: Limit response length\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Reasonable limit for a summary\n)\n```\n\n### 3. Otimize Prompts\n\n```python\n# ❌ Verbose prompt (more input tokens)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Concise prompt (fewer tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. Habilite o Caching\n\nAproveite o [caching semântico](/guides/caching):\n\n```python\n# For repeated similar queries, caching provides major savings\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministic = better cache hits\n)\n```\n\n### 5. Agrupe Requisições Semelhantes\n\n```python\n# ❌ Many small requests\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Fewer larger requests\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## Otimização de Desempenho\n\n### 1. Use Streaming para UX\n\nO streaming melhora o desempenho percebido:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. Escolha Modelos Rápidos para Uso Interativo\n\n| Caso de Uso | Recomendado | Latência |\n|----------|-------------|---------|\n| Interface de Chat | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms para o primeiro token |\n| Autocompletar (Tab completion) | `claude-haiku-4-5` | ~150ms para o primeiro token |\n| Processamento em segundo plano | `gpt-4o`, `claude-sonnet-4-5` | ~500ms para o primeiro token |\n\n### 3. Configure Timeouts\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 second timeout\n)\n```\n\n## Confiabilidade\n\n### 1. Implemente Retentativas\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. Trate Erros de Forma Adequada\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Check API key\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Queue for later or use backup\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Server error, retry later\n        schedule_retry(request)\n```\n\n### 3. Use Modelos de Fallback\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## Segurança\n\n### 1. Proteja suas Chaves de API\n\n```python\n# ❌ Never hardcode keys\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Use environment variables\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. Valide a Entrada do Usuário\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Add other validation as needed\n    return True\n```\n\n### 3. Defina Limites para Chaves de API\n\nCrie chaves de API separadas com limites de gastos para:\n- Desenvolvimento/testes\n- Produção\n- Diferentes aplicações\n\n## Monitoramento\n\n### 1. Acompanhe o Uso\n\nVerifique seu painel regularmente para:\n- Uso de tokens por modelo\n- Detalhamento de custos\n- Taxas de acerto de cache (cache hit rates)\n- Taxas de erro\n\n### 2. Registre Métricas Importantes\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. Configure Alertas\n\nConfigure alertas de saldo baixo em seu painel para evitar interrupções no serviço.\n\n## Checklist\n\n<AccordionGroup>\n  <Accordion title=\"Otimização de custos\">\n    - [ ] Usando o modelo apropriado para cada tarefa\n    - [ ] Definindo limites de max_tokens\n    - [ ] Prompts estão concisos\n    - [ ] Caching habilitado onde apropriado\n    - [ ] Agrupamento de requisições semelhantes\n  </Accordion>\n\n  <Accordion title=\"Desempenho\">\n    - [ ] Streaming para UX interativa\n    - [ ] Modelos rápidos para uso em tempo real\n    - [ ] Timeouts configurados\n  </Accordion>\n\n  <Accordion title=\"Confiabilidade\">\n    - [ ] Lógica de retentativa implementada\n    - [ ] Tratamento de erros configurado\n    - [ ] Modelos de fallback configurados\n  </Accordion>\n\n  <Accordion title=\"Segurança\">\n    - [ ] Chaves de API em variáveis de ambiente\n    - [ ] Validação de entrada\n    - [ ] Chaves separadas para dev/prod\n    - [ ] Limites de gastos definidos\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"أفضل الممارسات\"\ndescription: \"حسّن استخدامك لـ LemonData API من حيث التكلفة والأداء والموثوقية\"\n---\n\n## اختيار النموذج\n\nيمكن أن يؤثر اختيار النموذج الصحيح بشكل كبير على التكلفة والجودة.\n\n### توصيات بناءً على المهمة\n\n| المهمة | النماذج الموصى بها | السبب |\n|------|-------------------|-----------|\n| **أسئلة وأجوبة بسيطة** | `gpt-4o-mini`, `gemini-2.5-flash` | سريع، رخيص، وجيد بما يكفي |\n| **الاستنتاج المعقد** | `o3`, `claude-opus-4-5`, `deepseek-r1` | منطق وتخطيط أفضل |\n| **البرمجة** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | محسّن للكود |\n| **الكتابة الإبداعية** | `claude-sonnet-4-5`, `gpt-4o` | جودة نصوص أفضل |\n| **الرؤية/الصور** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | دعم أصلي للرؤية |\n| **سياق طويل** | `gemini-2.5-pro`, `claude-sonnet-4-5` | نوافذ `token` تزيد عن مليون |\n| **حساس للتكلفة** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | أفضل قيمة |\n\n### فئات التكلفة\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## تحسين التكلفة\n\n### 1. استخدم النماذج الأصغر أولاً\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. تعيين max_tokens\n\nقم دائماً بتعيين حد معقول لـ `max_tokens`:\n\n```python\n# ❌ Bad: No limit, could generate thousands of tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Good: Limit response length\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Reasonable limit for a summary\n)\n```\n\n### 3. تحسين الـ Prompts\n\n```python\n# ❌ Verbose prompt (more input tokens)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Concise prompt (fewer tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. تفعيل التخزين المؤقت (Caching)\n\nاستفد من [التخزين المؤقت الدلالي (semantic caching)](/guides/caching):\n\n```python\n# For repeated similar queries, caching provides major savings\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministic = better cache hits\n)\n```\n\n### 5. تجميع الطلبات المتشابهة (Batching)\n\n```python\n# ❌ Many small requests\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Fewer larger requests\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## تحسين الأداء\n\n### 1. استخدم البث (Streaming) لتحسين تجربة المستخدم\n\nيعمل البث على تحسين الأداء الملحوظ:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. اختر نماذج سريعة للاستخدام التفاعلي\n\n| حالة الاستخدام | الموصى به | زمن الاستجابة (Latency) |\n|----------|-------------|---------|\n| واجهة الدردشة | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms لأول `token` |\n| الإكمال التلقائي | `claude-haiku-4-5` | ~150ms لأول `token` |\n| المعالجة في الخلفية | `gpt-4o`, `claude-sonnet-4-5` | ~500ms لأول `token` |\n\n### 3. تعيين المهلات (Timeouts)\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 second timeout\n)\n```\n\n## الموثوقية\n\n### 1. تنفيذ محاولات الإعادة (Retries)\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. التعامل مع الأخطاء بسلاسة\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Check API key\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Queue for later or use backup\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Server error, retry later\n        schedule_retry(request)\n```\n\n### 3. استخدام نماذج احتياطية (Fallback Models)\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## الأمان\n\n### 1. حماية مفاتيح API\n\n```python\n# ❌ Never hardcode keys\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Use environment variables\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. التحقق من مدخلات المستخدم\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Add other validation as needed\n    return True\n```\n\n### 3. تعيين حدود لمفاتيح API\n\nقم بإنشاء مفاتيح `API` منفصلة مع حدود إنفاق لـ:\n- التطوير/الاختبار\n- الإنتاج (Production)\n- التطبيقات المختلفة\n\n## المراقبة\n\n### 1. تتبع الاستخدام\n\nتحقق من لوحة التحكم بانتظام من أجل:\n- استخدام الـ `token` حسب النموذج\n- تفاصيل التكلفة\n- معدلات نجاح التخزين المؤقت (Cache hit rates)\n- معدلات الخطأ\n\n### 2. تسجيل المقاييس الهامة\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. إعداد التنبيهات\n\nقم بتكوين تنبيهات انخفاض الرصيد في لوحة التحكم لتجنب انقطاع الخدمة.\n\n## قائمة التحقق\n\n<AccordionGroup>\n  <Accordion title=\"تحسين التكلفة\">\n    - [ ] استخدام النموذج المناسب لكل مهمة\n    - [ ] تعيين حدود `max_tokens`\n    - [ ] الـ `Prompts` موجزة\n    - [ ] تفعيل التخزين المؤقت حيثما كان ذلك مناسباً\n    - [ ] تجميع الطلبات المتشابهة\n  </Accordion>\n\n  <Accordion title=\"الأداء\">\n    - [ ] استخدام البث لتجربة مستخدم تفاعلية\n    - [ ] استخدام نماذج سريعة للاستخدام في الوقت الفعلي\n    - [ ] تكوين المهلات (Timeouts)\n  </Accordion>\n\n  <Accordion title=\"الموثوقية\">\n    - [ ] تنفيذ منطق محاولات الإعادة\n    - [ ] تفعيل معالجة الأخطاء\n    - [ ] تكوين النماذج الاحتياطية\n  </Accordion>\n\n  <Accordion title=\"الأمان\">\n    - [ ] وضع مفاتيح `API` في متغيرات البيئة\n    - [ ] التحقق من صحة المدخلات\n    - [ ] مفاتيح منفصلة للتطوير والإنتاج\n    - [ ] تعيين حدود الإنفاق\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Thực hành tốt nhất\"\ndescription: \"Tối ưu hóa việc sử dụng LemonData API của bạn về chi phí, hiệu suất và độ tin cậy\"\n---\n\n## Lựa chọn Mô hình\n\nViệc chọn đúng mô hình có thể ảnh hưởng đáng kể đến chi phí và chất lượng.\n\n### Khuyến nghị theo Nhiệm vụ\n\n| Nhiệm vụ | Mô hình khuyến nghị | Lý do |\n|------|-------------------|-----------|\n| **Hỏi đáp đơn giản** | `gpt-4o-mini`, `gemini-2.5-flash` | Nhanh, rẻ, đủ tốt |\n| **Suy luận phức tạp** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Logic và lập kế hoạch tốt hơn |\n| **Lập trình** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Được tối ưu hóa cho mã nguồn |\n| **Viết lách sáng tạo** | `claude-sonnet-4-5`, `gpt-4o` | Chất lượng văn phong tốt hơn |\n| **Thị giác/Hình ảnh** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Hỗ trợ thị giác gốc |\n| **Ngữ cảnh dài** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Cửa sổ ngữ cảnh trên 1M token |\n| **Nhạy cảm về chi phí** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Giá trị tốt nhất |\n\n### Các mức Chi phí\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## Tối ưu hóa Chi phí\n\n### 1. Sử dụng các mô hình nhỏ hơn trước\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Use cheaper models for simple tasks.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Start cheap, escalate if needed\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. Thiết lập max_tokens\n\nLuôn thiết lập một giới hạn `max_tokens` hợp lý:\n\n```python\n# ❌ Bad: No limit, could generate thousands of tokens\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Good: Limit response length\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Reasonable limit for a summary\n)\n```\n\n### 3. Tối ưu hóa Prompt\n\n```python\n# ❌ Verbose prompt (more input tokens)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Concise prompt (fewer tokens)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. Bật Caching\n\nTận dụng [semantic caching](/guides/caching):\n\n```python\n# For repeated similar queries, caching provides major savings\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministic = better cache hits\n)\n```\n\n### 5. Gộp các yêu cầu tương tự (Batching)\n\n```python\n# ❌ Many small requests\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Fewer larger requests\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## Tối ưu hóa Hiệu suất\n\n### 1. Sử dụng Streaming cho UX\n\nStreaming cải thiện hiệu suất cảm nhận:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. Chọn các mô hình nhanh cho mục đích tương tác\n\n| Trường hợp sử dụng | Khuyến nghị | Độ trễ |\n|----------|-------------|---------|\n| Giao diện Chat | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms cho token đầu tiên |\n| Tự động hoàn thành (Tab completion) | `claude-haiku-4-5` | ~150ms cho token đầu tiên |\n| Xử lý nền | `gpt-4o`, `claude-sonnet-4-5` | ~500ms cho token đầu tiên |\n\n### 3. Thiết lập Timeouts\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 second timeout\n)\n```\n\n## Độ tin cậy\n\n### 1. Triển khai cơ chế thử lại (Retries)\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. Xử lý lỗi một cách khéo léo\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Check API key\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Queue for later or use backup\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Server error, retry later\n        schedule_retry(request)\n```\n\n### 3. Sử dụng các mô hình dự phòng (Fallback)\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## Bảo mật\n\n### 1. Bảo vệ API Key\n\n```python\n# ❌ Never hardcode keys\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Use environment variables\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. Xác thực đầu vào của người dùng\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validate user input before sending to API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Add other validation as needed\n    return True\n```\n\n### 3. Thiết lập giới hạn cho API Key\n\nTạo các API key riêng biệt với giới hạn chi tiêu cho:\n- Phát triển/thử nghiệm\n- Sản xuất (Production)\n- Các ứng dụng khác nhau\n\n## Giám sát\n\n### 1. Theo dõi mức độ sử dụng\n\nKiểm tra dashboard của bạn thường xuyên để biết:\n- Mức sử dụng token theo mô hình\n- Phân bổ chi phí\n- Tỷ lệ khớp cache\n- Tỷ lệ lỗi\n\n### 2. Ghi nhật ký (Log) các chỉ số quan trọng\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. Thiết lập cảnh báo\n\nCấu hình cảnh báo số dư thấp trong dashboard của bạn để tránh gián đoạn dịch vụ.\n\n## Danh sách kiểm tra\n\n<AccordionGroup>\n  <Accordion title=\"Tối ưu hóa chi phí\">\n    - [ ] Sử dụng mô hình phù hợp cho từng nhiệm vụ\n    - [ ] Thiết lập giới hạn max_tokens\n    - [ ] Prompt ngắn gọn\n    - [ ] Bật caching ở những nơi phù hợp\n    - [ ] Gộp các yêu cầu tương tự\n  </Accordion>\n\n  <Accordion title=\"Hiệu suất\">\n    - [ ] Streaming cho UX tương tác\n    - [ ] Các mô hình nhanh cho việc sử dụng thời gian thực\n    - [ ] Đã cấu hình timeouts\n  </Accordion>\n\n  <Accordion title=\"Độ tin cậy\">\n    - [ ] Đã triển khai logic thử lại\n    - [ ] Đã thiết lập xử lý lỗi\n    - [ ] Đã cấu hình các mô hình dự phòng\n  </Accordion>\n\n  <Accordion title=\"Bảo mật\">\n    - [ ] API key nằm trong biến môi trường\n    - [ ] Xác thực đầu vào\n    - [ ] Các key riêng biệt cho dev/prod\n    - [ ] Đã thiết lập giới hạn chi tiêu\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Praktik Terbaik\"\ndescription: \"Optimalkan penggunaan LemonData API Anda untuk biaya, performa, dan keandalan\"\n---\n\n## Pemilihan Model\n\nMemilih model yang tepat dapat berdampak signifikan pada biaya dan kualitas.\n\n### Rekomendasi Berdasarkan Tugas\n\n| Tugas | Model yang Direkomendasikan | Alasan |\n|------|-------------------|-----------|\n| **Tanya Jawab Sederhana** | `gpt-4o-mini`, `gemini-2.5-flash` | Cepat, murah, cukup baik |\n| **Penalaran kompleks** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Logika dan perencanaan yang lebih baik |\n| **Pemrograman** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Dioptimalkan untuk kode |\n| **Penulisan kreatif** | `claude-sonnet-4-5`, `gpt-4o` | Kualitas prosa yang lebih baik |\n| **Visi/Gambar** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Dukungan visi bawaan |\n| **Konteks panjang** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Jendela token 1M+ |\n| **Sensitif terhadap biaya** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Nilai terbaik |\n\n### Tingkatan Biaya\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standard: claude-sonnet-4-5, gpt-4o\n$$   Budget:   gpt-4o-mini, gemini-2.5-flash\n$    Economy:  deepseek-v3.2, deepseek-r1\n```\n\n## Optimasi Biaya\n\n### 1. Gunakan Model yang Lebih Kecil Terlebih Dahulu\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Gunakan model yang lebih murah untuk tugas-tugas sederhana.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Mulai dengan yang murah, tingkatkan jika diperlukan\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. Atur max_tokens\n\nSelalu atur batas `max_tokens` yang wajar:\n\n```python\n# ❌ Buruk: Tanpa batas, dapat menghasilkan ribuan token\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ Baik: Batasi panjang respons\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Batas yang wajar untuk sebuah ringkasan\n)\n```\n\n### 3. Optimalkan Prompt\n\n```python\n# ❌ Prompt bertele-tele (lebih banyak token input)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Prompt ringkas (lebih sedikit token)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. Aktifkan Caching\n\nManfaatkan [semantic caching](/guides/caching):\n\n```python\n# Untuk kueri serupa yang berulang, caching memberikan penghematan besar\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministik = hit cache yang lebih baik\n)\n```\n\n### 5. Batch Permintaan Serupa\n\n```python\n# ❌ Banyak permintaan kecil\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Lebih sedikit permintaan yang lebih besar\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## Optimasi Performa\n\n### 1. Gunakan Streaming untuk UX\n\nStreaming meningkatkan persepsi performa:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. Pilih Model Cepat untuk Penggunaan Interaktif\n\n| Kasus Penggunaan | Direkomendasikan | Latensi |\n|----------|-------------|---------|\n| UI Chat | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms token pertama |\n| Penyelesaian tab | `claude-haiku-4-5` | ~150ms token pertama |\n| Pemrosesan latar belakang | `gpt-4o`, `claude-sonnet-4-5` | ~500ms token pertama |\n\n### 3. Atur Timeout\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # Timeout 60 detik\n)\n```\n\n## Keandalan\n\n### 1. Implementasikan Retry\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Terkena rate limit, menunggu {wait} detik...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Batas maksimal percobaan terlampaui\")\n```\n\n### 2. Tangani Error dengan Baik\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # Periksa API key\n    notify_admin(\"API key tidak valid\")\nexcept RateLimitError:\n    # Antrekan untuk nanti atau gunakan cadangan\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Saldo rendah\")\n    elif e.status_code >= 500:\n        # Error server, coba lagi nanti\n        schedule_retry(request)\n```\n\n### 3. Gunakan Model Fallback\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"Semua model gagal\")\n```\n\n## Keamanan\n\n### 1. Lindungi API Key\n\n```python\n# ❌ Jangan pernah melakukan hardcode pada key\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Gunakan variabel lingkungan\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. Validasi Input Pengguna\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"Validasi input pengguna sebelum mengirim ke API.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Pesan terlalu panjang\")\n    # Tambahkan validasi lain sesuai kebutuhan\n    return True\n```\n\n### 3. Atur Batas API Key\n\nBuat API key terpisah dengan batas pengeluaran untuk:\n- Pengembangan/pengujian\n- Produksi\n- Aplikasi yang berbeda\n\n## Pemantauan\n\n### 1. Lacak Penggunaan\n\nPeriksa dasbor Anda secara berkala untuk:\n- Penggunaan token berdasarkan model\n- Rincian biaya\n- Tingkat hit cache\n- Tingkat error\n\n### 2. Catat Metrik Penting\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. Siapkan Peringatan\n\nKonfigurasikan peringatan saldo rendah di dasbor Anda untuk menghindari gangguan layanan.\n\n## Daftar Periksa\n\n<AccordionGroup>\n  <Accordion title=\"Optimasi biaya\">\n    - [ ] Menggunakan model yang sesuai untuk setiap tugas\n    - [ ] Mengatur batas max_tokens\n    - [ ] Prompt ringkas\n    - [ ] Caching diaktifkan jika sesuai\n    - [ ] Melakukan batching pada permintaan serupa\n  </Accordion>\n\n  <Accordion title=\"Performa\">\n    - [ ] Streaming untuk UX interaktif\n    - [ ] Model cepat untuk penggunaan real-time\n    - [ ] Timeout dikonfigurasi\n  </Accordion>\n\n  <Accordion title=\"Keandalan\">\n    - [ ] Logika retry diimplementasikan\n    - [ ] Penanganan error tersedia\n    - [ ] Model fallback dikonfigurasi\n  </Accordion>\n\n  <Accordion title=\"Keamanan\">\n    - [ ] API key dalam variabel lingkungan\n    - [ ] Validasi input\n    - [ ] Key terpisah untuk dev/prod\n    - [ ] Batas pengeluaran diatur\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"En İyi Uygulamalar\"\ndescription: \"Maliyet, performans ve güvenilirlik için LemonData API kullanımınızı optimize edin\"\n---\n\n## Model Seçimi\n\nDoğru modeli seçmek, maliyet ve kaliteyi önemli ölçüde etkileyebilir.\n\n### Görev Bazlı Öneriler\n\n| Görev | Önerilen Modeller | Gerekçe |\n|------|-------------------|-----------|\n| **Basit Soru-Cevap** | `gpt-4o-mini`, `gemini-2.5-flash` | Hızlı, ucuz, yeterince iyi |\n| **Karmaşık akıl yürütme** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Daha iyi mantık ve planlama |\n| **Kodlama** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Kod için optimize edilmiş |\n| **Yaratıcı yazım** | `claude-sonnet-4-5`, `gpt-4o` | Daha iyi düzyazı kalitesi |\n| **Görüntü/Resim** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Yerel görüntü desteği |\n| **Uzun bağlam** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 1M+ token penceresi |\n| **Maliyet duyarlı** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | En iyi değer |\n\n### Maliyet Kademeleri\n\n```\n$$$$ Premium: o3, claude-opus-4-5, gpt-4o\n$$$  Standart: claude-sonnet-4-5, gpt-4o\n$$   Bütçe:   gpt-4o-mini, gemini-2.5-flash\n$    Ekonomi:  deepseek-v3.2, deepseek-r1\n```\n\n## Maliyet Optimizasyonu\n\n### 1. Önce Daha Küçük Modelleri Kullanın\n\n```python\ndef smart_query(question: str, complexity: str = \"auto\"):\n    \"\"\"Basit görevler için daha ucuz modeller kullanın.\"\"\"\n\n    if complexity == \"simple\":\n        model = \"gpt-4o-mini\"\n    elif complexity == \"complex\":\n        model = \"gpt-4o\"\n    else:\n        # Ucuz başlayın, gerekirse yükseltin\n        model = \"gpt-4o-mini\"\n\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    return response\n```\n\n### 2. max_tokens Değerini Ayarlayın\n\nHer zaman makul bir `max_tokens` sınırı belirleyin:\n\n```python\n# ❌ Kötü: Sınır yok, binlerce token oluşturabilir\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}]\n)\n\n# ✅ İyi: Yanıt uzunluğunu sınırla\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this article\"}],\n    max_tokens=500  # Bir özet için makul sınır\n)\n```\n\n### 3. Prompt'ları Optimize Edin\n\n```python\n# ❌ Ayrıntılı prompt (daha fazla giriş token'ı)\nprompt = \"\"\"\nI would like you to please help me by analyzing the following text\nand providing a comprehensive summary of the main points. Please be\nthorough but also concise in your response. The text is as follows:\n{text}\n\"\"\"\n\n# ✅ Kısa ve öz prompt (daha az token)\nprompt = \"Summarize the key points:\\n{text}\"\n```\n\n### 4. Önbelleğe Almayı Etkinleştirin\n\n[Anlamsal önbelleğe alma](/guides/caching) özelliğinden yararlanın:\n\n```python\n# Tekrarlanan benzer sorgular için önbelleğe alma büyük tasarruf sağlar\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n    temperature=0  # Deterministik = daha iyi önbellek isabeti\n)\n```\n\n### 5. Benzer İstekleri Gruplandırın (Batch)\n\n```python\n# ❌ Çok sayıda küçük istek\nfor question in questions:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n\n# ✅ Daha az sayıda büyük istek\ncombined_prompt = \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": f\"Answer each question:\\n{combined_prompt}\"}]\n)\n```\n\n## Performans Optimizasyonu\n\n### 1. Kullanıcı Deneyimi (UX) için Streaming Kullanın\n\nStreaming (akış), algılanan performansı artırır:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a long essay\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n### 2. Etkileşimli Kullanım için Hızlı Modeller Seçin\n\n| Kullanım Durumu | Önerilen | Gecikme |\n|----------|-------------|---------|\n| Sohbet Arayüzü | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms ilk token |\n| Sekme tamamlama | `claude-haiku-4-5` | ~150ms ilk token |\n| Arka plan işleme | `gpt-4o`, `claude-sonnet-4-5` | ~500ms ilk token |\n\n### 3. Zaman Aşımlarını (Timeout) Ayarlayın\n\n```python\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    timeout=60.0  # 60 saniyelik zaman aşımı\n)\n```\n\n## Güvenilirlik\n\n### 1. Yeniden Denemeleri (Retry) Uygulayın\n\n```python\nimport time\nfrom openai import RateLimitError, APIError\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError:\n            wait = 2 ** attempt\n            print(f\"Rate limited, waiting {wait}s...\")\n            time.sleep(wait)\n        except APIError as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(1)\n    raise Exception(\"Max retries exceeded\")\n```\n\n### 2. Hataları Düzgün Bir Şekilde Yönetin\n\n```python\nfrom openai import APIError, AuthenticationError, RateLimitError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError:\n    # API anahtarını kontrol et\n    notify_admin(\"Invalid API key\")\nexcept RateLimitError:\n    # Daha sonrası için sıraya al veya yedek kullan\n    add_to_queue(request)\nexcept APIError as e:\n    if e.status_code == 402:\n        notify_admin(\"Balance low\")\n    elif e.status_code >= 500:\n        # Sunucu hatası, daha sonra tekrar dene\n        schedule_retry(request)\n```\n\n### 3. Yedek (Fallback) Modeller Kullanın\n\n```python\nFALLBACK_CHAIN = [\"gpt-4o\", \"claude-sonnet-4-5\", \"gemini-2.5-flash\"]\n\ndef chat_with_fallback(messages):\n    for model in FALLBACK_CHAIN:\n        try:\n            return client.chat.completions.create(\n                model=model,\n                messages=messages\n            )\n        except APIError:\n            continue\n    raise Exception(\"All models failed\")\n```\n\n## Güvenlik\n\n### 1. API Anahtarlarını Koruyun\n\n```python\n# ❌ Anahtarları asla kodun içine gömmeyin\nclient = OpenAI(api_key=\"sk-abc123...\")\n\n# ✅ Ortam değişkenlerini kullanın\nimport os\nclient = OpenAI(api_key=os.environ[\"LEMONDATA_API_KEY\"])\n```\n\n### 2. Kullanıcı Girişini Doğrulayın\n\n```python\ndef validate_message(content: str) -> bool:\n    \"\"\"API'ye göndermeden önce kullanıcı girişini doğrulayın.\"\"\"\n    if len(content) > 100000:\n        raise ValueError(\"Message too long\")\n    # Gerektiğinde diğer doğrulamaları ekleyin\n    return True\n```\n\n### 3. API Anahtarı Sınırlarını Belirleyin\n\nAşağıdakiler için harcama limitli ayrı API anahtarları oluşturun:\n- Geliştirme/test\n- Üretim (Production)\n- Farklı uygulamalar\n\n## İzleme\n\n### 1. Kullanımı Takip Edin\n\nPanelinizi aşağıdakiler için düzenli olarak kontrol edin:\n- Modele göre token kullanımı\n- Maliyet dökümü\n- Önbellek isabet oranları\n- Hata oranları\n\n### 2. Önemli Metrikleri Günlüğe Kaydedin (Log)\n\n```python\nimport logging\n\nresponse = client.chat.completions.create(...)\n\nlogging.info({\n    \"model\": response.model,\n    \"prompt_tokens\": response.usage.prompt_tokens,\n    \"completion_tokens\": response.usage.completion_tokens,\n    \"total_tokens\": response.usage.total_tokens,\n})\n```\n\n### 3. Uyarılar Oluşturun\n\nHizmet kesintisini önlemek için panelinizde düşük bakiye uyarıları yapılandırın.\n\n## Kontrol Listesi\n\n<AccordionGroup>\n  <Accordion title=\"Maliyet optimizasyonu\">\n    - [ ] Her görev için uygun modelin kullanılması\n    - [ ] max_tokens sınırlarının belirlenmesi\n    - [ ] Prompt'ların kısa ve öz olması\n    - [ ] Uygun yerlerde önbelleğe almanın etkinleştirilmesi\n    - [ ] Benzer isteklerin gruplandırılması\n  </Accordion>\n\n  <Accordion title=\"Performans\">\n    - [ ] Etkileşimli UX için streaming kullanımı\n    - [ ] Gerçek zamanlı kullanım için hızlı modeller\n    - [ ] Zaman aşımlarının yapılandırılması\n  </Accordion>\n\n  <Accordion title=\"Güvenilirlik\">\n    - [ ] Yeniden deneme mantığının uygulanması\n    - [ ] Hata yönetiminin hazır olması\n    - [ ] Yedek modellerin yapılandırılması\n  </Accordion>\n\n  <Accordion title=\"Güvenlik\">\n    - [ ] API anahtarlarının ortam değişkenlerinde tutulması\n    - [ ] Giriş doğrulaması\n    - [ ] Geliştirme/üretim için ayrı anahtarlar\n    - [ ] Harcama limitlerinin belirlenmesi\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:31:02.747Z"
  },
  "guides/billing.mdx": {
    "sourceHash": "48863b377d9bf87e",
    "translations": {
      "zh": "---\ntitle: \"计费与定价\"\ndescription: \"了解 LemonData 的按量计费模式\"\n---\n\n## 概览\n\nLemonData 采用 **按量计费 (pay-as-you-go)** 模式。您只需为您使用的部分付费，无需订阅，也没有最低消费限制。\n\n## 计费方式\n\n1. **充值额度** 到您的账户\n2. **使用 API** - 费用将按请求扣除\n3. **监控使用情况** 在您的控制面板中\n4. **余额不足时充值**\n\n## 定价模型\n\n### 按 Token 计费\n\n大多数文本生成模型按每百万 token 计费：\n\n| 模型 | 输入 (每 1M tokens) | 输出 (每 1M tokens) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  显示的定价为平台折扣价。实际价格可能会有所变动。请查看 [定价页面](https://lemondata.cc/zh/pricing) 获取当前费率。\n</Note>\n\n### 按请求计费\n\n图像、视频和音频模型按请求计费：\n\n| 模型 | 单次请求价格 |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (每分钟) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Token 计数\n\nToken 是文本处理的基本单位：\n\n- 约 4 个字符 = 1 token (英文)\n- 约 1-2 个字符 = 1 token (中文)\n- 1 张图片 = 取决于尺寸和细节\n\n### 估算 Token\n\n```python\n# 粗略估算\ndef estimate_tokens(text):\n    return len(text) / 4  # 英文近似值\n\n# 实际计数 (适用于 OpenAI 模型)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## 使用情况追踪\n\n### 控制面板\n\n在 [控制面板](https://lemondata.cc/dashboard) 中监控您的使用情况：\n\n- 实时余额\n- 按模型分类的使用历史\n- 费用明细\n- API key 使用情况\n\n### API 响应\n\n每个响应都包含使用信息：\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## 成本优化\n\n<AccordionGroup>\n  <Accordion title=\"使用合适的模型\">\n    对于简单任务，使用较小的模型 (GPT-4o-mini, Gemini Flash)。\n  </Accordion>\n\n  <Accordion title=\"实施缓存\">\n    为重复的相同请求缓存响应。\n  </Accordion>\n\n  <Accordion title=\"优化提示词\">\n    在保持清晰的同时，尽量简洁地编写提示词。\n  </Accordion>\n\n  <Accordion title=\"设置 max_tokens\">\n    当不需要完整响应时，限制响应长度。\n  </Accordion>\n\n  <Accordion title=\"对长响应使用流式传输\">\n    流式传输不会产生额外费用，但能提升感知性能。\n  </Accordion>\n</AccordionGroup>\n\n## 余额不足提醒\n\n配置余额下降时的提醒：\n\n1. 前往 **控制面板 → 设置 → 通知**\n2. 设置您的阈值金额\n3. 接收邮件通知\n\n## 充值额度\n\n### 支付方式\n\n- 支付宝 (Alipay)\n- 微信支付 (WeChat Pay)\n- Stripe (Visa, Mastercard)\n\n### 步骤\n\n1. 登录 [控制面板](https://lemondata.cc/dashboard)\n2. 点击 **充值额度**\n3. 选择金额和支付方式\n4. 完成支付\n\n确认付款后，额度将立即到账。\n\n## API Key 限制\n\n您可以为单个 API key 设置支出限制：\n\n1. 前往 **控制面板 → API Keys**\n2. 点击某个 key 进行编辑\n3. 设置 **使用限制**\n\n达到限制后，使用该 key 的请求将返回 `402 Payment Required`。\n\n## 发票\n\n对于企业账户，可以提供发票：\n\n1. 前往 **控制面板 → 计费**\n2. 查看交易历史\n3. 下载 PDF 格式发票\n\n## 有疑问？\n\n如有计费咨询，请联系 support@lemondata.cc。",
      "zh-TW": "---\ntitle: \"帳單與定價\"\ndescription: \"瞭解 LemonData 的按量計費定價\"\n---\n\n## 概覽\n\nLemonData 採用 **按量計費 (pay-as-you-go)**。您只需為使用的部分付費，無需訂閱或最低消費承諾。\n\n## 帳單運作方式\n\n1. **儲值額度** 到您的帳戶\n2. **使用 API** - 費用按請求扣除\n3. **監控用量** 在您的控制面板中\n4. **餘額不足時儲值**\n\n## 定價模式\n\n### 按 Token 計費\n\n大多數文本生成模型按每百萬個 tokens 計費：\n\n| 模型 | 輸入 (每 1M tokens) | 輸出 (每 1M tokens) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  顯示的價格為平台折扣價。實際價格可能有所變動。請查看 [定價頁面](https://lemondata.cc/zh-TW/pricing) 以獲取最新費率。\n</Note>\n\n### 按請求計費\n\n圖像、影片和音訊模型按請求計費：\n\n| 模型 | 每次請求價格 |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (每分鐘) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Token 計數\n\nTokens 是文本處理的基本單位：\n\n- ~4 個字元 = 1 token (英文)\n- ~1-2 個字元 = 1 token (中文)\n- 1 張圖片 = 視尺寸和細節而定\n\n### 估算 Tokens\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)",
      "ja": "---\ntitle: \"請求と料金\"\ndescription: \"LemonDataの従量課金制料金体系について\"\n---\n\n## 概要\n\nLemonDataは**従量課金制（pay-as-you-go pricing）**を採用しています。サブスクリプションや最低利用料金はなく、使用した分だけお支払いいただきます。\n\n## 請求の仕組み\n\n1. アカウントに**クレジットを追加**する\n2. **APIを使用する** - リクエストごとに費用が差し引かれます\n3. ダッシュボードで**使用状況を監視**する\n4. 残高が少なくなったら**チャージ**する\n\n## 料金モデル\n\n### トークン単位の料金\n\nほとんどのテキスト生成モデルは、100万トークンあたりの価格が設定されています。\n\n| モデル | 入力（100万トークンあたり） | 出力（100万トークンあたり） |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  表示されている価格は割引後のプラットフォーム価格です。実際の価格は変動する場合があります。最新の料金については[料金ページ](https://lemondata.cc/ja/pricing)をご確認ください。\n</Note>\n\n### リクエスト単位の料金\n\n画像、動画、音声モデルはリクエストごとに課金されます。\n\n| モデル | リクエストあたりの料金 |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (1分あたり) | $0.006 |\n| TTS-1 | $0.015 |\n\n## トークンのカウント\n\nトークンはテキスト処理の基本単位です。\n\n- 約4文字 = 1トークン（英語）\n- 約1〜2文字 = 1トークン（中国語）\n- 画像1枚 = サイズと詳細度によって異なります\n\n### トークン数の見積もり\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## 使用状況の追跡\n\n### ダッシュボード\n\n[ダッシュボード](https://lemondata.cc/dashboard)で使用状況を確認できます。\n\n- リアルタイムの残高\n- モデル別の使用履歴\n- 費用の内訳\n- APIキーの使用状況\n\n### APIレスポンス\n\n各レスポンスには使用状況の情報が含まれています。\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## コストの最適化\n\n<AccordionGroup>\n  <Accordion title=\"適切なモデルの使用\">\n    単純なタスクには、より小規模なモデル（GPT-4o-mini、Gemini Flash）を使用してください。\n  </Accordion>\n\n  <Accordion title=\"キャッシングの実装\">\n    繰り返される同一のリクエストに対してレスポンスをキャッシュします。\n  </Accordion>\n\n  <Accordion title=\"プロンプトの最適化\">\n    明確さを維持しつつ、プロンプトを簡潔に保ちます。\n  </Accordion>\n\n  <Accordion title=\"max_tokensの設定\">\n    完全なレスポンスが必要ない場合は、レスポンスの長さを制限します。\n  </Accordion>\n\n  <Accordion title=\"長いレスポンスにはストリーミングを使用\">\n    ストリーミングに追加費用はかかりませんが、体感的なパフォーマンスが向上します。\n  </Accordion>\n</AccordionGroup>\n\n## 残高不足のアラート\n\n残高が低下した際のアラートを設定します。\n\n1. **ダッシュボード → 設定 → 通知**に移動します\n2. しきい値を設定します\n3. メール通知を受け取ります\n\n## クレジットの追加\n\n### 支払い方法\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### 手順\n\n1. [ダッシュボード](https://lemondata.cc/dashboard)にログインします\n2. **クレジットを追加**をクリックします\n3. 金額と支払い方法を選択します\n4. 支払いを完了します\n\nクレジットは支払い確認後、即座に追加されます。\n\n## APIキーの制限\n\n個々のAPIキーに使用制限を設定できます。\n\n1. **ダッシュボード → APIキー**に移動します\n2. 編集するキーをクリックします\n3. **使用制限（Usage Limit）**を設定します\n\n制限に達すると、そのキーを使用したリクエストは`402 Payment Required`を返します。\n\n## 請求書\n\nビジネスアカウントでは、請求書を利用できます。\n\n1. **ダッシュボード → 請求**に移動します\n2. 取引履歴を表示します\n3. 請求書をPDFとしてダウンロードします\n\n## ご質問がある場合\n\n請求に関するお問い合わせは support@lemondata.cc までご連絡ください。",
      "ko": "---\ntitle: \"결제 및 요금 정책\"\ndescription: \"LemonData의 종량제 요금 체계를 확인하세요\"\n---\n\n## 개요\n\nLemonData는 **종량제(pay-as-you-go) 요금제**를 사용합니다. 구독료나 최소 약정 없이 사용한 만큼만 비용을 지불하면 됩니다.\n\n## 결제 방식\n\n1. 계정에 **크레딧 추가**\n2. **API 사용** - 요청당 비용이 차감됩니다\n3. 대시보드에서 **사용량 모니터링**\n4. 잔액이 부족할 때 **충전**\n\n## 요금 모델\n\n### 토큰당 요금\n\n대부분의 텍스트 생성 모델은 100만 토큰당 요금이 책정됩니다:\n\n| 모델 | 입력 (100만 토큰당) | 출력 (100만 토큰당) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  표시된 가격은 할인된 플랫폼 가격입니다. 실제 가격은 변동될 수 있습니다. 현재 요율은 [요금 페이지](https://lemondata.cc/ko/pricing)에서 확인하세요.\n</Note>\n\n### 요청당 요금\n\n이미지, 비디오 및 오디오 모델은 요청당 요금이 책정됩니다:\n\n| 모델 | 요청당 가격 |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (분당) | $0.006 |\n| TTS-1 | $0.015 |\n\n## 토큰 계산\n\n토큰은 텍스트 처리의 기본 단위입니다:\n\n- 약 4자 = 1 토큰 (영어)\n- 약 1-2자 = 1 토큰 (중국어)\n- 이미지 1개 = 크기 및 세부 사항에 따라 다름\n\n### 토큰 추정\n\n```python\n# 대략적인 추정\ndef estimate_tokens(text):\n    return len(text) / 4  # 영어 기준 근사치\n\n# 실제 카운트 (OpenAI 모델용)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## 사용량 추적\n\n### 대시보드\n\n[대시보드](https://lemondata.cc/dashboard)에서 사용량을 모니터링하세요:\n\n- 실시간 잔액\n- 모델별 사용 기록\n- 비용 상세 내역\n- API 키 사용량\n\n### API 응답\n\n각 응답에는 사용량 정보가 포함됩니다:\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## 비용 최적화\n\n<AccordionGroup>\n  <Accordion title=\"적절한 모델 사용\">\n    단순한 작업에는 더 작은 모델(GPT-4o-mini, Gemini Flash)을 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"캐싱 구현\">\n    반복되는 동일한 요청에 대해 응답을 캐싱하세요.\n  </Accordion>\n\n  <Accordion title=\"프롬프트 최적화\">\n    명확성을 유지하면서 프롬프트를 간결하게 작성하세요.\n  </Accordion>\n\n  <Accordion title=\"max_tokens 설정\">\n    전체 응답이 필요하지 않은 경우 응답 길이를 제한하세요.\n  </Accordion>\n\n  <Accordion title=\"긴 응답에 스트리밍 사용\">\n    스트리밍은 추가 비용이 들지 않으며 체감 성능을 향상시킵니다.\n  </Accordion>\n</AccordionGroup>\n\n## 잔액 부족 알림\n\n잔액이 떨어졌을 때 알림을 받도록 설정하세요:\n\n1. **대시보드 → 설정 → 알림**으로 이동\n2. 임계값 금액 설정\n3. 이메일 알림 수신\n\n## 크레딧 추가\n\n### 결제 수단\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### 단계\n\n1. [대시보드](https://lemondata.cc/dashboard)에 로그인\n2. **크레딧 추가** 클릭\n3. 금액 및 결제 수단 선택\n4. 결제 완료\n\n결제 확인 후 크레딧이 즉시 추가됩니다.\n\n## API 키 제한\n\n개별 API 키에 지출 제한을 설정할 수 있습니다:\n\n1. **대시보드 → API 키**로 이동\n2. 편집할 키 클릭\n3. **사용량 제한** 설정\n\n제한에 도달하면 해당 키를 사용한 요청은 `402 Payment Required`를 반환합니다.\n\n## 인보이스(송장)\n\n비즈니스 계정의 경우 인보이스를 사용할 수 있습니다:\n\n1. **대시보드 → 결제**로 이동\n2. 거래 내역 보기\n3. 인보이스를 PDF로 다운로드\n\n## 질문이 있으신가요?\n\n결제 관련 문의는 support@lemondata.cc로 연락해 주세요.",
      "de": "---\ntitle: \"Abrechnung & Preise\"\ndescription: \"Verstehen Sie das Pay-as-you-go-Preismodell von LemonData\"\n---\n\n## Übersicht\n\nLemonData nutzt ein **Pay-as-you-go-Preismodell**. Sie zahlen nur für das, was Sie tatsächlich nutzen, ohne Abonnements oder Mindestverpflichtungen.\n\n## So funktioniert die Abrechnung\n\n1. **Guthaben hinzufügen** zu Ihrem Konto\n2. **Die API nutzen** – Kosten werden pro Anfrage abgezogen\n3. **Nutzung überwachen** in Ihrem Dashboard\n4. **Guthaben aufladen**, wenn Ihr Saldo niedrig ist\n\n## Preismodelle\n\n### Preisgestaltung pro Token\n\nDie meisten Textgenerierungsmodelle werden pro Million Token abgerechnet:\n\n| Modell | Input (pro 1 Mio. Token) | Output (pro 1 Mio. Token) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Die angezeigten Preise sind ermäßigte Plattformpreise. Die tatsächlichen Preise können variieren. Aktuelle Tarife finden Sie auf der [Preisseite](https://lemondata.cc/de/pricing).\n</Note>\n\n### Preisgestaltung pro Anfrage\n\nBild-, Video- und Audiomodelle werden pro Anfrage abgerechnet:\n\n| Modell | Preis pro Anfrage |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (pro Minute) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Token-Zählung\n\nToken sind die Basiseinheiten der Textverarbeitung:\n\n- ~4 Zeichen = 1 Token (Englisch)\n- ~1-2 Zeichen = 1 Token (Chinesisch)\n- 1 Bild = variiert je nach Größe und Detailgrad\n\n### Token-Schätzung\n\n```python\n# Rough estimation\ndef estimate_tokens(",
      "fr": "---\ntitle: \"Facturation et Tarification\"\ndescription: \"Comprendre la tarification à l'usage de LemonData\"\n---\n\n## Aperçu\n\nLemonData utilise une **tarification à l'usage (pay-as-you-go)**. Vous ne payez que pour ce que vous utilisez, sans abonnement ni engagement minimum.\n\n## Fonctionnement de la facturation\n\n1. **Ajoutez des crédits** à votre compte\n2. **Utilisez l'API** - les coûts sont déduits par requête\n3. **Suivez votre utilisation** dans votre tableau de bord\n4. **Rechargez** lorsque votre solde est bas\n\n## Modèles de tarification\n\n### Tarification par Token\n\nLa plupart des modèles de génération de texte sont facturés par million de tokens :\n\n| Modèle | Entrée (par 1M de tokens) | Sortie (par 1M de tokens) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Les prix affichés sont des prix de plateforme réduits. Les prix réels peuvent varier. Consultez la [page de tarification](https://lemondata.cc/fr/pricing) pour les tarifs actuels.\n</Note>\n\n### Tarification par requête\n\nLes modèles d'image, de vidéo et d'audio sont facturés par requête :\n\n| Modèle | Prix par requête |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (par minute) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Comptage des Tokens\n\nLes tokens sont les unités de base du traitement de texte :\n\n- ~4 caractères = 1 token (Anglais)\n- ~1-2 caractères = 1 token (Chinois)\n- 1 image = varie selon la taille et les détails\n\n### Estimation des Tokens\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## Suivi de l'utilisation\n\n### Tableau de bord\n\nSuivez votre utilisation dans le [Tableau de bord](https://lemondata.cc/dashboard) :\n\n- Solde en temps réel\n- Historique d'utilisation par modèle\n- Détail des coûts\n- Utilisation des clés API\n\n### Réponse de l'API\n\nChaque réponse inclut des informations sur l'utilisation :\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## Optimisation des coûts\n\n<AccordionGroup>\n  <Accordion title=\"Utiliser des modèles appropriés\">\n    Utilisez des modèles plus petits (GPT-4o-mini, Gemini Flash) pour les tâches simples.\n  </Accordion>\n\n  <Accordion title=\"Implémenter la mise en cache\">\n    Mettez en cache les réponses pour les requêtes identiques répétées.\n  </Accordion>\n\n  <Accordion title=\"Optimiser les prompts\">\n    Gardez des prompts concis tout en maintenant la clarté.\n  </Accordion>\n\n  <Accordion title=\"Définir max_tokens\">\n    Limitez la longueur de la réponse lorsque des réponses complètes ne sont pas nécessaires.\n  </Accordion>\n\n  <Accordion title=\"Utiliser le streaming pour les réponses longues\">\n    Le streaming ne coûte pas plus cher mais améliore la performance perçue.\n  </Accordion>\n</AccordionGroup>\n\n## Alertes de solde faible\n\nConfigurez des alertes lorsque votre solde diminue :\n\n1. Allez dans **Tableau de bord → Paramètres → Notifications**\n2. Définissez votre montant seuil\n3. Recevez des notifications par e-mail\n\n## Ajouter des crédits\n\n### Modes de paiement\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### Étapes\n\n1. Connectez-vous au [Tableau de bord](https://lemondata.cc/dashboard)\n2. Cliquez sur **Ajouter des crédits**\n3. Sélectionnez le montant et le mode de paiement\n4. Finalisez le paiement\n\nLes crédits sont ajoutés instantanément après la confirmation du paiement.\n\n## Limites des clés API\n\nVous pouvez définir des limites de dépenses sur chaque clé API :\n\n1. Allez dans **Tableau de bord → Clés API**\n2. Cliquez sur une clé pour la modifier\n3. Définissez une **Limite d'utilisation**\n\nLorsque la limite est atteinte, les requêtes avec cette clé renverront `402 Payment Required`.\n\n## Factures\n\nPour les comptes professionnels, des factures sont disponibles :\n\n1. Allez dans **Tableau de bord → Facturation**\n2. Consultez l'historique des transactions\n3. Téléchargez les factures au format PDF\n\n## Des questions ?\n\nContactez support@lemondata.cc pour toute question relative à la facturation.",
      "es": "---\ntitle: \"Facturación y Precios\"\ndescription: \"Entienda el modelo de precios de pago por uso de LemonData\"\n---\n\n## Resumen\n\nLemonData utiliza un **modelo de precios de pago por uso**. Solo paga por lo que utiliza, sin suscripciones ni compromisos mínimos.\n\n## Cómo funciona la facturación\n\n1. **Añada créditos** a su cuenta\n2. **Use la API**: los costes se deducen por solicitud\n3. **Supervise el uso** en su panel de control\n4. **Recargue** cuando su saldo sea bajo\n\n## Modelos de precios\n\n### Precios por Token\n\nLa mayoría de los modelos de generación de texto tienen un precio por millón de tokens:\n\n| Modelo | Entrada (por 1M de tokens) | Salida (por 1M de tokens) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Los precios mostrados son precios con descuento de la plataforma. Los precios reales pueden variar. Consulte la [página de precios](https://lemondata.cc/es/pricing) para ver las tarifas actuales.\n</Note>\n\n### Precios por solicitud\n\nLos modelos de imagen, vídeo y audio tienen un precio por solicitud:\n\n| Modelo | Precio por solicitud |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (por minuto) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Conteo de tokens\n\nLos tokens son las unidades básicas del procesamiento de texto:\n\n- ~4 caracteres = 1 token (inglés)\n- ~1-2 caracteres = 1 token (chino)\n- 1 imagen = varía según el tamaño y el detalle\n\n### Estimación de tokens\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## Seguimiento del uso\n\n### Panel de control\n\nSupervise su uso en el [Panel de control](https://lemondata.cc/dashboard):\n\n- Saldo en tiempo real\n- Historial de uso por modelo\n- Desglose de costes\n- Uso de claves API\n\n### Respuesta de la API\n\nCada respuesta incluye información de uso:\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## Optimización de costes\n\n<AccordionGroup>\n  <Accordion title=\"Use modelos adecuados\">\n    Utilice modelos más pequeños (GPT-4o-mini, Gemini Flash) para tareas sencillas.\n  </Accordion>\n\n  <Accordion title=\"Implemente el almacenamiento en caché\">\n    Almacene en caché las respuestas para solicitudes idénticas repetidas.\n  </Accordion>\n\n  <Accordion title=\"Optimice los prompts\">\n    Mantenga los prompts concisos manteniendo la claridad.\n  </Accordion>\n\n  <Accordion title=\"Establezca max_tokens\">\n    Limite la longitud de la respuesta cuando no se necesiten respuestas completas.\n  </Accordion>\n\n  <Accordion title=\"Use streaming para respuestas largas\">\n    El streaming no tiene un coste adicional pero mejora el rendimiento percibido.\n  </Accordion>\n</AccordionGroup>\n\n## Alertas de saldo bajo\n\nConfigure alertas para cuando su saldo disminuya:\n\n1. Vaya a **Panel de control → Configuración → Notificaciones**\n2. Establezca su importe de umbral\n3. Reciba notificaciones por correo electrónico\n\n## Añadir créditos\n\n### Métodos de pago\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### Pasos\n\n1. Inicie sesión en el [Panel de control](https://lemondata.cc/dashboard)\n2. Haga clic en **Añadir créditos**\n3. Seleccione el importe y el método de pago\n4. Complete el pago\n\nLos créditos se añaden instantáneamente tras la confirmación del pago.\n\n## Límites de las claves API\n\nPuede establecer límites de gasto en claves API individuales:\n\n1. Vaya a **Panel de control → Claves API**\n2. Haga clic en una clave para editarla\n3. Establezca el **Límite de uso**\n\nCuando se alcance el límite, las solicitudes con esa clave devolverán `402 Payment Required`.\n\n## Facturas\n\nPara cuentas de empresa, las facturas están disponibles:\n\n1. Vaya a **Panel de control → Facturación**\n2. Vea el historial de transacciones\n3. Descargue las facturas en formato PDF\n\n## ¿Preguntas?\n\nPóngase en contacto con support@lemondata.cc para consultas de facturación.",
      "pt": "---\ntitle: \"Faturamento e Preços\"\ndescription: \"Entenda o modelo de preços pay-as-you-go da LemonData\"\n---\n\n## Visão Geral\n\nA LemonData utiliza o modelo de **preços pay-as-you-go**. Você paga apenas pelo que usar, sem assinaturas ou compromissos mínimos.\n\n## Como Funciona o Faturamento\n\n1. **Adicione créditos** à sua conta\n2. **Use a API** - os custos são deduzidos por requisição\n3. **Monitore o uso** no seu dashboard\n4. **Recarregue** quando seu saldo estiver baixo\n\n## Modelos de Preços\n\n### Preços por Token\n\nA maioria dos modelos de geração de texto é precificada por milhão de tokens:\n\n| Modelo | Entrada (por 1M de tokens) | Saída (por 1M de tokens) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Os preços exibidos são preços com desconto da plataforma. Os preços reais podem variar. Verifique a [página de preços](https://lemondata.cc/pt/pricing) para as taxas atuais.\n</Note>\n\n### Preços por Requisição\n\nModelos de imagem, vídeo e áudio são precificados por requisição:\n\n| Modelo | Preço por Requisição |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (por minuto) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Contagem de Tokens\n\nTokens são as unidades básicas do processamento de texto:\n\n- ~4 caracteres = 1 token (Inglês)\n- ~1-2 caracteres = 1 token (Chinês)\n- 1 imagem = varia conforme o tamanho e detalhes\n\n### Estimando Tokens\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## Acompanhamento de Uso\n\n### Dashboard\n\nMonitore seu uso no [Dashboard](https://lemondata.cc/dashboard):\n\n- Saldo em tempo real\n- Histórico de uso por modelo\n- Detalhamento de custos\n- Uso por chave de API\n\n### API Response\n\nCada resposta inclui informações de uso:\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## Otimização de Custos\n\n<AccordionGroup>\n  <Accordion title=\"Use modelos apropriados\">\n    Use modelos menores (GPT-4o-mini, Gemini Flash) para tarefas simples.\n  </Accordion>\n\n  <Accordion title=\"Implemente cache\">\n    Faça o cache de respostas para requisições idênticas repetidas.\n  </Accordion>\n\n  <Accordion title=\"Otimize prompts\">\n    Mantenha os prompts concisos, mantendo a clareza.\n  </Accordion>\n\n  <Accordion title=\"Defina max_tokens\">\n    Limite o comprimento da resposta quando respostas completas não forem necessárias.\n  </Accordion>\n\n  <Accordion title=\"Use streaming para respostas longas\">\n    O streaming não custa extra, mas melhora a percepção de desempenho.\n  </Accordion>\n</AccordionGroup>\n\n## Alertas de Saldo Baixo\n\nConfigure alertas para quando seu saldo cair:\n\n1. Vá para **Dashboard → Configurações → Notificações**\n2. Defina o valor do seu limite\n3. Receba notificações por e-mail\n\n## Adicionando Créditos\n\n### Métodos de Pagamento\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### Passos\n\n1. Faça login no [Dashboard](https://lemondata.cc/dashboard)\n2. Clique em **Adicionar Créditos**\n3. Selecione o valor e o método de pagamento\n4. Conclua o pagamento\n\nOs créditos são adicionados instantaneamente após a confirmação do pagamento.\n\n## Limites de Chave de API\n\nVocê pode definir limites de gastos em chaves de API individuais:\n\n1. Vá para **Dashboard → Chaves de API**\n2. Clique em uma chave para editar\n3. Defina o **Limite de Uso**\n\nQuando o limite for atingido, as requisições com essa chave retornarão `402 Payment Required`.\n\n## Faturas\n\nPara contas empresariais, as faturas estão disponíveis:\n\n1. Vá para **Dashboard → Faturamento**\n2. Visualize o histórico de transações\n3. Baixe as faturas em PDF\n\n## Dúvidas?\n\nEntre em contato com support@lemondata.cc para consultas de faturamento.",
      "ar": "---\ntitle: \"الفواتير والتسعير\"\ndescription: \"تعرف على نظام تسعير الدفع حسب الاستخدام في LemonData\"\n---\n\n## نظرة عامة\n\nتستخدم LemonData **نظام تسعير الدفع حسب الاستخدام**. أنت تدفع فقط مقابل ما تستخدمه، دون اشتراكات أو التزامات حد أدنى.\n\n## كيف تعمل الفواتير\n\n1. **أضف رصيداً** إلى حسابك\n2. **استخدم الـ API** - يتم خصم التكاليف لكل طلب\n3. **راقب الاستخدام** في لوحة التحكم الخاصة بك\n4. **أعد الشحن** عندما ينخفض رصيدك\n\n## نماذج التسعير\n\n### التسعير لكل Token\n\nيتم تسعير معظم نماذج توليد النصوص لكل مليون token:\n\n| النموذج | المدخلات (لكل 1 مليون token) | المخرجات (لكل 1 مليون token) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  الأسعار المعروضة هي أسعار المنصة المخفضة. قد تختلف الأسعار الفعلية. تحقق من [صفحة التسعير](https://lemondata.cc/ar/pricing) لمعرفة الأسعار الحالية.\n</Note>\n\n### التسعير لكل طلب\n\nيتم تسعير نماذج الصور والفيديو والصوت لكل طلب:\n\n| النموذج | السعر لكل طلب |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (لكل دقيقة) | $0.006 |\n| TTS-1 | $0.015 |\n\n## حساب الـ Tokens\n\nالـ Tokens هي الوحدات الأساسية لمعالجة النصوص:\n\n- ~4 أحرف = 1 token (الإنجليزية)\n- ~1-2 حرف = 1 token (الصينية)\n- صورة واحدة = تختلف حسب الحجم والتفاصيل\n\n### تقدير الـ Tokens\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## تتبع الاستخدام\n\n### لوحة التحكم\n\nراقب استخدامك في [لوحة التحكم](https://lemondata.cc/dashboard):\n\n- الرصيد في الوقت الفعلي\n- سجل الاستخدام حسب النموذج\n- تفاصيل التكاليف\n- استخدام مفتاح الـ API\n\n### استجابة الـ API\n\nتتضمن كل استجابة معلومات الاستخدام:\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## تحسين التكلفة\n\n<AccordionGroup>\n  <Accordion title=\"استخدم النماذج المناسبة\">\n    استخدم نماذج أصغر (GPT-4o-mini، Gemini Flash) للمهام البسيطة.\n  </Accordion>\n\n  <Accordion title=\"تنفيذ التخزين المؤقت (Caching)\">\n    قم بتخزين الاستجابات مؤقتاً للطلبات المتطابقة المتكررة.\n  </Accordion>\n\n  <Accordion title=\"تحسين الأوامر (Prompts)\">\n    اجعل الأوامر موجزة مع الحفاظ على الوضوح.\n  </Accordion>\n\n  <Accordion title=\"تعيين max_tokens\">\n    حدد طول الاستجابة عندما لا تكون هناك حاجة لاستجابات كاملة.\n  </Accordion>\n\n  <Accordion title=\"استخدم البث (Streaming) للاستجابات الطويلة\">\n    البث لا يكلف مبالغ إضافية ولكنه يحسن الأداء الملحوظ.\n  </Accordion>\n</AccordionGroup>\n\n## تنبيهات انخفاض الرصيد\n\nقم بتكوين التنبيهات عندما ينخفض رصيدك:\n\n1. انتقل إلى **لوحة التحكم ← الإعدادات ← الإشعارات**\n2. حدد مبلغ الحد الأدنى الخاص بك\n3. تلقي إشعارات عبر البريد الإلكتروني\n\n## إضافة رصيد\n\n### طرق الدفع\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### الخطوات\n\n1. قم بتسجيل الدخول إلى [لوحة التحكم](https://lemondata.cc/dashboard)\n2. انقر على **إضافة رصيد**\n3. اختر المبلغ وطريقة الدفع\n4. أكمل عملية الدفع\n\nيتم إضافة الرصيد فوراً بعد تأكيد الدفع.\n\n## حدود مفتاح الـ API\n\nيمكنك تعيين حدود للإنفاق على مفاتيح الـ API الفردية:\n\n1. انتقل إلى **لوحة التحكم ← مفاتيح الـ API**\n2. انقر على مفتاح للتعديل\n3. حدد **حد الاستخدام**\n\nعند الوصول إلى الحد الأقصى، ستُرجع الطلبات باستخدام هذا المفتاح خطأ `402 Payment Required`.\n\n## الفواتير\n\nبالنسبة لحسابات الشركات، تتوفر الفواتير:\n\n1. انتقل إلى **لوحة التحكم ← الفواتير**\n2. عرض سجل المعاملات\n3. تحميل الفواتير بصيغة PDF\n\n## أسئلة؟\n\nتواصل مع support@lemondata.cc للاستفسارات المتعلقة بالفواتير.",
      "vi": "---\ntitle: \"Thanh toán & Giá cả\"\ndescription: \"Tìm hiểu mô hình giá pay-as-you-go của LemonData\"\n---\n\n## Tổng quan\n\nLemonData sử dụng **mô hình giá pay-as-you-go**. Bạn chỉ trả tiền cho những gì bạn sử dụng, không có phí đăng ký định kỳ hoặc cam kết tối thiểu.\n\n## Cách thức thanh toán hoạt động\n\n1. **Nạp credit** vào tài khoản của bạn\n2. **Sử dụng API** - chi phí được khấu trừ trên mỗi yêu cầu\n3. **Theo dõi mức sử dụng** trong dashboard của bạn\n4. **Nạp thêm tiền** khi số dư của bạn ở mức thấp\n\n## Các mô hình giá\n\n### Giá theo Token\n\nHầu hết các mô hình tạo văn bản được tính giá trên mỗi triệu token:\n\n| Model | Input (trên 1 triệu token) | Output (trên 1 triệu token) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Giá hiển thị là giá ưu đãi của nền tảng. Giá thực tế có thể thay đổi. Kiểm tra [trang giá cả](https://lemondata.cc/vi/pricing) để biết tỷ giá hiện tại.\n</Note>\n\n### Giá theo yêu cầu\n\nCác mô hình hình ảnh, video và âm thanh được tính giá trên mỗi yêu cầu:\n\n| Model | Giá mỗi yêu cầu |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (mỗi phút) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Cách tính Token\n\nToken là các đơn vị cơ bản của quá trình xử lý văn bản:\n\n- ~4 ký tự = 1 token (Tiếng Anh)\n- ~1-2 ký tự = 1 token (Tiếng Trung)\n- 1 hình ảnh = thay đổi tùy theo kích thước và độ chi tiết\n\n### Ước tính Token\n\n```python\n# Ước tính sơ bộ\ndef estimate_tokens(text):\n    return len(text) / 4  # Xấp xỉ cho tiếng Anh\n\n# Số lượng thực tế (cho các mô hình OpenAI)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## Theo dõi mức sử dụng\n\n### Dashboard\n\nTheo dõi mức sử dụng của bạn trong [Dashboard](https://lemondata.cc/dashboard):\n\n- Số dư thời gian thực\n- Lịch sử sử dụng theo mô hình\n- Phân bổ chi phí\n- Mức sử dụng API key\n\n### Phản hồi API\n\nMỗi phản hồi bao gồm thông tin sử dụng:\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## Tối ưu hóa chi phí\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng các mô hình phù hợp\">\n    Sử dụng các mô hình nhỏ hơn (GPT-4o-mini, Gemini Flash) cho các tác vụ đơn giản.\n  </Accordion>\n\n  <Accordion title=\"Triển khai bộ nhớ đệm (caching)\">\n    Lưu phản hồi vào bộ nhớ đệm cho các yêu cầu giống hệt nhau được lặp lại.\n  </Accordion>\n\n  <Accordion title=\"Tối ưu hóa prompt\">\n    Giữ cho các prompt ngắn gọn trong khi vẫn duy trì sự rõ ràng.\n  </Accordion>\n\n  <Accordion title=\"Thiết lập max_tokens\">\n    Giới hạn độ dài phản hồi khi không cần phản hồi đầy đủ.\n  </Accordion>\n\n  <Accordion title=\"Sử dụng streaming cho các phản hồi dài\">\n    Streaming không tốn thêm phí nhưng cải thiện hiệu suất cảm nhận được.\n  </Accordion>\n</AccordionGroup>\n\n## Cảnh báo số dư thấp\n\nCấu hình cảnh báo khi số dư của bạn giảm xuống:\n\n1. Đi tới **Dashboard → Settings → Notifications**\n2. Thiết lập số tiền ngưỡng của bạn\n3. Nhận thông báo qua email\n\n## Nạp Credit\n\n### Phương thức thanh toán\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### Các bước thực hiện\n\n1. Đăng nhập vào [Dashboard](https://lemondata.cc/dashboard)\n2. Nhấp vào **Add Credits**\n3. Chọn số tiền và phương thức thanh toán\n4. Hoàn tất thanh toán\n\nCredit được cộng ngay lập tức sau khi xác nhận thanh toán.\n\n## Giới hạn API Key\n\nBạn có thể thiết lập giới hạn chi tiêu cho từng API key riêng lẻ:\n\n1. Đi tới **Dashboard → API Keys**\n2. Nhấp vào một key để chỉnh sửa\n3. Thiết lập **Usage Limit**\n\nKhi đạt đến giới hạn, các yêu cầu với key đó sẽ trả về lỗi `402 Payment Required`.\n\n## Hóa đơn\n\nĐối với tài khoản doanh nghiệp, hóa đơn luôn có sẵn:\n\n1. Đi tới **Dashboard → Billing**\n2. Xem lịch sử giao dịch\n3. Tải xuống hóa đơn dưới dạng PDF\n\n## Câu hỏi?\n\nLiên hệ support@lemondata.cc nếu có thắc mắc về thanh toán.",
      "id": "---\ntitle: \"Penagihan & Harga\"\ndescription: \"Pahami harga pay-as-you-go dari LemonData\"\n---\n\n## Ringkasan\n\nLemonData menggunakan **harga pay-as-you-go**. Anda hanya membayar untuk apa yang Anda gunakan, tanpa langganan atau komitmen minimum.\n\n## Cara Kerja Penagihan\n\n1. **Tambahkan kredit** ke akun Anda\n2. **Gunakan API** - biaya dipotong per permintaan\n3. **Pantau penggunaan** di dashboard Anda\n4. **Isi ulang** saat saldo Anda rendah\n\n## Model Harga\n\n### Harga Per-Token\n\nSebagian besar model pembuatan teks dikenakan harga per juta token:\n\n| Model | Input (per 1M token) | Output (per 1M token) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Harga yang ditampilkan adalah harga platform yang didiskon. Harga aktual mungkin bervariasi. Periksa [halaman harga](https://lemondata.cc/id/pricing) untuk tarif saat ini.\n</Note>\n\n### Harga Per-Permintaan\n\nModel gambar, video, dan audio dikenakan harga per permintaan:\n\n| Model | Harga per Permintaan |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (per menit) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Penghitungan Token\n\nToken adalah unit dasar pemrosesan teks:\n\n- ~4 karakter = 1 token (Inggris)\n- ~1-2 karakter = 1 token (Mandarin)\n- 1 gambar = bervariasi berdasarkan ukuran dan detail\n\n### Mengestimasi Token\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder",
      "tr": "---\ntitle: \"Faturalandırma ve Fiyatlandırma\"\ndescription: \"LemonData'nın kullandıkça öde fiyatlandırmasını anlayın\"\n---\n\n## Genel Bakış\n\nLemonData **kullandıkça öde fiyatlandırması** kullanır. Abonelik veya minimum taahhüt olmadan yalnızca kullandığınız kadar ödersiniz.\n\n## Faturalandırma Nasıl Çalışır\n\n1. **Hesabınıza kredi ekleyin**\n2. **API'yi kullanın** - maliyetler istek başına düşülür\n3. **Panelinizden kullanımı izleyin**\n4. **Bakiyeniz azaldığında yükleme yapın**\n\n## Fiyatlandırma Modelleri\n\n### Token Başına Fiyatlandırma\n\nÇoğu metin oluşturma modeli, milyon token başına fiyatlandırılır:\n\n| Model | Girdi (1M token başına) | Çıktı (1M token başına) |\n|-------|----------------------|------------------------|\n| GPT-4o | $1.75 | $7.00 |\n| GPT-4o-mini | $0.105 | $0.42 |\n| Claude Opus 4.5 | $3.50 | $17.50 |\n| Claude Sonnet 4.5 | $2.10 | $10.50 |\n| Claude Haiku 4.5 | $0.70 | $3.50 |\n| Gemini 2.5 Pro | $0.875 | $7.00 |\n| Gemini 2.5 Flash | $0.1225 | $0.525 |\n| DeepSeek R1 | $0.385 | $1.533 |\n| DeepSeek V3.2 | $0.196 | $0.294 |\n\n<Note>\n  Gösterilen fiyatlar indirimli platform fiyatlarıdır. Gerçek fiyatlar değişiklik gösterebilir. Güncel oranlar için [fiyatlandırma sayfasını](https://lemondata.cc/tr/pricing) kontrol edin.\n</Note>\n\n### İstek Başına Fiyatlandırma\n\nGörüntü, video ve ses modelleri istek başına fiyatlandırılır:\n\n| Model | İstek Başına Fiyat |\n|-------|-------------------|\n| DALL-E 3 (1024x1024) | $0.04 |\n| Midjourney | $0.05 |\n| Sora Video (5s) | $0.20 |\n| Whisper (dakika başına) | $0.006 |\n| TTS-1 | $0.015 |\n\n## Token Sayımı\n\nTokenlar, metin işlemenin temel birimleridir:\n\n- ~4 karakter = 1 token (İngilizce)\n- ~1-2 karakter = 1 token (Çince)\n- 1 görüntü = boyut ve detaya göre değişir\n\n### Token Tahmini\n\n```python\n# Rough estimation\ndef estimate_tokens(text):\n    return len(text) / 4  # Approximate for English\n\n# Actual count (for OpenAI models)\nimport tiktoken\nencoder = tiktoken.encoding_for_model(\"gpt-4o\")\ntokens = encoder.encode(\"Your text here\")\nprint(f\"Token count: {len(tokens)}\")\n```\n\n## Kullanım Takibi\n\n### Panel\n\nKullanımınızı [Panel](https://lemondata.cc/dashboard) üzerinden izleyin:\n\n- Gerçek zamanlı bakiye\n- Modele göre kullanım geçmişi\n- Maliyet dökümü\n- API anahtarı kullanımı\n\n### API Yanıtı\n\nHer yanıt kullanım bilgilerini içerir:\n\n```json\n{\n  \"usage\": {\n    \"prompt_tokens\": 50,\n    \"completion_tokens\": 100,\n    \"total_tokens\": 150\n  }\n}\n```\n\n## Maliyet Optimizasyonu\n\n<AccordionGroup>\n  <Accordion title=\"Uygun modelleri kullanın\">\n    Basit görevler için daha küçük modeller (GPT-4o-mini, Gemini Flash) kullanın.\n  </Accordion>\n\n  <Accordion title=\"Önbelleğe almayı uygulayın\">\n    Tekrarlanan aynı istekler için yanıtları önbelleğe alın.\n  </Accordion>\n\n  <Accordion title=\"Promptları optimize edin\">\n    Netliği korurken promptları kısa tutun.\n  </Accordion>\n\n  <Accordion title=\"max_tokens değerini ayarlayın\">\n    Tam yanıtlar gerekmediğinde yanıt uzunluğunu sınırlayın.\n  </Accordion>\n\n  <Accordion title=\"Uzun yanıtlar için akış (streaming) kullanın\">\n    Akış (streaming) ekstra maliyet getirmez ancak algılanan performansı artırır.\n  </Accordion>\n</AccordionGroup>\n\n## Düşük Bakiye Uyarıları\n\nBakiyeniz düştüğünde uyarıları yapılandırın:\n\n1. **Panel → Ayarlar → Bildirimler** bölümüne gidin\n2. Eşik tutarınızı belirleyin\n3. E-posta bildirimleri alın\n\n## Kredi Ekleme\n\n### Ödeme Yöntemleri\n\n- Alipay (支付宝)\n- WeChat Pay (微信支付)\n- Stripe (Visa, Mastercard)\n\n### Adımlar\n\n1. [Panel](https://lemondata.cc/dashboard) hesabınıza giriş yapın\n2. **Kredi Ekle** butonuna tıklayın\n3. Tutarı ve ödeme yöntemini seçin\n4. Ödemeyi tamamlayın\n\nKrediler, ödeme onayından sonra anında eklenir.\n\n## API Anahtarı Limitleri\n\nBireysel API anahtarları üzerinde harcama limitleri belirleyebilirsiniz:\n\n1. **Panel → API Anahtarları** bölümüne gidin\n2. Düzenlemek için bir anahtara tıklayın\n3. **Kullanım Limiti** belirleyin\n\nLimite ulaşıldığında, o anahtarla yapılan istekler `402 Payment Required` hatası döndürecektir.\n\n## Faturalar\n\nKurumsal hesaplar için faturalar mevcuttur:\n\n1. **Panel → Faturalandırma** bölümüne gidin\n2. İşlem geçmişini görüntüleyin\n3. Faturaları PDF olarak indirin\n\n## Sorularınız mı var?\n\nFaturalandırma sorularınız için support@lemondata.cc ile iletişime geçin."
    },
    "updatedAt": "2026-01-26T05:31:28.698Z"
  },
  "guides/caching.mdx": {
    "sourceHash": "5e9ed9c3e5a1986a",
    "translations": {
      "zh": "---\ntitle: \"✨ 智能缓存\"\ndescription: \"通过上下文感知的语义缓存降低成本和延迟\"\n---\n\n## 概览\n\nLemonData 提供了一个智能缓存系统，可以显著降低您的 API 成本和响应延迟。我们的缓存不仅限于简单的请求匹配——它还能理解 Prompt 的**语义含义**。\n\n<CardGroup cols={2}>\n  <Card title=\"节省成本\" icon=\"piggy-bank\">\n    缓存命中仅按正常成本的一小部分计费。\n  </Card>\n  <Card title=\"更快的响应\" icon=\"bolt\">\n    缓存的响应会立即返回，无需模型推理。\n  </Card>\n  <Card title=\"上下文感知\" icon=\"brain\">\n    语义匹配即使在措辞不同的情况下也能找到相似的请求。\n  </Card>\n  <Card title=\"隐私控制\" icon=\"shield\">\n    完全控制缓存和共享的内容。\n  </Card>\n</CardGroup>\n\n## 工作原理\n\nLemonData 使用两层缓存系统：\n\n### 第一层：响应缓存（精确匹配）\n\n对于确定性请求（`temperature=0`），我们缓存精确的响应：\n\n- **匹配**：相同的 `model`、`messages` 和参数\n- **速度**：瞬时（微秒级）\n- **最适用于**：重复的相同查询\n\n### 第二层：语义缓存（相似度匹配）\n\n对于所有请求，我们还使用两阶段匹配算法检查语义相似度：\n\n- **第一阶段（仅查询）**：用户查询相似度 ≥95%\n- **第二阶段（全上下文）**：包含对话上下文的相似度 ≥85%\n- **最适用于**：FAQ 风格的查询、常见问题\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## 缓存 Header\n\n### 请求 Header\n\n按请求控制缓存行为：\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| Header | 值 | 效果 |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | 跳过缓存，获取新鲜响应 |\n| `Cache-Control: no-store` | - | 不缓存此响应 |\n\n### 响应 Header\n\n每个响应都包含缓存状态：\n\n```\nX-Cache: HIT           # Response served from cache\nX-Cache: MISS          # Fresh response from model\nX-Cache-Entry-Id: abc  # Cache entry ID (for feedback)\n```\n\n## 检查缓存状态\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## 缓存计费\n\n缓存命中的价格显著低于新鲜请求：\n\n| 类型 | 成本 |\n|------|------|\n| 缓存 HIT | **2折 (80% off)** |\n| 缓存 MISS | 全价 |\n\n具体的折扣显示在您的仪表板使用日志中。\n\n## 隐私控制\n\n### API Key 级别\n\n在仪表板中为每个 API Key 配置缓存行为：\n\n| 模式 | 描述 |\n|------|-------------|\n| **Default** | 启用缓存，可能与相似请求共享 |\n| **No Share** | 启用缓存，但响应对您的账户私有 |\n| **Disabled** | 完全不缓存 |\n\n### 请求级别\n\n按请求覆盖：\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## 缓存反馈\n\n如果您收到错误的缓存响应，可以进行报告：\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**反馈类型：**\n- `wrong_answer` - 事实错误\n- `outdated` - 信息陈旧\n- `irrelevant` - 与问题不匹配\n- `other` - 其他问题\n\n当一个缓存条目收到足够的负面反馈时，它会自动失效。\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"对可缓存的查询使用 temperature=0\">\n    确定性设置可最大化缓存命中率。\n  </Accordion>\n\n  <Accordion title=\"标准化 Prompt 格式\">\n    一致的格式可以提高语义匹配效果。\n  </Accordion>\n\n  <Accordion title=\"对时间敏感的查询使用 no-cache\">\n    时事、实时数据应跳过缓存。\n  </Accordion>\n\n  <Accordion title=\"监控缓存命中率\">\n    在仪表板中查看缓存统计信息和节省情况。\n  </Accordion>\n</AccordionGroup>\n\n## 何时不应使用缓存\n\n为以下情况禁用缓存：\n\n- **实时信息**：股票价格、天气、新闻\n- **个性化内容**：针对特定用户的推荐\n- **创意任务**：当需要多样性时\n- **敏感数据**：机密信息\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "zh-TW": "---\ntitle: \"✨ 智慧快取\"\ndescription: \"透過上下文感知的語義快取降低成本與延遲\"\n---\n\n## 總覽\n\nLemonData 提供智慧快取系統，可顯著降低您的 API 成本與回應延遲。我們的快取不僅僅是簡單的請求匹配，它還能理解您提示詞（prompts）的**語義（semantic meaning）**。\n\n<CardGroup cols={2}>\n  <Card title=\"節省成本\" icon=\"piggy-bank\">\n    快取命中（Cache hits）僅按正常成本的一小部分計費。\n  </Card>\n  <Card title=\"更快速的回應\" icon=\"bolt\">\n    快取的回應會立即回傳，無需進行模型推論。\n  </Card>\n  <Card title=\"上下文感知\" icon=\"brain\">\n    語義匹配即使在措辭不同的情況下也能找到相似的請求。\n  </Card>\n  <Card title=\"隱私控制\" icon=\"shield\">\n    完全控制快取與分享的內容。\n  </Card>\n</CardGroup>\n\n## 運作原理\n\nLemonData 使用雙層快取系統：\n\n### 第一層：回應快取（精確匹配）\n\n對於確定性請求（`temperature=0`），我們快取精確的回應：\n\n- **匹配條件**：相同的模型、訊息與參數\n- **速度**：即時（微秒級）\n- **適用於**：重複的相同查詢\n\n### 第二層：語義快取（相似度匹配）\n\n對於所有請求，我們還會使用兩階段匹配演算法檢查語義相似度：\n\n- **第一階段（僅查詢）**：使用者查詢相似度 ≥95%\n- **第二階段（完整上下文）**：包含對話上下文的相似度 ≥85%\n- **適用於**：FAQ 類型的查詢、常見問題\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## 快取標頭\n\n### 請求標頭\n\n控制每個請求的快取行為：\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| 標頭 | 數值 | 效果 |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | 跳過快取，獲取全新回應 |\n| `Cache-Control: no-store` | - | 不要快取此回應 |\n\n### 回應標頭\n\n每個回應都包含快取狀態：\n\n```\nX-Cache: HIT           # 回應來自快取\nX-Cache: MISS          # 來自模型的全新回應\nX-Cache-Entry-Id: abc  # 快取項目 ID（用於回饋）\n```\n\n## 檢查快取狀態\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## 快取計費\n\n快取命中的費用顯著低於全新請求：\n\n| 類型 | 成本 |\n|------|------|\n| 快取命中 (HIT) | **2 折 (80% off)** |\n| 快取未命中 (MISS) | 原價 |\n\n確切的折扣顯示在您的儀表板使用日誌中。\n\n## 隱私控制\n\n### API Key 層級\n\n在儀表板中為每個 API Key 設定快取行為：\n\n| 模式 | 描述 |\n|------|-------------|\n| **預設** | 啟用快取，可能與相似請求共享 |\n| **不共享** | 啟用快取，但回應僅限您的帳戶私有 |\n| **已停用** | 完全不使用快取 |\n\n### 請求層級\n\n覆蓋單個請求的設定：\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## 快取回饋\n\n如果您收到錯誤的快取回應，可以進行回報：\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**回饋類型：**\n- `wrong_answer` - 事實錯誤\n- `outdated` - 資訊已過時\n- `irrelevant` - 與問題不符\n- `other` - 其他問題\n\n當快取項目收到足夠的負面回饋時，它將自動失效。\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"對可快取的查詢使用 temperature=0\">\n    確定性設定可最大化快取命中率。\n  </Accordion>\n\n  <Accordion title=\"標準化提示詞格式\">\n    一致的格式化可改善語義匹配。\n  </Accordion>\n\n  <Accordion title=\"對時效性查詢使用 no-cache\">\n    時事、即時數據應跳過快取。\n  </Accordion>\n\n  <Accordion title=\"監控快取命中率\">\n    在儀表板中查看快取統計數據與節省金額。\n  </Accordion>\n</AccordionGroup>\n\n## 何時不應使用快取\n\n針對以下情況停用快取：\n\n- **即時資訊**：股票價格、天氣、新聞\n- **個人化內容**：針對特定使用者的推薦\n- **創意任務**：當需要多樣性時\n- **敏感數據**：機密資訊\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "ja": "---\ntitle: \"✨ インテリジェント・キャッシング\"\ndescription: \"コンテキストを考慮したセマンティック・キャッシングにより、コストとレイテンシを削減します\"\n---\n\n## 概要\n\nLemonDataは、APIコストとレスポンスのレイテンシを大幅に削減できるインテリジェント・キャッシング・システムを提供します。当社のキャッシングは単なるリクエストのマッチングにとどまらず、プロンプトの**セマンティックな意味（意味論的な意味）**を理解します。\n\n<CardGroup cols={2}>\n  <Card title=\"コスト削減\" icon=\"piggy-bank\">\n    キャッシュヒット時は、通常のコストの数分の一で請求されます。\n  </Card>\n  <Card title=\"レスポンスの高速化\" icon=\"bolt\">\n    キャッシュされたレスポンスは即座に返され、モデルの推論は不要です。\n  </Card>\n  <Card title=\"コンテキスト対応\" icon=\"brain\">\n    セマンティック・マッチングにより、言い回しが異なる場合でも類似のリクエストを特定します。\n  </Card>\n  <Card title=\"プライバシー・コントロール\" icon=\"shield\">\n    何をキャッシュし、共有するかを完全に制御できます。\n  </Card>\n</CardGroup>\n\n## 仕組み\n\nLemonDataは2層のキャッシング・システムを使用しています：\n\n### レイヤー1：レスポンス・キャッシュ（完全一致）\n\n決定論的なリクエスト（`temperature=0`）の場合、正確なレスポンスをキャッシュします：\n\n- **一致条件**：同一のモデル、メッセージ、およびパラメータ\n- **速度**：即時（マイクロ秒単位）\n- **最適な用途**：繰り返される同一のクエリ\n\n### レイヤー2：セマンティック・キャッシュ（類似性一致）\n\nすべてのリクエストに対して、2段階のマッチング・アルゴリズムを使用してセマンティックな類似性もチェックします：\n\n- **ステージ1（クエリのみ）**：ユーザー・クエリで95%以上の類似性\n- **ステージ2（フル・コンテキスト）**：会話のコンテキストを含めて85%以上の類似性\n- **最適な用途**：FAQ形式のクエリ、よくある質問\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ 同じキャッシュレスポンス（高いセマンティック類似性）\n```\n\n## キャッシュ・ヘッダー\n\n### リクエスト・ヘッダー\n\nリクエストごとにキャッシングの動作を制御します：\n\n```bash\n# キャッシュルックアップをスキップし、常にモデルを呼び出す\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| ヘッダー | 値 | 効果 |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | キャッシュをスキップし、新しいレスポンスを取得 |\n| `Cache-Control: no-store` | - | このレスポンスをキャッシュしない |\n\n### レスポンス・ヘッダー\n\nすべてのレスポンスにキャッシュ・ステータスが含まれます：\n\n```\nX-Cache: HIT           # キャッシュから提供されたレスポンス\nX-Cache: MISS          # モデルからの新しいレスポンス\nX-Cache-Entry-Id: abc  # キャッシュ・エントリID（フィードバック用）\n```\n\n## キャッシュ・ステータスの確認\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# レスポンス・ヘッダーからキャッシュ・ステータスを確認する\n# （生のHTTPレスポンスで利用可能）\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## キャッシュの請求\n\nキャッシュヒットは、新しいリクエストよりも大幅に安価です：\n\n| タイプ | コスト |\n|------|------|\n| キャッシュ HIT | **80% オフ** |\n| キャッシュ MISS | 定価 |\n\n正確な割引額は、ダッシュボードの使用状況ログに表示されます。\n\n## プライバシー・コントロール\n\n### APIキー・レベル\n\nダッシュボードで各APIキーのキャッシング動作を設定します：\n\n| モード | 説明 |\n|------|-------------|\n| **デフォルト** | キャッシュ有効。類似のリクエストと共有される可能性があります |\n| **共有なし** | キャッシュ有効。ただし、レスポンスはお客様のアカウント専用となります |\n| **無効** | キャッシングを一切行わない |\n\n### リクエスト・レベル\n\nリクエストごとに上書きします：\n\n```bash\n# このリクエストのキャッシングを無効にする\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## キャッシュ・フィードバック\n\n誤ったキャッシュ・レスポンスを受け取った場合は、報告することができます：\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**フィードバックの種類：**\n- `wrong_answer` - 事実と異なる\n- `outdated` - 情報が古い\n- `irrelevant` - 質問と一致しない\n- `other` - その他の問題\n\nキャッシュ・エントリが十分な負のフィードバックを受けると、自動的に無効化されます。\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"キャッシュ可能なクエリには temperature=0 を使用する\">\n    決定論的な設定により、キャッシュヒット率が最大化されます。\n  </Accordion>\n\n  <Accordion title=\"プロンプトの形式を標準化する\">\n    一貫したフォーマットにより、セマンティック・マッチングが向上します。\n  </Accordion>\n\n  <Accordion title=\"時間に敏感なクエリには no-cache を使用する\">\n    時事問題やリアルタイム・データはキャッシュをスキップする必要があります。\n  </Accordion>\n\n  <Accordion title=\"キャッシュヒット率を監視する\">\n    ダッシュボードでキャッシュの統計と節約額を確認してください。\n  </Accordion>\n</AccordionGroup>\n\n## キャッシュすべきでない場合\n\n以下の場合、キャッシングを無効にしてください：\n\n- **リアルタイム情報**：株価、天気、ニュース\n- **パーソナライズされたコンテンツ**：ユーザー固有の推奨事項\n- **クリエイティブなタスク**：多様性が求められる場合\n- **機密データ**：機密情報\n\n```python\n# 時間に敏感なクエリの場合\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "ko": "---\ntitle: \"✨ 지능형 캐싱\"\ndescription: \"문맥 인식 시맨틱 캐싱으로 비용과 지연 시간을 줄이세요\"\n---\n\n## 개요\n\nLemonData는 API 비용과 응답 지연 시간을 크게 줄일 수 있는 지능형 캐싱 시스템을 제공합니다. 당사의 캐싱은 단순한 요청 매칭을 넘어 프롬프트의 **시맨틱 의미(semantic meaning)**를 이해합니다.\n\n<CardGroup cols={2}>\n  <Card title=\"비용 절감\" icon=\"piggy-bank\">\n    캐시 히트(Cache hit)는 정상 비용의 일부만 청구됩니다.\n  </Card>\n  <Card title=\"빠른 응답\" icon=\"bolt\">\n    캐시된 응답은 즉시 반환되며, 모델 추론이 필요하지 않습니다.\n  </Card>\n  <Card title=\"문맥 인식\" icon=\"brain\">\n    시맨틱 매칭은 표현이 다르더라도 유사한 요청을 찾아냅니다.\n  </Card>\n  <Card title=\"개인정보 제어\" icon=\"shield\">\n    캐싱 및 공유 대상에 대한 전체 제어 권한을 제공합니다.\n  </Card>\n</CardGroup>\n\n## 작동 원리\n\nLemonData는 2계층 캐싱 시스템을 사용합니다:\n\n### 계층 1: 응답 캐시 (정확한 일치)\n\n결정론적 요청(`temperature=0`)의 경우, 정확한 응답을 캐싱합니다:\n\n- **일치 항목**: 동일한 모델, 메시지 및 파라미터\n- **속도**: 즉시 (마이크로초)\n- **최적 용도**: 반복되는 동일한 쿼리\n\n### 계층 2: 시맨틱 캐시 (유사도 일치)\n\n모든 요청에 대해 2단계 매칭 알고리즘을 사용하여 시맨틱 유사성도 확인합니다:\n\n- **1단계 (쿼리 전용)**: 사용자 쿼리에 대해 95% 이상의 유사도\n- **2단계 (전체 문맥)**: 대화 문맥을 포함하여 85% 이상의 유사도\n- **최적 용도**: FAQ 스타일의 쿼리, 일반적인 질문\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## 캐시 헤더\n\n### 요청 헤더\n\n요청별로 캐싱 동작을 제어합니다:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| 헤더 | 값 | 효과 |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | 캐시 건너뛰기, 새로운 응답 |\n| `Cache-Control: no-store` | - | 이 응답을 캐싱하지 않음 |\n\n### 응답 헤더\n\n모든 응답에는 캐시 상태가 포함됩니다:\n\n```\nX-Cache: HIT           # 캐시에서 응답 제공\nX-Cache: MISS          # 모델의 새로운 응답\nX-Cache-Entry-Id: abc  # 캐시 엔트리 ID (피드백용)\n```\n\n## 캐시 상태 확인\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## 캐시 과금\n\n캐시 히트는 새로운 요청보다 훨씬 저렴합니다:\n\n| 유형 | 비용 |\n|------|------|\n| 캐시 HIT | **80% 할인** |\n| 캐시 MISS | 정가 |\n\n정확한 할인율은 대시보드 사용 로그에 표시됩니다.\n\n## 개인정보 제어\n\n### API 키 수준\n\n대시보드에서 각 API 키에 대한 캐싱 동작을 구성합니다:\n\n| 모드 | 설명 |\n|------|-------------|\n| **Default** | 캐시 활성화, 유사한 요청과 공유될 수 있음 |\n| **No Share** | 캐시 활성화, 하지만 응답은 귀하의 계정에만 비공개로 유지됨 |\n| **Disabled** | 캐싱 사용 안 함 |\n\n### 요청 수준\n\n요청별 재정의:\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## 캐시 피드백\n\n잘못된 캐시 응답을 받은 경우 이를 보고할 수 있습니다:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**피드백 유형:**\n- `wrong_answer` - 사실과 다름\n- `outdated` - 정보가 오래됨\n- `irrelevant` - 질문과 일치하지 않음\n- `other` - 기타 문제\n\n캐시 엔트리에 부정적인 피드백이 충분히 쌓이면 자동으로 무효화됩니다.\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"캐싱 가능한 쿼리에 temperature=0 사용\">\n    결정론적 설정은 캐시 히트율을 극대화합니다.\n  </Accordion>\n\n  <Accordion title=\"프롬프트 형식 표준화\">\n    일관된 포맷팅은 시맨틱 매칭을 개선합니다.\n  </Accordion>\n\n  <Accordion title=\"시간에 민감한 쿼리에 no-cache 사용\">\n    시사 이슈, 실시간 데이터는 캐시를 건너뛰어야 합니다.\n  </Accordion>\n\n  <Accordion title=\"캐시 히트율 모니터링\">\n    대시보드에서 캐시 통계 및 절감액을 확인하세요.\n  </Accordion>\n</AccordionGroup>\n\n## 캐싱을 하지 말아야 할 경우\n\n다음에 대해 캐싱을 비활성화하세요:\n\n- **실시간 정보**: 주가, 날씨, 뉴스\n- **개인화된 콘텐츠**: 사용자별 추천\n- **창의적 작업**: 다양성이 필요한 경우\n- **민감한 데이터**: 기밀 정보\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "de": "---\ntitle: \"✨ Intelligentes Caching\"\ndescription: \"Reduzieren Sie Kosten und Latenz mit kontextbewusstem semantischem Caching\"\n---\n\n## Übersicht\n\nLemonData bietet ein intelligentes Caching-System, das Ihre API-Kosten und Antwortlatenz erheblich reduzieren kann. Unser Caching geht über einfaches Request-Matching hinaus – es versteht die **semantische Bedeutung** Ihrer Prompts.\n\n<CardGroup cols={2}>\n  <Card title=\"Kostenersparnis\" icon=\"piggy-bank\">\n    Cache-Hits werden zu einem Bruchteil der normalen Kosten abgerechnet.\n  </Card>\n  <Card title=\"Schnellere Antworten\" icon=\"bolt\">\n    Gecachte Antworten werden sofort zurückgegeben, es ist keine Modell-Inferenz erforderlich.\n  </Card>\n  <Card title=\"Kontextbewusst\" icon=\"brain\">\n    Semantisches Matching findet ähnliche Anfragen, selbst bei unterschiedlicher Formulierung.\n  </Card>\n  <Card title=\"Datenschutzkontrollen\" icon=\"shield\">\n    Volle Kontrolle darüber, was gecacht und geteilt wird.\n  </Card>\n</CardGroup>\n\n## Funktionsweise\n\nLemonData verwendet ein zweistufiges Caching-System:\n\n### Ebene 1: Response-Cache (Exakte Übereinstimmung)\n\nFür deterministische Anfragen (`temperature=0`) cachen wir die exakte Antwort:\n\n- **Übereinstimmung**: Identisches Modell, Nachrichten und Parameter\n- **Geschwindigkeit**: Sofort (Mikrosekunden)\n- **Ideal für**: Wiederholte identische Abfragen\n\n### Ebene 2: Semantischer Cache (Ähnlichkeitsabgleich)\n\nFür alle Anfragen prüfen wir zusätzlich die semantische Ähnlichkeit mithilfe eines zweistufigen Matching-Algorithmus:\n\n- **Stufe 1 (Nur Query)**: ≥95% Ähnlichkeit bei der Benutzeranfrage\n- **Stufe 2 (Vollständiger Kontext)**: ≥85% Ähnlichkeit einschließlich des Konversationskontexts\n- **Ideal für**: FAQ-ähnliche Abfragen, häufige Fragen\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## Cache-Header\n\n### Request-Header\n\nSteuern Sie das Caching-Verhalten pro Anfrage:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| Header | Wert | Effekt |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | Cache überspringen, frische Antwort |\n| `Cache-Control: no-store` | - | Diese Antwort nicht cachen |\n\n### Response-Header\n\nJede Antwort enthält einen Cache-Status:\n\n```\nX-Cache: HIT           # Response served from cache\nX-Cache: MISS          # Fresh response from model\nX-Cache-Entry-Id: abc  # Cache entry ID (for feedback)\n```\n\n## Cache-Status überprüfen\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## Cache-Abrechnung\n\nCache-Hits sind deutlich günstiger als frische Anfragen:\n\n| Typ | Kosten |\n|------|------|\n| Cache HIT | **80% Rabatt** |\n| Cache MISS | Voller Preis |\n\nDer genaue Rabatt wird in den Nutzungsprotokollen Ihres Dashboards angezeigt.\n\n## Datenschutzkontrollen\n\n### API-Key-Ebene\n\nKonfigurieren Sie das Caching-Verhalten für jeden API-Key in Ihrem Dashboard:\n\n| Modus | Beschreibung |\n|------|-------------|\n| **Default** | Cache aktiviert, kann mit ähnlichen Anfragen geteilt werden |\n| **No Share** | Cache aktiviert, aber Antworten sind privat für Ihr Konto |\n| **Disabled** | Keinerlei Caching |\n\n### Request-Ebene\n\nPro Anfrage überschreiben:\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## Cache-Feedback\n\nWenn Sie eine fehlerhafte gecachte Antwort erhalten, können Sie dies melden:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**Feedback-Typen:**\n- `wrong_answer` - Sachlich falsch\n- `outdated` - Informationen sind veraltet\n- `irrelevant` - Passt nicht zur Frage\n- `other` - Andere Probleme\n\nWenn ein Cache-Eintrag genügend negatives Feedback erhält, wird er automatisch ungültig gemacht.\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Verwenden Sie temperature=0 für cachbare Abfragen\">\n    Deterministische Einstellungen maximieren die Cache-Hit-Raten.\n  </Accordion>\n\n  <Accordion title=\"Prompt-Formate standardisieren\">\n    Einheitliche Formatierung verbessert das semantische Matching.\n  </Accordion>\n\n  <Accordion title=\"Verwenden Sie no-cache für zeitkritische Abfragen\">\n    Aktuelle Ereignisse und Echtzeitdaten sollten den Cache überspringen.\n  </Accordion>\n\n  <Accordion title=\"Cache-Hit-Raten überwachen\">\n    Überprüfen Sie Ihr Dashboard auf Cache-Statistiken und Einsparungen.\n  </Accordion>\n</AccordionGroup>\n\n## Wann Sie NICHT cachen sollten\n\nDeaktivieren Sie das Caching für:\n\n- **Echtzeit-Informationen**: Aktienkurse, Wetter, Nachrichten\n- **Personalisierte Inhalte**: Benutzerspezifische Empfehlungen\n- **Kreative Aufgaben**: Wenn Abwechslung erwünscht ist\n- **Sensible Daten**: Vertrauliche Informationen\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "fr": "---\ntitle: \"✨ Mise en cache intelligente\"\ndescription: \"Réduisez les coûts et la latence grâce à une mise en cache sémantique sensible au contexte\"\n---\n\n## Aperçu\n\nLemonData propose un système de mise en cache intelligente qui peut réduire considérablement vos coûts d'API et la latence des réponses. Notre mise en cache va au-delà de la simple correspondance de requêtes - elle comprend la **signification sémantique** de vos prompts.\n\n<CardGroup cols={2}>\n  <Card title=\"Économies de coûts\" icon=\"piggy-bank\">\n    Les hits de cache sont facturés à une fraction du coût normal.\n  </Card>\n  <Card title=\"Réponses plus rapides\" icon=\"bolt\">\n    Les réponses mises en cache sont renvoyées instantanément, aucune inférence de modèle n'est nécessaire.\n  </Card>\n  <Card title=\"Sensible au contexte\" icon=\"brain\">\n    La correspondance sémantique trouve des requêtes similaires même avec une formulation différente.\n  </Card>\n  <Card title=\"Contrôles de confidentialité\" icon=\"shield\">\n    Contrôle total sur ce qui est mis en cache et partagé.\n  </Card>\n</CardGroup>\n\n## Fonctionnement\n\nLemonData utilise un système de mise en cache à deux couches :\n\n### Couche 1 : Cache de réponse (Correspondance exacte)\n\nPour les requêtes déterministes (`temperature=0`), nous mettons en cache la réponse exacte :\n\n- **Correspondance** : Modèle, messages et paramètres identiques\n- **Vitesse** : Instantanée (microsecondes)\n- **Idéal pour** : Les requêtes identiques répétées\n\n### Couche 2 : Cache sémantique (Correspondance par similitude)\n\nPour toutes les requêtes, nous vérifions également la similitude sémantique à l'aide d'un algorithme de correspondance en deux étapes :\n\n- **Étape 1 (Requête uniquement)** : ≥95 % de similitude sur la requête utilisateur\n- **Étape 2 (Contexte complet)** : ≥85 % de similitude incluant le contexte de la conversation\n- **Idéal pour** : Les requêtes de type FAQ, les questions courantes\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## En-têtes de cache\n\n### En-têtes de requête\n\nContrôlez le comportement de mise en cache par requête :\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| En-tête | Valeur | Effet |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | Ignorer le cache, réponse fraîche |\n| `Cache-Control: no-store` | - | Ne pas mettre cette réponse en cache |\n\n### En-têtes de réponse\n\nChaque réponse inclut un statut de cache :\n\n```\nX-Cache: HIT           # Response served from cache\nX-Cache: MISS          # Fresh response from model\nX-Cache-Entry-Id: abc  # Cache entry ID (for feedback)\n```\n\n## Vérification du statut du cache\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## Facturation du cache\n\nLes hits de cache sont nettement moins chers que les requêtes fraîches :\n\n| Type | Coût |\n|------|------|\n| Cache HIT | **-80 %** |\n| Cache MISS | Plein tarif |\n\nLa remise exacte est affichée dans les journaux d'utilisation de votre tableau de bord.\n\n## Contrôles de confidentialité\n\n### Niveau Clé API\n\nConfigurez le comportement de mise en cache pour chaque clé API dans votre tableau de bord :\n\n| Mode | Description |\n|------|-------------|\n| **Par défaut** | Cache activé, peut être partagé avec des requêtes similaires |\n| **Pas de partage** | Cache activé, mais les réponses sont privées pour votre compte |\n| **Désactivé** | Aucune mise en cache |\n\n### Niveau Requête\n\nSurcharger par requête :\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## Feedback du cache\n\nSi vous recevez une réponse mise en cache incorrecte, vous pouvez la signaler :\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**Types de feedback :**\n- `wrong_answer` - Factuellement incorrect\n- `outdated` - L'information est périmée\n- `irrelevant` - Ne correspond pas à la question\n- `other` - Autres problèmes\n\nLorsqu'une entrée de cache reçoit suffisamment de retours négatifs, elle est automatiquement invalidée.\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Utilisez temperature=0 pour les requêtes pouvant être mises en cache\">\n    Les paramètres déterministes maximisent les taux de hit de cache.\n  </Accordion>\n\n  <Accordion title=\"Standardisez les formats de prompt\">\n    Un formatage cohérent améliore la correspondance sémantique.\n  </Accordion>\n\n  <Accordion title=\"Utilisez no-cache pour les requêtes sensibles au facteur temps\">\n    Les événements actuels et les données en temps réel devraient ignorer le cache.\n  </Accordion>\n\n  <Accordion title=\"Surveillez les taux de hit de cache\">\n    Consultez votre tableau de bord pour les statistiques de cache et les économies réalisées.\n  </Accordion>\n</AccordionGroup>\n\n## Quand NE PAS mettre en cache\n\nDésactivez la mise en cache pour :\n\n- **Informations en temps réel** : Cours de la bourse, météo, actualités\n- **Contenu personnalisé** : Recommandations spécifiques à l'utilisateur\n- **Tâches créatives** : Lorsque la variété est souhaitée\n- **Données sensibles** : Informations confidentielles\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "es": "---\ntitle: \"✨ Caching Inteligente\"\ndescription: \"Reduzca costos y latencia con un caching semántico consciente del contexto\"\n---\n\n## Resumen\n\nLemonData proporciona un sistema de caching inteligente que puede reducir significativamente sus costos de API y la latencia de respuesta. Nuestro caching va más allá de la simple coincidencia de solicitudes: entiende el **significado semántico** de sus prompts.\n\n<CardGroup cols={2}>\n  <Card title=\"Ahorro de Costos\" icon=\"piggy-bank\">\n    Los aciertos de cache (cache hits) se facturan a una fracción del costo normal.\n  </Card>\n  <Card title=\"Respuestas más Rápidas\" icon=\"bolt\">\n    Las respuestas en cache se devuelven al instante, sin necesidad de inferencia del modelo.\n  </Card>\n  <Card title=\"Consciente del Contexto\" icon=\"brain\">\n    La coincidencia semántica encuentra solicitudes similares incluso con una redacción diferente.\n  </Card>\n  <Card title=\"Controles de Privacidad\" icon=\"shield\">\n    Control total sobre lo que se almacena en cache y se comparte.\n  </Card>\n</CardGroup>\n\n## Cómo Funciona\n\nLemonData utiliza un sistema de caching de dos capas:\n\n### Capa 1: Cache de Respuesta (Coincidencia Exacta)\n\nPara solicitudes deterministas (`temperature=0`), almacenamos en cache la respuesta exacta:\n\n- **Coincidencia**: Modelo, mensajes y parámetros idénticos\n- **Velocidad**: Instantánea (microsegundos)\n- **Ideal para**: Consultas idénticas repetidas\n\n### Capa 2: Cache Semántico (Coincidencia por Similitud)\n\nPara todas las solicitudes, también verificamos la similitud semántica utilizando un algoritmo de coincidencia de dos etapas:\n\n- **Etapa 1 (Solo consulta)**: ≥95% de similitud en la consulta del usuario\n- **Etapa 2 (Contexto completo)**: ≥85% de similitud incluyendo el contexto de la conversación\n- **Ideal para**: Consultas tipo FAQ, preguntas comunes\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## Encabezados de Cache\n\n### Encabezados de Solicitud\n\nControle el comportamiento del caching por solicitud:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| Encabezado | Valor | Efecto |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | Omitir cache, respuesta fresca |\n| `Cache-Control: no-store` | - | No almacenar esta respuesta en cache |\n\n### Encabezados de Respuesta\n\nCada respuesta incluye el estado del cache:\n\n```\nX-Cache: HIT           # Response served from cache\nX-Cache: MISS          # Fresh response from model\nX-Cache-Entry-Id: abc  # Cache entry ID (for feedback)\n```\n\n## Comprobación del Estado del Cache\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## Facturación del Cache\n\nLos aciertos de cache son significativamente más baratos que las solicitudes frescas:\n\n| Tipo | Costo |\n|------|------|\n| Cache HIT | **80% de descuento** |\n| Cache MISS | Precio completo |\n\nEl descuento exacto se muestra en los registros de uso de su dashboard.\n\n## Controles de Privacidad\n\n### Nivel de API Key\n\nConfigure el comportamiento del caching para cada API key en su dashboard:\n\n| Modo | Descripción |\n|------|-------------|\n| **Default** | Cache habilitado, puede compartirse con solicitudes similares |\n| **No Share** | Cache habilitado, pero las respuestas son privadas para su cuenta |\n| **Disabled** | Sin caching en absoluto |\n\n### Nivel de Solicitud\n\nAnular por solicitud:\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## Feedback del Cache\n\nSi recibe una respuesta en cache incorrecta, puede reportarla:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**Tipos de feedback:**\n- `wrong_answer` - Fácticamente incorrecto\n- `outdated` - La información está desactualizada\n- `irrelevant` - No coincide con la pregunta\n- `other` - Otros problemas\n\nCuando una entrada de cache recibe suficiente feedback negativo, se invalida automáticamente.\n\n## Mejores Prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Use temperature=0 para consultas cacheables\">\n    Los ajustes deterministas maximizan las tasas de acierto de cache.\n  </Accordion>\n\n  <Accordion title=\"Estandarice los formatos de prompt\">\n    Un formato consistente mejora la coincidencia semántica.\n  </Accordion>\n\n  <Accordion title=\"Use no-cache para consultas sensibles al tiempo\">\n    Eventos actuales y datos en tiempo real deben omitir el cache.\n  </Accordion>\n\n  <Accordion title=\"Monitoree las tasas de acierto de cache\">\n    Consulte su dashboard para ver estadísticas de cache y ahorros.\n  </Accordion>\n</AccordionGroup>\n\n## Cuándo NO usar cache\n\nDeshabilite el caching para:\n\n- **Información en tiempo real**: Precios de acciones, clima, noticias\n- **Contenido personalizado**: Recomendaciones específicas del usuario\n- **Tareas creativas**: Cuando se desea variedad\n- **Datos sensibles**: Información confidencial\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "pt": "---\ntitle: \"✨ Caching Inteligente\"\ndescription: \"Reduza custos e latência com caching semântico sensível ao contexto\"\n---\n\n## Visão Geral\n\nA LemonData fornece um sistema de caching inteligente que pode reduzir significativamente seus custos de API e a latência de resposta. Nosso caching vai além da simples correspondência de requisições - ele entende o **significado semântico** dos seus prompts.\n\n<CardGroup cols={2}>\n  <Card title=\"Economia de Custos\" icon=\"piggy-bank\">\n    Cache hits são faturados por uma fração do custo normal.\n  </Card>\n  <Card title=\"Respostas Mais Rápidas\" icon=\"bolt\">\n    Respostas em cache são retornadas instantaneamente, sem necessidade de inferência do modelo.\n  </Card>\n  <Card title=\"Sensível ao Contexto\" icon=\"brain\">\n    A correspondência semântica encontra requisições semelhantes, mesmo com redações diferentes.\n  </Card>\n  <Card title=\"Controles de Privacidade\" icon=\"shield\">\n    Controle total sobre o que é armazenado em cache e compartilhado.\n  </Card>\n</CardGroup>\n\n## Como Funciona\n\nA LemonData utiliza um sistema de caching de duas camadas:\n\n### Camada 1: Cache de Resposta (Correspondência Exata)\n\nPara requisições determinísticas (`temperature=0`), armazenamos a resposta exata em cache:\n\n- **Correspondência**: Modelo, mensagens e parâmetros idênticos\n- **Velocidade**: Instantânea (microssegundos)\n- **Ideal para**: Consultas idênticas repetidas\n\n### Camada 2: Cache Semântico (Correspondência por Similaridade)\n\nPara todas as requisições, também verificamos a similaridade semântica usando um algoritmo de correspondência de dois estágios:\n\n- **Estágio 1 (Apenas consulta)**: ≥95% de similaridade na consulta do usuário\n- **Estágio 2 (Contexto completo)**: ≥85% de similaridade incluindo o contexto da conversa\n- **Ideal para**: Consultas estilo FAQ, perguntas comuns\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## Headers de Cache\n\n### Headers de Requisição\n\nControle o comportamento de caching por requisição:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| Header | Valor | Efeito |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | Pular cache, resposta nova |\n| `Cache-Control: no-store` | - | Não armazenar esta resposta em cache |\n\n### Headers de Resposta\n\nCada resposta inclui o status do cache:\n\n```\nX-Cache: HIT           # Resposta servida a partir do cache\nX-Cache: MISS          # Resposta nova do modelo\nX-Cache-Entry-Id: abc  # ID da entrada de cache (para feedback)\n```\n\n## Verificando o Status do Cache\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## Faturamento de Cache\n\nCache hits são significativamente mais baratos do que requisições novas:\n\n| Tipo | Custo |\n|------|------|\n| Cache HIT | **80% de desconto** |\n| Cache MISS | Preço total |\n\nO desconto exato é exibido nos logs de uso do seu dashboard.\n\n## Controles de Privacidade\n\n### Nível de Chave de API\n\nConfigure o comportamento de caching para cada chave de API em seu dashboard:\n\n| Modo | Descrição |\n|------|-------------|\n| **Default** | Cache ativado, pode ser compartilhado com requisições semelhantes |\n| **No Share** | Cache ativado, mas as respostas são privadas para sua conta |\n| **Disabled** | Nenhum caching |\n\n### Nível de Requisição\n\nSobrescrever por requisição:\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## Feedback de Cache\n\nSe você receber uma resposta em cache incorreta, poderá reportá-la:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**Tipos de feedback:**\n- `wrong_answer` - Factualmente incorreto\n- `outdated` - A informação está desatualizada\n- `irrelevant` - Não corresponde à pergunta\n- `other` - Outros problemas\n\nQuando uma entrada de cache recebe feedback negativo suficiente, ela é invalidada automaticamente.\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Use temperature=0 para consultas passíveis de cache\">\n    Configurações determinísticas maximizam as taxas de cache hit.\n  </Accordion>\n\n  <Accordion title=\"Padronize os formatos de prompt\">\n    A formatação consistente melhora a correspondência semântica.\n  </Accordion>\n\n  <Accordion title=\"Use no-cache para consultas sensíveis ao tempo\">\n    Eventos atuais e dados em tempo real devem pular o cache.\n  </Accordion>\n\n  <Accordion title=\"Monitore as taxas de cache hit\">\n    Verifique seu dashboard para estatísticas de cache e economia.\n  </Accordion>\n</AccordionGroup>\n\n## Quando NÃO usar Cache\n\nDesative o caching para:\n\n- **Informações em tempo real**: Preços de ações, clima, notícias\n- **Conteúdo personalizado**: Recomendações específicas do usuário\n- **Tarefas criativas**: Quando a variedade é desejada\n- **Dados sensíveis**: Informações confidenciais\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "ar": "---\ntitle: \"✨ التخزين المؤقت الذكي\"\ndescription: \"خفض التكاليف وزمن الاستجابة باستخدام التخزين المؤقت الدلالي المدرك للسياق\"\n---\n\n## نظرة عامة\n\nتوفر LemonData نظام تخزين مؤقت ذكي يمكنه تقليل تكاليف API وزمن استجابة الطلبات بشكل كبير. يتجاوز نظام التخزين المؤقت لدينا مجرد مطابقة الطلبات البسيطة - فهو يفهم **المعنى الدلالي** للمطالبات الخاصة بك.\n\n<CardGroup cols={2}>\n  <Card title=\"توفير التكاليف\" icon=\"piggy-bank\">\n    يتم احتساب رسوم عمليات Cache hits بجزء بسيط من التكلفة العادية.\n  </Card>\n  <Card title=\"استجابات أسرع\" icon=\"bolt\">\n    يتم إرجاع الاستجابات المخزنة مؤقتاً فوراً، دون الحاجة إلى استنتاج النموذج.\n  </Card>\n  <Card title=\"مدرك للسياق\" icon=\"brain\">\n    تجد المطابقة الدلالية الطلبات المتشابهة حتى مع اختلاف الصياغة.\n  </Card>\n  <Card title=\"عناصر التحكم في الخصوصية\" icon=\"shield\">\n    تحكم كامل في ما يتم تخزينه مؤقتاً ومشاركته.\n  </Card>\n</CardGroup>\n\n## كيف يعمل\n\nتستخدم LemonData نظام تخزين مؤقت ثنائي الطبقات:\n\n### الطبقة 1: تخزين الاستجابة المؤقت (مطابقة تامة)\n\nبالنسبة للطلبات الحتمية (`temperature=0`)، نقوم بتخزين الاستجابة الدقيقة مؤقتاً:\n\n- **المطابقة**: نموذج ورسائل ومعلمات متطابقة\n- **السرعة**: فورية (ميكروثانية)\n- **الأفضل لـ**: الاستعلامات المتكررة المتطابقة\n\n### الطبقة 2: التخزين المؤقت الدلالي (مطابقة التشابه)\n\nبالنسبة لجميع الطلبات، نتحقق أيضاً من التشابه الدلالي باستخدام خوارزمية مطابقة ثنائية المراحل:\n\n- **المرحلة 1 (الاستعلام فقط)**: تشابه ≥95% في استعلام المستخدم\n- **المرحلة 2 (السياق الكامل)**: تشابه ≥85% بما في ذلك سياق المحادثة\n- **الأفضل لـ**: استعلامات نمط الأسئلة الشائعة، والأسئلة المتكررة\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## رؤوس التخزين المؤقت (Cache Headers)\n\n### رؤوس الطلب (Request Headers)\n\nالتحكم في سلوك التخزين المؤقت لكل طلب:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| الرأس | القيمة | التأثير |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | تخطي التخزين المؤقت، استجابة جديدة |\n| `Cache-Control: no-store` | - | لا تقم بتخزين هذه الاستجابة مؤقتاً |\n\n### رؤوس الاستجابة (Response Headers)\n\nتتضمن كل استجابة حالة التخزين المؤقت:\n\n```\nX-Cache: HIT           # Response served from cache\nX-Cache: MISS          # Fresh response from model\nX-Cache-Entry-Id: abc  # Cache entry ID (for feedback)\n```\n\n## التحقق من حالة التخزين المؤقت\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## فوترة التخزين المؤقت\n\nتعد عمليات Cache hits أرخص بكثير من الطلبات الجديدة:\n\n| النوع | التكلفة |\n|------|------|\n| Cache HIT | **خصم 80%** |\n| Cache MISS | السعر الكامل |\n\nيظهر الخصم الدقيق في سجلات الاستخدام بلوحة التحكم الخاصة بك.\n\n## عناصر التحكم في الخصوصية\n\n### مستوى مفتاح API\n\nتكوين سلوك التخزين المؤقت لكل مفتاح API في لوحة التحكم الخاصة بك:\n\n| الوضع | الوصف |\n|------|-------------|\n| **Default** | التخزين المؤقت مفعل، قد تتم المشاركة مع طلبات مماثلة |\n| **No Share** | التخزين المؤقت مفعل، لكن الاستجابات خاصة بحسابك فقط |\n| **Disabled** | لا يوجد تخزين مؤقت على الإطلاق |\n\n### مستوى الطلب\n\nالتجاوز لكل طلب:\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## ملاحظات التخزين المؤقت\n\nإذا تلقيت استجابة مخزنة مؤقتاً غير صحيحة، يمكنك الإبلاغ عنها:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**أنواع الملاحظات:**\n- `wrong_answer` - غير صحيح واقعياً\n- `outdated` - المعلومات قديمة\n- `irrelevant` - لا يطابق السؤال\n- `other` - مشكلات أخرى\n\nعندما يتلقى إدخال التخزين المؤقت ما يكفي من الملاحظات السلبية، يتم إبطال مفعوله تلقائياً.\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"استخدم temperature=0 للاستعلامات القابلة للتخزين المؤقت\">\n    تزيد الإعدادات الحتمية من معدلات نجاح التخزين المؤقت (cache hit rates).\n  </Accordion>\n\n  <Accordion title=\"توحيد تنسيقات المطالبات\">\n    يؤدي التنسيق المتسق إلى تحسين المطابقة الدلالية.\n  </Accordion>\n\n  <Accordion title=\"استخدم no-cache للاستعلامات الحساسة للوقت\">\n    يجب أن تتخطى الأحداث الجارية والبيانات في الوقت الفعلي التخزين المؤقت.\n  </Accordion>\n\n  <Accordion title=\"مراقبة معدلات نجاح التخزين المؤقت\">\n    تحقق من لوحة التحكم الخاصة بك للحصول على إحصائيات التخزين المؤقت والتوفير.\n  </Accordion>\n</AccordionGroup>\n\n## متى لا يجب التخزين المؤقت\n\nقم بتعطيل التخزين المؤقت لـ:\n\n- **المعلومات في الوقت الفعلي**: أسعار الأسهم، الطقس، الأخبار\n- **المحتوى المخصص**: التوصيات الخاصة بالمستخدم\n- **المهام الإبداعية**: عندما يكون التنوع مطلوباً\n- **البيانات الحساسة**: المعلومات السرية\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "vi": "---\ntitle: \"✨ Caching Thông Minh\"\ndescription: \"Giảm chi phí và độ trễ với bộ nhớ đệm ngữ nghĩa nhận diện ngữ cảnh\"\n---\n\n## Tổng quan\n\nLemonData cung cấp một hệ thống caching thông minh có thể giảm đáng kể chi phí API và độ trễ phản hồi của bạn. Hệ thống caching của chúng tôi vượt xa việc khớp yêu cầu đơn thuần - nó hiểu được **ý nghĩa ngữ nghĩa** trong các prompt của bạn.\n\n<CardGroup cols={2}>\n  <Card title=\"Tiết kiệm chi phí\" icon=\"piggy-bank\">\n    Các lượt cache hit được tính phí chỉ bằng một phần nhỏ so với chi phí thông thường.\n  </Card>\n  <Card title=\"Phản hồi nhanh hơn\" icon=\"bolt\">\n    Các phản hồi đã lưu trong cache được trả về ngay lập tức, không cần suy luận mô hình.\n  </Card>\n  <Card title=\"Nhận diện ngữ cảnh\" icon=\"brain\">\n    Khớp ngữ nghĩa tìm thấy các yêu cầu tương tự ngay cả khi cách diễn đạt khác nhau.\n  </Card>\n  <Card title=\"Kiểm soát quyền riêng tư\" icon=\"shield\">\n    Toàn quyền kiểm soát những gì được lưu vào cache và chia sẻ.\n  </Card>\n</CardGroup>\n\n## Cách thức hoạt động\n\nLemonData sử dụng hệ thống caching hai lớp:\n\n### Lớp 1: Response Cache (Khớp chính xác)\n\nĐối với các yêu cầu mang tính xác định (`temperature=0`), chúng tôi lưu trữ phản hồi chính xác:\n\n- **Khớp**: Model, messages và các tham số giống hệt nhau\n- **Tốc độ**: Tức thì (micro giây)\n- **Tốt nhất cho**: Các truy vấn lặp lại giống hệt nhau\n\n### Lớp 2: Semantic Cache (Khớp tương đồng)\n\nĐối với tất cả các yêu cầu, chúng tôi cũng kiểm tra sự tương đồng về ngữ nghĩa bằng thuật toán khớp hai giai đoạn:\n\n- **Giai đoạn 1 (Chỉ truy vấn)**: Độ tương đồng ≥95% trên truy vấn của người dùng\n- **Giai đoạn 2 (Toàn bộ ngữ cảnh)**: Độ tương đồng ≥85% bao gồm cả ngữ cảnh hội thoại\n- **Tốt nhất cho**: Các truy vấn kiểu FAQ, các câu hỏi phổ biến\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## Cache Headers\n\n### Request Headers\n\nKiểm soát hành vi caching trên mỗi yêu cầu:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| Header | Giá trị | Hiệu quả |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | Bỏ qua cache, lấy phản hồi mới |\n| `Cache-Control: no-store` | - | Không lưu phản hồi này vào cache |\n\n### Response Headers\n\nMỗi phản hồi đều bao gồm trạng thái cache:\n\n```\nX-Cache: HIT           # Phản hồi được cung cấp từ cache\nX-Cache: MISS          # Phản hồi mới từ model\nX-Cache-Entry-Id: abc  # ID mục nhập cache (để phản hồi)\n```\n\n## Kiểm tra trạng thái Cache\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Check cache status from response headers\n# (Available in raw HTTP response)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## Thanh toán Cache\n\nCác lượt cache hit rẻ hơn đáng kể so với các yêu cầu mới:\n\n| Loại | Chi phí |\n|------|------|\n| Cache HIT | **Giảm 80%** |\n| Cache MISS | Giá gốc |\n\nMức chiết khấu chính xác được hiển thị trong nhật ký sử dụng trên dashboard của bạn.\n\n## Kiểm soát quyền riêng tư\n\n### Cấp độ API Key\n\nCấu hình hành vi caching cho từng API key trong dashboard của bạn:\n\n| Chế độ | Mô tả |\n|------|-------------|\n| **Default** | Đã bật cache, có thể chia sẻ với các yêu cầu tương tự |\n| **No Share** | Đã bật cache, nhưng các phản hồi là riêng tư đối với tài khoản của bạn |\n| **Disabled** | Hoàn toàn không sử dụng cache |\n\n### Cấp độ yêu cầu\n\nGhi đè trên mỗi yêu cầu:\n\n```bash\n# Disable caching for this request\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## Phản hồi Cache\n\nNếu bạn nhận được phản hồi từ cache không chính xác, bạn có thể báo cáo nó:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**Các loại phản hồi:**\n- `wrong_answer` - Sai lệch về sự thật\n- `outdated` - Thông tin đã cũ\n- `irrelevant` - Không khớp với câu hỏi\n- `other` - Các vấn đề khác\n\nKhi một mục nhập cache nhận đủ phản hồi tiêu cực, nó sẽ tự động bị vô hiệu hóa.\n\n## Thực hành tốt nhất\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng temperature=0 cho các truy vấn có thể lưu cache\">\n    Các thiết lập mang tính xác định giúp tối đa hóa tỷ lệ cache hit.\n  </Accordion>\n\n  <Accordion title=\"Chuẩn hóa định dạng prompt\">\n    Định dạng nhất quán giúp cải thiện việc khớp ngữ nghĩa.\n  </Accordion>\n\n  <Accordion title=\"Sử dụng no-cache cho các truy vấn nhạy cảm với thời gian\">\n    Các sự kiện hiện tại, dữ liệu thời gian thực nên bỏ qua cache.\n  </Accordion>\n\n  <Accordion title=\"Theo dõi tỷ lệ cache hit\">\n    Kiểm tra dashboard của bạn để biết số liệu thống kê cache và mức tiết kiệm.\n  </Accordion>\n</AccordionGroup>\n\n## Khi nào KHÔNG nên sử dụng Cache\n\nTắt caching cho:\n\n- **Thông tin thời gian thực**: Giá cổ phiếu, thời tiết, tin tức\n- **Nội dung cá nhân hóa**: Các đề xuất dành riêng cho người dùng\n- **Các tác vụ sáng tạo**: Khi cần sự đa dạng\n- **Dữ liệu nhạy cảm**: Thông tin bảo mật\n\n```python\n# For time-sensitive queries\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "id": "---\ntitle: \"✨ Intelligent Caching\"\ndescription: \"Kurangi biaya dan latensi dengan semantic caching yang sadar konteks\"\n---\n\n## Ringkasan\n\nLemonData menyediakan sistem caching cerdas yang dapat secara signifikan mengurangi biaya API dan latensi respons Anda. Caching kami melampaui pencocokan permintaan sederhana - sistem ini memahami **makna semantik** dari prompt Anda.\n\n<CardGroup cols={2}>\n  <Card title=\"Penghematan Biaya\" icon=\"piggy-bank\">\n    Cache hit ditagih dengan biaya yang jauh lebih rendah dari biaya normal.\n  </Card>\n  <Card title=\"Respons Lebih Cepat\" icon=\"bolt\">\n    Respons yang di-cache dikembalikan secara instan, tidak memerlukan inferensi model.\n  </Card>\n  <Card title=\"Sadar Konteks\" icon=\"brain\">\n    Pencocokan semantik menemukan permintaan serupa bahkan dengan kata-kata yang berbeda.\n  </Card>\n  <Card title=\"Kontrol Privasi\" icon=\"shield\">\n    Kontrol penuh atas apa yang di-cache dan dibagikan.\n  </Card>\n</CardGroup>\n\n## Cara Kerja\n\nLemonData menggunakan sistem caching dua lapis:\n\n### Lapisan 1: Response Cache (Pencocokan Persis)\n\nUntuk permintaan deterministik (`temperature=0`), kami men-cache respons yang persis:\n\n- **Pencocokan**: Model, pesan, dan parameter yang identik\n- **Kecepatan**: Instan (mikrodetik)\n- **Terbaik untuk**: Kueri identik yang berulang\n\n### Lapisan 2: Semantic Cache (Pencocokan Kemiripan)\n\nUntuk semua permintaan, kami juga memeriksa kemiripan semantik menggunakan algoritma pencocokan dua tahap:\n\n- **Tahap 1 (Hanya kueri)**: ≥95% kemiripan pada kueri pengguna\n- **Tahap 2 (Konteks penuh)**: ≥85% kemiripan termasuk konteks percakapan\n- **Terbaik untuk**: Kueri gaya FAQ, pertanyaan umum\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## Header Cache\n\n### Header Permintaan\n\nKontrol perilaku caching per permintaan:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Cache-Control: no-cache\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [...]}'\n```\n\n| Header | Nilai | Efek |\n|--------|-------|--------|\n| `Cache-Control: no-cache` | - | Lewati cache, respons baru |\n| `Cache-Control: no-store` | - | Jangan cache respons ini |\n\n### Header Respons\n\nSetiap respons menyertakan status cache:\n\n```\nX-Cache: HIT           # Respons dilayani dari cache\nX-Cache: MISS          # Respons baru dari model\nX-Cache-Entry-Id: abc  # ID entri cache (untuk umpan balik)\n```\n\n## Memeriksa Status Cache\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\n# Periksa status cache dari header respons\n# (Tersedia dalam respons HTTP mentah)\nprint(f\"Cache: {response._raw_response.headers.get('X-Cache')}\")\n```\n\n## Penagihan Cache\n\nCache hit secara signifikan lebih murah daripada permintaan baru:\n\n| Tipe | Biaya |\n|------|------|\n| Cache HIT | **Diskon 80%** |\n| Cache MISS | Harga penuh |\n\nDiskon tepatnya ditampilkan di log penggunaan dasbor Anda.\n\n## Kontrol Privasi\n\n### Tingkat API Key\n\nKonfigurasikan perilaku caching untuk setiap API key di dasbor Anda:\n\n| Mode | Deskripsi |\n|------|-------------|\n| **Default** | Cache diaktifkan, dapat dibagikan dengan permintaan serupa |\n| **No Share** | Cache diaktifkan, tetapi respons bersifat pribadi untuk akun Anda |\n| **Disabled** | Tidak ada caching sama sekali |\n\n### Tingkat Permintaan\n\nTimpa per permintaan:\n\n```bash\n# Nonaktifkan caching untuk permintaan ini\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Cache-Control: no-store\" \\\n  -d '...'\n```\n\n## Umpan Balik Cache\n\nJika Anda menerima respons cache yang salah, Anda dapat melaporkannya:\n\n```bash\ncurl -X POST https://api.lemondata.cc/v1/cache/feedback \\\n  -H \"Authorization: Bearer sk-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"cache_entry_id\": \"abc123\",\n    \"feedback_type\": \"wrong_answer\",\n    \"description\": \"Response was outdated\"\n  }'\n```\n\n**Tipe umpan balik:**\n- `wrong_answer` - Secara faktual salah\n- `outdated` - Informasi sudah usang\n- `irrelevant` - Tidak cocok dengan pertanyaan\n- `other` - Masalah lainnya\n\nKetika sebuah entri cache menerima cukup banyak umpan balik negatif, entri tersebut akan dibatalkan secara otomatis.\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Gunakan temperature=0 untuk kueri yang dapat di-cache\">\n    Pengaturan deterministik memaksimalkan tingkat cache hit.\n  </Accordion>\n\n  <Accordion title=\"Standarisasi format prompt\">\n    Pemformatan yang konsisten meningkatkan pencocokan semantik.\n  </Accordion>\n\n  <Accordion title=\"Gunakan no-cache untuk kueri yang sensitif terhadap waktu\">\n    Peristiwa terkini, data real-time harus melewati cache.\n  </Accordion>\n\n  <Accordion title=\"Pantau tingkat cache hit\">\n    Periksa dasbor Anda untuk statistik cache dan penghematan.\n  </Accordion>\n</AccordionGroup>\n\n## Kapan TIDAK menggunakan Cache\n\nNonaktifkan caching untuk:\n\n- **Informasi real-time**: Harga saham, cuaca, berita\n- **Konten yang dipersonalisasi**: Rekomendasi khusus pengguna\n- **Tugas kreatif**: Saat variasi diinginkan\n- **Data sensitif**: Informasi rahasia\n\n```python\n# Untuk kueri yang sensitif terhadap waktu\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the current stock price of AAPL?\"}],\n    extra_headers={\"Cache-Control\": \"no-cache\"}\n)\n```",
      "tr": "---\ntitle: \"✨ Akıllı Önbellekleme\"\ndescription: \"Bağlam duyarlı semantik önbellekleme ile maliyetleri ve gecikmeyi azaltın\"\n---\n\n## Genel Bakış\n\nLemonData, API maliyetlerinizi ve yanıt gecikmenizi önemli ölçüde azaltabilen akıllı bir önbellekleme sistemi sunar. Önbelleklememiz basit istek eşleştirmenin ötesine geçer - istemlerinizin (prompts) **semantik anlamını** anlar.\n\n<CardGroup cols={2}>\n  <Card title=\"Maliyet Tasarrufu\" icon=\"piggy-bank\">\n    Önbellek isabetleri (cache hits), normal maliyetin çok küçük bir kısmı ile ücretlendirilir.\n  </Card>\n  <Card title=\"Daha Hızlı Yanıtlar\" icon=\"bolt\">\n    Önbelleğe alınan yanıtlar anında döndürülür, model çıkarımı (inference) gerekmez.\n  </Card>\n  <Card title=\"Bağlam Duyarlı\" icon=\"brain\">\n    Semantik eşleştirme, farklı kelimelerle ifade edilse bile benzer istekleri bulur.\n  </Card>\n  <Card title=\"Gizlilik Kontrolleri\" icon=\"shield\">\n    Neyin önbelleğe alınacağı ve paylaşılacağı üzerinde tam kontrol.\n  </Card>\n</CardGroup>\n\n## Nasıl Çalışır?\n\nLemonData iki katmanlı bir önbellekleme sistemi kullanır:\n\n### Katman 1: Yanıt Önbelleği (Tam Eşleşme)\n\nDeterministik istekler (`temperature=0`) için tam yanıtı önbelleğe alıyoruz:\n\n- **Eşleşme**: Özdeş model, mesajlar ve parametreler\n- **Hız**: Anında (mikrosaniye)\n- **En iyi kullanım**: Tekrarlanan özdeş sorgular\n\n### Katman 2: Semantik Önbellek (Benzerlik Eşleşmesi)\n\nTüm istekler için, iki aşamalı bir eşleştirme algoritması kullanarak semantik benzerliği de kontrol ediyoruz:\n\n- **Aşama 1 (Yalnızca sorgu)**: Kullanıcı sorgusunda ≥%95 benzerlik\n- **Aşama 2 (Tam bağlam)**: Konuşma bağlamı dahil ≥%85 benzerlik\n- **En iyi kullanım**: SSS tarzı sorgular, yaygın sorular\n\n```\nUser A: \"What is the capital of France?\"\nUser B: \"Tell me the capital city of France\"\n→ Same cached response (high semantic similarity)\n```\n\n## Önbellek Başlıkları\n\n### İstek Başlıkları\n\nİstek başına önbellekleme davranışını kontrol edin:\n\n```bash\n# Skip cache lookup, always call the model\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-key\" \\"
    },
    "updatedAt": "2026-01-26T05:32:00.434Z"
  },
  "guides/error-handling.mdx": {
    "sourceHash": "6a1e9cc9c69e1d58",
    "translations": {
      "zh": "---\ntitle: \"错误处理\"\ndescription: \"优雅地处理 API 错误\"\n---\n\n## 错误响应格式\n\n所有错误都返回一致的 JSON 格式：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## HTTP 状态码\n\n| 代码 | 描述 |\n|------|-------------|\n| 400 | Bad Request - 参数无效 |\n| 401 | Unauthorized - API key 无效或缺失 |\n| 402 | Payment Required - 余额不足 |\n| 403 | Forbidden - 访问被拒绝或模型未授权 |\n| 404 | Not Found - 未找到模型或资源 |\n| 413 | Payload Too Large - 输入或文件大小超限 |\n| 429 | Too Many Requests - 已达到速率限制 |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - 上游供应商错误 |\n| 503 | Service Unavailable - 所有渠道均失败 |\n| 504 | Gateway Timeout - 请求超时 |\n\n## 错误类型\n\n### 身份验证错误 (401)\n\n| 类型 | 代码 | 描述 |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API key 缺失或无效 |\n| `expired_api_key` | `expired_api_key` | API key 已被撤销 |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### 支付错误 (402)\n\n| 类型 | 代码 | 描述 |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | 账户余额过低 |\n| `quota_exceeded` | `quota_exceeded` | 已达到 API key 使用限制 |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### 访问错误 (403)\n\n| 类型 | 代码 | 描述 |\n|------|------|-------------|\n| `access_denied` | `access_denied` | 资源访问被拒绝 |\n| `access_denied` | `model_not_allowed` | 此 API key 不允许使用该模型 |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### 验证错误 (400)\n\n| 类型 | 描述 |\n|------|-------------|\n| `invalid_request_error` | 请求参数无效 |\n| `context_length_exceeded` | 输入内容对模型而言过长 |\n| `model_not_found` | 请求的模型不存在 |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### 速率限制错误 (429)\n\n当您超过速率限制时：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**包含的响应头：**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### 负载过大 (413)\n\n当输入或文件大小超过限制时：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\n常见原因：\n- 图片文件过大（最大 20MB）\n- 音频文件过大（最大 25MB）\n- 输入文本超过模型上下文长度\n\n### 上游错误 (502, 503)\n\n| 类型 | 描述 |\n|------|-------------|\n| `upstream_error` | 供应商返回错误 |\n| `all_channels_failed` | 没有可用的供应商 |\n| `timeout_error` | 请求超时 |\n\n## 在 Python 中处理错误\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## 在 JavaScript 中处理错误\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"实现指数退避\">\n    当受到速率限制时，在重试之间逐渐延长等待时间：\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"设置超时\">\n    始终设置合理的超时时间，以避免请求挂起：\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"记录错误以便调试\">\n    记录完整的错误响应（包括请求 ID）以便寻求支持：\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"处理特定于模型的错误\">\n    某些模型有特定要求（例如最大 token 数、图像格式）。在发起请求前验证输入。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"錯誤處理\"\ndescription: \"優雅地處理 API 錯誤\"\n---\n\n## 錯誤回應格式\n\n所有錯誤都會回傳一致的 JSON 格式：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // 選填，用於驗證錯誤\n  }\n}\n```\n\n## HTTP 狀態碼\n\n| 代碼 | 描述 |\n|------|-------------|\n| 400 | Bad Request - 參數無效 |\n| 401 | Unauthorized - API key 無效或缺失 |\n| 402 | Payment Required - 餘額不足 |\n| 403 | Forbidden - 存取被拒絕或不允許使用該模型 |\n| 404 | Not Found - 找不到模型或資源 |\n| 413 | Payload Too Large - 輸入或檔案大小超過限制 |\n| 429 | Too Many Requests - 超過速率限制 |\n| 500 | Internal Server Error - 伺服器內部錯誤 |\n| 502 | Bad Gateway - 上游供應商錯誤 |\n| 503 | Service Unavailable - 所有管道皆失敗 |\n| 504 | Gateway Timeout - 請求逾時 |\n\n## 錯誤類型\n\n### 認證錯誤 (401)\n\n| 類型 | 代碼 | 描述 |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API key 缺失或無效 |\n| `expired_api_key` | `expired_api_key` | API key 已被撤銷 |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### 支付錯誤 (402)\n\n| 類型 | 代碼 | 描述 |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | 帳戶餘額過低 |\n| `quota_exceeded` | `quota_exceeded` | 已達到 API key 使用限制 |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### 存取錯誤 (403)\n\n| 類型 | 代碼 | 描述 |\n|------|------|-------------|\n| `access_denied` | `access_denied` | 存取資源被拒絕 |\n| `access_denied` | `model_not_allowed` | 此 API key 不允許使用該模型 |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### 驗證錯誤 (400)\n\n| 類型 | 描述 |\n|------|-------------|\n| `invalid_request_error` | 請求參數無效 |\n| `context_length_exceeded` | 輸入內容對模型而言過長 |\n| `model_not_found` | 請求的模型不存在 |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### 速率限制錯誤 (429)\n\n當您超過速率限制時：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**包含的標頭 (Headers)：**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### 內容過大 (413)\n\n當輸入或檔案大小超過限制時：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\n常見原因：\n- 圖片檔案過大 (最大 20MB)\n- 音訊檔案過大 (最大 25MB)\n- 輸入文字超過模型上下文長度 (context length)\n\n### 上游錯誤 (502, 503)\n\n| 類型 | 描述 |\n|------|-------------|\n| `upstream_error` | 供應商回傳錯誤 |\n| `all_channels_failed` | 無可用供應商 |\n| `timeout_error` | 請求逾時 |\n\n## 在 Python 中處理錯誤\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## 在 JavaScript 中處理錯誤\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"實作指數退避 (Exponential Backoff)\">\n    當遇到速率限制時，在重試之間逐漸增加等待時間：\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"設定逾時時間 (Timeouts)\">\n    務必設定合理的逾時時間，以避免請求掛起：\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 秒逾時\n    ```\n  </Accordion>\n\n  <Accordion title=\"記錄錯誤以供偵錯\">\n    記錄完整的錯誤回應（包括 request ID）以便尋求支援：\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"處理特定模型的錯誤\">\n    某些模型有特定要求（例如：最大 token 數、圖片格式）。\n    在發送請求前驗證輸入。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"エラーハンドリング\"\ndescription: \"APIエラーを適切に処理する\"\n---\n\n## エラーレスポンス形式\n\nすべてのエラーは一貫したJSON形式で返されます：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## HTTPステータスコード\n\n| コード | 説明 |\n|------|-------------|\n| 400 | Bad Request - 無効なパラメータ |\n| 401 | Unauthorized - APIキーが無効または不足 |\n| 402 | Payment Required - 残高不足 |\n| 403 | Forbidden - アクセス拒否またはモデルが許可されていない |\n| 404 | Not Found - モデルまたはリソースが見つからない |\n| 413 | Payload Too Large - 入力またはファイルサイズが上限を超過 |\n| 429 | Too Many Requests - レート制限を超過 |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - アップストリームプロバイダーのエラー |\n| 503 | Service Unavailable - すべてのチャネルが失敗 |\n| 504 | Gateway Timeout - リクエストがタイムアウト |\n\n## エラータイプ\n\n### 認証エラー (401)\n\n| タイプ | コード | 説明 |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | APIキーが不足しているか無効です |\n| `expired_api_key` | `expired_api_key` | APIキーが失効しています |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### 支払いエラー (402)\n\n| タイプ | コード | 説明 |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | アカウント残高が不足しています |\n| `quota_exceeded` | `quota_exceeded` | APIキーの使用制限に達しました |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### アクセスエラー (403)\n\n| タイプ | コード | 説明 |\n|------|------|-------------|\n| `access_denied` | `access_denied` | リソースへのアクセスが拒否されました |\n| `access_denied` | `model_not_allowed` | このAPIキーではモデルの使用が許可されていません |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### バリデーションエラー (400)\n\n| タイプ | 説明 |\n|------|-------------|\n| `invalid_request_error` | リクエストパラメータが無効です |\n| `context_length_exceeded` | 入力がモデルの制限に対して長すぎます |\n| `model_not_found` | リクエストされたモデルが存在しません |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### レート制限エラー (429)\n\nレート制限を超えた場合：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**含まれるヘッダー:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### ペイロード過大 (413)\n\n入力またはファイルサイズが制限を超えた場合：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\n主な原因：\n- 画像ファイルが大きすぎる（最大 20MB）\n- 音声ファイルが大きすぎる（最大 25MB）\n- 入力テキストがモデルのコンテキスト長を超えている\n\n### アップストリームエラー (502, 503)\n\n| タイプ | 説明 |\n|------|-------------|\n| `upstream_error` | プロバイダーがエラーを返しました |\n| `all_channels_failed` | 利用可能なプロバイダーがありません |\n| `timeout_error` | リクエストがタイムアウトしました |\n\n## Pythonでのエラーハンドリング\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## JavaScriptでのエラーハンドリング\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"エクスポネンシャルバックオフの実装\">\n    レート制限が発生した際、リトライの間隔を段階的に長くします：\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"タイムアウトの設定\">\n    リクエストのハングを避けるため、常に適切なタイムアウトを設定してください：\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"デバッグ用のエラーログ記録\">\n    サポートのために、リクエストIDを含む完全なエラーレスポンスをログに記録します：\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"モデル固有のエラーの処理\">\n    一部のモデルには特定の要件（最大トークン数、画像形式など）があります。リクエストを行う前に入力を検証してください。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"오류 처리\"\ndescription: \"API 오류를 원활하게 처리하기\"\n---\n\n## 오류 응답 형식\n\n모든 오류는 일관된 JSON 형식을 반환합니다:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## HTTP 상태 코드\n\n| 코드 | 설명 |\n|------|-------------|\n| 400 | Bad Request - 잘못된 파라미터 |\n| 401 | Unauthorized - 유효하지 않거나 누락된 API 키 |\n| 402 | Payment Required - 잔액 부족 |\n| 403 | Forbidden - 액세스 거부 또는 허용되지 않은 모델 |\n| 404 | Not Found - 모델 또는 리소스를 찾을 수 없음 |\n| 413 | Payload Too Large - 입력 또는 파일 크기 초과 |\n| 429 | Too Many Requests - 속도 제한 초과 |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - 업스트림 제공자 오류 |\n| 503 | Service Unavailable - 모든 채널 실패 |\n| 504 | Gateway Timeout - 요청 시간 초과 |\n\n## 오류 유형\n\n### 인증 오류 (401)\n\n| 유형 | 코드 | 설명 |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API 키가 누락되었거나 유효하지 않음 |\n| `expired_api_key` | `expired_api_key` | API 키가 취소됨 |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### 결제 오류 (402)\n\n| 유형 | 코드 | 설명 |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | 계정 잔액이 너무 낮음 |\n| `quota_exceeded` | `quota_exceeded` | API 키 사용량 한도 도달 |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### 액세스 오류 (403)\n\n| 유형 | 코드 | 설명 |\n|------|------|-------------|\n| `access_denied` | `access_denied` | 리소스 액세스 거부됨 |\n| `access_denied` | `model_not_allowed` | 이 API 키에 허용되지 않은 모델 |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### 유효성 검사 오류 (400)\n\n| 유형 | 설명 |\n|------|-------------|\n| `invalid_request_error` | 요청 파라미터가 유효하지 않음 |\n| `context_length_exceeded` | 모델에 비해 입력이 너무 김 |\n| `model_not_found` | 요청한 모델이 존재하지 않음 |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### 속도 제한 오류 (429)\n\n속도 제한을 초과한 경우:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**포함된 헤더:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### 페이로드 너무 큼 (413)\n\n입력 또는 파일 크기가 제한을 초과한 경우:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\n일반적인 원인:\n- 이미지 파일이 너무 큼 (최대 20MB)\n- 오디오 파일이 너무 큼 (최대 25MB)\n- 입력 텍스트가 모델 컨텍스트 길이를 초과함\n\n### 업스트림 오류 (502, 503)\n\n| 유형 | 설명 |\n|------|-------------|\n| `upstream_error` | 제공자가 오류를 반환함 |\n| `all_channels_failed` | 사용 가능한 제공자 없음 |\n| `timeout_error` | 요청 시간 초과 |\n\n## Python에서 오류 처리하기\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## JavaScript에서 오류 처리하기\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"지수 백오프 구현\">\n    속도 제한이 발생하면 재시도 간격을 점진적으로 늘리십시오:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"타임아웃 설정\">\n    요청이 중단되는 것을 방지하기 위해 항상 적절한 타임아웃을 설정하십시오:\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"디버깅을 위한 오류 로깅\">\n    지원을 받기 위해 요청 ID를 포함한 전체 오류 응답을 로깅하십시오:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"모델별 오류 처리\">\n    일부 모델에는 특정 요구 사항(예: 최대 토큰, 이미지 형식)이 있습니다.\n    요청을 보내기 전에 입력을 유효성 검사하십시오.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Fehlerbehandlung\"\ndescription: \"API-Fehler elegant behandeln\"\n---\n\n## Fehlerantwort-Format\n\nAlle Fehler geben ein konsistentes JSON-Format zurück:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## HTTP-Statuscodes\n\n| Code | Beschreibung |\n|------|-------------|\n| 400 | Bad Request – Ungültige Parameter |\n| 401 | Unauthorized – Ungültiger oder fehlender API-Key |\n| 402 | Payment Required – Unzureichendes Guthaben |\n| 403 | Forbidden – Zugriff verweigert oder Modell nicht erlaubt |\n| 404 | Not Found – Modell oder Ressource nicht gefunden |\n| 413 | Payload Too Large – Eingabe- oder Dateigröße überschritten |\n| 429 | Too Many Requests – Rate-Limit überschritten |\n| 500 | Interner Serverfehler |\n| 502 | Bad Gateway – Fehler beim Upstream-Provider |\n| 503 | Service Unavailable – Alle Kanäle fehlgeschlagen |\n| 504 | Gateway Timeout – Zeitüberschreitung der Anfrage |\n\n## Fehlertypen\n\n### Authentifizierungsfehler (401)\n\n| Typ | Code | Beschreibung |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API-Key fehlt oder ist ungültig |\n| `expired_api_key` | `expired_api_key` | API-Key wurde widerrufen |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Zahlungsfehler (402)\n\n| Typ | Code | Beschreibung |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | Kontoguthaben ist zu niedrig |\n| `quota_exceeded` | `quota_exceeded` | Nutzungslimit des API-Keys erreicht |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Zugriffsfehler (403)\n\n| Typ | Code | Beschreibung |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Zugriff auf Ressource verweigert |\n| `access_denied` | `model_not_allowed` | Modell für diesen API-Key nicht erlaubt |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Validierungsfehler (400)\n\n| Typ | Beschreibung |\n|------|-------------|\n| `invalid_request_error` | Anfrageparameter sind ungültig |\n| `context_length_exceeded` | Eingabe zu lang für das Modell |\n| `model_not_found` | Angefordertes Modell existiert nicht |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Rate-Limit-Fehler (429)\n\nWenn Sie Rate-Limits überschreiten:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**Enthaltene Header:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Payload Too Large (413)\n\nWenn die Eingabe- oder Dateigröße die Limits überschreitet:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nHäufige Ursachen:\n- Bilddatei zu groß (max. 20 MB)\n- Audiodatei zu groß (max. 25 MB)\n- Eingabetext überschreitet die Kontextlänge des Modells\n\n### Upstream-Fehler (502, 503)\n\n| Typ | Beschreibung |\n|------|-------------|\n| `upstream_error` | Provider hat einen Fehler zurückgegeben |\n| `all_channels_failed` | Keine verfügbaren Provider |\n| `timeout_error` | Zeitüberschreitung der Anfrage |\n\n## Fehlerbehandlung in Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## Fehlerbehandlung in JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Exponentielles Backoff implementieren\">\n    Warten Sie bei einem Rate-Limit schrittweise länger zwischen den Versuchen:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Timeouts festlegen\">\n    Legen Sie immer angemessene Timeouts fest, um hängende Anfragen zu vermeiden:\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"Fehler für das Debugging protokollieren\">\n    Protokollieren Sie die vollständige Fehlerantwort einschließlich der Request-ID für den Support:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Modellspezifische Fehler behandeln\">\n    Einige Modelle haben spezifische Anforderungen (z. B. maximale Tokens, Bildformate).\n    Validieren Sie Eingaben, bevor Sie Anfragen stellen.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"Gestion des erreurs\"\ndescription: \"Gérer les erreurs d'API avec élégance\"\n---\n\n## Format de réponse d'erreur\n\nToutes les erreurs renvoient un format JSON cohérent :\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## Codes d'état HTTP\n\n| Code | Description |\n|------|-------------|\n| 400 | Bad Request - Paramètres invalides |\n| 401 | Unauthorized - Clé API invalide ou manquante |\n| 402 | Payment Required - Solde insuffisant |\n| 403 | Forbidden - Accès refusé ou modèle non autorisé |\n| 404 | Not Found - Modèle ou ressource non trouvé |\n| 413 | Payload Too Large - Taille de l'entrée ou du fichier dépassée |\n| 429 | Too Many Requests - Limite de débit dépassée |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - Erreur du fournisseur en amont |\n| 503 | Service Unavailable - Tous les canaux ont échoué |\n| 504 | Gateway Timeout - La requête a expiré |\n\n## Types d'erreurs\n\n### Erreurs d'authentification (401)\n\n| Type | Code | Description |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | La clé API est manquante ou invalide |\n| `expired_api_key` | `expired_api_key` | La clé API a été révoquée |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Erreurs de paiement (402)\n\n| Type | Code | Description |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | Le solde du compte est trop bas |\n| `quota_exceeded` | `quota_exceeded` | Limite d'utilisation de la clé API atteinte |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Erreurs d'accès (403)\n\n| Type | Code | Description |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Accès à la ressource refusé |\n| `access_denied` | `model_not_allowed` | Modèle non autorisé pour cette clé API |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Erreurs de validation (400)\n\n| Type | Description |\n|------|-------------|\n| `invalid_request_error` | Les paramètres de la requête sont invalides |\n| `context_length_exceeded` | Entrée trop longue pour le modèle |\n| `model_not_found` | Le modèle demandé n'existe pas |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Erreurs de limite de débit (429)\n\nLorsque vous dépassez les limites de débit :\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**En-têtes inclus :**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Payload Too Large (413)\n\nLorsque la taille de l'entrée ou du fichier dépasse les limites :\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nCauses courantes :\n- Fichier image trop volumineux (max 20 Mo)\n- Fichier audio trop volumineux (max 25 Mo)\n- Le texte d'entrée dépasse la longueur de contexte du modèle\n\n### Erreurs en amont (502, 503)\n\n| Type | Description |\n|------|-------------|\n| `upstream_error` | Le fournisseur a renvoyé une erreur |\n| `all_channels_failed` | Aucun fournisseur disponible |\n| `timeout_error` | La requête a expiré |\n\n## Gestion des erreurs en Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## Gestion des erreurs en JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Implémenter un backoff exponentiel\">\n    En cas de limite de débit, attendez progressivement plus longtemps entre les tentatives :\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Définir des timeouts\">\n    Définissez toujours des délais d'expiration (timeouts) raisonnables pour éviter les requêtes bloquées :\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"Journaliser les erreurs pour le débogage\">\n    Journalisez la réponse d'erreur complète, y compris l'ID de requête pour le support :\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Gérer les erreurs spécifiques au modèle\">\n    Certains modèles ont des exigences spécifiques (par ex., max tokens, formats d'image).\n    Validez les entrées avant d'effectuer des requêtes.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Manejo de Errores\"\ndescription: \"Maneje los errores de la API de forma adecuada\"\n---\n\n## Formato de Respuesta de Error\n\nTodos los errores devuelven un formato JSON consistente:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## Códigos de Estado HTTP\n\n| Código | Descripción |\n|------|-------------|\n| 400 | Bad Request - Parámetros inválidos |\n| 401 | Unauthorized - API key inválida o faltante |\n| 402 | Payment Required - Saldo insuficiente |\n| 403 | Forbidden - Acceso denegado o modelo no permitido |\n| 404 | Not Found - Modelo o recurso no encontrado |\n| 413 | Payload Too Large - Tamaño de entrada o archivo excedido |\n| 429 | Too Many Requests - Límite de tasa excedido |\n| 500 | Internal Server Error - Error interno del servidor |\n| 502 | Bad Gateway - Error del proveedor upstream |\n| 503 | Service Unavailable - Todos los canales fallaron |\n| 504 | Gateway Timeout - La solicitud agotó el tiempo de espera |\n\n## Tipos de Error\n\n### Errores de Autenticación (401)\n\n| Tipo | Código | Descripción |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | La API key falta o es inválida |\n| `expired_api_key` | `expired_api_key` | La API key ha sido revocada |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Errores de Pago (402)\n\n| Tipo | Código | Descripción |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | El saldo de la cuenta es demasiado bajo |\n| `quota_exceeded` | `quota_exceeded` | Se ha alcanzado el límite de uso de la API key |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Errores de Acceso (403)\n\n| Tipo | Código | Descripción |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Acceso al recurso denegado |\n| `access_denied` | `model_not_allowed` | Modelo no permitido para esta API key |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Errores de Validación (400)\n\n| Tipo | Descripción |\n|------|-------------|\n| `invalid_request_error` | Los parámetros de la solicitud son inválidos |\n| `context_length_exceeded` | Entrada demasiado larga para el modelo |\n| `model_not_found` | El modelo solicitado no existe |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Errores de Límite de Tasa (429)\n\nCuando excede los límites de tasa:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**Encabezados incluidos:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Payload Too Large (413)\n\nCuando el tamaño de la entrada o del archivo excede los límites:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nCausas comunes:\n- Archivo de imagen demasiado grande (máx. 20MB)\n- Archivo de audio demasiado grande (máx. 25MB)\n- El texto de entrada excede la longitud de contexto del modelo\n\n### Errores de Upstream (502, 503)\n\n| Tipo | Descripción |\n|------|-------------|\n| `upstream_error` | El proveedor devolvió un error |\n| `all_channels_failed` | No hay proveedores disponibles |\n| `timeout_error` | La solicitud agotó el tiempo de espera |\n\n## Manejo de Errores en Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## Manejo de Errores en JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Mejores Prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Implementar exponential backoff\">\n    Cuando se aplique el límite de tasa, espere progresivamente más tiempo entre reintentos:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Establecer timeouts\">\n    Establezca siempre timeouts razonables para evitar solicitudes colgadas:\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"Registrar errores para depuración\">\n    Registre la respuesta de error completa, incluyendo el ID de la solicitud para soporte:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Manejar errores específicos del modelo\">\n    Algunos modelos tienen requisitos específicos (por ejemplo, tokens máximos, formatos de imagen).\n    Valide las entradas antes de realizar solicitudes.\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Tratamento de Erros\"\ndescription: \"Trate erros de API de forma elegante\"\n---\n\n## Formato de Resposta de Erro\n\nTodos os erros retornam um formato JSON consistente:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Opcional, para erros de validação\n  }\n}\n```\n\n## Códigos de Status HTTP\n\n| Código | Descrição |\n|------|-------------|\n| 400 | Bad Request - Parâmetros inválidos |\n| 401 | Unauthorized - Chave de API inválida ou ausente |\n| 402 | Payment Required - Saldo insuficiente |\n| 403 | Forbidden - Acesso negado ou modelo não permitido |\n| 404 | Not Found - Modelo ou recurso não encontrado |\n| 413 | Payload Too Large - Tamanho de entrada ou arquivo excedido |\n| 429 | Too Many Requests - Limite de taxa excedido |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - Erro do provedor upstream |\n| 503 | Service Unavailable - Todos os canais falharam |\n| 504 | Gateway Timeout - A requisição expirou |\n\n## Tipos de Erro\n\n### Erros de Autenticação (401)\n\n| Tipo | Código | Descrição |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | Chave de API ausente ou inválida |\n| `expired_api_key` | `expired_api_key` | Chave de API foi revogada |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Erros de Pagamento (402)\n\n| Tipo | Código | Descrição |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | Saldo da conta está muito baixo |\n| `quota_exceeded` | `quota_exceeded` | Limite de uso da chave de API atingido |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Erros de Acesso (403)\n\n| Tipo | Código | Descrição |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Acesso ao recurso negado |\n| `access_denied` | `model_not_allowed` | Modelo não permitido para esta chave de API |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Erros de Validação (400)\n\n| Tipo | Descrição |\n|------|-------------|\n| `invalid_request_error` | Parâmetros da requisição são inválidos |\n| `context_length_exceeded` | Entrada muito longa para o modelo |\n| `model_not_found` | O modelo solicitado não existe |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Erros de Limite de Taxa (429)\n\nQuando você excede os limites de taxa:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**Headers incluídos:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Payload Muito Grande (413)\n\nQuando o tamanho da entrada ou do arquivo excede os limites:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nCausas comuns:\n- Arquivo de imagem muito grande (máx. 20MB)\n- Arquivo de áudio muito grande (máx. 25MB)\n- Texto de entrada excede o comprimento de contexto do modelo\n\n### Erros de Upstream (502, 503)\n\n| Tipo | Descrição |\n|------|-------------|\n| `upstream_error` | O provedor retornou um erro |\n| `all_channels_failed` | Nenhum provedor disponível |\n| `timeout_error` | A requisição expirou |\n\n## Tratamento de Erros em Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## Tratamento de Erros em JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Implemente exponential backoff\">\n    Quando houver limite de taxa, aguarde progressivamente mais tempo entre as tentativas:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Defina timeouts\">\n    Sempre defina timeouts razoáveis para evitar requisições travadas:\n    ```python\n    client = OpenAI(timeout=60.0)  # Timeout de 60 segundos\n    ```\n  </Accordion>\n\n  <Accordion title=\"Registre logs de erros para depuração\">\n    Registre a resposta de erro completa, incluindo o ID da requisição para suporte:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Trate erros específicos do modelo\">\n    Alguns modelos possuem requisitos específicos (ex: tokens máximos, formatos de imagem).\n    Valide as entradas antes de fazer as requisições.\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"معالجة الأخطاء\"\ndescription: \"معالجة أخطاء API بسلاسة\"\n---\n\n## تنسيق استجابة الخطأ\n\nتعيد جميع الأخطاء تنسيق JSON ثابتاً:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // اختياري، لأخطاء التحقق من الصحة\n  }\n}\n```\n\n## رموز حالة HTTP\n\n| الرمز | الوصف |\n|------|-------------|\n| 400 | Bad Request - معلمات غير صالحة |\n| 401 | Unauthorized - مفتاح API غير صالح أو مفقود |\n| 402 | Payment Required - رصيد غير كافٍ |\n| 403 | Forbidden - تم رفض الوصول أو النموذج غير مسموح به |\n| 404 | Not Found - النموذج أو المورد غير موجود |\n| 413 | Payload Too Large - تم تجاوز حجم المدخلات أو الملف |\n| 429 | Too Many Requests - تم تجاوز حد المعدل (Rate limit) |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - خطأ من المزود الرئيسي (Upstream) |\n| 503 | Service Unavailable - فشلت جميع القنوات |\n| 504 | Gateway Timeout - انتهت مهلة الطلب |\n\n## أنواع الأخطاء\n\n### أخطاء المصادقة (401)\n\n| النوع | الرمز | الوصف |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | مفتاح API مفقود أو غير صالح |\n| `expired_api_key` | `expired_api_key` | تم إلغاء مفتاح API |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### أخطاء الدفع (402)\n\n| النوع | الرمز | الوصف |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | رصيد الحساب منخفض جداً |\n| `quota_exceeded` | `quota_exceeded` | تم الوصول إلى حد استخدام مفتاح API |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### أخطاء الوصول (403)\n\n| النوع | الرمز | الوصف |\n|------|------|-------------|\n| `access_denied` | `access_denied` | تم رفض الوصول إلى المورد |\n| `access_denied` | `model_not_allowed` | النموذج غير مسموح به لمفتاح API هذا |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### أخطاء التحقق من الصحة (400)\n\n| النوع | الوصف |\n|------|-------------|\n| `invalid_request_error` | معلمات الطلب غير صالحة |\n| `context_length_exceeded` | المدخلات طويلة جداً بالنسبة للنموذج |\n| `model_not_found` | النموذج المطلوب غير موجود |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### أخطاء حد المعدل (429)\n\nعندما تتجاوز حدود المعدل (rate limits):\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**الرؤوس (Headers) المضمنة:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### الحمولة كبيرة جداً (413)\n\nعندما يتجاوز حجم المدخلات أو الملف الحدود المسموح بها:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nالأسباب الشائعة:\n- ملف الصورة كبير جداً (بحد أقصى 20 ميجابايت)\n- ملف الصوت كبير جداً (بحد أقصى 25 ميجابايت)\n- نص المدخلات يتجاوز طول سياق (context length) النموذج\n\n### أخطاء المزود (502, 503)\n\n| النوع | الوصف |\n|------|-------------|\n| `upstream_error` | أعاد المزود خطأً |\n| `all_channels_failed` | لا يوجد مزودون متاحون |\n| `timeout_error` | انتهت مهلة الطلب |\n\n## معالجة الأخطاء في Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## معالجة الأخطاء في JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"تنفيذ التراجع الأسي (Exponential backoff)\">\n    عند تقييد المعدل، انتظر لفترات أطول تدريجياً بين محاولات إعادة المحاولة:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"تعيين مهلات زمنية (Timeouts)\">\n    قم دائماً بتعيين مهلات زمنية معقولة لتجنب الطلبات المعلقة:\n    ```python\n    client = OpenAI(timeout=60.0)  # مهلة 60 ثانية\n    ```\n  </Accordion>\n\n  <Accordion title=\"تسجيل الأخطاء لتصحيح الأخطاء (Debugging)\">\n    قم بتسجيل استجابة الخطأ الكاملة بما في ذلك معرف الطلب (request ID) للدعم الفني:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"معالجة الأخطاء الخاصة بالنموذج\">\n    بعض النماذج لها متطلبات محددة (مثل الحد الأقصى للرموز المميزة \"tokens\"، تنسيقات الصور).\n    تحقق من صحة المدخلات قبل إجراء الطلبات.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Xử lý lỗi\"\ndescription: \"Xử lý các lỗi API một cách khéo léo\"\n---\n\n## Định dạng phản hồi lỗi\n\nTất cả các lỗi đều trả về một định dạng JSON nhất quán:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## Mã trạng thái HTTP\n\n| Mã | Mô tả |\n|------|-------------|\n| 400 | Bad Request - Các tham số không hợp lệ |\n| 401 | Unauthorized - API key không hợp lệ hoặc bị thiếu |\n| 402 | Payment Required - Số dư không đủ |\n| 403 | Forbidden - Truy cập bị từ chối hoặc mô hình không được phép |\n| 404 | Not Found - Không tìm thấy mô hình hoặc tài nguyên |\n| 413 | Payload Too Large - Kích thước đầu vào hoặc tệp vượt quá giới hạn |\n| 429 | Too Many Requests - Vượt quá giới hạn tốc độ (Rate limit) |\n| 500 | Internal Server Error - Lỗi máy chủ nội bộ |\n| 502 | Bad Gateway - Lỗi từ nhà cung cấp thượng nguồn (Upstream provider) |\n| 503 | Service Unavailable - Tất cả các kênh đều thất bại |\n| 504 | Gateway Timeout - Yêu cầu quá hạn |\n\n## Các loại lỗi\n\n### Lỗi xác thực (401)\n\n| Loại | Mã | Mô tả |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API key bị thiếu hoặc không hợp lệ |\n| `expired_api_key` | `expired_api_key` | API key đã bị thu hồi |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Lỗi thanh toán (402)\n\n| Loại | Mã | Mô tả |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | Số dư tài khoản quá thấp |\n| `quota_exceeded` | `quota_exceeded` | Đã đạt đến giới hạn sử dụng API key |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Lỗi truy cập (403)\n\n| Loại | Mã | Mô tả |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Truy cập vào tài nguyên bị từ chối |\n| `access_denied` | `model_not_allowed` | Mô hình không được phép cho API key này |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Lỗi xác thực dữ liệu (400)\n\n| Loại | Mô tả |\n|------|-------------|\n| `invalid_request_error` | Các tham số yêu cầu không hợp lệ |\n| `context_length_exceeded` | Đầu vào quá dài đối với mô hình |\n| `model_not_found` | Mô hình được yêu cầu không tồn tại |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Lỗi giới hạn tốc độ (429)\n\nKhi bạn vượt quá giới hạn tốc độ (rate limits):\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**Các Header bao gồm:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Dữ liệu quá lớn (413)\n\nKhi kích thước đầu vào hoặc tệp vượt quá giới hạn:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nCác nguyên nhân phổ biến:\n- Tệp hình ảnh quá lớn (tối đa 20MB)\n- Tệp âm thanh quá lớn (tối đa 25MB)\n- Văn bản đầu vào vượt quá độ dài ngữ cảnh (context length) của mô hình\n\n### Lỗi từ phía thượng nguồn (502, 503)\n\n| Loại | Mô tả |\n|------|-------------|\n| `upstream_error` | Nhà cung cấp trả về một lỗi |\n| `all_channels_failed` | Không có nhà cung cấp nào khả dụng |\n| `timeout_error` | Yêu cầu đã quá hạn |\n\n## Xử lý lỗi trong Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## Xử lý lỗi trong JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Các phương pháp hay nhất\n\n<AccordionGroup>\n  <Accordion title=\"Triển khai exponential backoff\">\n    Khi bị giới hạn tốc độ, hãy đợi lâu dần giữa các lần thử lại:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Thiết lập thời gian chờ (timeouts)\">\n    Luôn thiết lập thời gian chờ hợp lý để tránh các yêu cầu bị treo:\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 second timeout\n    ```\n  </Accordion>\n\n  <Accordion title=\"Ghi nhật ký lỗi để gỡ lỗi\">\n    Ghi lại toàn bộ phản hồi lỗi bao gồm request ID để được hỗ trợ:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Xử lý các lỗi đặc thù của mô hình\">\n    Một số mô hình có các yêu cầu cụ thể (ví dụ: max tokens, định dạng hình ảnh).\n    Hãy xác thực đầu vào trước khi thực hiện yêu cầu.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Penanganan Kesalahan\"\ndescription: \"Menangani kesalahan API dengan baik\"\n---\n\n## Format Respons Kesalahan\n\nSemua kesalahan mengembalikan format JSON yang konsisten:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Opsional, untuk kesalahan validasi\n  }\n}\n```\n\n## Kode Status HTTP\n\n| Kode | Deskripsi |\n|------|-------------|\n| 400 | Bad Request - Parameter tidak valid |\n| 401 | Unauthorized - API key tidak valid atau hilang |\n| 402 | Payment Required - Saldo tidak mencukupi |\n| 403 | Forbidden - Akses ditolak atau model tidak diizinkan |\n| 404 | Not Found - Model atau sumber daya tidak ditemukan |\n| 413 | Payload Too Large - Ukuran input atau file terlampaui |\n| 429 | Too Many Requests - Batas laju (rate limit) terlampaui |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - Kesalahan penyedia upstream |\n| 503 | Service Unavailable - Semua saluran gagal |\n| 504 | Gateway Timeout - Permintaan waktu habis (timed out) |\n\n## Tipe Kesalahan\n\n### Kesalahan Autentikasi (401)\n\n| Tipe | Kode | Deskripsi |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API key hilang atau tidak valid |\n| `expired_api_key` | `expired_api_key` | API key telah dicabut |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Kesalahan Pembayaran (402)\n\n| Tipe | Kode | Deskripsi |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | Saldo akun terlalu rendah |\n| `quota_exceeded` | `quota_exceeded` | Batas penggunaan API key tercapai |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Kesalahan Akses (403)\n\n| Tipe | Kode | Deskripsi |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Akses ke sumber daya ditolak |\n| `access_denied` | `model_not_allowed` | Model tidak diizinkan untuk API key ini |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Kesalahan Validasi (400)\n\n| Tipe | Deskripsi |\n|------|-------------|\n| `invalid_request_error` | Parameter permintaan tidak valid |\n| `context_length_exceeded` | Input terlalu panjang untuk model |\n| `model_not_found` | Model yang diminta tidak ada |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Kesalahan Batas Laju (429)\n\nSaat Anda melampaui batas laju (rate limits):\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**Header yang disertakan:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Payload Terlalu Besar (413)\n\nSaat ukuran input atau file melampaui batas:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nPenyebab umum:\n- File gambar terlalu besar (maks 20MB)\n- File audio terlalu besar (maks 25MB)\n- Teks input melampaui panjang konteks model\n\n### Kesalahan Upstream (502, 503)\n\n| Tipe | Deskripsi |\n|------|-------------|\n| `upstream_error` | Penyedia mengembalikan kesalahan |\n| `all_channels_failed` | Tidak ada penyedia yang tersedia |\n| `timeout_error` | Permintaan waktu habis (timed out) |\n\n## Menangani Kesalahan di Python\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## Menangani Kesalahan di JavaScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Terapkan exponential backoff\">\n    Saat terkena pembatasan laju (rate limited), tunggu lebih lama secara progresif di antara percobaan ulang:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Atur timeout\">\n    Selalu atur timeout yang wajar untuk menghindari permintaan yang menggantung:\n    ```python\n    client = OpenAI(timeout=60.0)  # Timeout 60 detik\n    ```\n  </Accordion>\n\n  <Accordion title=\"Catat log kesalahan untuk debugging\">\n    Catat log respons kesalahan lengkap termasuk ID permintaan untuk dukungan:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Tangani kesalahan spesifik model\">\n    Beberapa model memiliki persyaratan khusus (misalnya, token maksimum, format gambar).\n    Validasi input sebelum membuat permintaan.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"Hata Yönetimi\"\ndescription: \"API hatalarını sorunsuz bir şekilde yönetin\"\n---\n\n## Hata Yanıt Formatı\n\nTüm hatalar tutarlı bir JSON formatı döndürür:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Human-readable error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\",\n    \"param\": \"parameter_name\"  // Optional, for validation errors\n  }\n}\n```\n\n## HTTP Durum Kodları\n\n| Kod | Açıklama |\n|------|-------------|\n| 400 | Bad Request - Geçersiz parametreler |\n| 401 | Unauthorized - Geçersiz veya eksik API anahtarı |\n| 402 | Payment Required - Yetersiz bakiye |\n| 403 | Forbidden - Erişim reddedildi veya modele izin verilmiyor |\n| 404 | Not Found - Model veya kaynak bulunamadı |\n| 413 | Payload Too Large - Girdi veya dosya boyutu aşıldı |\n| 429 | Too Many Requests - Hız sınırı aşıldı |\n| 500 | Internal Server Error |\n| 502 | Bad Gateway - Üst sağlayıcı hatası |\n| 503 | Service Unavailable - Tüm kanallar başarısız oldu |\n| 504 | Gateway Timeout - İstek zaman aşımına uğradı |\n\n## Hata Türleri\n\n### Kimlik Doğrulama Hataları (401)\n\n| Tür | Kod | Açıklama |\n|------|------|-------------|\n| `invalid_api_key` | `invalid_api_key` | API anahtarı eksik veya geçersiz |\n| `expired_api_key` | `expired_api_key` | API anahtarı iptal edilmiş |\n\n```python\nfrom openai import OpenAI, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept AuthenticationError as e:\n    print(f\"Authentication failed: {e.message}\")\n```\n\n### Ödeme Hataları (402)\n\n| Tür | Kod | Açıklama |\n|------|------|-------------|\n| `insufficient_quota` | `insufficient_quota` | Hesap bakiyesi çok düşük |\n| `quota_exceeded` | `quota_exceeded` | API anahtarı kullanım sınırına ulaşıldı |\n\n```python\nfrom openai import OpenAI, APIStatusError\n\ntry:\n    response = client.chat.completions.create(...)\nexcept APIStatusError as e:\n    if e.status_code == 402:\n        print(\"Please top up your account balance\")\n```\n\n### Erişim Hataları (403)\n\n| Tür | Kod | Açıklama |\n|------|------|-------------|\n| `access_denied` | `access_denied` | Kaynağa erişim reddedildi |\n| `access_denied` | `model_not_allowed` | Bu API anahtarı için modele izin verilmiyor |\n\n```json\n{\n  \"error\": {\n    \"message\": \"You don't have permission to access this model\",\n    \"type\": \"access_denied\",\n    \"code\": \"model_not_allowed\"\n  }\n}\n```\n\n### Doğrulama Hataları (400)\n\n| Tür | Açıklama |\n|------|-------------|\n| `invalid_request_error` | İstek parametreleri geçersiz |\n| `context_length_exceeded` | Girdi model için çok uzun |\n| `model_not_found` | İstenen model mevcut değil |\n\n```json\n{\n  \"error\": {\n    \"message\": \"model: 'invalid-model' is not a valid model\",\n    \"type\": \"model_not_found\",\n    \"param\": \"model\"\n  }\n}\n```\n\n### Hız Sınırı Hataları (429)\n\nHız sınırlarını aştığınızda:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_error\",\n    \"code\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n**Dahil edilen başlıklar:**\n\n```\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1234567890\nRetry-After: 60\n```\n\n### Payload Çok Büyük (413)\n\nGirdi veya dosya boyutu sınırları aştığında:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Input size exceeds maximum allowed\",\n    \"type\": \"invalid_request_error\",\n    \"code\": \"payload_too_large\"\n  }\n}\n```\n\nYaygın nedenler:\n- Resim dosyası çok büyük (maks 20MB)\n- Ses dosyası çok büyük (maks 25MB)\n- Girdi metni model bağlam uzunluğunu aşıyor\n\n### Üst Sunucu Hataları (502, 503)\n\n| Tür | Açıklama |\n|------|-------------|\n| `upstream_error` | Sağlayıcı bir hata döndürdü |\n| `all_channels_failed` | Kullanılabilir sağlayıcı yok |\n| `timeout_error` | İstek zaman aşımına uğradı |\n\n## Python'da Hataları Yönetme\n\n```python\nfrom openai import OpenAI, APIError, RateLimitError, APIConnectionError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef chat_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt < max_retries - 1:\n                import time\n                time.sleep(2 ** attempt)  # Exponential backoff\n                continue\n            raise\n        except APIConnectionError as e:\n            print(f\"Connection error: {e}\")\n            raise\n        except APIError as e:\n            print(f\"API error: {e.status_code} - {e.message}\")\n            raise\n```\n\n## JavaScript'te Hataları Yönetme\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nasync function chatWithRetry(messages, maxRetries = 3) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await client.chat.completions.create({\n        model: 'gpt-4o',\n        messages\n      });\n    } catch (error) {\n      if (error instanceof OpenAI.RateLimitError) {\n        if (attempt < maxRetries - 1) {\n          await new Promise(r => setTimeout(r, 2 ** attempt * 1000));\n          continue;\n        }\n      }\n      throw error;\n    }\n  }\n}\n```\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Üstel geri çekilme uygulayın\">\n    Hız sınırına takıldığınızda, denemeler arasında kademeli olarak daha uzun süre bekleyin:\n    ```python\n    wait_time = 2 ** attempt  # 1s, 2s, 4s, 8s...\n    ```\n  </Accordion>\n\n  <Accordion title=\"Zaman aşımlarını ayarlayın\">\n    Asılı kalan istekleri önlemek için her zaman makul zaman aşımları belirleyin:\n    ```python\n    client = OpenAI(timeout=60.0)  # 60 saniyelik zaman aşımı\n    ```\n  </Accordion>\n\n  <Accordion title=\"Hata ayıklama için hataları günlüğe kaydedin\">\n    Destek almak için istek kimliği (request ID) dahil olmak üzere tam hata yanıtını günlüğe kaydedin:\n    ```python\n    except APIError as e:\n        logger.error(f\"API Error: {e.status_code} - {e.message}\")\n    ```\n  </Accordion>\n\n  <Accordion title=\"Modele özgü hataları yönetin\">\n    Bazı modellerin belirli gereksinimleri vardır (örneğin, maksimum token sayısı, resim formatları). İstek yapmadan önce girdileri doğrulayın.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:32:25.458Z"
  },
  "guides/rate-limits.mdx": {
    "sourceHash": "68ef10ccbd00bf44",
    "translations": {
      "zh": "---\ntitle: \"速率限制\"\ndescription: \"了解并处理速率限制\"\n---\n\n## 概览\n\nLemonData 实施速率限制以确保公平使用和平台稳定性。限制因账户层级而异。\n\n## 速率限制层级\n\n| 层级 | 请求数/分钟 | 描述 |\n|------|-------------|-------------|\n| **User** | 60 | 所有账户的默认层级 |\n| **Partner** | 300 | 适用于集成合作伙伴 |\n| **VIP** | 1,000 | 高用量用户 |\n\n<Note>\n  速率限制可能会发生变化。如需自定义限制，请联系 support@lemondata.cc。\n</Note>\n\n## 速率限制响应头\n\n每个 API 响应都包含速率限制信息：\n\n```\nX-RateLimit-Limit: 60          # 您每分钟的限制\nX-RateLimit-Remaining: 55      # 剩余请求数\nX-RateLimit-Reset: 1234567890  # 限制重置的 Unix 时间戳\n```\n\n## 超过速率限制\n\n当您超过限制时，将收到 `429` 响应：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n附带额外的响应头：\n```\nRetry-After: 60  # 重试前需要等待的秒数\n```\n\n## 处理速率限制\n\n### 指数退避\n\n为自动重试实现指数退避：\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 秒\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### 请求队列\n\n对于高并发应用，请实现请求队列：\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # 如果需要，等待以遵守速率限制\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### 批量处理\n\n对于批量操作，请进行带延迟的分批处理：\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # 批次之间的停顿\n    return results\n```\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"监控您的用量\">\n    跟踪速率限制响应头，主动保持在限制范围内。\n  </Accordion>\n\n  <Accordion title=\"实现缓存\">\n    对相同请求的响应进行缓存，以减少 API 调用。\n  </Accordion>\n\n  <Accordion title=\"使用合适的模型\">\n    更快的模型（如 gpt-4o-mini）允许更高的吞吐量。\n  </Accordion>\n\n  <Accordion title=\"联系我们获取更高限制\">\n    如果您需要更高的限制，请联系 support@lemondata.cc。\n  </Accordion>\n</AccordionGroup>\n\n## 升级您的层级\n\n如需申请层级升级：\n\n1. 登录您的 [控制面板](https://lemondata.cc/dashboard)\n2. 前往 **Settings → Account**\n3. 联系支持团队并说明您的使用场景\n\n或者发送邮件至 support@lemondata.cc，并提供以下信息：\n- 您的账户邮箱\n- 预期的请求量\n- 使用场景描述",
      "zh-TW": "---\ntitle: \"速率限制\"\ndescription: \"了解並處理速率限制\"\n---\n\n## 概覽\n\nLemonData 實施速率限制以確保公平使用和平台穩定性。限制因帳戶層級而異。\n\n## 速率限制層級\n\n| 層級 | 請求數/分鐘 | 描述 |\n|------|-------------|-------------|\n| **User** | 60 | 所有帳戶的預設層級 |\n| **Partner** | 300 | 針對整合合作夥伴 |\n| **VIP** | 1,000 | 高用量使用者 |\n\n<Note>\n  速率限制可能會有所變動。請聯繫 support@lemondata.cc 以獲取自定義限制。\n</Note>\n\n## 速率限制標頭\n\n每個 API 回應都包含速率限制資訊：\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## 已超出速率限制\n\n當您超出限制時，您將收到 `429` 回應：\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n附帶額外標頭：\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## 處理速率限制\n\n### 指數退避 (Exponential Backoff)\n\n實作指數退避以進行自動重試：\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### 請求隊列\n\n對於高用量應用程式，請實作請求隊列：\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### 批次處理\n\n對於大量操作，請使用帶有延遲的批次處理：\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"監控您的使用情況\">\n    追蹤速率限制標頭以主動保持在限制範圍內。\n  </Accordion>\n\n  <Accordion title=\"實作快取\">\n    針對相同的請求快取回應，以減少 API 呼叫。\n  </Accordion>\n\n  <Accordion title=\"使用合適的模型\">\n    更快的模型（如 gpt-4o-mini）可提供更高的吞吐量。\n  </Accordion>\n\n  <Accordion title=\"聯繫我們以獲取更高限制\">\n    如果您需要更高的限制，請聯繫 support@lemondata.cc。\n  </Accordion>\n</AccordionGroup>\n\n## 升級您的層級\n\n若要申請層級升級：\n\n1. 登入您的 [控制台](https://lemondata.cc/dashboard)\n2. 前往 **Settings → Account**\n3. 聯繫支援團隊並說明您的使用場景\n\n或發送電子郵件至 support@lemondata.cc，並提供：\n- 您的帳戶電子郵件\n- 預期的請求量\n- 使用場景描述",
      "ja": "---\ntitle: \"レート制限\"\ndescription: \"レート制限の理解と対応\"\n---\n\n## 概要\n\nLemonDataは、公平な利用とプラットフォームの安定性を確保するためにレート制限を導入しています。制限はアカウントのティアによって異なります。\n\n## レート制限ティア\n\n| ティア | リクエスト/分 | 説明 |\n|------|-------------|-------------|\n| **User** | 60 | すべてのアカウントのデフォルトティア |\n| **Partner** | 300 | 統合パートナー向け |\n| **VIP** | 1,000 | 大量のリクエストを行うユーザー |\n\n<Note>\n  レート制限は変更される場合があります。カスタム制限については support@lemondata.cc までお問い合わせください。\n</Note>\n\n## レート制限ヘッダー\n\nすべての API レスポンスには、レート制限に関する情報が含まれています。\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## レート制限の超過\n\n制限を超えると、`429` レスポンスが返されます。\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n追加のヘッダー：\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## レート制限への対応\n\n### 指数バックオフ\n\n自動再試行のために指数バックオフを実装します。\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### リクエストキューイング\n\n大量のリクエストを処理するアプリケーションの場合は、リクエストキューを実装してください。\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### バッチ処理\n\n一括操作の場合は、遅延を挟みながらバッチ処理を行います。\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"利用状況の監視\">\n    レート制限ヘッダーを追跡し、プロアクティブに制限内に収まるようにします。\n  </Accordion>\n\n  <Accordion title=\"キャッシュの実装\">\n    同一のリクエストに対するレスポンスをキャッシュし、API 呼び出し回数を削減します。\n  </Accordion>\n\n  <Accordion title=\"適切なモデルの使用\">\n    より高速なモデル（gpt-4o-mini など）を使用することで、スループットを向上させることができます。\n  </Accordion>\n\n  <Accordion title=\"制限の引き上げについてはお問い合わせください\">\n    より高い制限が必要な場合は、support@lemondata.cc までご連絡ください。\n  </Accordion>\n</AccordionGroup>\n\n## ティアのアップグレード\n\nティアのアップグレードをリクエストするには：\n\n1. [ダッシュボード](https://lemondata.cc/dashboard)にログインします\n2. **Settings → Account** に移動します\n3. ユースケースを添えてサポートに連絡します\n\nまたは、以下の内容を support@lemondata.cc までメールでお送りください：\n- アカウントのメールアドレス\n- 予想されるリクエスト量\n- ユースケースの説明",
      "ko": "---\ntitle: \"속도 제한\"\ndescription: \"속도 제한의 이해 및 처리 방법\"\n---\n\n## 개요\n\nLemonData는 공정한 사용과 플랫폼 안정성을 보장하기 위해 속도 제한(rate limits)을 적용합니다. 제한 사항은 계정 등급에 따라 다릅니다.\n\n## 속도 제한 등급\n\n| 등급 | 분당 요청 수 (Requests/min) | 설명 |\n|------|-------------|-------------|\n| **User** | 60 | 모든 계정의 기본 등급 |\n| **Partner** | 300 | 통합 파트너용 |\n| **VIP** | 1,000 | 대량 사용 유저 |\n\n<Note>\n  속도 제한은 변경될 수 있습니다. 맞춤형 제한이 필요한 경우 support@lemondata.cc로 문의해 주세요.\n</Note>\n\n## 속도 제한 헤더\n\n모든 API 응답에는 속도 제한 정보가 포함됩니다:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## 속도 제한 초과\n\n제한을 초과하면 `429` 응답을 받게 됩니다:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\n추가 헤더 포함:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## 속도 제한 처리\n\n### 지수 백오프 (Exponential Backoff)\n\n자동 재시도를 위해 지수 백오프를 구현하세요:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### 요청 큐잉 (Request Queuing)\n\n대량의 요청이 발생하는 애플리케이션의 경우, 요청 큐를 구현하세요:\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### 배치 처리 (Batch Processing)\n\n대량 작업의 경우, 지연 시간을 두고 배치 단위로 처리하세요:\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"사용량 모니터링\">\n    속도 제한 헤더를 추적하여 선제적으로 제한 범위 내를 유지하세요.\n  </Accordion>\n\n  <Accordion title=\"캐싱 구현\">\n    동일한 요청에 대한 응답을 캐싱하여 API 호출 횟수를 줄이세요.\n  </Accordion>\n\n  <Accordion title=\"적절한 모델 사용\">\n    더 빠른 모델(예: gpt-4o-mini)은 더 높은 처리량을 제공합니다.\n  </Accordion>\n\n  <Accordion title=\"높은 제한이 필요한 경우 문의\">\n    더 높은 제한이 필요한 경우 support@lemondata.cc로 문의하세요.\n  </Accordion>\n</AccordionGroup>\n\n## 등급 업그레이드\n\n등급 업그레이드를 요청하려면:\n\n1. [대시보드](https://lemondata.cc/dashboard)에 로그인합니다.\n2. **Settings → Account**로 이동합니다.\n3. 사용 사례와 함께 고객 지원팀에 문의하세요.\n\n또는 다음 내용을 포함하여 support@lemondata.cc로 이메일을 보내주세요:\n- 계정 이메일\n- 예상 요청량\n- 사용 사례 설명",
      "de": "\n---\ntitle: \"Rate-Limits\"\ndescription: \"Verständnis und Handhabung von Rate-Limits\"\n---\n\n## Übersicht\n\nLemonData implementiert Rate-Limits, um eine faire Nutzung und Plattformstabilität zu gewährleisten. Die Limits variieren je nach Account-Stufe.\n\n## Rate-Limit-Stufen\n\n| Stufe | Anfragen/Min. | Beschreibung |\n|------|-------------|-------------|\n| **User** | 60 | Standard-Stufe für alle Accounts |\n| **Partner** | 300 | Für Integrationspartner |\n| **VIP** | 1.000 | Nutzer mit hohem Volumen |\n\n<Note>\n  Rate-Limits können sich ändern. Kontaktieren Sie support@lemondata.cc für individuelle Limits.\n</Note>\n\n## Rate-Limit-Header\n\nJede API-Antwort enthält Informationen zum Rate-Limit:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## Rate-Limit überschritten\n\nWenn Sie das Limit überschreiten, erhalten Sie eine `429`-Antwort:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nMit zusätzlichem Header:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## Handhabung von Rate-Limits\n\n### Exponential Backoff\n\nImplementieren Sie Exponential Backoff für automatische Wiederholungsversuche:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### Request-Queuing\n\nImplementieren Sie für Anwendungen mit hohem Volumen eine Anfrage-Warteschlange (Request Queue):\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm",
      "fr": "---\ntitle: \"Limites de débit\"\ndescription: \"Comprendre et gérer les limites de débit\"\n---\n\n## Aperçu\n\nLemonData implémente des limites de débit pour garantir une utilisation équitable et la stabilité de la plateforme. Les limites varient selon le niveau du compte.\n\n## Niveaux de limites de débit\n\n| Niveau | Requêtes/min | Description |\n|------|-------------|-------------|\n| **Utilisateur** | 60 | Niveau par défaut pour tous les comptes |\n| **Partenaire** | 300 | Pour les partenaires d'intégration |\n| **VIP** | 1 000 | Utilisateurs à volume élevé |\n\n<Note>\n  Les limites de débit sont sujettes à modification. Contactez support@lemondata.cc pour des limites personnalisées.\n</Note>\n\n## En-têtes de limite de débit\n\nChaque réponse API inclut des informations sur la limite de débit :\n\n```\nX-RateLimit-Limit: 60          # Votre limite par minute\nX-RateLimit-Remaining: 55      # Requêtes restantes\nX-RateLimit-Reset: 1234567890  # Horodatage Unix lors de la réinitialisation de la limite\n```\n\n## Limite de débit dépassée\n\nLorsque vous dépassez la limite, vous recevrez une réponse `429` :\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nAvec un en-tête supplémentaire :\n```\nRetry-After: 60  # Secondes à attendre avant de réessayer\n```\n\n## Gestion des limites de débit\n\n### Backoff exponentiel\n\nImplémentez un backoff exponentiel pour les tentatives automatiques :\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 secondes\n            print(f\"Limite de débit atteinte. Attente de {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### Mise en file d'attente des requêtes\n\nPour les applications à volume élevé, implémentez une file d'attente de requêtes :\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Attendre si nécessaire pour respecter la limite de débit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### Traitement par lots\n\nPour les opérations en masse, traitez par lots avec des délais :\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause entre les lots\n    return results\n```\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Surveillez votre utilisation\">\n    Suivez les en-têtes de limite de débit pour rester proactivement en dessous des limites.\n  </Accordion>\n\n  <Accordion title=\"Implémentez la mise en cache\">\n    Mettez en cache les réponses pour les requêtes identiques afin de réduire les appels API.\n  </Accordion>\n\n  <Accordion title=\"Utilisez des modèles appropriés\">\n    Les modèles plus rapides (comme gpt-4o-mini) permettent un débit plus élevé.\n  </Accordion>\n\n  <Accordion title=\"Contactez-nous pour des limites plus élevées\">\n    Si vous avez besoin de limites plus élevées, contactez support@lemondata.cc.\n  </Accordion>\n</AccordionGroup>\n\n## Mise à niveau de votre niveau\n\nPour demander une mise à niveau de niveau :\n\n1. Connectez-vous à votre [Tableau de bord](https://lemondata.cc/dashboard)\n2. Allez dans **Paramètres → Compte**\n3. Contactez le support avec votre cas d'utilisation\n\nOu envoyez un e-mail à support@lemondata.cc avec :\n- L'e-mail de votre compte\n- Le volume de requêtes attendu\n- La description du cas d'utilisation",
      "pt": "---\ntitle: \"Limites de Taxa\"\ndescription: \"Entendendo e gerenciando limites de taxa\"\n---\n\n## Visão Geral\n\nA LemonData implementa limites de taxa para garantir o uso justo e a estabilidade da plataforma. Os limites variam de acordo com o nível da conta.\n\n## Níveis de Limite de Taxa\n\n| Nível | Requisições/min | Descrição |\n|------|-------------|-------------|\n| **Usuário** | 60 | Nível padrão para todas as contas |\n| **Parceiro** | 300 | Para parceiros de integração |\n| **VIP** | 1.000 | Usuários de alto volume |\n\n<Note>\n  Os limites de taxa estão sujeitos a alterações. Entre em contato com support@lemondata.cc para limites personalizados.\n</Note>\n\n## Cabeçalhos de Limite de Taxa\n\nCada resposta da API inclui informações sobre o limite de taxa:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## Limite de Taxa Excedido\n\nQuando você exceder o limite, receberá uma resposta `429`:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nCom cabeçalho adicional:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## Gerenciando Limites de Taxa\n\n### Backoff Exponencial\n\nImplemente backoff exponencial para tentativas automáticas:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### Fila de Requisições\n\nPara aplicações de alto volume, implemente uma fila de requisições:\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_",
      "ar": "---\ntitle: \"حدود المعدل (Rate Limits)\"\ndescription: \"فهم والتعامل مع حدود المعدل\"\n---\n\n## نظرة عامة\n\nتطبق LemonData حدوداً للمعدل (Rate Limits) لضمان الاستخدام العادل واستقرار المنصة. تختلف الحدود حسب فئة الحساب.\n\n## فئات حدود المعدل\n\n| الفئة | الطلبات/الدقيقة | الوصف |\n|------|-------------|-------------|\n| **User** | 60 | الفئة الافتراضية لجميع الحسابات |\n| **Partner** | 300 | لشركاء التكامل |\n| **VIP** | 1,000 | المستخدمون ذوو الأحجام العالية |\n\n<Note>\n  تخضع حدود المعدل للتغيير. تواصل مع support@lemondata.cc للحصول على حدود مخصصة.\n</Note>\n\n## ترويسات حدود المعدل (Rate Limit Headers)\n\nيتضمن كل استجابة API معلومات حول حدود المعدل:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## تجاوز حد المعدل\n\nعند تجاوز الحد، ستتلقى استجابة `429`:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nمع ترويسة إضافية:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## التعامل مع حدود المعدل\n\n### التراجع الأسي (Exponential Backoff)\n\nقم بتنفيذ التراجع الأسي (Exponential Backoff) لإعادة المحاولة تلقائياً:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### جدولة الطلبات (Request Queuing)\n\nللتطبيقات ذات الأحجام العالية، قم بتنفيذ زمام طلبات (Request Queue):\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### المعالجة بالدفعة (Batch Processing)\n\nللعمليات الضخمة، قم بالمعالجة في دفعات مع فترات تأخير:\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"راقب استخدامك\">\n    تتبع ترويسات حدود المعدل للبقاء ضمن الحدود بشكل استباقي.\n  </Accordion>\n\n  <Accordion title=\"قم بتنفيذ التخزين المؤقت (Caching)\">\n    قم بتخزين الاستجابات للطلبات المتطابقة لتقليل استدعاءات API.\n  </Accordion>\n\n  <Accordion title=\"استخدم النماذج المناسبة\">\n    تسمح النماذج الأسرع (مثل gpt-4o-mini) بإنتاجية أعلى.\n  </Accordion>\n\n  <Accordion title=\"تواصل معنا للحصول على حدود أعلى\">\n    إذا كنت بحاجة إلى حدود أعلى، تواصل مع support@lemondata.cc.\n  </Accordion>\n</AccordionGroup>\n\n## ترقية فئتك\n\nلطلب ترقية الفئة:\n\n1. قم بتسجيل الدخول إلى [لوحة التحكم (Dashboard)](https://lemondata.cc/dashboard)\n2. انتقل إلى **Settings ← Account**\n3. تواصل مع الدعم مع توضيح حالة الاستخدام الخاصة بك\n\nأو أرسل بريداً إلكترونياً إلى support@lemondata.cc يتضمن:\n- بريد حسابك الإلكتروني\n- حجم الطلبات المتوقع\n- وصف حالة الاستخدام",
      "vi": "---\ntitle: \"Giới hạn Tốc độ\"\ndescription: \"Tìm hiểu và xử lý các giới hạn tốc độ\"\n---\n\n## Tổng quan\n\nLemonData áp dụng các giới hạn tốc độ để đảm bảo việc sử dụng công bằng và sự ổn định của nền tảng. Các giới hạn thay đổi tùy theo cấp độ tài khoản.\n\n## Các cấp độ Giới hạn Tốc độ\n\n| Cấp độ | Yêu cầu/phút | Mô tả |\n|------|-------------|-------------|\n| **User** | 60 | Cấp độ mặc định cho tất cả tài khoản |\n| **Partner** | 300 | Dành cho các đối tác tích hợp |\n| **VIP** | 1,000 | Người dùng có lưu lượng lớn |\n\n<Note>\n  Các giới hạn tốc độ có thể thay đổi. Liên hệ support@lemondata.cc để biết các giới hạn tùy chỉnh.\n</Note>\n\n## Các Header Giới hạn Tốc độ\n\nMọi API phản hồi đều bao gồm thông tin về giới hạn tốc độ:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## Vượt quá Giới hạn Tốc độ\n\nKhi bạn vượt quá giới hạn, bạn sẽ nhận được phản hồi `429`:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nVới header bổ sung:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## Xử lý Giới hạn Tốc độ\n\n### Exponential Backoff\n\nTriển khai exponential backoff để tự động thử lại:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### Hàng đợi Yêu cầu\n\nĐối với các ứng dụng có lưu lượng lớn, hãy triển khai một hàng đợi yêu cầu:\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### Xử lý theo lô\n\nĐối với các hoạt động hàng loạt, hãy xử lý theo lô với các khoảng trễ:\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## Các phương pháp hay nhất\n\n<AccordionGroup>\n  <Accordion title=\"Theo dõi mức độ sử dụng của bạn\">\n    Theo dõi các header giới hạn tốc độ để chủ động duy trì dưới mức giới hạn.\n  </Accordion>\n\n  <Accordion title=\"Triển khai bộ nhớ đệm (caching)\">\n    Lưu phản hồi vào bộ nhớ đệm cho các yêu cầu giống hệt nhau để giảm số lượng lệnh gọi API.\n  </Accordion>\n\n  <Accordion title=\"Sử dụng các mô hình phù hợp\">\n    Các mô hình nhanh hơn (như gpt-4o-mini) cho phép thông lượng cao hơn.\n  </Accordion>\n\n  <Accordion title=\"Liên hệ với chúng tôi để có giới hạn cao hơn\">\n    Nếu bạn cần giới hạn cao hơn, hãy liên hệ support@lemondata.cc.\n  </Accordion>\n</AccordionGroup>\n\n## Nâng cấp Cấp độ của bạn\n\nĐể yêu cầu nâng cấp cấp độ:\n\n1. Đăng nhập vào [Dashboard](https://lemondata.cc/dashboard) của bạn\n2. Đi tới **Settings → Account**\n3. Liên hệ bộ phận hỗ trợ với trường hợp sử dụng của bạn\n\nHoặc gửi email đến support@lemondata.cc với:\n- Email tài khoản của bạn\n- Lưu lượng yêu cầu dự kiến\n- Mô tả trường hợp sử dụng",
      "id": "---\ntitle: \"Rate Limits\"\ndescription: \"Memahami dan menangani rate limits\"\n---\n\n## Gambaran Umum\n\nLemonData menerapkan rate limits untuk memastikan penggunaan yang adil dan stabilitas platform. Batasan bervariasi berdasarkan tingkatan akun.\n\n## Tingkatan Rate Limit\n\n| Tingkatan | Permintaan/menit | Deskripsi |\n|------|-------------|-------------|\n| **User** | 60 | Tingkatan default untuk semua akun |\n| **Partner** | 300 | Untuk mitra integrasi |\n| **VIP** | 1,000 | Pengguna bervolume tinggi |\n\n<Note>\n  Rate limits dapat berubah sewaktu-waktu. Hubungi support@lemondata.cc untuk batasan khusus.\n</Note>\n\n## Header Rate Limit\n\nSetiap respons API menyertakan informasi rate limit:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## Rate Limit Terlampaui\n\nSaat Anda melampaui batas, Anda akan menerima respons `429`:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nDengan header tambahan:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## Menangani Rate Limits\n\n### Exponential Backoff\n\nTerapkan exponential backoff untuk percobaan ulang otomatis:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### Request Queuing\n\nUntuk aplikasi bervolume tinggi, terapkan antrean permintaan (request queue):\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### Batch Processing\n\nUntuk operasi massal, proses dalam batch dengan jeda:\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Pantau penggunaan Anda\">\n    Lacak header rate limit untuk tetap berada di bawah batasan secara proaktif.\n  </Accordion>\n\n  <Accordion title=\"Terapkan caching\">\n    Simpan respons dalam cache untuk permintaan yang identik guna mengurangi panggilan API.\n  </Accordion>\n\n  <Accordion title=\"Gunakan model yang sesuai\">\n    Model yang lebih cepat (seperti gpt-4o-mini) memungkinkan throughput yang lebih tinggi.\n  </Accordion>\n\n  <Accordion title=\"Hubungi kami untuk batasan yang lebih tinggi\">\n    Jika Anda membutuhkan batasan yang lebih tinggi, hubungi support@lemondata.cc.\n  </Accordion>\n</AccordionGroup>\n\n## Meningkatkan Tingkatan Anda\n\nUntuk meminta peningkatan tingkatan:\n\n1. Masuk ke [Dashboard](https://lemondata.cc/dashboard) Anda\n2. Buka **Settings → Account**\n3. Hubungi dukungan dengan kasus penggunaan (use case) Anda\n\nAtau kirim email ke support@lemondata.cc dengan:\n- Email akun Anda\n- Volume permintaan yang diharapkan\n- Deskripsi kasus penggunaan (use case)",
      "tr": "---\ntitle: \"Hız Limitleri\"\ndescription: \"Hız limitlerini anlama ve yönetme\"\n---\n\n## Genel Bakış\n\nLemonData, adil kullanım ve platform kararlılığını sağlamak için hız limitleri uygular. Limitler hesap katmanına göre değişiklik gösterir.\n\n## Hız Limiti Katmanları\n\n| Katman | İstek/dak | Açıklama |\n|------|-------------|-------------|\n| **User** | 60 | Tüm hesaplar için varsayılan katman |\n| **Partner** | 300 | Entegrasyon ortakları için |\n| **VIP** | 1,000 | Yüksek hacimli kullanıcılar |\n\n<Note>\n  Hız limitleri değişikliğe tabidir. Özel limitler için support@lemondata.cc ile iletişime geçin.\n</Note>\n\n## Hız Limiti Başlıkları\n\nHer API yanıtı hız limiti bilgilerini içerir:\n\n```\nX-RateLimit-Limit: 60          # Dakika başına limitiniz\nX-RateLimit-Remaining: 55      # Kalan istek sayısı\nX-RateLimit-Reset: 1234567890  # Limitin sıfırlanacağı Unix zaman damgası\n```\n\n## Hız Limiti Aşıldı\n\nLimiti aştığınızda bir `429` yanıtı alırsınız:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nEk başlık ile:\n```\nRetry-After: 60  # Yeniden denemeden önce beklenecek saniye\n```\n\n## Hız Limitlerini Yönetme\n\n### Üstel Geri Çekilme (Exponential Backoff)\n\nOtomatik yeniden denemeler için üstel geri çekilme uygulayın:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 saniye\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### İstek Kuyruğa Alma\n\nYüksek hacimli uygulamalar için bir istek kuyruğu uygulayın:\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Hız limitine uymak için gerekirse bekle\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### Toplu İşleme (Batch Processing)\n\nToplu işlemler için, gecikmelerle birlikte gruplar halinde işleyin:\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Gruplar arasında duraklat\n    return results\n```\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Kullanımınızı izleyin\">\n    Limitlerin altında kalmak için hız limiti başlıklarını proaktif olarak takip edin.\n  </Accordion>\n\n  <Accordion title=\"Önbelleğe almayı uygulayın\">\n    API çağrılarını azaltmak için aynı isteklerin yanıtlarını önbelleğe alın.\n  </Accordion>\n\n  <Accordion title=\"Uygun modelleri kullanın\">\n    Daha hızlı modeller (gpt-4o-mini gibi) daha fazla işleme kapasitesi sağlar.\n  </Accordion>\n\n  <Accordion title=\"Daha yüksek limitler için bizimle iletişime geçin\">\n    Daha yüksek limitlere ihtiyacınız varsa support@lemondata.cc ile iletişime geçin.\n  </Accordion>\n</AccordionGroup>\n\n## Katmanınızı Yükseltme\n\nKatman yükseltme talebinde bulunmak için:\n\n1. [Panelinize](https://lemondata.cc/dashboard) giriş yapın\n2. **Ayarlar → Hesap** bölümüne gidin\n3. Kullanım durumunuzla birlikte destek ekibiyle iletişime geçin\n\nVeya şu bilgileri içeren bir e-postayı support@lemondata.cc adresine gönderin:\n- Hesap e-postanız\n- Beklenen istek hacmi\n- Kullanım durumu açıklaması",
      "es": "---\ntitle: \"Límites de Tasa\"\ndescription: \"Entendiendo y manejando los límites de tasa\"\n---\n\n## Resumen\n\nLemonData implementa límites de tasa para asegurar un uso justo y la estabilidad de la plataforma. Los límites varían según el nivel de la cuenta.\n\n## Niveles de Límites de Tasa\n\n| Nivel | Solicitudes/min | Descripción |\n|------|-------------|-------------|\n| **User** | 60 | Nivel predeterminado para todas las cuentas |\n| **Partner** | 300 | Para socios de integración |\n| **VIP** | 1,000 | Usuarios de alto volumen |\n\n<Note>\n  Los límites de tasa están sujetos a cambios. Contacte a support@lemondata.cc para límites personalizados.\n</Note>\n\n## Encabezados de Límites de Tasa\n\nCada respuesta de la API incluye información sobre los límites de tasa:\n\n```\nX-RateLimit-Limit: 60          # Your limit per minute\nX-RateLimit-Remaining: 55      # Requests remaining\nX-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets\n```\n\n## Límite de Tasa Excedido\n\nCuando exceda el límite, recibirá una respuesta `429`:\n\n```json\n{\n  \"error\": {\n    \"message\": \"Rate limit exceeded. Please slow down.\",\n    \"type\": \"rate_limit_exceeded\"\n  }\n}\n```\n\nCon un encabezado adicional:\n```\nRetry-After: 60  # Seconds to wait before retrying\n```\n\n## Manejo de Límites de Tasa\n\n### Backoff Exponencial\n\nImplemente backoff exponencial para reintentos automáticos:\n\n```python\nimport time\nfrom openai import OpenAI, RateLimitError\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\ndef make_request_with_backoff(messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages\n            )\n        except RateLimitError as e:\n            if attempt == max_retries - 1:\n                raise\n\n            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n```\n\n### Cola de Solicitudes\n\nPara aplicaciones de alto volumen, implemente una cola de solicitudes:\n\n```python\nimport asyncio\nfrom collections import deque\n\nclass RateLimitedClient:\n    def __init__(self, requests_per_minute=60):\n        self.rpm = requests_per_minute\n        self.interval = 60 / requests_per_minute\n        self.last_request = 0\n\n    async def request(self, messages):\n        # Wait if needed to respect rate limit\n        now = asyncio.get_event_loop().time()\n        wait_time = max(0, self.last_request + self.interval - now)\n        if wait_time > 0:\n            await asyncio.sleep(wait_time)\n\n        self.last_request = asyncio.get_event_loop().time()\n        return await self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages\n        )\n```\n\n### Procesamiento por Lotes\n\nPara operaciones masivas, procese en lotes con retrasos:\n\n```python\ndef process_batch(items, batch_size=50, delay=1):\n    results = []\n    for i in range(0, len(items), batch_size):\n        batch = items[i:i + batch_size]\n        for item in batch:\n            result = client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": item}]\n            )\n            results.append(result)\n        time.sleep(delay)  # Pause between batches\n    return results\n```\n\n## Mejores Prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Monitoree su uso\">\n    Rastree los encabezados de límites de tasa para mantenerse bajo los límites de manera proactiva.\n  </Accordion>\n\n  <Accordion title=\"Implemente almacenamiento en caché\">\n    Almacene en caché las respuestas para solicitudes idénticas para reducir las llamadas a la API.\n  </Accordion>\n\n  <Accordion title=\"Use modelos apropiados\">\n    Los modelos más rápidos (como gpt-4o-mini) permiten un mayor rendimiento.\n  </Accordion>\n\n  <Accordion title=\"Contáctenos para límites más altos\">\n    Si necesita límites más altos, contacte a support@lemondata.cc.\n  </Accordion>\n</AccordionGroup>\n\n## Actualización de su Nivel\n\nPara solicitar una actualización de nivel:\n\n1. Inicie sesión en su [Dashboard](https://lemondata.cc/dashboard)\n2. Vaya a **Settings → Account**\n3. Contacte a soporte con su caso de uso\n\nO envíe un correo electrónico a support@lemondata.cc con:\n- El correo electrónico de su cuenta\n- Volumen de solicitudes esperado\n- Descripción del caso de uso"
    },
    "updatedAt": "2026-01-26T05:32:49.446Z"
  },
  "guides/sdks.mdx": {
    "sourceHash": "e3f8f81c73640b4d",
    "translations": {
      "zh": "---\ntitle: \"SDK 与库\"\ndescription: \"LemonData 官方及社区 SDK\"\n---\n\n## 概览\n\nLemonData **兼容 OpenAI**，这意味着您只需更改 base URL 即可使用任何 OpenAI SDK。我们还支持原生的 Anthropic 和 Google Gemini SDK。\n\n## 官方 SDK\n\n由于 LemonData 兼容 OpenAI，请使用官方的 OpenAI SDK：\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## 配置\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## 环境变量\n\n我们建议为 API 密钥使用环境变量：\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\n对于 Anthropic 原生请求，请使用 Anthropic SDK：\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain 集成\n\n请参阅 [LangChain 集成](/integrations/langchain) 指南以了解框架集成。",
      "zh-TW": "---\ntitle: \"SDK 與函式庫\"\ndescription: \"LemonData 的官方與社群 SDK\"\n---\n\n## 概覽\n\nLemonData **相容於 OpenAI**，這意味著您只需更改 base URL 即可使用任何 OpenAI SDK。我們也支援原生的 Anthropic 和 Google Gemini SDK。\n\n## 官方 SDK\n\n由於 LemonData 相容於 OpenAI，請使用官方的 OpenAI SDK：\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## 配置\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# 使用任何模型\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # 或 claude-sonnet-4-5, gemini-2.5-flash 等\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## 環境變數\n\n我們建議使用環境變數來管理 API 金鑰：\n\n```bash\n# .env 或 shell 設定檔\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\n對於 Anthropic 原生請求，請使用 Anthropic SDK：\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain 整合\n\n請參閱 [LangChain 整合](/integrations/langchain) 指南以了解框架整合。",
      "ja": "---\ntitle: \"SDKとライブラリ\"\ndescription: \"LemonDataの公式およびコミュニティSDK\"\n---\n\n## 概要\n\nLemonDataは**OpenAI互換**です。つまり、ベースURLを変更するだけで、任意のOpenAI SDKを使用できます。また、ネイティブのAnthropicおよびGoogle Gemini SDKもサポートしています。\n\n## 公式SDK\n\nLemonDataはOpenAI互換であるため、公式のOpenAI SDKを使用してください：\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## 設定\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# 任意のモデルを使用\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # または claude-sonnet-4-5, gemini-2.5-flash など\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## 環境変数\n\nAPIキーには環境変数を使用することをお勧めします：\n\n```bash\n# .env またはシェルプロファイル\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\nAnthropicネイティブのリクエストには、Anthropic SDKを使用してください：\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChainの統合\n\nフレームワークの統合については、[LangChainの統合](/integrations/langchain)ガイドを参照してください。",
      "ko": "---\ntitle: \"SDK 및 라이브러리\"\ndescription: \"LemonData를 위한 공식 및 커뮤니티 SDK\"\n---\n\n## 개요\n\nLemonData는 **OpenAI와 호환**되므로, `base URL` 변경만으로 모든 OpenAI SDK를 사용할 수 있습니다. 또한 Anthropic 및 Google Gemini 네이티브 SDK도 지원합니다.\n\n## 공식 SDK\n\nLemonData는 OpenAI와 호환되므로, 공식 OpenAI SDK를 사용하세요:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## 설정\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# 모든 모델 사용 가능\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # 또는 claude-sonnet-4-5, gemini-2.5-flash 등\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## 환경 변수\n\nAPI 키 관리를 위해 환경 변수 사용을 권장합니다:\n\n```bash\n# .env 또는 쉘 프로필\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\nAnthropic 네이티브 요청의 경우, Anthropic SDK를 사용하세요:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain 연동\n\n프레임워크 연동에 대해서는 [LangChain 연동](/integrations/langchain) 가이드를 참조하세요.",
      "de": "---\ntitle: \"SDKs & Bibliotheken\"\ndescription: \"Offizielle und Community-SDKs für LemonData\"\n---\n\n## Übersicht\n\nLemonData ist **OpenAI-kompatibel**, was bedeutet, dass Sie jedes OpenAI SDK mit einer einfachen Änderung der Basis-URL verwenden können. Wir unterstützen auch native Anthropic und Google Gemini SDKs.\n\n## Offizielle SDKs\n\nDa LemonData OpenAI-kompatibel ist, verwenden Sie die offiziellen OpenAI SDKs:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Konfiguration\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Umgebungsvariablen\n\nWir empfehlen die Verwendung von Umgebungsvariablen für API-Keys:\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\nFür Anthropic-native Anfragen verwenden Sie das Anthropic SDK:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain Integration\n\nWeitere Informationen zur Framework-Integration finden Sie im Leitfaden zur [LangChain-Integration](/integrations/langchain).",
      "fr": "---\ntitle: \"SDKs et Bibliothèques\"\ndescription: \"SDKs officiels et communautaires pour LemonData\"\n---\n\n## Aperçu\n\nLemonData est **compatible avec OpenAI**, ce qui signifie que vous pouvez utiliser n'importe quel SDK OpenAI en changeant simplement l'URL de base. Nous supportons également les SDK natifs Anthropic et Google Gemini.\n\n## SDKs officiels\n\nPuisque LemonData est compatible avec OpenAI, utilisez les SDK officiels d'OpenAI :\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Configuration\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Variables d'environnement\n\nNous recommandons d'utiliser des variables d'environnement pour les clés API :\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## SDK Anthropic\n\nPour les requêtes natives Anthropic, utilisez le SDK Anthropic :\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## Intégration LangChain\n\nConsultez le guide [Intégration LangChain](/integrations/langchain) pour l'intégration du framework.",
      "es": "---\ntitle: \"SDKs y Librerías\"\ndescription: \"SDKs oficiales y de la comunidad para LemonData\"\n---\n\n## Visión general\n\nLemonData es **compatible con OpenAI**, lo que significa que puedes usar cualquier SDK de OpenAI con solo cambiar la URL base. También soportamos SDKs nativos de Anthropic y Google Gemini.\n\n## SDKs Oficiales\n\nDado que LemonData es compatible con OpenAI, utiliza los SDKs oficiales de OpenAI:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Configuración\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Variables de Entorno\n\nRecomendamos usar variables de entorno para las API keys:\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## SDK de Anthropic\n\nPara solicitudes nativas de Anthropic, utiliza el SDK de Anthropic:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## Integración con LangChain\n\nConsulta la guía de [Integración con LangChain](/integrations/langchain) para la integración con el framework.",
      "pt": "---\ntitle: \"SDKs e Bibliotecas\"\ndescription: \"SDKs oficiais e da comunidade para LemonData\"\n---\n\n## Visão Geral\n\nO LemonData é **compatível com OpenAI**, o que significa que você pode usar qualquer SDK da OpenAI apenas alterando a URL base. Também oferecemos suporte a SDKs nativos da Anthropic e do Google Gemini.\n\n## SDKs Oficiais\n\nComo o LemonData é compatível com OpenAI, utilize os SDKs oficiais da OpenAI:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Configuração\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Variáveis de Ambiente\n\nRecomendamos o uso de variáveis de ambiente para chaves de API:\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## SDK da Anthropic\n\nPara requisições nativas da Anthropic, use o SDK da Anthropic:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## Integração com LangChain\n\nConsulte o guia de [Integração com LangChain](/integrations/langchain) para integração com o framework.",
      "ar": "---\ntitle: \"مجموعات SDK والمكتبات\"\ndescription: \"مجموعات SDK الرسمية والمجتمعية لـ LemonData\"\n---\n\n## نظرة عامة\n\nإن LemonData **متوافق مع OpenAI**، مما يعني أنه يمكنك استخدام أي SDK خاص بـ OpenAI بمجرد تغيير base URL فقط. كما ندعم أيضاً مجموعات SDK الأصلية لـ Anthropic و Google Gemini.\n\n## مجموعات SDK الرسمية\n\nبما أن LemonData متوافق مع OpenAI، استخدم مجموعات SDK الرسمية لـ OpenAI:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## الإعداد\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## متغيرات البيئة\n\nنوصي باستخدام متغيرات البيئة لمفاتيح API:\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## SDK الخاص بـ Anthropic\n\nللطلبات الأصلية الخاصة بـ Anthropic، استخدم SDK الخاص بـ Anthropic:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## التكامل مع LangChain\n\nراجع دليل [التكامل مع LangChain](/integrations/langchain) للتعرف على كيفية التكامل مع إطار العمل.",
      "vi": "---\ntitle: \"SDKs & Thư viện\"\ndescription: \"Các SDK chính thức và từ cộng đồng cho LemonData\"\n---\n\n## Tổng quan\n\nLemonData **tương thích với OpenAI**, điều đó có nghĩa là bạn có thể sử dụng bất kỳ OpenAI SDK nào chỉ với việc thay đổi base URL. Chúng tôi cũng hỗ trợ các SDK Anthropic và Google Gemini gốc.\n\n## Các SDK chính thức\n\nVì LemonData tương thích với OpenAI, hãy sử dụng các OpenAI SDK chính thức:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Cấu hình\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Sử dụng bất kỳ mô hình nào\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # hoặc claude-sonnet-4-5, gemini-2.5-flash, v.v.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Biến môi trường\n\nChúng tôi khuyên bạn nên sử dụng các biến môi trường cho API key:\n\n```bash\n# .env hoặc shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\nĐối với các yêu cầu Anthropic gốc, hãy sử dụng Anthropic SDK:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## Tích hợp LangChain\n\nXem hướng dẫn [Tích hợp LangChain](/integrations/langchain) để biết cách tích hợp framework.",
      "id": "---\ntitle: \"SDK & Library\"\ndescription: \"SDK resmi dan komunitas untuk LemonData\"\n---\n\n## Ringkasan\n\nLemonData bersifat **kompatibel dengan OpenAI**, yang berarti Anda dapat menggunakan SDK OpenAI apa pun hanya dengan mengubah URL dasar. Kami juga mendukung SDK asli Anthropic dan Google Gemini.\n\n## SDK Resmi\n\nKarena LemonData kompatibel dengan OpenAI, gunakan SDK resmi OpenAI:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Konfigurasi\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Variabel Lingkungan\n\nKami menyarankan penggunaan variabel lingkungan untuk kunci API:\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## SDK Anthropic\n\nUntuk permintaan asli Anthropic, gunakan SDK Anthropic:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## Integrasi LangChain\n\nLihat panduan [Integrasi LangChain](/integrations/langchain) untuk integrasi framework.",
      "tr": "---\ntitle: \"SDK'lar ve Kütüphaneler\"\ndescription: \"LemonData için resmi ve topluluk SDK'ları\"\n---\n\n## Genel Bakış\n\nLemonData **OpenAI uyumludur**, bu da herhangi bir OpenAI SDK'sını sadece bir base URL değişikliği ile kullanabileceğiniz anlamına gelir. Ayrıca yerel Anthropic ve Google Gemini SDK'larını da destekliyoruz.\n\n## Resmi SDK'lar\n\nLemonData OpenAI uyumlu olduğu için resmi OpenAI SDK'larını kullanın:\n\n<CardGroup cols={2}>\n  <Card title=\"Python\" icon=\"python\" href=\"https://github.com/openai/openai-python\">\n    `pip install openai`\n  </Card>\n  <Card title=\"Node.js\" icon=\"node-js\" href=\"https://github.com/openai/openai-node\">\n    `npm install openai`\n  </Card>\n  <Card title=\"Go\" icon=\"golang\" href=\"https://github.com/sashabaranov/go-openai\">\n    `go get github.com/sashabaranov/go-openai`\n  </Card>\n  <Card title=\".NET\" icon=\"microsoft\" href=\"https://github.com/openai/openai-dotnet\">\n    `dotnet add package OpenAI`\n  </Card>\n</CardGroup>\n\n## Yapılandırma\n\n### Python\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Use any model\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # or claude-sonnet-4-5, gemini-2.5-flash, etc.\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n### JavaScript / TypeScript\n\n```javascript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n```\n\n### Go\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    openai \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: \"user\", Content: \"Hello!\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n### .NET / C#\n\n```csharp\nusing OpenAI;\n\nvar options = new OpenAIClientOptions\n{\n    Endpoint = new Uri(\"https://api.lemondata.cc/v1\")\n};\n\nvar client = new OpenAIClient(\"sk-your-api-key\", options);\n\nvar chat = client.GetChatClient(\"gpt-4o\");\nvar response = await chat.CompleteChatAsync(\"Hello!\");\n\nConsole.WriteLine(response.Value.Content[0].Text);\n```\n\n### cURL\n\n```bash\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n## Ortam Değişkenleri\n\nAPI anahtarları için ortam değişkenlerini kullanmanızı öneririz:\n\n```bash\n# .env or shell profile\nexport LEMONDATA_API_KEY=\"sk-your-api-key\"\nexport LEMONDATA_BASE_URL=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"LEMONDATA_API_KEY\"),\n    base_url=os.environ.get(\"LEMONDATA_BASE_URL\")\n)\n```\n\n## Anthropic SDK\n\nAnthropic'e özgü istekler için Anthropic SDK'sını kullanın:\n\n```python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n```\n\n## LangChain Entegrasyonu\n\nFramework entegrasyonu için [LangChain Entegrasyonu](/integrations/langchain) kılavuzuna bakın."
    },
    "updatedAt": "2026-01-26T05:33:37.160Z"
  },
  "guides/streaming.mdx": {
    "sourceHash": "4627c5d08f266809",
    "translations": {
      "zh": "---\ntitle: \"流式传输\"\ndescription: \"实现实时流式响应\"\n---\n\n## 概览\n\n流式传输允许你在响应生成时接收部分内容，从而为聊天应用提供更好的用户体验。\n\n## 启用流式传输\n\n在请求中设置 `stream: true`：\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## 流式响应格式\n\n流中的每个数据块（chunk）都遵循以下格式：\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## 处理流结束\n\n流以以下方式结束：\n- `finish_reason: \"stop\"` - 正常完成\n- `finish_reason: \"length\"` - 达到 `max_tokens` 限制\n- `finish_reason: \"tool_calls\"` - 模型想要调用工具\n- `data: [DONE]` - 最终消息\n\n## 收集完整响应\n\n在流式传输时收集完整响应：\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## 异步流式传输\n\n对于异步应用：\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Web 应用示例\n\n对于 Web 聊天界面：\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // 添加到你的 UI\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "zh-TW": "---\ntitle: \"串流 (Streaming)\"\ndescription: \"實作即時串流回應\"\n---\n\n## 總覽\n\n串流允許您在回應生成時即時接收部分內容，為聊天應用程式提供更好的使用者體驗。\n\n## 啟用串流\n\n在您的請求中設定 `stream: true`：\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## 串流回應格式\n\n串流中的每個區塊 (chunk) 都遵循以下格式：\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## 處理串流結束\n\n串流結束於：\n- `finish_reason: \"stop\"` - 正常完成\n- `finish_reason: \"length\"` - 達到 `max_tokens` 限制\n- `finish_reason: \"tool_calls\"` - 模型想要呼叫工具\n- `data: [DONE]` - 最後一則訊息\n\n## 收集完整回應\n\n要在串流時收集完整的回應：\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## 非同步串流 (Async Streaming)\n\n對於非同步應用程式：\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## 網頁應用程式範例\n\n對於網頁聊天介面：\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Append to your UI\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "ja": "---\ntitle: \"ストリーミング\"\ndescription: \"リアルタイムのストリーミングレスポンスを実装する\"\n---\n\n## 概要\n\nストリーミングを使用すると、レスポンスが生成されるたびに部分的に受け取ることができ、チャットアプリケーションにおいてより優れたユーザーエクスペリエンスを提供できます。\n\n## ストリーミングの有効化\n\nリクエストで `stream: true` を設定します：\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## ストリーミングレスポンスの形式\n\nストリーム内の各チャンクは以下の形式に従います：\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## ストリーム終了の処理\n\nストリームは以下で終了します：\n- `finish_reason: \"stop\"` - 通常の完了\n- `finish_reason: \"length\"` - `max_tokens` の制限に到達\n- `finish_reason: \"tool_calls\"` - モデルがツールを呼び出そうとしている\n- `data: [DONE]` - 最終メッセージ\n\n## 完全なレスポンスの収集\n\nストリーミング中に完全なレスポンスを収集するには：\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## 非同期ストリーミング\n\n非同期アプリケーションの場合：\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Webアプリケーションの例\n\nWebチャットインターフェースの場合：\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // UIに追加\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "ko": "---\ntitle: \"스트리밍\"\ndescription: \"실시간 스트리밍 응답 구현하기\"\n---\n\n## 개요\n\n스트리밍을 사용하면 응답이 생성되는 대로 부분적으로 수신할 수 있어, 채팅 애플리케이션에서 더 나은 사용자 경험을 제공할 수 있습니다.\n\n## 스트리밍 활성화\n\n요청에서 `stream: true`로 설정하세요:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## 스트림 응답 형식\n\n스트림의 각 청크(chunk)는 다음 형식을 따릅니다:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## 스트림 종료 처리\n\n스트림은 다음과 같이 종료됩니다:\n- `finish_reason: \"stop\"` - 정상 완료\n- `finish_reason: \"length\"` - `max_tokens` 제한 도달\n- `finish_reason: \"tool_calls\"` - 모델이 도구 호출을 원하는 경우\n- `data: [DONE]` - 최종 메시지\n\n## 전체 응답 수집하기\n\n스트리밍 중에 전체 응답을 수집하려면:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## 비동기 스트리밍\n\n비동기 애플리케이션의 경우:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## 웹 애플리케이션 예시\n\n웹 채팅 인터페이스의 경우:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Append to your UI\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "de": "---\ntitle: \"Streaming\"\ndescription: \"Implementieren Sie Streaming-Antworten in Echtzeit\"\n---\n\n## Übersicht\n\nStreaming ermöglicht es Ihnen, Teilantworten zu erhalten, während diese generiert werden, was eine bessere Benutzererfahrung für Chat-Anwendungen bietet.\n\n## Streaming aktivieren\n\nSetzen Sie `stream: true` in Ihrer Anfrage:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Format der Streaming-Antwort\n\nJeder Chunk im Stream folgt diesem Format:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Umgang mit dem Stream-Ende\n\nDer Stream endet mit:\n- `finish_reason: \"stop\"` - Normale Fertigstellung\n- `finish_reason: \"length\"` - `max_tokens` Limit erreicht\n- `finish_reason: \"tool_calls\"` - Modell möchte ein Tool aufrufen\n- `data: [DONE]` - Letzte Nachricht\n\n## Sammeln der vollständigen Antwort\n\nUm die vollständige Antwort während des Streamings zu sammeln:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Asynchrones Streaming\n\nFür asynchrone Anwendungen:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Beispiel für eine Web-Anwendung\n\nFür eine Web-Chat-Schnittstelle:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Append to your UI\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "fr": "---\ntitle: \"Streaming\"\ndescription: \"Implémentez des réponses en streaming en temps réel\"\n---\n\n## Aperçu\n\nLe streaming vous permet de recevoir des réponses partielles au fur et à mesure de leur génération, offrant ainsi une meilleure expérience utilisateur pour les applications de chat.\n\n## Activer le streaming\n\nDéfinissez `stream: true` dans votre requête :\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Format de réponse du flux\n\nChaque fragment (chunk) du flux suit ce format :\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Gestion de la fin du flux\n\nLe flux se termine par :\n- `finish_reason: \"stop\"` - Complétion normale\n- `finish_reason: \"length\"` - Limite `max_tokens` atteinte\n- `finish_reason: \"tool_calls\"` - Le modèle souhaite appeler un outil\n- `data: [DONE]` - Message final\n\n## Collecter la réponse complète\n\nPour collecter la réponse complète pendant le streaming :\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Streaming asynchrone\n\nPour les applications asynchrones :\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Exemple d'application Web\n\nPour une interface de chat web :\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Ajouter à votre interface utilisateur\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "es": "---\ntitle: \"Streaming\"\ndescription: \"Implementa respuestas de streaming en tiempo real\"\n---\n\n## Descripción general\n\nEl streaming permite recibir respuestas parciales a medida que se generan, proporcionando una mejor experiencia de usuario para aplicaciones de chat.\n\n## Habilitar Streaming\n\nEstablece `stream: true` en tu solicitud:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Formato de respuesta de stream\n\nCada fragmento (chunk) en el stream sigue este formato:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Manejo del fin del stream\n\nEl stream termina con:\n- `finish_reason: \"stop\"` - Completado normal\n- `finish_reason: \"length\"` - Se alcanzó el límite de `max_tokens`\n- `finish_reason: \"tool_calls\"` - El modelo desea llamar a una herramienta\n- `data: [DONE]` - Mensaje final\n\n## Recopilación de la respuesta completa\n\nPara recopilar la respuesta completa durante el streaming:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Streaming asíncrono\n\nPara aplicaciones asíncronas:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Ejemplo de aplicación web\n\nPara una interfaz de chat web:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Añadir a tu interfaz de usuario\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "pt": "---\ntitle: \"Streaming\"\ndescription: \"Implemente respostas em streaming em tempo real\"\n---\n\n## Visão Geral\n\nO streaming permite que você receba respostas parciais à medida que são geradas, proporcionando uma melhor experiência de usuário para aplicações de chat.\n\n## Habilitar Streaming\n\nDefina `stream: true` em sua requisição:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Formato de Resposta do Stream\n\nCada chunk no stream segue este formato:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Lidando com o Fim do Stream\n\nO stream termina com:\n- `finish_reason: \"stop\"` - Conclusão normal\n- `finish_reason: \"length\"` - Atingiu o limite de `max_tokens`\n- `finish_reason: \"tool_calls\"` - O modelo deseja chamar uma ferramenta\n- `data: [DONE]` - Mensagem final\n\n## Coletando a Resposta Completa\n\nPara coletar a resposta completa durante o streaming:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Streaming Assíncrono\n\nPara aplicações assíncronas:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Exemplo de Aplicação Web\n\nPara uma interface de chat web:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Append to your UI\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "ar": "---\ntitle: \"البث (Streaming)\"\ndescription: \"تنفيذ استجابات البث المباشر في الوقت الفعلي\"\n---\n\n## نظرة عامة\n\nيتيح لك البث (Streaming) تلقي استجابات جزئية أثناء إنشائها، مما يوفر تجربة مستخدم أفضل لتطبيقات الدردشة.\n\n## تفعيل البث (Streaming)\n\nقم بتعيين `stream: true` في طلبك:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## تنسيق استجابة البث\n\nيتبع كل جزء (chunk) في البث هذا التنسيق:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## معالجة نهاية البث\n\nينتهي البث بـ:\n- `finish_reason: \"stop\"` - اكتمال طبيعي\n- `finish_reason: \"length\"` - الوصول إلى حد `max_tokens`\n- `finish_reason: \"tool_calls\"` - النموذج يرغب في استدعاء أداة (tool)\n- `data: [DONE]` - الرسالة النهائية\n\n## تجميع الاستجابة الكاملة\n\nلتجميع الاستجابة الكاملة أثناء البث:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## البث غير المتزامن (Async Streaming)\n\nللتطبيقات غير المتزامنة (async):\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## مثال لتطبيق ويب\n\nلواجهة دردشة ويب:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Append to your UI\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "vi": "---\ntitle: \"Streaming\"\ndescription: \"Triển khai phản hồi streaming thời gian thực\"\n---\n\n## Tổng quan\n\nStreaming cho phép bạn nhận các phản hồi từng phần khi chúng được tạo ra, mang lại trải nghiệm người dùng tốt hơn cho các ứng dụng chat.\n\n## Bật Streaming\n\nThiết lập `stream: true` trong request của bạn:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Định dạng Phản hồi Stream\n\nMỗi chunk trong stream tuân theo định dạng sau:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Xử lý Kết thúc Stream\n\nStream kết thúc với:\n- `finish_reason: \"stop\"` - Hoàn thành bình thường\n- `finish_reason: \"length\"` - Chạm giới hạn `max_tokens`\n- `finish_reason: \"tool_calls\"` - Model muốn gọi một tool\n- `data: [DONE]` - Tin nhắn cuối cùng\n\n## Thu thập Phản hồi Đầy đủ\n\nĐể thu thập phản hồi đầy đủ trong khi streaming:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Streaming Bất đồng bộ (Async)\n\nĐối với các ứng dụng bất đồng bộ:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Ví dụ Ứng dụng Web\n\nĐối với một giao diện chat web:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Thêm vào giao diện người dùng của bạn\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "id": "---\ntitle: \"Streaming\"\ndescription: \"Implementasikan respons streaming real-time\"\n---\n\n## Overview\n\nStreaming memungkinkan Anda untuk menerima respons parsial saat sedang dibuat, memberikan pengalaman pengguna yang lebih baik untuk aplikasi chat.\n\n## Enable Streaming\n\nAtur `stream: true` dalam permintaan Anda:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Stream Response Format\n\nSetiap chunk dalam stream mengikuti format berikut:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Handling Stream End\n\nStream berakhir dengan:\n- `finish_reason: \"stop\"` - Penyelesaian normal\n- `finish_reason: \"length\"` - Mencapai batas `max_tokens`\n- `finish_reason: \"tool_calls\"` - Model ingin memanggil sebuah tool\n- `data: [DONE]` - Pesan terakhir\n\n## Collecting Full Response\n\nUntuk mengumpulkan respons lengkap saat streaming:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Async Streaming\n\nUntuk aplikasi async:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Web Application Example\n\nUntuk antarmuka chat web:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // Tambahkan ke UI Anda\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```",
      "tr": "---\ntitle: \"Streaming\"\ndescription: \"Gerçek zamanlı streaming yanıtlarını uygulayın\"\n---\n\n## Genel Bakış\n\nStreaming, yanıtların oluşturuldukça parça parça alınmasını sağlayarak sohbet uygulamaları için daha iyi bir kullanıcı deneyimi sunar.\n\n## Streaming'i Etkinleştirme\n\nİsteğinizde `stream: true` olarak ayarlayın:\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    \"stream\": true\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a short poem\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst stream = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Write a short poem' }],\n  stream: true\n});\n\nfor await (const chunk of stream) {\n  const content = chunk.choices[0]?.delta?.content;\n  if (content) {\n    process.stdout.write(content);\n  }\n}\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n\n    stream, _ := client.CreateChatCompletionStream(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model:  \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Write a short poem\"},\n            },\n            Stream: true,\n        },\n    )\n    defer stream.Close()\n\n    for {\n        response, err := stream.Recv()\n        if err == io.EOF {\n            break\n        }\n        fmt.Print(response.Choices[0].Delta.Content)\n    }\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Write a short poem']],\n        'stream' => true\n    ]),\n    CURLOPT_WRITEFUNCTION => function($ch, $data) {\n        echo $data;\n        return strlen($data);\n    }\n]);\n\ncurl_exec($ch);\ncurl_close($ch);\n```\n\n</CodeGroup>\n\n## Stream Yanıt Formatı\n\nStream'deki her bir parça (chunk) şu formatı izler:\n\n```\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\" world\"},\"finish_reason\":null}]}\n\ndata: {\"id\":\"chatcmpl-xxx\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\n\ndata: [DONE]\n```\n\n## Stream Sonlanmasını Yönetme\n\nStream şunlarla sona erer:\n- `finish_reason: \"stop\"` - Normal tamamlanma\n- `finish_reason: \"length\"` - `max_tokens` sınırına ulaşıldı\n- `finish_reason: \"tool_calls\"` - Model bir araç (tool) çağırmak istiyor\n- `data: [DONE]` - Final mesajı\n\n## Tam Yanıtı Biriktirme\n\nStreaming sırasında tam yanıtı biriktirmek için:\n\n```python\nfull_response = \"\"\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        content = chunk.choices[0].delta.content\n        full_response += content\n        print(content, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n```\n\n## Asenkron Streaming\n\nAsenkron uygulamalar için:\n\n```python\nimport asyncio\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(\n        api_key=\"sk-your-api-key\",\n        base_url=\"https://api.lemondata.cc/v1\"\n    )\n\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.choices[0].delta.content:\n            print(chunk.choices[0].delta.content, end=\"\")\n\nasyncio.run(main())\n```\n\n## Web Uygulaması Örneği\n\nBir web sohbet arayüzü için:\n\n```javascript\nasync function streamChat(message) {\n  const response = await fetch('https://api.lemondata.cc/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': 'Bearer sk-your-api-key',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gpt-4o',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n').filter(line => line.startsWith('data: '));\n\n    for (const line of lines) {\n      const data = line.slice(6);\n      if (data === '[DONE]') return;\n\n      const parsed = JSON.parse(data);\n      const content = parsed.choices[0]?.delta?.content;\n      if (content) {\n        // UI'ınıza ekleyin\n        document.getElementById('output').textContent += content;\n      }\n    }\n  }\n}\n```"
    },
    "updatedAt": "2026-01-26T05:33:57.157Z"
  },
  "integrations/ai-chat-apps.mdx": {
    "sourceHash": "85da55df5d05cff6",
    "translations": {
      "zh": "---\ntitle: \"AI 聊天应用\"\ndescription: \"配置热门 AI 聊天应用以使用 LemonData API\"\n---\n\n## 概览\n\nLemonData 兼容任何支持 OpenAI 协议的聊天应用。本指南涵盖了热门桌面端和移动端聊天应用的配置方法。\n\n## 支持的应用\n\n| 应用 | 平台 | 功能 |\n|-------------|----------|----------|\n| LobeChat | Web、桌面端、自托管 | 多供应商、插件、RAG |\n| CherryStudio | Windows、macOS、Linux | 丰富的插件、多供应商 |\n| Chatbox | Windows、macOS、Linux | 轻量、快速 |\n| BotGem | iOS、macOS | 原生 Apple 体验 |\n| TypingMind | Web、桌面端 | 自定义角色、插件 |\n| OpenCat | iOS、macOS | 原生、离线支持 |\n| ChatWise | Windows、macOS | 团队协作 |\n\n## 配置步骤\n\n### 步骤 1：获取您的 API Key\n\n1. 登录 [LemonData 控制面板](https://lemondata.cc/dashboard)\n2. 导航至 [API Keys](https://lemondata.cc/dashboard/api)\n3. 创建并复制您的 API Key（格式：`sk-...`）\n\n### 步骤 2：配置您的应用\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **对于自托管 LobeChat：**\n\n    设置环境变量：\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **对于 LobeChat 云端版：**\n    1. 打开 **设置** → **AI 服务商**\n    2. 选择 **OpenAI**\n    3. 输入您的 LemonData API Key\n    4. 设置自定义端点：`https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. 打开 **设置** → **模型供应商**\n    2. 点击 **添加供应商**\n    3. 选择 **OpenAI API 兼容**\n    4. 输入：\n       - **名称**：LemonData\n       - **API 基础 URL**：`https://api.lemondata.cc/v1`\n       - **API Key**：`sk-your-lemondata-key`\n    5. 添加模型：`gpt-4o`、`claude-sonnet-4-5` 等\n    6. 保存并测试\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. 打开 **设置** → **AI 模型**\n    2. 选择 **OpenAI API 兼容**\n    3. 输入：\n       - **API 主机**：`https://api.lemondata.cc/v1`\n       - **API Key**：`sk-your-lemondata-key`\n       - **模型**：`gpt-4o`\n    4. 点击 **验证** 以测试连接\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. 打开 **设置** → **供应商**\n    2. 点击 **添加自定义供应商**\n    3. 输入：\n       - **名称**：LemonData\n       - **基础 URL**：`https://api.lemondata.cc/v1`\n       - **API Key**：`sk-your-lemondata-key`\n    4. 添加您偏好的模型\n    5. 保存配置\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. 点击 **设置**（齿轮图标）\n    2. 选择 **自定义端点**\n    3. 输入：\n       - **端点 URL**：`https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**：`sk-your-lemondata-key`\n    4. 配置模型并保存\n  </Tab>\n</Tabs>\n\n### 步骤 3：添加模型\n\n配置供应商后，添加您想要使用的模型：\n\n| 类别 | 模型 |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## 配置参考\n\n| 字段 | 值 |\n|-------|-------|\n| 基础 URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API 类型 | OpenAI 兼容 |\n\n**JSON 配置（适用于支持该格式的应用）：**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"连接错误\">\n    - 验证基础 URL 是否准确为 `https://api.lemondata.cc/v1`\n    - 检查末尾是否有斜杠（如有请移除）\n    - 验证网络连接和防火墙设置\n  </Accordion>\n\n  <Accordion title=\"401 未授权\">\n    - 仔细检查您的 API Key\n    - 在 [LemonData 控制面板](https://lemondata.cc/dashboard/api) 中验证 Key 是否处于激活状态\n    - 确保 Key 中没有多余的空格\n  </Accordion>\n\n  <Accordion title=\"402 余额不足\">\n    - 在控制面板中检查您的账户余额\n    - 为您的账户充值\n  </Accordion>\n\n  <Accordion title=\"404 模型不可用\">\n    - 验证模型名称拼写是否正确\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh/models) 查看模型可用性\n    - 尝试使用其他模型\n  </Accordion>\n\n  <Accordion title=\"响应缓慢\">\n    - 尝试更快的模型（例如使用 `gpt-4o-mini` 代替 `gpt-4o`）\n    - 如果可行，请减小 `max_tokens` 参数\n    - 检查您的网络连接\n  </Accordion>\n</AccordionGroup>\n\n## 提示\n\n<AccordionGroup>\n  <Accordion title=\"使用合适的模型\">\n    根据任务匹配模型 —— 快速对话使用 GPT-4o-mini，复杂任务使用 Claude 或 GPT-4o。\n  </Accordion>\n\n  <Accordion title=\"保存常用提示词\">\n    大多数应用支持保存提示词或角色。为常见任务创建模板。\n  </Accordion>\n\n  <Accordion title=\"监控使用情况\">\n    在 LemonData 控制面板中查看您的使用情况，以跟踪成本并管理预算。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"AI 聊天應用程式\"\ndescription: \"設定熱門 AI 聊天應用程式以使用 LemonData API\"\n---\n\n## 概覽\n\nLemonData 可與任何相容 OpenAI 的聊天應用程式配合使用。本指南涵蓋了熱門桌面端和行動端聊天應用程式的設定。\n\n## 支援的應用程式\n\n| 應用程式 | 平台 | 功能 |\n|-------------|----------|----------|\n| LobeChat | 網頁、桌面端、自託管 | 多供應商、外掛程式、RAG |\n| CherryStudio | Windows, macOS, Linux | 豐富的外掛程式、多個供應商 |\n| Chatbox | Windows, macOS, Linux | 輕量、快速 |\n| BotGem | iOS, macOS | 原生 Apple 體驗 |\n| TypingMind | 網頁、桌面端 | 自定義人格、外掛程式 |\n| OpenCat | iOS, macOS | 原生、離線支援 |\n| ChatWise | Windows, macOS | 團隊協作 |\n\n## 設定步驟\n\n### 步驟 1：獲取您的 API Key\n\n1. 登入 [LemonData 控制台](https://lemondata.cc/dashboard)\n2. 導覽至 [API Keys](https://lemondata.cc/dashboard/api)\n3. 建立並複製您的 API Key（格式：`sk-...`）\n\n### 步驟 2：設定您的應用程式\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **針對自託管 LobeChat：**\n\n    設定環境變數：\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **針對 LobeChat Cloud：**\n    1. 開啟 **Settings** → **AI Service Provider**\n    2. 選擇 **OpenAI**\n    3. 輸入您的 LemonData API Key\n    4. 設定自定義端點：`https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. 開啟 **Settings** → **Model Provider**\n    2. 點擊 **Add Provider**\n    3. 選擇 **OpenAI API Compatible**\n    4. 輸入：\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. 新增模型：`gpt-4o`, `claude-sonnet-4-5` 等\n    6. 儲存並測試\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. 開啟 **Settings** → **AI Model**\n    2. 選擇 **OpenAI API Compatible**\n    3. 輸入：\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. 點擊 **Verify** 以測試連線\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. 開啟 **Settings** → **Providers**\n    2. 點擊 **Add Custom Provider**\n    3. 輸入：\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. 新增您偏好的模型\n    5. 儲存設定\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. 點擊 **Settings**（齒輪圖示）\n    2. 選擇 **Custom Endpoint**\n    3. 輸入：\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. 設定模型並儲存\n  </Tab>\n</Tabs>\n\n### 步驟 3：新增模型\n\n設定供應商後，新增您想要使用的模型：\n\n| 類別 | 模型 |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## 設定參考\n\n| 欄位 | 值 |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API 類型 | OpenAI Compatible |\n\n**JSON 設定（適用於支援的應用程式）：**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## 疑難排解\n\n<AccordionGroup>\n  <Accordion title=\"連線錯誤\">\n    - 驗證 Base URL 是否準確為 `https://api.lemondata.cc/v1`\n    - 檢查結尾是否有斜線（如有請移除）\n    - 驗證網路連線和防火牆設定\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - 再次檢查您的 API Key\n    - 在 [LemonData 控制台](https://lemondata.cc/dashboard/api) 中驗證 Key 是否處於啟用狀態\n    - 確保 Key 中沒有多餘的空格\n  </Accordion>\n\n  <Accordion title=\"402 Insufficient Balance\">\n    - 在控制台中檢查您的帳戶餘額\n    - 為您的帳戶儲值\n  </Accordion>\n\n  <Accordion title=\"404 Model Not Available\">\n    - 驗證模型名稱拼寫是否正確\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh-TW/models) 檢查模型可用性\n    - 嘗試不同的模型\n  </Accordion>\n\n  <Accordion title=\"回應緩慢\">\n    - 嘗試更快的模型（例如使用 `gpt-4o-mini` 代替 `gpt-4o`）\n    - 如果可行，請減少 `max_tokens` 參數\n    - 檢查您的網路連線\n  </Accordion>\n</AccordionGroup>\n\n## 提示\n\n<AccordionGroup>\n  <Accordion title=\"使用合適的模型\">\n    根據任務選擇模型：快速聊天使用 GPT-4o-mini，複雜任務使用 Claude 或 GPT-4o。\n  </Accordion>\n\n  <Accordion title=\"儲存常用提示詞\">\n    大多數應用程式支援儲存提示詞或人格。為常見任務建立範本。\n  </Accordion>\n\n  <Accordion title=\"監控使用情況\">\n    在 LemonData 控制台中檢查您的使用情況，以追蹤成本並管理預算。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"AIチャットアプリケーション\"\ndescription: \"人気のAIチャットアプリケーションをLemonData APIで使用するように設定します\"\n---\n\n## 概要\n\nLemonDataは、OpenAI互換のあらゆるチャットアプリケーションで動作します。このガイドでは、人気のデスクトップおよびモバイルチャットアプリの設定方法について説明します。\n\n## サポートされているアプリケーション\n\n| アプリケーション | プラットフォーム | 機能 |\n|-------------|----------|----------|\n| LobeChat | Web, デスクトップ, セルフホスト | マルチプロバイダー、プラグイン、RAG |\n| CherryStudio | Windows, macOS, Linux | 豊富なプラグイン、マルチプロバイダー |\n| Chatbox | Windows, macOS, Linux | 軽量、高速 |\n| BotGem | iOS, macOS | ネイティブなApple体験 |\n| TypingMind | Web, デスクトップ | カスタムペルソナ、プラグイン |\n| OpenCat | iOS, macOS | ネイティブ、オフラインサポート |\n| ChatWise | Windows, macOS | チームコラボレーション |\n\n## 設定手順\n\n### ステップ 1: APIキーの取得\n\n1. [LemonDataダッシュボード](https://lemondata.cc/dashboard)にログインします\n2. [APIキー](https://lemondata.cc/dashboard/api)に移動します\n3. APIキーを作成してコピーします（形式: `sk-...`）\n\n### ステップ 2: アプリの設定\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **セルフホスト型LobeChatの場合:**\n\n    環境変数を設定します:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **LobeChat Cloudの場合:**\n    1. **設定** → **AIサービスプロバイダー**を開きます\n    2. **OpenAI**を選択します\n    3. LemonDataのAPIキーを入力します\n    4. カスタムエンドポイントを設定します: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. **設定** → **モデルプロバイダー**を開きます\n    2. **プロバイダーを追加**をクリックします\n    3. **OpenAI API互換**を選択します\n    4. 以下を入力します:\n       - **名前**: LemonData\n       - **APIベースURL**: `https://api.lemondata.cc/v1`\n       - **APIキー**: `sk-your-lemondata-key`\n    5. モデルを追加します: `gpt-4o`, `claude-sonnet-4-5`など\n    6. 保存してテストします\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. **設定** → **AIモデル**を開きます\n    2. **OpenAI API互換**を選択します\n    3. 以下を入力します:\n       - **APIホスト**: `https://api.lemondata.cc/v1`\n       - **APIキー**: `sk-your-lemondata-key`\n       - **モデル**: `gpt-4o`\n    4. **検証**をクリックして接続をテストします\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. **設定** → **プロバイダー**を開きます\n    2. **カスタムプロバイダーを追加**をタップします\n    3. 以下を入力します:\n       - **名前**: LemonData\n       - **ベースURL**: `https://api.lemondata.cc/v1`\n       - **APIキー**: `sk-your-lemondata-key`\n    4. お好みのモデルを追加します\n    5. 設定を保存します\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. **設定**（歯車アイコン）をクリックします\n    2. **カスタムエンドポイント**を選択します\n    3. 以下を入力します:\n       - **エンドポイントURL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **APIキー**: `sk-your-lemondata-key`\n    4. モデルを設定して保存します\n  </Tab>\n</Tabs>\n\n### ステップ 3: モデルの追加\n\nプロバイダーを設定した後、使用したいモデルを追加します:\n\n| カテゴリ | モデル |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## 設定リファレンス\n\n| フィールド | 値 |\n|-------|-------|\n| ベースURL | `https://api.lemondata.cc/v1` |\n| APIキー | `sk-your-lemondata-key` |\n| APIタイプ | OpenAI互換 |\n\n**JSON設定（サポートしているアプリの場合）:**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"接続エラー\">\n    - ベースURLが正確に `https://api.lemondata.cc/v1` であることを確認してください\n    - 末尾のスラッシュを確認してください（存在する場合は削除してください）\n    - インターネット接続とファイアウォールの設定を確認してください\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - APIキーを再確認してください\n    - [LemonDataダッシュボード](https://lemondata.cc/dashboard/api)でキーが有効であることを確認してください\n    - キーに余分なスペースが含まれていないことを確認してください\n  </Accordion>\n\n  <Accordion title=\"402 Insufficient Balance\">\n    - ダッシュボードでアカウント残高を確認してください\n    - アカウントにクレジットを追加してください\n  </Accordion>\n\n  <Accordion title=\"404 Model Not Available\">\n    - モデル名の綴りが正しいか確認してください\n    - [lemondata.cc/en/models](https://lemondata.cc/ja/models) でモデルの利用可能性を確認してください\n    - 別のモデルを試してください\n  </Accordion>\n\n  <Accordion title=\"レスポンスが遅い\">\n    - より高速なモデルを試してください（例：`gpt-4o` の代わりに `gpt-4o-mini`）\n    - 利用可能な場合は `max_tokens` パラメータを減らしてください\n    - ネットワーク接続を確認してください\n  </Accordion>\n</AccordionGroup>\n\n## ヒント\n\n<AccordionGroup>\n  <Accordion title=\"適切なモデルを使用する\">\n    タスクに合わせてモデルを選択してください。クイックチャットには GPT-4o-mini、複雑なタスクには Claude または GPT-4o を使用します。\n  </Accordion>\n\n  <Accordion title=\"よく使うプロンプトを保存する\">\n    ほとんどのアプリはプロンプトやペルソナの保存をサポートしています。一般的なタスク用のテンプレートを作成しましょう。\n  </Accordion>\n\n  <Accordion title=\"使用状況を監視する\">\n    LemonDataダッシュボードで使用状況を確認し、コストを追跡して予算を管理しましょう。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"AI 채팅 애플리케이션\"\ndescription: \"LemonData API를 사용하도록 인기 있는 AI 채팅 애플리케이션을 설정합니다\"\n---\n\n## 개요\n\nLemonData는 모든 OpenAI 호환 채팅 애플리케이션과 연동됩니다. 이 가이드는 인기 있는 데스크톱 및 모바일 채팅 앱의 설정 방법을 다룹니다.\n\n## 지원되는 애플리케이션\n\n| 애플리케이션 | 플랫폼 | 기능 |\n|-------------|----------|----------|\n| LobeChat | 웹, 데스크톱, 셀프 호스팅 | 멀티 프로바이더, 플러그인, RAG |\n| CherryStudio | Windows, macOS, Linux | 풍부한 플러그인, 다중 프로바이더 |\n| Chatbox | Windows, macOS, Linux | 가볍고 빠름 |\n| BotGem | iOS, macOS | 네이티브 Apple 경험 |\n| TypingMind | 웹, 데스크톱 | 커스텀 페르소나, 플러그인 |\n| OpenCat | iOS, macOS | 네이티브, 오프라인 지원 |\n| ChatWise | Windows, macOS | 팀 협업 |\n\n## 설정 단계\n\n### 1단계: API 키 가져오기\n\n1. [LemonData 대시보드](https://lemondata.cc/dashboard)에 로그인합니다\n2. [API Keys](https://lemondata.cc/dashboard/api)로 이동합니다\n3. API 키를 생성하고 복사합니다 (형식: `sk-...`)\n\n### 2단계: 앱 설정하기\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **셀프 호스팅 LobeChat의 경우:**\n\n    환경 변수를 설정합니다:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **LobeChat Cloud의 경우:**\n    1. **설정** → **AI 서비스 프로바이더**를 엽니다\n    2. **OpenAI**를 선택합니다\n    3. LemonData API 키를 입력합니다\n    4. 커스텀 엔드포인트를 설정합니다: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. **설정** → **모델 프로바이더**를 엽니다\n    2. **프로바이더 추가**를 클릭합니다\n    3. **OpenAI API 호환**을 선택합니다\n    4. 다음을 입력합니다:\n       - **이름**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API 키**: `sk-your-lemondata-key`\n    5. 모델 추가: `gpt-4o`, `claude-sonnet-4-5` 등\n    6. 저장 및 테스트\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. **설정** → **AI 모델**을 엽니다\n    2. **OpenAI API 호환**을 선택합니다\n    3. 다음을 입력합니다:\n       - **API 호스트**: `https://api.lemondata.cc/v1`\n       - **API 키**: `sk-your-lemondata-key`\n       - **모델**: `gpt-4o`\n    4. **연결 테스트**를 위해 확인을 클릭합니다\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. **설정** → **프로바이더**를 엽니다\n    2. **커스텀 프로바이더 추가**를 탭합니다\n    3. 다음을 입력합니다:\n       - **이름**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API 키**: `sk-your-lemondata-key`\n    4. 선호하는 모델을 추가합니다\n    5. 설정 저장\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. **설정** (톱니바퀴 아이콘)을 클릭합니다\n    2. **커스텀 엔드포인트**를 선택합니다\n    3. 다음을 입력합니다:\n       - **엔드포인트 URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API 키**: `sk-your-lemondata-key`\n    4. 모델을 설정하고 저장합니다\n  </Tab>\n</Tabs>\n\n### 3단계: 모델 추가\n\n프로바이더를 설정한 후, 사용하려는 모델을 추가합니다:\n\n| 카테고리 | 모델 |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## 설정 참조\n\n| 필드 | 값 |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API Type | OpenAI Compatible |\n\n**JSON 설정 (지원하는 앱의 경우):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"연결 오류\">\n    - Base URL이 정확히 `https://api.lemondata.cc/v1`인지 확인합니다\n    - 끝에 슬래시(/)가 있는지 확인합니다 (있는 경우 제거)\n    - 인터넷 연결 및 방화벽 설정을 확인합니다\n  </Accordion>\n\n  <Accordion title=\"401 권한 없음\">\n    - API 키를 다시 확인합니다\n    - [LemonData 대시보드](https://lemondata.cc/dashboard/api)에서 키가 활성 상태인지 확인합니다\n    - 키에 불필요한 공백이 포함되지 않았는지 확인합니다\n  </Accordion>\n\n  <Accordion title=\"402 잔액 부족\">\n    - 대시보드에서 계정 잔액을 확인합니다\n    - 계정에 크레딧을 충전합니다\n  </Accordion>\n\n  <Accordion title=\"404 모델을 사용할 수 없음\">\n    - 모델 이름의 철자가 정확한지 확인합니다\n    - [lemondata.cc/en/models](https://lemondata.cc/ko/models)에서 모델 가용성을 확인합니다\n    - 다른 모델을 시도해 보세요\n  </Accordion>\n\n  <Accordion title=\"느린 응답\">\n    - 더 빠른 모델을 시도해 보세요 (예: `gpt-4o` 대신 `gpt-4o-mini`)\n    - 가능한 경우 `max_tokens` 파라미터를 줄입니다\n    - 네트워크 연결을 확인합니다\n  </Accordion>\n</AccordionGroup>\n\n## 팁\n\n<AccordionGroup>\n  <Accordion title=\"적절한 모델 사용\">\n    작업에 맞는 모델을 선택하세요. 간단한 채팅에는 GPT-4o-mini를, 복잡한 작업에는 Claude 또는 GPT-4o를 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"자주 사용하는 프롬프트 저장\">\n    대부분의 앱은 프롬프트나 페르소나 저장을 지원합니다. 일반적인 작업을 위한 템플릿을 만드세요.\n  </Accordion>\n\n  <Accordion title=\"사용량 모니터링\">\n    LemonData 대시보드에서 사용량을 확인하여 비용을 추적하고 예산을 관리하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"KI-Chat-Anwendungen\"\ndescription: \"Konfigurieren Sie beliebte KI-Chat-Anwendungen für die Nutzung der LemonData API\"\n---\n\n## Übersicht\n\nLemonData funktioniert mit jeder OpenAI-kompatiblen Chat-Anwendung. Dieser Leitfaden beschreibt die Konfiguration für beliebte Desktop- und Mobil-Chat-Apps.\n\n## Unterstützte Anwendungen\n\n| Anwendung | Plattform | Funktionen |\n|-------------|----------|----------|\n| LobeChat | Web, Desktop, Self-hosted | Multi-Provider, Plugins, RAG |\n| CherryStudio | Windows, macOS, Linux | Umfangreiche Plugins, mehrere Provider |\n| Chatbox | Windows, macOS, Linux | Leichtgewichtig, schnell |\n| BotGem | iOS, macOS | Natives Apple-Erlebnis |\n| TypingMind | Web, Desktop | Benutzerdefinierte Personas, Plugins |\n| OpenCat | iOS, macOS | Nativ, Offline-Unterstützung |\n| ChatWise | Windows, macOS | Team-Zusammenarbeit |\n\n## Konfigurationsschritte\n\n### Schritt 1: API-Key abrufen\n\n1. Melden Sie sich im [LemonData Dashboard](https://lemondata.cc/dashboard) an\n2. Navigieren Sie zu [API Keys](https://lemondata.cc/dashboard/api)\n3. Erstellen und kopieren Sie Ihren API-Key (Format: `sk-...`)\n\n### Schritt 2: App konfigurieren\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Für selbstgehostetes LobeChat:**\n\n    Umgebungsvariablen festlegen:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **Für LobeChat Cloud:**\n    1. Öffnen Sie **Settings** → **AI Service Provider**\n    2. Wählen Sie **OpenAI**\n    3. Geben Sie Ihren LemonData API-Key ein\n    4. Benutzerdefinierten Endpoint festlegen: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. Öffnen Sie **Settings** → **Model Provider**\n    2. Klicken Sie auf **Add Provider**\n    3. Wählen Sie **OpenAI API Compatible**\n    4. Geben Sie Folgendes ein:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. Modelle hinzufügen: `gpt-4o`, `claude-sonnet-4-5`, etc.\n    6. Speichern und testen\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. Öffnen Sie **Settings** → **AI Model**\n    2. Wählen Sie **OpenAI API Compatible**\n    3. Geben Sie Folgendes ein:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. Klicken Sie auf **Verify**, um die Verbindung zu testen\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. Öffnen Sie **Settings** → **Providers**\n    2. Tippen Sie auf **Add Custom Provider**\n    3. Geben Sie Folgendes ein:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Fügen Sie Ihre bevorzugten Modelle hinzu\n    5. Konfiguration speichern\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. Klicken Sie auf **Settings** (Zahnrad-Icon)\n    2. Wählen Sie **Custom Endpoint**\n    3. Geben Sie Folgendes ein:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Modelle konfigurieren und speichern\n  </Tab>\n</Tabs>\n\n### Schritt 3: Modelle hinzufügen\n\nNach der Konfiguration des Providers fügen Sie die Modelle hinzu, die Sie verwenden möchten:\n\n| Kategorie | Modelle |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Konfigurationsreferenz\n\n| Feld | Wert |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API Type | OpenAI Compatible |\n\n**JSON-Konfiguration (für Apps, die dies unterstützen):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Verbindungsfehler\">\n    - Überprüfen Sie, ob die Base URL exakt `https://api.lemondata.cc/v1` entspricht\n    - Achten Sie auf abschließende Schrägstriche (entfernen, falls vorhanden)\n    - Überprüfen Sie die Internetverbindung und Firewall-Einstellungen\n  </Accordion>\n\n  <Accordion title=\"401 Nicht autorisiert\">\n    - Überprüfen Sie Ihren API-Key erneut\n    - Stellen Sie sicher, dass der Key im [LemonData Dashboard](https://lemondata.cc/dashboard/api) aktiv ist\n    - Stellen Sie sicher, dass keine zusätzlichen Leerzeichen im Key enthalten sind\n  </Accordion>\n\n  <Accordion title=\"402 Unzureichendes Guthaben\">\n    - Überprüfen Sie Ihr Kontoguthaben im Dashboard\n    - Laden Sie Guthaben auf Ihr Konto auf\n  </Accordion>\n\n  <Accordion title=\"404 Modell nicht verfügbar\">\n    - Überprüfen Sie die korrekte Schreibweise des Modellnamens\n    - Prüfen Sie die Modellverfügbarkeit unter [lemondata.cc/en/models](https://lemondata.cc/de/models)\n    - Versuchen Sie ein anderes Modell\n  </Accordion>\n\n  <Accordion title=\"Langsame Antworten\">\n    - Versuchen Sie ein schnelleres Modell (z. B. `gpt-4o-mini` anstelle von `gpt-4o`)\n    - Reduzieren Sie den Parameter `max_tokens`, falls verfügbar\n    - Überprüfen Sie Ihre Netzwerkverbindung\n  </Accordion>\n</AccordionGroup>\n\n## Tipps\n\n<AccordionGroup>\n  <Accordion title=\"Passende Modelle verwenden\">\n    Wählen Sie das Modell passend zur Aufgabe – nutzen Sie GPT-4o-mini für kurze Chats, Claude oder GPT-4o für komplexe Aufgaben.\n  </Accordion>\n\n  <Accordion title=\"Häufig genutzte Prompts speichern\">\n    Die meisten Apps unterstützen das Speichern von Prompts oder Personas. Erstellen Sie Vorlagen für gängige Aufgaben.\n  </Accordion>\n\n  <Accordion title=\"Nutzung überwachen\">\n    Überprüfen Sie Ihre Nutzung im LemonData Dashboard, um Kosten zu verfolgen und Ihr Budget zu verwalten.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"Applications de Chat IA\"\ndescription: \"Configurez les applications de chat IA populaires pour utiliser l'API LemonData\"\n---\n\n## Aperçu\n\nLemonData fonctionne avec n'importe quelle application de chat compatible OpenAI. Ce guide couvre la configuration des applications de chat populaires pour bureau et mobile.\n\n## Applications prises en charge\n\n| Application | Plateforme | Fonctionnalités |\n|-------------|----------|----------|\n| LobeChat | Web, Bureau, Auto-hébergé | Multi-fournisseur, plugins, RAG |\n| CherryStudio | Windows, macOS, Linux | Plugins riches, fournisseurs multiples |\n| Chatbox | Windows, macOS, Linux | Léger, rapide |\n| BotGem | iOS, macOS | Expérience Apple native |\n| TypingMind | Web, Bureau | Personas personnalisés, plugins |\n| OpenCat | iOS, macOS | Natif, support hors ligne |\n| ChatWise | Windows, macOS | Collaboration d'équipe |\n\n## Étapes de configuration\n\n### Étape 1 : Obtenir votre clé API\n\n1. Connectez-vous au [Tableau de bord LemonData](https://lemondata.cc/dashboard)\n2. Accédez aux [Clés API](https://lemondata.cc/dashboard/api)\n3. Créez et copiez votre clé API (format : `sk-...`)\n\n### Étape 2 : Configurer votre application\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Pour LobeChat auto-hébergé :**\n\n    Définissez les variables d'environnement :\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **Pour LobeChat Cloud :**\n    1. Ouvrez **Settings** → **AI Service Provider**\n    2. Sélectionnez **OpenAI**\n    3. Entrez votre clé API LemonData\n    4. Définissez le point de terminaison personnalisé : `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. Ouvrez **Settings** → **Model Provider**\n    2. Cliquez sur **Add Provider**\n    3. Sélectionnez **OpenAI API Compatible**\n    4. Entrez :\n       - **Name** : LemonData\n       - **API Base URL** : `https://api.lemondata.cc/v1`\n       - **API Key** : `sk-your-lemondata-key`\n    5. Ajoutez des modèles : `gpt-4o`, `claude-sonnet-4-5`, etc.\n    6. Enregistrez et testez\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. Ouvrez **Settings** → **AI Model**\n    2. Sélectionnez **OpenAI API Compatible**\n    3. Entrez :\n       - **API Host** : `https://api.lemondata.cc/v1`\n       - **API Key** : `sk-your-lemondata-key`\n       - **Model** : `gpt-4o`\n    4. Cliquez sur **Verify** pour tester la connexion\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. Ouvrez **Settings** → **Providers**\n    2. Appuyez sur **Add Custom Provider**\n    3. Entrez :\n       - **Name** : LemonData\n       - **Base URL** : `https://api.lemondata.cc/v1`\n       - **API Key** : `sk-your-lemondata-key`\n    4. Ajoutez vos modèles préférés\n    5. Enregistrez la configuration\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. Cliquez sur **Settings** (icône d'engrenage)\n    2. Sélectionnez **Custom Endpoint**\n    3. Entrez :\n       - **Endpoint URL** : `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key** : `sk-your-lemondata-key`\n    4. Configurez les modèles et enregistrez\n  </Tab>\n</Tabs>\n\n### Étape 3 : Ajouter des modèles\n\nAprès avoir configuré le fournisseur, ajoutez les modèles que vous souhaitez utiliser :\n\n| Catégorie | Modèles |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Référence de configuration\n\n| Champ | Valeur |\n|-------|-------|\n| URL de base | `https://api.lemondata.cc/v1` |\n| Clé API | `sk-your-lemondata-key` |\n| Type d'API | Compatible OpenAI |\n\n**Configuration JSON (pour les applications qui le prennent en charge) :**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Dépannage\n\n<AccordionGroup>\n  <Accordion title=\"Erreur de connexion\">\n    - Vérifiez que l'URL de base est exactement `https://api.lemondata.cc/v1`\n    - Vérifiez les barres obliques de fin (supprimez-les si présentes)\n    - Vérifiez la connectivité Internet et les paramètres du pare-feu\n  </Accordion>\n\n  <Accordion title=\"401 Non autorisé\">\n    - Vérifiez à nouveau votre clé API\n    - Vérifiez que la clé est active dans le [Tableau de bord LemonData](https://lemondata.cc/dashboard/api)\n    - Assurez-vous qu'il n'y a pas d'espaces supplémentaires dans la clé\n  </Accordion>\n\n  <Accordion title=\"402 Solde insuffisant\">\n    - Vérifiez le solde de votre compte dans le tableau de bord\n    - Ajoutez des crédits à votre compte\n  </Accordion>\n\n  <Accordion title=\"404 Modèle non disponible\">\n    - Vérifiez que le nom du modèle est correctement orthographié\n    - Vérifiez la disponibilité du modèle sur [lemondata.cc/en/models](https://lemondata.cc/fr/models)\n    - Essayez un autre modèle\n  </Accordion>\n\n  <Accordion title=\"Réponses lentes\">\n    - Essayez un modèle plus rapide (par ex., `gpt-4o-mini` au lieu de `gpt-4o`)\n    - Réduisez le paramètre `max_tokens` si disponible\n    - Vérifiez votre connexion réseau\n  </Accordion>\n</AccordionGroup>\n\n## Conseils\n\n<AccordionGroup>\n  <Accordion title=\"Utilisez les modèles appropriés\">\n    Adaptez le modèle à votre tâche - utilisez GPT-4o-mini pour les discussions rapides, Claude ou GPT-4o pour les tâches complexes.\n  </Accordion>\n\n  <Accordion title=\"Enregistrez les prompts fréquemment utilisés\">\n    La plupart des applications permettent d'enregistrer des prompts ou des personas. Créez des modèles pour les tâches courantes.\n  </Accordion>\n\n  <Accordion title=\"Surveillez l'utilisation\">\n    Consultez votre utilisation dans le tableau de bord LemonData pour suivre les coûts et gérer votre budget.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Aplicaciones de Chat de IA\"\ndescription: \"Configure aplicaciones de chat de IA populares para usar la API de LemonData\"\n---\n\n## Visión general\n\nLemonData funciona con cualquier aplicación de chat compatible con OpenAI. Esta guía cubre la configuración para aplicaciones de chat populares de escritorio y móviles.\n\n## Aplicaciones compatibles\n\n| Aplicación | Plataforma | Características |\n|-------------|----------|----------|\n| LobeChat | Web, Escritorio, Auto-alojado | Multi-proveedor, plugins, RAG |\n| CherryStudio | Windows, macOS, Linux | Plugins enriquecidos, múltiples proveedores |\n| Chatbox | Windows, macOS, Linux | Ligero, rápido |\n| BotGem | iOS, macOS | Experiencia nativa de Apple |\n| TypingMind | Web, Escritorio | Personas personalizadas, plugins |\n| OpenCat | iOS, macOS | Nativo, soporte offline |\n| ChatWise | Windows, macOS | Colaboración en equipo |\n\n## Pasos de configuración\n\n### Paso 1: Obtenga su API Key\n\n1. Inicie sesión en el [Dashboard de LemonData](https://lemondata.cc/dashboard)\n2. Navegue a [API Keys](https://lemondata.cc/dashboard/api)\n3. Cree y copie su API key (formato: `sk-...`)\n\n### Paso 2: Configure su aplicación\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Para LobeChat auto-alojado:**\n\n    Establezca las variables de entorno:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **Para LobeChat Cloud:**\n    1. Abra **Settings** → **AI Service Provider**\n    2. Seleccione **OpenAI**\n    3. Ingrese su API key de LemonData\n    4. Establezca el endpoint personalizado: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. Abra **Settings** → **Model Provider**\n    2. Haga clic en **Add Provider**\n    3. Seleccione **OpenAI API Compatible**\n    4. Ingrese:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. Agregue modelos: `gpt-4o`, `claude-sonnet-4-5`, etc.\n    6. Guarde y pruebe\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. Abra **Settings** → **AI Model**\n    2. Seleccione **OpenAI API Compatible**\n    3. Ingrese:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. Haga clic en **Verify** para probar la conexión\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. Abra **Settings** → **Providers**\n    2. Toque en **Add Custom Provider**\n    3. Ingrese:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Agregue sus modelos preferidos\n    5. Guarde la configuración\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. Haga clic en **Settings** (icono de engranaje)\n    2. Seleccione **Custom Endpoint**\n    3. Ingrese:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Configure los modelos y guarde\n  </Tab>\n</Tabs>\n\n### Paso 3: Agregue modelos\n\nDespués de configurar el proveedor, agregue los modelos que desea utilizar:\n\n| Categoría | Modelos |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Referencia de configuración\n\n| Campo | Valor |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| Tipo de API | OpenAI Compatible |\n\n**Configuración JSON (para aplicaciones que lo soportan):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Solución de problemas\n\n<AccordionGroup>\n  <Accordion title=\"Error de conexión\">\n    - Verifique que la Base URL sea exactamente `https://api.lemondata.cc/v1`\n    - Busque barras diagonales al final (elimínelas si están presentes)\n    - Verifique la conectividad a internet y la configuración del firewall\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Verifique nuevamente su API key\n    - Verifique que la clave esté activa en el [Dashboard de LemonData](https://lemondata.cc/dashboard/api)\n    - Asegúrese de que no haya espacios adicionales en la clave\n  </Accordion>\n\n  <Accordion title=\"402 Insufficient Balance\">\n    - Verifique el saldo de su cuenta en el dashboard\n    - Agregue créditos a su cuenta\n  </Accordion>\n\n  <Accordion title=\"404 Model Not Available\">\n    - Verifique que el nombre del modelo esté escrito correctamente\n    - Verifique la disponibilidad del modelo en [lemondata.cc/en/models](https://lemondata.cc/es/models)\n    - Pruebe con un modelo diferente\n  </Accordion>\n\n  <Accordion title=\"Respuestas lentas\">\n    - Pruebe un modelo más rápido (por ejemplo, `gpt-4o-mini` en lugar de `gpt-4o`)\n    - Reduzca el parámetro `max_tokens` si está disponible\n    - Verifique su conexión de red\n  </Accordion>\n</AccordionGroup>\n\n## Consejos\n\n<AccordionGroup>\n  <Accordion title=\"Use los modelos adecuados\">\n    Adapte el modelo a su tarea: use GPT-4o-mini para chats rápidos, Claude o GPT-4o para tareas complejas.\n  </Accordion>\n\n  <Accordion title=\"Guarde los prompts utilizados con frecuencia\">\n    La mayoría de las aplicaciones permiten guardar prompts o personas. Cree plantillas para tareas comunes.\n  </Accordion>\n\n  <Accordion title=\"Monitoree el uso\">\n    Consulte su uso en el dashboard de LemonData para realizar un seguimiento de los costos y gestionar su presupuesto.\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Aplicativos de Chat de IA\"\ndescription: \"Configure aplicativos de chat de IA populares para usar a API da LemonData\"\n---\n\n## Visão Geral\n\nA LemonData funciona com qualquer aplicativo de chat compatível com OpenAI. Este guia aborda a configuração para aplicativos de chat populares de desktop e dispositivos móveis.\n\n## Aplicativos Suportados\n\n| Aplicativo | Plataforma | Recursos |\n|-------------|----------|----------|\n| LobeChat | Web, Desktop, Auto-hospedado | Multi-provedor, plugins, RAG |\n| CherryStudio | Windows, macOS, Linux | Plugins avançados, múltiplos provedores |\n| Chatbox | Windows, macOS, Linux | Leve, rápido |\n| BotGem | iOS, macOS | Experiência Apple nativa |\n| TypingMind | Web, Desktop | Personas personalizadas, plugins |\n| OpenCat | iOS, macOS | Nativo, suporte offline |\n| ChatWise | Windows, macOS | Colaboração em equipe |\n\n## Passos de Configuração\n\n### Passo 1: Obtenha sua Chave de API\n\n1. Faça login no [Painel da LemonData](https://lemondata.cc/dashboard)\n2. Navegue até [Chaves de API](https://lemondata.cc/dashboard/api)\n3. Crie e copie sua chave de API (formato: `sk-...`)\n\n### Passo 2: Configure seu Aplicativo\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Para LobeChat auto-hospedado:**\n\n    Defina as variáveis de ambiente:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **Para LobeChat Cloud:**\n    1. Abra **Settings** → **AI Service Provider**\n    2. Selecione **OpenAI**\n    3. Insira sua chave de API da LemonData\n    4. Defina o endpoint personalizado: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. Abra **Settings** → **Model Provider**\n    2. Clique em **Add Provider**\n    3. Selecione **OpenAI API Compatible**\n    4. Insira:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. Adicione modelos: `gpt-4o`, `claude-sonnet-4-5`, etc.\n    6. Salve e teste\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. Abra **Settings** → **AI Model**\n    2. Selecione **OpenAI API Compatible**\n    3. Insira:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. Clique em **Verify** para testar a conexão\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. Abra **Settings** → **Providers**\n    2. Toque em **Add Custom Provider**\n    3. Insira:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Adicione seus modelos preferidos\n    5. Salve a configuração\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. Clique em **Settings** (ícone de engrenagem)\n    2. Selecione **Custom Endpoint**\n    3. Insira:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Configure os modelos e salve\n  </Tab>\n</Tabs>\n\n### Passo 3: Adicione Modelos\n\nApós configurar o provedor, adicione os modelos que deseja usar:\n\n| Categoria | Modelos |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Referência de Configuração\n\n| Campo | Valor |\n|-------|-------|\n| URL Base | `https://api.lemondata.cc/v1` |\n| Chave de API | `sk-your-lemondata-key` |\n| Tipo de API | Compatível com OpenAI |\n\n**Configuração JSON (para aplicativos que a suportam):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Solução de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Erro de Conexão\">\n    - Verifique se a URL Base é exatamente `https://api.lemondata.cc/v1`\n    - Verifique se há barras no final (remova se houver)\n    - Verifique a conectividade com a internet e as configurações de firewall\n  </Accordion>\n\n  <Accordion title=\"401 Não Autorizado\">\n    - Verifique novamente sua chave de API\n    - Verifique se a chave está ativa no [Painel da LemonData](https://lemondata.cc/dashboard/api)\n    - Certifique-se de que não há espaços extras na chave\n  </Accordion>\n\n  <Accordion title=\"402 Saldo Insuficiente\">\n    - Verifique o saldo da sua conta no painel\n    - Adicione créditos à sua conta\n  </Accordion>\n\n  <Accordion title=\"404 Modelo Não Disponível\">\n    - Verifique se o nome do modelo está escrito corretamente\n    - Verifique a disponibilidade do modelo em [lemondata.cc/en/models](https://lemondata.cc/pt/models)\n    - Tente um modelo diferente\n  </Accordion>\n\n  <Accordion title=\"Respostas Lentas\">\n    - Tente um modelo mais rápido (ex: `gpt-4o-mini` em vez de `gpt-4o`)\n    - Reduza o parâmetro `max_tokens` se disponível\n    - Verifique sua conexão de rede\n  </Accordion>\n</AccordionGroup>\n\n## Dicas\n\n<AccordionGroup>\n  <Accordion title=\"Use modelos apropriados\">\n    Combine o modelo com sua tarefa - use GPT-4o-mini para chats rápidos, Claude ou GPT-4o para tarefas complexas.\n  </Accordion>\n\n  <Accordion title=\"Salve prompts usados com frequência\">\n    A maioria dos aplicativos suporta salvar prompts ou personas. Crie modelos para tarefas comuns.\n  </Accordion>\n\n  <Accordion title=\"Monitore o uso\">\n    Verifique seu uso no painel da LemonData para acompanhar os custos e gerenciar seu orçamento.\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"تطبيقات دردشة الذكاء الاصطناعي\"\ndescription: \"تكوين تطبيقات دردشة الذكاء الاصطناعي الشهيرة لاستخدام LemonData API\"\n---\n\n## نظرة عامة\n\nيعمل LemonData مع أي تطبيق دردشة متوافق مع OpenAI. يغطي هذا الدليل خطوات التكوين لتطبيقات الدردشة الشهيرة على أجهزة الكمبيوتر والهواتف المحمولة.\n\n## التطبيقات المدعومة\n\n| التطبيق | المنصة | الميزات |\n|-------------|----------|----------|\n| LobeChat | الويب، سطح المكتب، استضافة ذاتية | متعدد المزودين، إضافات، RAG |\n| CherryStudio | ويندوز، ماك، لينكس | إضافات غنية، مزودون متعددون |\n| Chatbox | ويندوز، ماك، لينكس | خفيف الوزن، سريع |\n| BotGem | iOS، macOS | تجربة Apple أصلية |\n| TypingMind | الويب، سطح المكتب | شخصيات مخصصة، إضافات |\n| OpenCat | iOS، macOS | أصلي، دعم العمل بدون اتصال |\n| ChatWise | ويندوز، ماك | تعاون الفريق |\n\n## خطوات التكوين\n\n### الخطوة 1: الحصول على API Key الخاص بك\n\n1. قم بتسجيل الدخول إلى [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. انتقل إلى [API Keys](https://lemondata.cc/dashboard/api)\n3. قم بإنشاء ونسخ API key الخاص بك (بصيغة: `sk-...`)\n\n### الخطوة 2: تكوين تطبيقك\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **بالنسبة لـ LobeChat المستضاف ذاتياً:**\n\n    قم بتعيين متغيرات البيئة:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **بالنسبة لـ LobeChat Cloud:**\n    1. افتح **Settings** ← **AI Service Provider**\n    2. اختر **OpenAI**\n    3. أدخل LemonData API key الخاص بك\n    4. قم بتعيين endpoint مخصص: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. افتح **Settings** ← **Model Provider**\n    2. انقر فوق **Add Provider**\n    3. اختر **OpenAI API Compatible**\n    4. أدخل:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. أضف النماذج: `gpt-4o`، `claude-sonnet-4-5`، إلخ.\n    6. احفظ واختبر\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. افتح **Settings** ← **AI Model**\n    2. اختر **OpenAI API Compatible**\n    3. أدخل:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. انقر فوق **Verify** لاختبار الاتصال\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. افتح **Settings** ← **Providers**\n    2. اضغط على **Add Custom Provider**\n    3. أدخل:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. أضف نماذجك المفضلة\n    5. احفظ التكوين\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. انقر فوق **Settings** (أيقونة الترس)\n    2. اختر **Custom Endpoint**\n    3. أدخل:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. قم بتكوين النماذج واحفظ\n  </Tab>\n</Tabs>\n\n### الخطوة 3: إضافة النماذج\n\nبعد تكوين المزود، أضف النماذج التي تريد استخدامها:\n\n| الفئة | النماذج |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## مرجع التكوين\n\n| الحقل | القيمة |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| نوع API | متوافق مع OpenAI |\n\n**تكوين JSON (للتطبيقات التي تدعم ذلك):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## استكشاف الأخطاء وإصلاحها\n\n<AccordionGroup>\n  <Accordion title=\"خطأ في الاتصال\">\n    - تحقق من أن Base URL هو بالضبط `https://api.lemondata.cc/v1`\n    - تحقق من وجود شرطات مائلة في النهاية (قم بإزالتها إذا وجدت)\n    - تحقق من الاتصال بالإنترنت وإعدادات جدار الحماية\n  </Accordion>\n\n  <Accordion title=\"401 غير مصرح به\">\n    - تحقق مرة أخرى من API key الخاص بك\n    - تحقق من أن المفتاح نشط في [LemonData Dashboard](https://lemondata.cc/dashboard/api)\n    - تأكد من عدم وجود مسافات إضافية في المفتاح\n  </Accordion>\n\n  <Accordion title=\"402 رصيد غير كافٍ\">\n    - تحقق من رصيد حسابك في لوحة التحكم\n    - أضف رصيداً إلى حسابك\n  </Accordion>\n\n  <Accordion title=\"404 النموذج غير متوفر\">\n    - تحقق من كتابة اسم النموذج بشكل صحيح\n    - تحقق من توفر النموذج في [lemondata.cc/en/models](https://lemondata.cc/ar/models)\n    - جرب نموذجاً مختلفاً\n  </Accordion>\n\n  <Accordion title=\"استجابات بطيئة\">\n    - جرب نموذجاً أسرع (على سبيل المثال، `gpt-4o-mini` بدلاً من `gpt-4o`)\n    - قم بتقليل معلمة `max_tokens` إذا كانت متاحة\n    - تحقق من اتصال الشبكة الخاص بك\n  </Accordion>\n</AccordionGroup>\n\n## نصائح\n\n<AccordionGroup>\n  <Accordion title=\"استخدم النماذج المناسبة\">\n    طابق النموذج مع مهمتك - استخدم GPT-4o-mini للدردشات السريعة، وClaude أو GPT-4o للمهام المعقدة.\n  </Accordion>\n\n  <Accordion title=\"احفظ المطالبات المستخدمة بشكل متكرر\">\n    تدعم معظم التطبيقات حفظ المطالبات (prompts) أو الشخصيات. قم بإنشاء قوالب للمهام الشائعة.\n  </Accordion>\n\n  <Accordion title=\"مراقبة الاستخدام\">\n    تحقق من استخدامك في لوحة تحكم LemonData لتتبع التكاليف وإدارة ميزانيتك.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Ứng dụng Chat AI\"\ndescription: \"Cấu hình các ứng dụng chat AI phổ biến để sử dụng LemonData API\"\n---\n\n## Tổng quan\n\nLemonData hoạt động với bất kỳ ứng dụng chat nào tương thích với OpenAI. Hướng dẫn này bao gồm cách cấu hình cho các ứng dụng chat phổ biến trên máy tính và thiết bị di động.\n\n## Các ứng dụng được hỗ trợ\n\n| Ứng dụng | Nền tảng | Tính năng |\n|-------------|----------|----------|\n| LobeChat | Web, Máy tính, Tự lưu trữ (Self-hosted) | Đa nhà cung cấp, plugin, RAG |\n| CherryStudio | Windows, macOS, Linux | Plugin phong phú, nhiều nhà cung cấp |\n| Chatbox | Windows, macOS, Linux | Nhẹ, nhanh |\n| BotGem | iOS, macOS | Trải nghiệm Apple nguyên bản |\n| TypingMind | Web, Máy tính | Persona tùy chỉnh, plugin |\n| OpenCat | iOS, macOS | Nguyên bản, hỗ trợ ngoại tuyến |\n| ChatWise | Windows, macOS | Cộng tác nhóm |\n\n## Các bước cấu hình\n\n### Bước 1: Lấy API Key của bạn\n\n1. Đăng nhập vào [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Đi tới [API Keys](https://lemondata.cc/dashboard/api)\n3. Tạo và sao chép API key của bạn (định dạng: `sk-...`)\n\n### Bước 2: Cấu hình ứng dụng của bạn\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Đối với LobeChat tự lưu trữ (Self-hosted):**\n\n    Thiết lập các biến môi trường:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **Đối với LobeChat Cloud:**\n    1. Mở **Settings** → **AI Service Provider**\n    2. Chọn **OpenAI**\n    3. Nhập LemonData API key của bạn\n    4. Thiết lập endpoint tùy chỉnh: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. Mở **Settings** → **Model Provider**\n    2. Nhấp vào **Add Provider**\n    3. Chọn **OpenAI API Compatible**\n    4. Nhập:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. Thêm các model: `gpt-4o`, `claude-sonnet-4-5`, v.v.\n    6. Lưu và kiểm tra\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. Mở **Settings** → **AI Model**\n    2. Chọn **OpenAI API Compatible**\n    3. Nhập:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. Nhấp vào **Verify** để kiểm tra kết nối\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. Mở **Settings** → **Providers**\n    2. Nhấn vào **Add Custom Provider**\n    3. Nhập:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Thêm các model ưa thích của bạn\n    5. Lưu cấu hình\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. Nhấp vào **Settings** (biểu tượng bánh răng)\n    2. Chọn **Custom Endpoint**\n    3. Nhập:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Cấu hình các model và lưu\n  </Tab>\n</Tabs>\n\n### Bước 3: Thêm các Model\n\nSau khi cấu hình nhà cung cấp, hãy thêm các model bạn muốn sử dụng:\n\n| Danh mục | Model |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Tham chiếu cấu hình\n\n| Trường | Giá trị |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API Type | OpenAI Compatible |\n\n**Cấu hình JSON (dành cho các ứng dụng hỗ trợ):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Xử lý sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Lỗi kết nối\">\n    - Xác minh Base URL chính xác là `https://api.lemondata.cc/v1`\n    - Kiểm tra các dấu gạch chéo ở cuối (xóa nếu có)\n    - Kiểm tra kết nối internet và cài đặt tường lửa\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized (Không được phép)\">\n    - Kiểm tra kỹ API key của bạn\n    - Xác minh key đang hoạt động trong [LemonData Dashboard](https://lemondata.cc/dashboard/api)\n    - Đảm bảo không có khoảng trắng thừa trong key\n  </Accordion>\n\n  <Accordion title=\"402 Insufficient Balance (Số dư không đủ)\">\n    - Kiểm tra số dư tài khoản trong dashboard\n    - Nạp thêm tiền vào tài khoản của bạn\n  </Accordion>\n\n  <Accordion title=\"404 Model Not Available (Model không khả dụng)\">\n    - Xác minh tên model được viết chính xác\n    - Kiểm tra tính khả dụng của model tại [lemondata.cc/en/models](https://lemondata.cc/vi/models)\n    - Thử một model khác\n  </Accordion>\n\n  <Accordion title=\"Phản hồi chậm\">\n    - Thử một model nhanh hơn (ví dụ: `gpt-4o-mini` thay vì `gpt-4o`)\n    - Giảm tham số `max_tokens` nếu có\n    - Kiểm tra kết nối mạng của bạn\n  </Accordion>\n</AccordionGroup>\n\n## Mẹo\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng các model phù hợp\">\n    Chọn model phù hợp với tác vụ của bạn - sử dụng GPT-4o-mini cho các cuộc trò chuyện nhanh, Claude hoặc GPT-4o cho các tác vụ phức tạp.\n  </Accordion>\n\n  <Accordion title=\"Lưu các prompt thường dùng\">\n    Hầu hết các ứng dụng đều hỗ trợ lưu prompt hoặc persona. Hãy tạo các mẫu cho những tác vụ phổ biến.\n  </Accordion>\n\n  <Accordion title=\"Theo dõi mức sử dụng\">\n    Kiểm tra mức sử dụng của bạn trong LemonData dashboard để theo dõi chi phí và quản lý ngân sách.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Aplikasi Chat AI\"\ndescription: \"Konfigurasikan aplikasi chat AI populer untuk menggunakan LemonData API\"\n---\n\n## Ringkasan\n\nLemonData bekerja dengan aplikasi chat apa pun yang kompatibel dengan OpenAI. Panduan ini mencakup konfigurasi untuk aplikasi chat desktop dan seluler yang populer.\n\n## Aplikasi yang Didukung\n\n| Aplikasi | Platform | Fitur |\n|-------------|----------|----------|\n| LobeChat | Web, Desktop, Self-hosted | Multi-provider, plugin, RAG |\n| CherryStudio | Windows, macOS, Linux | Plugin yang kaya, beberapa provider |\n| Chatbox | Windows, macOS, Linux | Ringan, cepat |\n| BotGem | iOS, macOS | Pengalaman Apple native |\n| TypingMind | Web, Desktop | Persona kustom, plugin |\n| OpenCat | iOS, macOS | Native, dukungan offline |\n| ChatWise | Windows, macOS | Kolaborasi tim |\n\n## Langkah-langkah Konfigurasi\n\n### Langkah 1: Dapatkan API Key Anda\n\n1. Masuk ke [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Buka [API Keys](https://lemondata.cc/dashboard/api)\n3. Buat dan salin API key Anda (format: `sk-...`)\n\n### Langkah 2: Konfigurasikan Aplikasi Anda\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Untuk LobeChat Self-hosted:**\n\n    Atur environment variables:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **Untuk LobeChat Cloud:**\n    1. Buka **Settings** → **AI Service Provider**\n    2. Pilih **OpenAI**\n    3. Masukkan LemonData API key Anda\n    4. Atur endpoint kustom: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. Buka **Settings** → **Model Provider**\n    2. Klik **Add Provider**\n    3. Pilih **OpenAI API Compatible**\n    4. Masukkan:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. Tambahkan model: `gpt-4o`, `claude-sonnet-4-5`, dll.\n    6. Simpan dan uji\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. Buka **Settings** → **AI Model**\n    2. Pilih **OpenAI API Compatible**\n    3. Masukkan:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. Klik **Verify** untuk menguji koneksi\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. Buka **Settings** → **Providers**\n    2. Ketuk **Add Custom Provider**\n    3. Masukkan:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Tambahkan model pilihan Anda\n    5. Simpan konfigurasi\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. Klik **Settings** (ikon gerigi)\n    2. Pilih **Custom Endpoint**\n    3. Masukkan:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Konfigurasikan model dan simpan\n  </Tab>\n</Tabs>\n\n### Langkah 3: Tambahkan Model\n\nSetelah mengonfigurasi provider, tambahkan model yang ingin Anda gunakan:\n\n| Kategori | Model |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Referensi Konfigurasi\n\n| Bidang | Nilai |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API Type | OpenAI Compatible |\n\n**Konfigurasi JSON (untuk aplikasi yang mendukungnya):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Pemecahan Masalah\n\n<AccordionGroup>\n  <Accordion title=\"Kesalahan Koneksi\">\n    - Verifikasi Base URL tepat `https://api.lemondata.cc/v1`\n    - Periksa garis miring di akhir (hapus jika ada)\n    - Verifikasi konektivitas internet dan pengaturan firewall\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Periksa kembali API key Anda\n    - Verifikasi bahwa key aktif di [LemonData Dashboard](https://lemondata.cc/dashboard/api)\n    - Pastikan tidak ada spasi tambahan pada key\n  </Accordion>\n\n  <Accordion title=\"402 Saldo Tidak Mencukupi\">\n    - Periksa saldo akun Anda di dashboard\n    - Tambahkan kredit ke akun Anda\n  </Accordion>\n\n  <Accordion title=\"404 Model Tidak Tersedia\">\n    - Verifikasi nama model dieja dengan benar\n    - Periksa ketersediaan model di [lemondata.cc/en/models](https://lemondata.cc/id/models)\n    - Coba model lain\n  </Accordion>\n\n  <Accordion title=\"Respons Lambat\">\n    - Coba model yang lebih cepat (misalnya, `gpt-4o-mini` alih-alih `gpt-4o`)\n    - Kurangi parameter `max_tokens` jika tersedia\n    - Periksa koneksi jaringan Anda\n  </Accordion>\n</AccordionGroup>\n\n## Tips\n\n<AccordionGroup>\n  <Accordion title=\"Gunakan model yang sesuai\">\n    Sesuaikan model dengan tugas Anda - gunakan GPT-4o-mini untuk chat cepat, Claude atau GPT-4o untuk tugas yang kompleks.\n  </Accordion>\n\n  <Accordion title=\"Simpan prompt yang sering digunakan\">\n    Sebagian besar aplikasi mendukung penyimpanan prompt atau persona. Buat templat untuk tugas-tugas umum.\n  </Accordion>\n\n  <Accordion title=\"Pantau penggunaan\">\n    Periksa penggunaan Anda di dashboard LemonData untuk melacak biaya dan mengelola anggaran Anda.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"AI Sohbet Uygulamaları\"\ndescription: \"Popüler AI sohbet uygulamalarını LemonData API kullanacak şekilde yapılandırın\"\n---\n\n## Genel Bakış\n\nLemonData, herhangi bir OpenAI uyumlu sohbet uygulamasıyla çalışır. Bu kılavuz, popüler masaüstü ve mobil sohbet uygulamaları için yapılandırmayı kapsar.\n\n## Desteklenen Uygulamalar\n\n| Uygulama | Platform | Özellikler |\n|-------------|----------|----------|\n| LobeChat | Web, Masaüstü, Self-hosted | Çoklu sağlayıcı, eklentiler, RAG |\n| CherryStudio | Windows, macOS, Linux | Zengin eklentiler, çoklu sağlayıcılar |\n| Chatbox | Windows, macOS, Linux | Hafif, hızlı |\n| BotGem | iOS, macOS | Yerel Apple deneyimi |\n| TypingMind | Web, Masaüstü | Özel personalar, eklentiler |\n| OpenCat | iOS, macOS | Yerel, çevrimdışı destek |\n| ChatWise | Windows, macOS | Ekip iş birliği |\n\n## Yapılandırma Adımları\n\n### Adım 1: API Anahtarınızı Alın\n\n1. [LemonData Dashboard](https://lemondata.cc/dashboard) paneline giriş yapın\n2. [API Keys](https://lemondata.cc/dashboard/api) bölümüne gidin\n3. API anahtarınızı oluşturun ve kopyalayın (format: `sk-...`)\n\n### Adım 2: Uygulamanızı Yapılandırın\n\n<Tabs>\n  <Tab title=\"LobeChat\">\n    **Kendi sunucunuzda barındırılan LobeChat için:**\n\n    Ortam değişkenlerini ayarlayın:\n    ```bash\n    OPENAI_API_KEY=sk-your-lemondata-key\n    OPENAI_PROXY_URL=https://api.lemondata.cc/v1\n    ```\n\n    **LobeChat Cloud için:**\n    1. **Settings** → **AI Service Provider** kısmını açın\n    2. **OpenAI** seçeneğini belirleyin\n    3. LemonData API anahtarınızı girin\n    4. Özel uç noktayı (endpoint) ayarlayın: `https://api.lemondata.cc/v1`\n  </Tab>\n  <Tab title=\"CherryStudio\">\n    1. **Settings** → **Model Provider** kısmını açın\n    2. **Add Provider** butonuna tıklayın\n    3. **OpenAI API Compatible** seçeneğini belirleyin\n    4. Şunları girin:\n       - **Name**: LemonData\n       - **API Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    5. Modelleri ekleyin: `gpt-4o`, `claude-sonnet-4-5`, vb.\n    6. Kaydedin ve test edin\n  </Tab>\n  <Tab title=\"Chatbox\">\n    1. **Settings** → **AI Model** kısmını açın\n    2. **OpenAI API Compatible** seçeneğini belirleyin\n    3. Şunları girin:\n       - **API Host**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n       - **Model**: `gpt-4o`\n    4. Bağlantıyı test etmek için **Verify** butonuna tıklayın\n  </Tab>\n  <Tab title=\"BotGem\">\n    1. **Settings** → **Providers** kısmını açın\n    2. **Add Custom Provider** seçeneğine dokunun\n    3. Şunları girin:\n       - **Name**: LemonData\n       - **Base URL**: `https://api.lemondata.cc/v1`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Tercih ettiğiniz modelleri ekleyin\n    5. Yapılandırmayı kaydedin\n  </Tab>\n  <Tab title=\"TypingMind\">\n    1. **Settings** (dişli simgesi) kısmına tıklayın\n    2. **Custom Endpoint** seçeneğini belirleyin\n    3. Şunları girin:\n       - **Endpoint URL**: `https://api.lemondata.cc/v1/chat/completions`\n       - **API Key**: `sk-your-lemondata-key`\n    4. Modelleri yapılandırın ve kaydedin\n  </Tab>\n</Tabs>\n\n### Adım 3: Modelleri Ekleyin\n\nSağlayıcıyı yapılandırdıktan sonra kullanmak istediğiniz modelleri ekleyin:\n\n| Kategori | Modeller |\n|----------|--------|\n| OpenAI | `gpt-4o`, `gpt-4o-mini`, `o1`, `o3-mini` |\n| Anthropic | `claude-sonnet-4-5`, `claude-opus-4-5` |\n| Google | `gemini-2.5-flash`, `gemini-2.5-pro` |\n| DeepSeek | `deepseek-r1`, `deepseek-chat` |\n\n## Yapılandırma Referansı\n\n| Alan | Değer |\n|-------|-------|\n| Base URL | `https://api.lemondata.cc/v1` |\n| API Key | `sk-your-lemondata-key` |\n| API Type | OpenAI Compatible |\n\n**JSON Yapılandırması (destekleyen uygulamalar için):**\n\n```json\n{\n  \"provider\": \"LemonData\",\n  \"baseURL\": \"https://api.lemondata.cc/v1\",\n  \"apiKey\": \"sk-your-lemondata-key\",\n  \"models\": [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-5\",\n    \"gemini-2.5-flash\"\n  ]\n}\n```\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Bağlantı Hatası\">\n    - Base URL'in tam olarak `https://api.lemondata.cc/v1` olduğunu doğrulayın\n    - Sondaki eğik çizgileri kontrol edin (varsa kaldırın)\n    - İnternet bağlantısını ve güvenlik duvarı ayarlarını doğrulayın\n  </Accordion>\n\n  <Accordion title=\"401 Yetkisiz (Unauthorized)\">\n    - API anahtarınızı tekrar kontrol edin\n    - Anahtarın [LemonData Dashboard](https://lemondata.cc/dashboard/api) üzerinde aktif olduğunu doğrulayın\n    - Anahtarda fazladan boşluk olmadığından emin olun\n  </Accordion>\n\n  <Accordion title=\"402 Yetersiz Bakiye (Insufficient Balance)\">\n    - Kontrol panelinden hesap bakiyenizi kontrol edin\n    - Hesabınıza kredi ekleyin\n  </Accordion>\n\n  <Accordion title=\"404 Model Mevcut Değil (Model Not Available)\">\n    - Model adının doğru yazıldığını doğrulayın\n    - [lemondata.cc/en/models](https://lemondata.cc/tr/models) adresinden modelin kullanılabilirliğini kontrol edin\n    - Farklı bir model deneyin\n  </Accordion>\n\n  <Accordion title=\"Yavaş Yanıtlar\">\n    - Daha hızlı bir model deneyin (örneğin, `gpt-4o` yerine `gpt-4o-mini`)\n    - Varsa `max_tokens` parametresini azaltın\n    - Ağ bağlantınızı kontrol edin\n  </Accordion>\n</AccordionGroup>\n\n## İpuçları\n\n<AccordionGroup>\n  <Accordion title=\"Uygun modelleri kullanın\">\n    Modeli görevinize göre seçin - hızlı sohbetler için GPT-4o-mini, karmaşık görevler için Claude veya GPT-4o kullanın.\n  </Accordion>\n\n  <Accordion title=\"Sık kullanılan istemleri kaydedin\">\n    Çoğu uygulama istemleri (prompts) veya personaları kaydetmeyi destekler. Yaygın görevler için şablonlar oluşturun.\n  </Accordion>\n\n  <Accordion title=\"Kullanımı izleyin\">\n    Maliyetleri takip etmek ve bütçenizi yönetmek için LemonData kontrol panelinden kullanımınızı kontrol edin.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:34:32.289Z"
  },
  "integrations/anthropic-sdk.mdx": {
    "sourceHash": "087784da6b737f39",
    "translations": {
      "zh": "---\ntitle: \"Anthropic SDK\"\ndescription: \"在 Claude 模型中通过 Anthropic SDK 使用 LemonData\"\n---\n\n## 概览\n\nLemonData 支持原生的 Anthropic Messages API 格式。通过 LemonData 使用官方 Anthropic SDK 即可访问 Claude 模型。\n\n## 安装\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## 配置\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  注意：对于 Anthropic SDK，base URL 为 `https://api.lemondata.cc`（不包含 `/v1`）。\n</Note>\n\n## 基础用法\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## 使用 System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## 流式传输\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## 视觉 (Vision)\n\n```python\nimport base64\n\n# 来自 URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# 来自 base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## 工具调用 (Tool Use)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# 检查工具调用\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## 深度思考 (Extended Thinking) (Claude Opus 4.5)\n\n对于支持深度思考的模型：\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# 访问思考区块\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## 可用的 Claude 模型\n\n| 模型 | 最适用于 |\n|-------|----------|\n| `claude-opus-4-5` | 复杂推理，深度思考 |\n| `claude-sonnet-4-5` | 通用场景，编程 |\n| `claude-haiku-4-5` | 快速响应 |\n\n## 错误处理\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## 对比：OpenAI SDK vs Anthropic SDK\n\n两者均可配合 LemonData 用于 Claude 模型：\n\n| 特性 | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | 在 `messages` 数组中 | 独立的 `system` 参数 |\n| 深度思考 (Extended thinking) | 不支持 | 支持 |\n\n请根据您的偏好或现有代码库进行选择。",
      "zh-TW": "---\ntitle: \"Anthropic SDK\"\ndescription: \"搭配 Anthropic SDK 使用 LemonData 存取 Claude 模型\"\n---\n\n## 概覽\n\nLemonData 支援原生的 Anthropic Messages API 格式。請搭配 LemonData 使用官方 Anthropic SDK 來存取 Claude 模型。\n\n## 安裝\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## 配置\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  注意：Anthropic SDK 的 base URL 為 `https://api.lemondata.cc`（不含 `/v1`）。\n</Note>\n\n## 基本用法\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## 使用 System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## 串流 (Streaming)\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## 視覺 (Vision)\n\n```python\nimport base64\n\n# From URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# From base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## 工具使用 (Tool Use)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Check for tool use\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## 延伸思考 (Extended Thinking) (Claude Opus 4.5)\n\n適用於支援延伸思考的模型：\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Access thinking blocks\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## 可用的 Claude 模型\n\n| 模型 | 最適合 |\n|-------|----------|\n| `claude-opus-4-5` | 複雜推理、延伸思考 |\n| `claude-sonnet-4-5` | 通用用途、程式編寫 |\n| `claude-haiku-4-5` | 快速回應 |\n\n## 錯誤處理\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## 比較：OpenAI SDK vs Anthropic SDK\n\n兩者皆可搭配 LemonData 用於 Claude 模型：\n\n| 特性 | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | 在 `messages` 陣列中 | 獨立的 `system` 參數 |\n| 延伸思考 (Extended thinking) | 不支援 | 支援 |\n\n請根據您的偏好或現有的程式碼庫進行選擇。",
      "ja": "---\ntitle: \"Anthropic SDK\"\ndescription: \"ClaudeモデルでAnthropic SDKとLemonDataを使用する\"\n---\n\n## 概要\n\nLemonDataは、ネイティブのAnthropic Messages API形式をサポートしています。Claudeモデルにアクセスするには、LemonDataで公式のAnthropic SDKを使用してください。\n\n## インストール\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## 設定\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  注意: Anthropic SDKの場合、ベースURLは `https://api.lemondata.cc` （`/v1` なし）となります。\n</Note>\n\n## 基本的な使い方\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## システムプロンプトの使用\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## ストリーミング\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## ビジョン（画像認識）\n\n```python\nimport base64\n\n# URLから\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# base64から\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## ツール利用（Tool Use）\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# ツール利用の確認\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## 思考の拡張（Extended Thinking - Claude Opus 4.5）\n\n思考の拡張をサポートしているモデルの場合:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# 思考ブロックへのアクセス\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## 利用可能なClaudeモデル\n\n| モデル | 最適な用途 |\n|-------|----------|\n| `claude-opus-4-5` | 複雑な推論、思考の拡張 |\n| `claude-sonnet-4-5` | 汎用、コーディング |\n| `claude-haiku-4-5` | 高速なレスポンス |\n\n## エラーハンドリング\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## 比較: OpenAI SDK vs Anthropic SDK\n\nどちらもClaudeモデルでLemonDataを使用できます:\n\n| 機能 | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| ベースURL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| エンドポイント | `/chat/completions` | `/v1/messages` |\n| システムプロンプト | `messages`配列内 | 個別の`system`パラメータ |\n| 思考の拡張（Extended Thinking） | 非対応 | 対応 |\n\n好みや既存のコードベースに基づいて選択してください。",
      "ko": "---\ntitle: \"Anthropic SDK\"\ndescription: \"Claude 모델을 위해 Anthropic SDK와 함께 LemonData를 사용하세요\"\n---\n\n## 개요\n\nLemonData는 네이티브 Anthropic Messages API 형식을 지원합니다. Claude 모델에 액세스하려면 LemonData와 함께 공식 Anthropic SDK를 사용하세요.\n\n## 설치\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## 설정\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  참고: Anthropic SDK의 경우 base URL은 `https://api.lemondata.cc` (`/v1` 제외)입니다.\n</Note>\n\n## 기본 사용법\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## System Prompt 사용\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## 스트리밍\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Vision\n\n```python\nimport base64\n\n# URL에서 가져오기\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# base64에서 가져오기\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## 도구 사용 (Tool Use)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# 도구 사용 확인\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nExtended thinking을 지원하는 모델의 경우:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# thinking 블록 액세스\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## 사용 가능한 Claude 모델\n\n| 모델 | 최적 용도 |\n|-------|----------|\n| `claude-opus-4-5` | 복잡한 추론, extended thinking |\n| `claude-sonnet-4-5` | 일반적인 용도, 코딩 |\n| `claude-haiku-4-5` | 빠른 응답 |\n\n## 오류 처리\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## 비교: OpenAI SDK vs Anthropic SDK\n\n두 SDK 모두 Claude 모델을 위해 LemonData와 함께 작동합니다:\n\n| 기능 | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | `messages` 배열 내 | 별도의 `system` 파라미터 |\n| Extended thinking | 지원되지 않음 | 지원됨 |\n\n선호도나 기존 코드베이스에 따라 선택하세요.",
      "de": "---\ntitle: \"Anthropic SDK\"\ndescription: \"Nutzen Sie LemonData mit dem Anthropic SDK für Claude-Modelle\"\n---\n\n## Übersicht\n\nLemonData unterstützt das native Anthropic Messages API-Format. Verwenden Sie das offizielle Anthropic SDK mit LemonData, um auf Claude-Modelle zuzugreifen.\n\n## Installation\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Konfiguration\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Hinweis: Die Basis-URL für das Anthropic SDK ist `https://api.lemondata.cc` (ohne `/v1`).\n</Note>\n\n## Grundlegende Verwendung\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## Mit System-Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Vision\n\n```python\nimport base64\n\n# From URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# From base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## Tool-Nutzung\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Check for tool use\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nFür Modelle, die Extended Thinking unterstützen:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Access thinking blocks\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## Verfügbare Claude-Modelle\n\n| Modell | Bestens geeignet für |\n|-------|----------|\n| `claude-opus-4-5` | Komplexes logisches Schließen, Extended Thinking |\n| `claude-sonnet-4-5` | Allgemeine Zwecke, Programmierung |\n| `claude-haiku-4-5` | Schnelle Antworten |\n\n## Fehlerbehandlung\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## Vergleich: OpenAI SDK vs. Anthropic SDK\n\nBeide funktionieren mit LemonData für Claude-Modelle:\n\n| Funktion | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Basis-URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpunkt | `/chat/completions` | `/v1/messages` |\n| System-Prompt | Im `messages`-Array | Separater `system`-Parameter |\n| Extended Thinking | Nicht unterstützt | Unterstützt |\n\nWählen Sie basierend auf Ihren Vorlieben oder Ihrer bestehenden Codebasis.",
      "fr": "---\ntitle: \"SDK Anthropic\"\ndescription: \"Utilisez LemonData avec le SDK Anthropic pour les modèles Claude\"\n---\n\n## Présentation\n\nLemonData prend en charge le format natif de l'API Anthropic Messages. Utilisez le SDK Anthropic officiel avec LemonData pour accéder aux modèles Claude.\n\n## Installation\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Configuration\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Remarque : L'URL de base est `https://api.lemondata.cc` (sans `/v1`) pour le SDK Anthropic.\n</Note>\n\n## Utilisation de base\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## Avec un System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Vision\n\n```python\nimport base64\n\n# Depuis une URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# Depuis base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## Utilisation d'outils (Tool Use)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Vérifier l'utilisation d'outils\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nPour les modèles qui prennent en charge l'Extended Thinking :\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Accéder aux blocs de pensée\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## Modèles Claude disponibles\n\n| Modèle | Idéal pour |\n|-------|----------|\n| `claude-opus-4-5` | Raisonnement complexe, Extended Thinking |\n| `claude-sonnet-4-5` | Usage général, codage |\n| `claude-haiku-4-5` | Réponses rapides |\n\n## Gestion des erreurs\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## Comparaison : SDK OpenAI vs SDK Anthropic\n\nLes deux fonctionnent avec LemonData pour les modèles Claude :\n\n| Fonctionnalité | SDK OpenAI | SDK Anthropic |\n|---------|-----------|---------------|\n| URL de base | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | Dans le tableau `messages` | Paramètre `system` séparé |\n| Extended thinking | Non pris en charge | Pris en charge |\n\nChoisissez en fonction de vos préférences ou de votre base de code existante.",
      "es": "---\ntitle: \"SDK de Anthropic\"\ndescription: \"Use LemonData con el SDK de Anthropic para modelos Claude\"\n---\n\n## Descripción general\n\nLemonData es compatible con el formato nativo de la API de Messages de Anthropic. Utilice el SDK oficial de Anthropic con LemonData para acceder a los modelos Claude.\n\n## Instalación\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Configuración\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Nota: La URL base es `https://api.lemondata.cc` (sin `/v1`) para el SDK de Anthropic.\n</Note>\n\n## Uso básico\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## Con System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Visión\n\n```python\nimport base64\n\n# Desde URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# Desde base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## Uso de herramientas\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Verificar el uso de herramientas\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nPara modelos que admiten extended thinking:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Acceder a los bloques de pensamiento\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## Modelos Claude disponibles\n\n| Modelo | Ideal para |\n|-------|----------|\n| `claude-opus-4-5` | Razonamiento complejo, extended thinking |\n| `claude-sonnet-4-5` | Propósito general, programación |\n| `claude-haiku-4-5` | Respuestas rápidas |\n\n## Manejo de errores\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## Comparación: SDK de OpenAI vs SDK de Anthropic\n\nAmbos funcionan con LemonData para los modelos Claude:\n\n| Característica | SDK de OpenAI | SDK de Anthropic |\n|---------|-----------|---------------|\n| URL base | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | En el array de mensajes | Parámetro `system` separado |\n| Extended thinking | No compatible | Compatible |\n\nElija según su preferencia o base de código existente.",
      "pt": "---\ntitle: \"Anthropic SDK\"\ndescription: \"Use o LemonData com o Anthropic SDK para modelos Claude\"\n---\n\n## Visão Geral\n\nO LemonData suporta o formato nativo da Anthropic Messages API. Use o Anthropic SDK oficial com o LemonData para acessar os modelos Claude.\n\n## Instalação\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Configuração\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Nota: A URL base é `https://api.lemondata.cc` (sem `/v1`) para o Anthropic SDK.\n</Note>\n\n## Uso Básico\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## Com System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Visão\n\n```python\nimport base64\n\n# From URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n",
      "ar": "---\ntitle: \"Anthropic SDK\"\ndescription: \"استخدم LemonData مع Anthropic SDK لنماذج Claude\"\n---\n\n## نظرة عامة\n\nيدعم LemonData تنسيق Anthropic Messages API الأصلي. استخدم Anthropic SDK الرسمي مع LemonData للوصول إلى نماذج Claude.\n\n## التثبيت\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## الإعداد\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  ملاحظة: عنوان URL الأساسي هو `https://api.lemondata.cc` (بدون `/v1`) لـ Anthropic SDK.\n</Note>\n\n## الاستخدام الأساسي\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## مع System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## البث (Streaming)\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## الرؤية (Vision)\n\n```python\nimport base64\n\n# From URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# From base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## استخدام الأدوات (Tool Use)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Check for tool use\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## التفكير الموسع (Claude Opus 4.5)\n\nلنماذج التي تدعم التفكير الموسع (extended thinking):\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Access thinking blocks\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## نماذج Claude المتاحة\n\n| النموذج | الأفضل لـ |\n|-------|----------|\n| `claude-opus-4-5` | الاستنتاج المعقد، التفكير الموسع |\n| `claude-sonnet-4-5` | الأغراض العامة، البرمجة |\n| `claude-haiku-4-5` | الاستجابات السريعة |\n\n## معالجة الأخطاء\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## مقارنة: OpenAI SDK مقابل Anthropic SDK\n\nكلاهما يعمل مع LemonData لنماذج Claude:\n\n| الميزة | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | في مصفوفة `messages` | بارامتر `system` منفصل |\n| التفكير الموسع | غير مدعوم | مدعوم |\n\nاختر بناءً على تفضيلك أو قاعدة الكود الحالية لديك.",
      "vi": "---\ntitle: \"Anthropic SDK\"\ndescription: \"Sử dụng LemonData với Anthropic SDK cho các mô hình Claude\"\n---\n\n## Tổng quan\n\nLemonData hỗ trợ định dạng Anthropic Messages API gốc. Sử dụng Anthropic SDK chính thức với LemonData để truy cập các mô hình Claude.\n\n## Cài đặt\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Cấu hình\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Lưu ý: Base URL là `https://api.lemondata.cc` (không có `/v1`) đối với Anthropic SDK.\n</Note>\n\n## Cách sử dụng cơ bản\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## Với System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Vision\n\n```python\nimport base64\n\n# Từ URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# Từ base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## Sử dụng Tool\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Kiểm tra việc sử dụng tool\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nĐối với các mô hình hỗ trợ extended thinking:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Truy cập các thinking block\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## Các mô hình Claude hiện có\n\n| Model | Phù hợp nhất cho |\n|-------|----------|\n| `claude-opus-4-5` | Suy luận phức tạp, extended thinking |\n| `claude-sonnet-4-5` | Mục đích tổng quát, lập trình |\n| `claude-haiku-4-5` | Phản hồi nhanh |\n\n## Xử lý lỗi\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## So sánh: OpenAI SDK và Anthropic SDK\n\nCả hai đều hoạt động với LemonData cho các mô hình Claude:\n\n| Tính năng | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | Trong mảng `messages` | Tham số `system` riêng biệt |\n| Extended thinking | Không được hỗ trợ | Được hỗ trợ |\n\nChọn dựa trên sở thích của bạn hoặc cơ sở mã nguồn hiện có.",
      "id": "---\ntitle: \"Anthropic SDK\"\ndescription: \"Gunakan LemonData dengan Anthropic SDK untuk model Claude\"\n---\n\n## Ringkasan\n\nLemonData mendukung format native Anthropic Messages API. Gunakan Anthropic SDK resmi dengan LemonData untuk mengakses model Claude.\n\n## Instalasi\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Konfigurasi\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Catatan: Base URL adalah `https://api.lemondata.cc` (tanpa `/v1`) untuk Anthropic SDK.\n</Note>\n\n## Penggunaan Dasar\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## Dengan System Prompt\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Vision\n\n```python\nimport base64\n\n# Dari URL\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# Dari base64\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## Penggunaan Tool\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Periksa penggunaan tool\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nUntuk model yang mendukung extended thinking:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Akses blok thinking\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## Model Claude yang Tersedia\n\n| Model | Terbaik Untuk |\n|-------|----------|\n| `claude-opus-4-5` | Penalaran kompleks, extended thinking |\n| `claude-sonnet-4-5` | Tujuan umum, coding |\n| `claude-haiku-4-5` | Respons cepat |\n\n## Penanganan Error\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## Perbandingan: OpenAI SDK vs Anthropic SDK\n\nKeduanya berfungsi dengan LemonData untuk model Claude:\n\n| Fitur | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | Dalam array messages | Parameter `system` terpisah |\n| Extended thinking | Tidak didukung | Didukung |\n\nPilih berdasarkan preferensi Anda atau codebase yang sudah ada.",
      "tr": "---\ntitle: \"Anthropic SDK\"\ndescription: \"Claude modelleri için Anthropic SDK ile LemonData kullanın\"\n---\n\n## Genel Bakış\n\nLemonData, yerel Anthropic Messages API formatını destekler. Claude modellerine erişmek için LemonData ile resmi Anthropic SDK'sını kullanın.\n\n## Kurulum\n\n<CodeGroup>\n\n```bash Python\npip install anthropic\n```\n\n```bash Node.js\nnpm install @anthropic-ai/sdk\n```\n\n</CodeGroup>\n\n## Yapılandırma\n\n<CodeGroup>\n\n```python Python\nfrom anthropic import Anthropic\n\nclient = Anthropic(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc\"\n)\n```\n\n```javascript JavaScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc'\n});\n```\n\n</CodeGroup>\n\n<Note>\n  Not: Anthropic SDK için base URL `https://api.lemondata.cc` şeklindedir (`/v1` olmadan).\n</Note>\n\n## Temel Kullanım\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, Claude!\"}\n    ]\n)\n\nprint(message.content[0].text)\n```\n\n## System Prompt ile Kullanım\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=\"You are a helpful coding assistant.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to reverse a string\"}\n    ]\n)\n```\n\n## Streaming (Akış)\n\n```python\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n```\n\n## Vision (Görüntü İşleme)\n\n```python\nimport base64\n\n# URL üzerinden\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"url\",\n                    \"url\": \"https://example.com/image.jpg\"\n                }\n            }\n        ]\n    }]\n)\n\n# base64 üzerinden\nwith open(\"image.png\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this image\"},\n            {\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/png\",\n                    \"data\": image_data\n                }\n            }\n        ]\n    }]\n)\n```\n\n## Tool Use (Araç Kullanımı)\n\n```python\nmessage = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[{\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather for a location\",\n        \"input_schema\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n            },\n            \"required\": [\"location\"]\n        }\n    }],\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]\n)\n\n# Araç kullanımı kontrolü\nfor block in message.content:\n    if block.type == \"tool_use\":\n        print(f\"Tool: {block.name}\")\n        print(f\"Input: {block.input}\")\n```\n\n## Extended Thinking (Claude Opus 4.5)\n\nExtended thinking özelliğini destekleyen modeller için:\n\n```python\nmessage = client.messages.create(\n    model=\"claude-opus-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\"role\": \"user\", \"content\": \"Solve this complex math problem...\"}]\n)\n\n# Düşünme (thinking) bloklarına erişim\nfor block in message.content:\n    if block.type == \"thinking\":\n        print(f\"Thinking: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"Response: {block.text}\")\n```\n\n## Mevcut Claude Modelleri\n\n| Model | En İyi Kullanım Alanı |\n|-------|----------|\n| `claude-opus-4-5` | Karmaşık muhakeme, extended thinking |\n| `claude-sonnet-4-5` | Genel amaçlı, kodlama |\n| `claude-haiku-4-5` | Hızlı yanıtlar |\n\n## Hata Yönetimi\n\n```python\nfrom anthropic import APIError, APIStatusError, APIConnectionError\n\ntry:\n    message = client.messages.create(\n        model=\"claude-sonnet-4-5\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept APIStatusError as e:\n    if e.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"API error: {e.status_code}\")\nexcept APIConnectionError:\n    print(\"Connection error\")\nexcept APIError as e:\n    print(f\"Unexpected error: {e}\")\n```\n\n## Karşılaştırma: OpenAI SDK vs Anthropic SDK\n\nHer ikisi de Claude modelleri için LemonData ile çalışır:\n\n| Özellik | OpenAI SDK | Anthropic SDK |\n|---------|-----------|---------------|\n| Base URL | `https://api.lemondata.cc/v1` | `https://api.lemondata.cc` |\n| Endpoint | `/chat/completions` | `/v1/messages` |\n| System prompt | `messages` dizisi içinde | Ayrı `system` parametresi |\n| Extended thinking | Desteklenmiyor | Destekleniyor |\n\nTercihinize veya mevcut kod tabanınıza göre seçim yapın."
    },
    "updatedAt": "2026-01-26T05:34:56.076Z"
  },
  "integrations/claude-code-skill.mdx": {
    "sourceHash": "4b324268a3a3c322",
    "translations": {
      "zh": "---\ntitle: \"✨ Claude Code 技能\"\ndescription: \"5 分钟快速入门指南 - 让 Claude Code 为您自动集成任何 LemonData API\"\n---\n\n<Note>\n此技能让您只需用简单的语言描述需求，即可集成 LemonData 的**数百个 AI API**！\n</Note>\n\n## 此技能可以做什么？\n\n当您想在代码中使用 AI 功能（如 GPT-4、图像生成、语音识别等）时，此技能将：\n\n1. ✅ **自动搜索** LemonData 的数百个 API\n2. ✅ **找到最适合**您需求的 API\n3. ✅ **生成完整的、可运行的代码**\n4. ✅ **配置您的 API Key**\n5. ✅ **提供使用示例**和最佳实践\n\n**简而言之：只需用自然语言描述您的需求，即可获得完整的 API 集成代码！**\n\n## 步骤 1：安装技能\n\n<Tabs>\n  <Tab title=\"npx (推荐)\">\n    最简单的安装方式：\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    这将自动为所有检测到的编程助手（Claude Code, Cursor, Copilot 等）安装该技能。\n  </Tab>\n  <Tab title=\"分享 GitHub URL\">\n    1. 在终端或 IDE 中打开 **Claude Code**\n    2. 在聊天框中粘贴此链接：\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code 将识别其为技能仓库并帮助您进行安装\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **个人安装**（在所有项目中可用）：\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **特定项目安装**（通过 git 与团队共享）：\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### 验证安装\n\n询问 Claude Code：\n```\nWhat skills are available?\n```\n\n如果您看到 `lemondata-api-integration`，则表示安装成功！\n\n## 步骤 2：获取您的 API Key\n\n在使用该技能之前，您需要一个 LemonData API Key。\n\n<Tip>\n**您不需要提前配置 API Key！** 当您使用该技能时，Claude Code 会自动询问您的 API Key 并将其包含在生成的代码中。\n</Tip>\n\n<Steps>\n  <Step title=\"访问 LemonData\">\n    前往 [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"登录\">\n    创建账号或登录\n  </Step>\n  <Step title=\"获取 API Key\">\n    导航至 [控制面板 → API Keys](https://lemondata.cc/dashboard/api) 并创建一个新密钥\n  </Step>\n  <Step title=\"复制密钥\">\n    您的密钥以 `sk-...` 开头 - 请妥善保存\n  </Step>\n</Steps>\n\n<Warning>\n**重要提示：**\n- 所有 LemonData API 都使用相同的 API Key\n- 不要与他人分享您的密钥\n- 不要将您的密钥提交到 GitHub\n</Warning>\n\n## 步骤 3：开始使用\n\n使用该技能就像聊天一样简单！\n\n### 示例 1：使用 GPT-4\n\n**您说：**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code 将：**\n1. 询问您的 API Key\n2. 搜索 GPT-4 相关 API\n3. 显示可用的 API 选项\n4. 生成完整的 Python 代码\n5. 说明如何使用它\n\n**您得到：**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### 示例 2：生成图像\n\n**您说：**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code 将：**\n- 搜索图像生成 API\n- 生成 JavaScript 代码\n- 包含图像下载和保存代码\n\n### 示例 3：语音识别\n\n**您说：**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code 将：**\n- 询问您正在使用的编程语言\n- 搜索语音识别 API\n- 以您选择的语言生成代码\n- 提供音频文件处理示例\n\n## 支持的功能\n\n此技能可以帮助您集成所有这些 AI 能力：\n\n| 功能类型 | 示例 |\n|-------------|----------|\n| 💬 对话 | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 图像生成 | Midjourney, Flux, Stable Diffusion |\n| 🎬 视频生成 | Sora, Runway, Kling, Luma AI |\n| 🎵 音乐生成 | Suno |\n| 🗿 3D 模型 | Tripo3D |\n| 🎤 音频 | Text-to-Speech, Speech-to-Text |\n| 📊 向量 (Embeddings) | text-embedding-3 |\n| 🔄 重排序 (Rerank) | bce-reranker, qwen3-rerank |\n\n## 使用技巧\n\n### 1. 直接说明您的需求\n\n<Tabs>\n  <Tab title=\"❌ 不要这样做\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ 而要这样做\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. 指定您的语言\n\n如果您有特定的语言要求，直接说出来即可：\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. 描述您的场景\n\n如果您有特定需求，请详细描述：\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## 重要注意事项\n\n### API Key 安全\n\n<CardGroup cols={2}>\n  <Card title=\"❌ 切勿这样做\" icon=\"xmark\" color=\"#ef4444\">\n    - 将 API Key 直接放入前端代码\n    - 将 API Key 提交到 GitHub\n    - 与他人分享 API Key\n  </Card>\n  <Card title=\"✅ 最佳实践\" icon=\"check\" color=\"#22c55e\">\n    - 使用环境变量\n    - 对于网站，从后端调用 API\n    - 使用 Next.js、Express 等框架\n  </Card>\n</CardGroup>\n\n### 成本管理\n\n- 每次 API 调用都会消耗额度\n- 在您的[控制面板](https://lemondata.cc/dashboard)中设置使用限制\n- 测试时使用少量数据\n\n### 错误处理\n\n生成的代码包含基础的错误处理，但您可能还希望：\n- 添加重试机制\n- 记录错误日志\n- 提供用户友好的错误提示\n\n## 常见问题\n\n<AccordionGroup>\n  <Accordion title=\"技能没有自动触发？\">\n    尝试在请求中提及 “LemonData” 或 “LemonData API”，或者更具体地描述您的需求：\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"找不到我想要的 API？\">\n    尝试不同的关键词：\n    - “chat” → “dialogue” 或 “conversation”\n    - “image” → “picture” 或 “generation”\n    - “voice” → “audio” 或 “speech”\n  </Accordion>\n\n  <Accordion title=\"生成的代码有错误？\">\n    检查以下几点：\n    1. API Key 是否正确？\n    2. 是否安装了必要的库？（例如 Python 的 `openai`）\n    3. 您能否访问 LemonData 的服务器？\n    4. 您的 API Key 是否有足够的额度？\n  </Accordion>\n\n  <Accordion title=\"如何更新技能？\">\n    再次向 Claude Code 发送 GitHub 链接：\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## 资源\n\n<CardGroup cols={2}>\n  <Card title=\"LemonData 官网\" icon=\"globe\" href=\"https://lemondata.cc\">\n    主站和控制面板\n  </Card>\n  <Card title=\"API 文档\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    完整的 API 参考\n  </Card>\n  <Card title=\"模型\" icon=\"robot\" href=\"https://lemondata.cc/zh/models\">\n    浏览所有可用模型\n  </Card>\n  <Card title=\"价格\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    比官方价格低 30%\n  </Card>\n</CardGroup>\n\n## 立即开始！\n\n现在您已经知道如何使用此技能了。尝试询问 Claude Code：\n\n```\nI want to use GPT-4 in my project\n```\n\n或者\n\n```\nHelp me integrate image generation\n```\n\nClaude Code 将自动使用此技能为您完成所有工作！\n\n<Info>\n**有疑问？** 查看我们的 [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) 或联系 [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "zh-TW": "---\ntitle: \"✨ Claude Code 技能\"\ndescription: \"5 分鐘快速入門指南 - 讓 Claude Code 為您自動整合任何 LemonData API\"\n---\n\n<Note>\n這項技能讓您只需用簡單的英文描述需求，即可整合 LemonData **數百個 AI API** 中的任何一個！\n</Note>\n\n## 這項技能可以做什麼？\n\n當您想在代碼中使用 AI 功能（如 GPT-4、圖像生成、語音識別等）時，此技能將：\n\n1. ✅ **自動搜尋** LemonData 的數百個 API\n2. ✅ **找到最適合**您需求的 API\n3. ✅ **生成完整、可運行的代碼**\n4. ✅ **配置您的 API Key**\n5. ✅ **提供使用範例**和最佳實踐\n\n**簡而言之：只需用自然語言描述您的需求，即可獲得完整的 API 整合代碼！**\n\n## 步驟 1：安裝技能\n\n<Tabs>\n  <Tab title=\"npx (推薦)\">\n    最簡單的安裝方式：\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    這將自動為所有偵測到的編程助手（Claude Code, Cursor, Copilot 等）安裝此技能。\n  </Tab>\n  <Tab title=\"分享 GitHub URL\">\n    1. 在您的終端機或 IDE 中打開 **Claude Code**\n    2. 在對話中貼上此連結：\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code 會將其識別為技能儲存庫並協助您安裝\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **個人安裝**（適用於所有專案）：\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **專案特定安裝**（透過 git 與團隊共享）：\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### 驗證安裝\n\n詢問 Claude Code：\n```\nWhat skills are available?\n```\n\n如果您看到 `lemondata-api-integration`，則表示安裝成功！\n\n## 步驟 2：獲取您的 API Key\n\n在使用此技能之前，您需要一個 LemonData API Key。\n\n<Tip>\n**您不需要提前配置 API Key！** 當您使用此技能時，Claude Code 會自動詢問您的 API Key 並將其包含在生成的代碼中。\n</Tip>\n\n<Steps>\n  <Step title=\"訪問 LemonData\">\n    前往 [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"登入\">\n    建立帳號或登入\n  </Step>\n  <Step title=\"獲取 API Key\">\n    導覽至 [Dashboard → API Keys](https://lemondata.cc/dashboard/api) 並建立一個新金鑰\n  </Step>\n  <Step title=\"複製金鑰\">\n    您的金鑰以 `sk-...` 開頭 - 請安全地保存它\n  </Step>\n</Steps>\n\n<Warning>\n**重要提示：**\n- 所有 LemonData API 都使用相同的 API Key\n- 不要與他人分享您的金鑰\n- 不要將您的金鑰提交到 GitHub\n</Warning>\n\n## 步驟 3：開始使用\n\n使用此技能就像聊天一樣簡單！\n\n### 範例 1：使用 GPT-4\n\n**您說：**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code 將：**\n1. 詢問您的 API Key\n2. 搜尋 GPT-4 相關的 API\n3. 顯示可用的 API 選項\n4. 生成完整的 Python 代碼\n5. 解釋如何使用它\n\n**您得到：**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### 範例 2：生成圖像\n\n**您說：**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code 將：**\n- 搜尋圖像生成 API\n- 生成 JavaScript 代碼\n- 包含圖像下載和保存代碼\n\n### 範例 3：語音識別\n\n**您說：**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code 將：**\n- 詢問您使用的程式語言\n- 搜尋語音識別 API\n- 以您選擇的語言生成代碼\n- 提供音訊檔案處理範例\n\n## 支援的功能\n\n此技能可以幫助您整合所有這些 AI 能力：\n\n| 功能類型 | 範例 |\n|-------------|----------|\n| 💬 對話 | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 圖像生成 | Midjourney, Flux, Stable Diffusion |\n| 🎬 影片生成 | Sora, Runway, Kling, Luma AI |\n| 🎵 音樂生成 | Suno |\n| 🗿 3D 模型 | Tripo3D |\n| 🎤 音訊 | 文字轉語音 (TTS), 語音轉文字 (STT) |\n| 📊 向量嵌入 | text-embedding-3 |\n| 🔄 重排序 | bce-reranker, qwen3-rerank |\n\n## 使用技巧\n\n### 1. 直接說明您的需求\n\n<Tabs>\n  <Tab title=\"❌ 不要這樣做\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ 建議這樣做\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. 指定您的語言\n\n如果您有特定的語言需求，直接說明即可：\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. 描述您的場景\n\n如果您有特定需求，請詳細描述：\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## 重要注意事項\n\n### API Key 安全性\n\n<CardGroup cols={2}>\n  <Card title=\"❌ 絕不要這樣做\" icon=\"xmark\" color=\"#ef4444\">\n    - 將 API Key 直接放在前端代碼中\n    - 將 API Key 提交到 GitHub\n    - 與他人分享 API Key\n  </Card>\n  <Card title=\"✅ 最佳實踐\" icon=\"check\" color=\"#22c55e\">\n    - 使用環境變數\n    - 對於網站，從後端調用 API\n    - 使用 Next.js, Express 等框架\n  </Card>\n</CardGroup>\n\n### 成本管理\n\n- 每次 API 調用都會消耗額度\n- 在您的 [儀表板](https://lemondata.cc/dashboard) 中設置使用限制\n- 測試時使用小數據量\n\n### 錯誤處理\n\n生成的代碼包含基本的錯誤處理，但您可能還需要：\n- 添加重試機制\n- 記錄錯誤日誌\n- 提供用戶友好的錯誤訊息\n\n## 常見問題 (FAQ)\n\n<AccordionGroup>\n  <Accordion title=\"技能沒有自動觸發？\">\n    嘗試在請求中提到 \"LemonData\" 或 \"LemonData API\"，或更具體地描述您的需求：\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"找不到我想要的 API？\">\n    嘗試不同的關鍵字：\n    - \"chat\" → \"dialogue\" 或 \"conversation\"\n    - \"image\" → \"picture\" 或 \"generation\"\n    - \"voice\" → \"audio\" 或 \"speech\"\n  </Accordion>\n\n  <Accordion title=\"生成的代碼有錯誤？\">\n    檢查以下幾點：\n    1. API Key 是否正確？\n    2. 是否安裝了必要的函式庫？（例如 Python 的 `openai`）\n    3. 您是否可以訪問 LemonData 的伺服器？\n    4. 您的 API Key 是否有足夠的額度？\n  </Accordion>\n\n  <Accordion title=\"如何更新技能？\">\n    再次將 GitHub 連結發送給 Claude Code：\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## 資源\n\n<CardGroup cols={2}>\n  <Card title=\"LemonData 網站\" icon=\"globe\" href=\"https://lemondata.cc\">\n    主網站和儀表板\n  </Card>\n  <Card title=\"API 文件\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    完整的 API 參考指南\n  </Card>\n  <Card title=\"模型\" icon=\"robot\" href=\"https://lemondata.cc/zh-TW/models\">\n    瀏覽所有可用模型\n  </Card>\n  <Card title=\"價格\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    比官方價格低 30%\n  </Card>\n</CardGroup>\n\n## 開始使用！\n\n現在您已經知道如何使用這項技能了。嘗試詢問 Claude Code：\n\n```\nI want to use GPT-4 in my project\n```\n\n或者\n\n```\nHelp me integrate image generation\n```\n\nClaude Code 將自動使用此技能為您完成所有工作！\n\n<Info>\n**有任何問題嗎？** 請查看我們的 [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) 或聯繫 [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "ja": "---\ntitle: \"✨ Claude Code スキル\"\ndescription: \"5分でわかるクイックスタートガイド - Claude Code があらゆる LemonData API を自動的に統合します\"\n---\n\n<Note>\nこのスキルを使用すると、必要な内容を自然な言葉で説明するだけで、LemonData が提供する**数百もの AI API**を統合できます。\n</Note>\n\n## このスキルでできること\n\nコード内で AI 機能（GPT-4、画像生成、音声認識など）を使用したい場合、このスキルは以下のことを行います：\n\n1. ✅ LemonData の数百の API を**自動的に検索**\n2. ✅ ニーズに**最適な API を特定**\n3. ✅ **完全で実行可能なコードを生成**\n4. ✅ **API Key を設定**\n5. ✅ **使用例**とベストプラクティスを提供\n\n**要約：自然言語でニーズを伝えるだけで、完全な API 統合コードを取得できます！**\n\n## ステップ 1：スキルのインストール\n\n<Tabs>\n  <Tab title=\"npx（推奨）\">\n    最も簡単なインストール方法：\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    これにより、検出されたすべてのコーディングエージェント（Claude Code, Cursor, Copilot など）にスキルが自動的にインストールされます。\n  </Tab>\n  <Tab title=\"GitHub URL を共有\">\n    1. ターミナルまたは IDE で **Claude Code** を開きます\n    2. チャットにこのリンクを貼り付けます：\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code がスキルリポジトリとして認識し、インストールをサポートします\n  </Tab>\n  <Tab title=\"Git クローン\">\n    **個人用インストール**（すべてのプロジェクトで使用可能）：\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **プロジェクト固有のインストール**（git を介してチームと共有）：\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### インストールの確認\n\nClaude Code に尋ねる：\n```\nWhat skills are available?\n```\n\n`lemondata-api-integration` が表示されれば、インストールは成功です！\n\n## ステップ 2：API Key の取得\n\nスキルを使用する前に、LemonData の API Key が必要です。\n\n<Tip>\n**事前に API Key を設定する必要はありません！** スキルを使用すると、Claude Code が自動的に API Key を尋ね、生成されたコードに含めてくれます。\n</Tip>\n\n<Steps>\n  <Step title=\"LemonData にアクセス\">\n    [lemondata.cc](https://lemondata.cc) に移動します\n  </Step>\n  <Step title=\"サインイン\">\n    アカウントを作成するか、ログインします\n  </Step>\n  <Step title=\"API Key を取得\">\n    [ダッシュボード → API Keys](https://lemondata.cc/dashboard/api) に移動し、新しいキーを作成します\n  </Step>\n  <Step title=\"キーをコピー\">\n    キーは `sk-...` で始まります。安全に保存してください\n  </Step>\n</Steps>\n\n<Warning>\n**重要：**\n- すべての LemonData API は同じ API Key を使用します\n- キーを他人に教えないでください\n- キーを GitHub にコミットしないでください\n</Warning>\n\n## ステップ 3：使用を開始する\n\nスキルの使い方は、チャットするのと同じくらい簡単です！\n\n### 例 1：GPT-4 を使用する\n\n**ユーザー：**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code の動作：**\n1. API Key を尋ねる\n2. GPT-4 関連の API を検索する\n3. 利用可能な API オプションを表示する\n4. 完全な Python コードを生成する\n5. 使用方法を説明する\n\n**取得結果：**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### 例 2：画像を生成する\n\n**ユーザー：**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code の動作：**\n- 画像生成 API を検索\n- JavaScript コードを生成\n- 画像のダウンロードと保存コードを含める\n\n### 例 3：音声認識\n\n**ユーザー：**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code の動作：**\n- 使用しているプログラミング言語を尋ねる\n- 音声認識 API を検索\n- 選択した言語でコードを生成\n- オーディオファイルの処理例を提供\n\n## サポートされている機能\n\nこのスキルは、以下のすべての AI 機能の統合をサポートします：\n\n| 機能タイプ | 例 |\n|-------------|----------|\n| 💬 チャット | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 画像生成 | Midjourney, Flux, Stable Diffusion |\n| 🎬 動画生成 | Sora, Runway, Kling, Luma AI |\n| 🎵 音楽生成 | Suno |\n| 🗿 3D モデル | Tripo3D |\n| 🎤 オーディオ | Text-to-Speech, Speech-to-Text |\n| 📊 埋め込み (Embeddings) | text-embedding-3 |\n| 🔄 リランク (Rerank) | bce-reranker, qwen3-rerank |\n\n## 使用のヒント\n\n### 1. ニーズを伝えるだけ\n\n<Tabs>\n  <Tab title=\"❌ 非推奨\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ 推奨\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. 言語を指定する\n\n特定の言語が必要な場合は、それを伝えるだけです：\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. シナリオを説明する\n\n具体的なニーズがある場合は、詳細に説明してください：\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## 重要な注意事項\n\n### API Key のセキュリティ\n\n<CardGroup cols={2}>\n  <Card title=\"❌ 絶対にしないでください\" icon=\"xmark\" color=\"#ef4444\">\n    - フロントエンドコードに API Key を直接記述する\n    - GitHub に API Key をコミットする\n    - API Key を他人に共有する\n  </Card>\n  <Card title=\"✅ ベストプラクティス\" icon=\"check\" color=\"#22c55e\">\n    - 環境変数を使用する\n    - Web サイトの場合はバックエンドから API を呼び出す\n    - Next.js や Express などのフレームワークを使用する\n  </Card>\n</CardGroup>\n\n### コスト管理\n\n- 各 API 呼び出しでクレジットが消費されます\n- [ダッシュボード](https://lemondata.cc/dashboard)で利用制限を設定してください\n- テストには少量のデータを使用してください\n\n### エラー処理\n\n生成されたコードには基本的なエラー処理が含まれていますが、以下の対応を検討してください：\n- リトライメカニズムの追加\n- エラーのログ記録\n- ユーザーフレンドリーなエラーメッセージの提供\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"スキルが自動的にトリガーされませんか？\">\n    リクエスト内で「LemonData」または「LemonData API」に言及するか、ニーズをより具体的に説明してみてください：\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"目的の API が見つかりませんか？\">\n    別のキーワードを試してください：\n    - 「chat」 → 「dialogue」または「conversation」\n    - 「image」 → 「picture」または「generation」\n    - 「voice」 → 「audio」または「speech」\n  </Accordion>\n\n  <Accordion title=\"生成されたコードにエラーがありますか？\">\n    以下の点を確認してください：\n    1. API Key は正しいですか？\n    2. 必要なライブラリはインストールされていますか？（例：Python の場合は `openai`）\n    3. LemonData のサーバーにアクセスできますか？\n    4. API Key に十分なクレジットがありますか？\n  </Accordion>\n\n  <Accordion title=\"スキルを更新するには？\">\n    GitHub リンクを再度 Claude Code に送信してください：\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## リソース\n\n<CardGroup cols={2}>\n  <Card title=\"LemonData ウェブサイト\" icon=\"globe\" href=\"https://lemondata.cc\">\n    メインサイトおよびダッシュボード\n  </Card>\n  <Card title=\"API ドキュメント\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    完全な API リファレンス\n  </Card>\n  <Card title=\"モデル\" icon=\"robot\" href=\"https://lemondata.cc/ja/models\">\n    利用可能なすべてのモデルを閲覧\n  </Card>\n  <Card title=\"料金\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    公式価格より 30% 低価格\n  </Card>\n</CardGroup>\n\n## さあ、始めましょう！\n\nこれでスキルの使い方がわかりました。Claude Code にこう尋ねてみてください：\n\n```\nI want to use GPT-4 in my project\n```\n\nまたは\n\n```\nHelp me integrate image generation\n```\n\nClaude Code が自動的にこのスキルを使用して、すべての作業を完了します！\n\n<Info>\n**ご質問がありますか？** [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) を確認するか、[support@lemondata.cc](mailto:support@lemondata.cc) までお問い合わせください。\n</Info>",
      "ko": "---\ntitle: \"✨ Claude Code 스킬\"\ndescription: \"5분 퀵 스타트 가이드 - Claude Code가 모든 LemonData API를 자동으로 연동해 드립니다\"\n---\n\n<Note>\n이 스킬을 사용하면 필요한 내용을 일상적인 영어로 설명하는 것만으로 LemonData의 **수백 가지 AI API**를 연동할 수 있습니다!\n</Note>\n\n## 이 스킬로 무엇을 할 수 있나요?\n\n코드에서 AI 기능(GPT-4, 이미지 생성, 음성 인식 등)을 사용하고 싶을 때, 이 스킬은 다음을 수행합니다:\n\n1. ✅ LemonData의 수백 가지 API를 **자동으로 검색**합니다\n2. ✅ 요구 사항에 **가장 적합한 API**를 찾습니다\n3. ✅ **완성된 실행 가능한 코드**를 생성합니다\n4. ✅ **API Key**를 설정합니다\n5. ✅ **사용 예시**와 모범 사례를 제공합니다\n\n**요약하자면: 자연어로 요구 사항을 설명하기만 하면 완성된 API 연동 코드를 얻을 수 있습니다!**\n\n## 1단계: 스킬 설치하기\n\n<Tabs>\n  <Tab title=\"npx (권장)\">\n    가장 쉬운 설치 방법:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    감지된 모든 코딩 에이전트(Claude Code, Cursor, Copilot 등)에 스킬을 자동으로 설치합니다.\n  </Tab>\n  <Tab title=\"GitHub URL 공유\">\n    1. 터미널이나 IDE에서 **Claude Code**를 엽니다\n    2. 채팅창에 이 링크를 붙여넣으세요:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code가 이를 스킬 저장소로 인식하고 설치를 도와줍니다\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **개인 설치** (모든 프로젝트에서 사용 가능):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **프로젝트별 설치** (git을 통해 팀과 공유):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### 설치 확인\n\nClaude Code에게 물어보세요:\n```\nWhat skills are available?\n```\n\n만약 `lemondata-api-integration`이 보인다면 설치에 성공한 것입니다!\n\n## 2단계: API Key 발급받기\n\n스킬을 사용하기 전에 LemonData API Key가 필요합니다.\n\n<Tip>\n**API Key를 미리 설정할 필요가 없습니다!** 스킬을 사용할 때 Claude Code가 자동으로 API Key를 요청하고 생성된 코드에 포함시켜 줍니다.\n</Tip>\n\n<Steps>\n  <Step title=\"LemonData 방문\">\n    [lemondata.cc](https://lemondata.cc)로 이동합니다\n  </Step>\n  <Step title=\"로그인\">\n    계정을 생성하거나 로그인합니다\n  </Step>\n  <Step title=\"API Key 발급\">\n    [Dashboard → API Keys](https://lemondata.cc/dashboard/api)로 이동하여 새 키를 생성합니다\n  </Step>\n  <Step title=\"키 복사\">\n    `sk-...`로 시작하는 키를 안전하게 저장하세요\n  </Step>\n</Steps>\n\n<Warning>\n**중요:**\n- 모든 LemonData API는 동일한 API Key를 사용합니다\n- 키를 타인과 공유하지 마세요\n- GitHub에 키를 커밋하지 마세요\n</Warning>\n\n## 3단계: 사용 시작하기\n\n스킬 사용은 채팅만큼 간단합니다!\n\n### 예시 1: GPT-4 사용하기\n\n**사용자:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code의 동작:**\n1. API Key를 요청합니다\n2. GPT-4 관련 API를 검색합니다\n3. 사용 가능한 API 옵션을 보여줍니다\n4. 완성된 Python 코드를 생성합니다\n5. 사용 방법을 설명합니다\n\n**결과물:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### 예시 2: 이미지 생성\n\n**사용자:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code의 동작:**\n- 이미지 생성 API를 검색합니다\n- JavaScript 코드를 생성합니다\n- 이미지 다운로드 및 저장 코드를 포함합니다\n\n### 예시 3: 음성 인식\n\n**사용자:**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code의 동작:**\n- 사용 중인 프로그래밍 언어를 묻습니다\n- 음성 인식 API를 검색합니다\n- 선택한 언어로 코드를 생성합니다\n- 오디오 파일 처리 예시를 제공합니다\n\n## 지원 기능\n\n이 스킬은 다음과 같은 모든 AI 기능의 연동을 도와줍니다:\n\n| 기능 유형 | 예시 |\n|-------------|----------|\n| 💬 채팅 | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 이미지 생성 | Midjourney, Flux, Stable Diffusion |\n| 🎬 비디오 생성 | Sora, Runway, Kling, Luma AI |\n| 🎵 음악 생성 | Suno |\n| 🗿 3D 모델 | Tripo3D |\n| 🎤 오디오 | Text-to-Speech, Speech-to-Text |\n| 📊 임베딩 | text-embedding-3 |\n| 🔄 리랭크 | bce-reranker, qwen3-rerank |\n\n## 사용 팁\n\n### 1. 요구 사항을 명확히 말씀하세요\n\n<Tabs>\n  <Tab title=\"❌ 이렇게 하지 마세요\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ 대신 이렇게 하세요\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. 언어를 지정하세요\n\n특정 언어 요구 사항이 있는 경우 말씀해 주세요:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. 시나리오를 설명하세요\n\n구체적인 요구 사항이 있다면 자세히 설명해 주세요:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## 중요 고려 사항\n\n### API Key 보안\n\n<CardGroup cols={2}>\n  <Card title=\"❌ 절대 하지 마세요\" icon=\"xmark\" color=\"#ef4444\">\n    - 프론트엔드 코드에 API Key를 직접 넣기\n    - GitHub에 API Key 커밋하기\n    - 타인과 API Key 공유하기\n  </Card>\n  <Card title=\"✅ 권장 사항\" icon=\"check\" color=\"#22c55e\">\n    - 환경 변수 사용하기\n    - 웹사이트의 경우 백엔드에서 API 호출하기\n    - Next.js, Express와 같은 프레임워크 사용하기\n  </Card>\n</CardGroup>\n\n### 비용 관리\n\n- 각 API 호출 시 크레딧이 소모됩니다\n- [대시보드](https://lemondata.cc/dashboard)에서 사용량 제한을 설정하세요\n- 테스트 시에는 적은 양의 데이터를 사용하세요\n\n### 에러 처리\n\n생성된 코드에는 기본적인 에러 처리가 포함되어 있지만, 다음을 추가하는 것이 좋습니다:\n- 재시도 메커니즘 추가\n- 에러 로그 기록\n- 사용자 친화적인 에러 메시지 제공\n\n## 자주 묻는 질문 (FAQ)\n\n<AccordionGroup>\n  <Accordion title=\"스킬이 자동으로 실행되지 않나요?\">\n    요청 시 \"LemonData\" 또는 \"LemonData API\"를 언급하거나, 요구 사항을 더 구체적으로 설명해 보세요:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"원하는 API를 찾을 수 없나요?\">\n    다른 키워드를 시도해 보세요:\n    - \"chat\" → \"dialogue\" 또는 \"conversation\"\n    - \"image\" → \"picture\" 또는 \"generation\"\n    - \"voice\" → \"audio\" 또는 \"speech\"\n  </Accordion>\n\n  <Accordion title=\"생성된 코드에 오류가 있나요?\">\n    다음 사항을 확인하세요:\n    1. API Key가 정확한가요?\n    2. 필요한 라이브러리가 설치되었나요? (예: Python의 `openai`)\n    3. LemonData 서버에 접속 가능한가요?\n    4. API Key에 충분한 크레딧이 있나요?\n  </Accordion>\n\n  <Accordion title=\"스킬을 어떻게 업데이트하나요?\">\n    Claude Code에 GitHub 링크를 다시 보내주세요:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## 리소스\n\n<CardGroup cols={2}>\n  <Card title=\"LemonData 웹사이트\" icon=\"globe\" href=\"https://lemondata.cc\">\n    메인 웹사이트 및 대시보드\n  </Card>\n  <Card title=\"API 문서\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    전체 API 레퍼런스\n  </Card>\n  <Card title=\"모델\" icon=\"robot\" href=\"https://lemondata.cc/ko/models\">\n    사용 가능한 모든 모델 둘러보기\n  </Card>\n  <Card title=\"가격\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    공식 가격보다 30% 저렴\n  </Card>\n</CardGroup>\n\n## 시작해 보세요!\n\n이제 스킬 사용법을 익히셨습니다. Claude Code에게 이렇게 물어보세요:\n\n```\nI want to use GPT-4 in my project\n```\n\n또는\n\n```\nHelp me integrate image generation\n```\n\nClaude Code가 자동으로 이 스킬을 사용하여 모든 작업을 완료해 드립니다!\n\n<Info>\n**궁금한 점이 있으신가요?** [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues)를 확인하거나 [support@lemondata.cc](mailto:support@lemondata.cc)로 문의해 주세요\n</Info>",
      "de": "---\ntitle: \"✨ Claude Code Skill\"\ndescription: \"5-Minuten-Schnellstartanleitung – Lassen Sie Claude Code automatisch jede LemonData-API für Sie integrieren\"\n---\n\n<Note>\nMit diesem Skill können Sie jede der **Hunderte von KI-APIs** von LemonData integrieren, indem Sie einfach in einfacher Sprache beschreiben, was Sie benötigen!\n</Note>\n\n## Was kann dieser Skill?\n\nWenn Sie KI-Funktionen in Ihren Code integrieren möchten (wie GPT-4, Bildgenerierung, Spracherkennung usw.), wird dieser Skill:\n\n1. ✅ **Automatisch die Hunderte von APIs** von LemonData durchsuchen\n2. ✅ **Die beste API** für Ihre Anforderungen finden\n3. ✅ **Vollständigen, ausführbaren Code** generieren\n4. ✅ **Ihren API Key** konfigurieren\n5. ✅ **Anwendungsbeispiele** und Best Practices bereitstellen\n\n**Kurz gesagt: Beschreiben Sie Ihre Anforderungen einfach in natürlicher Sprache und erhalten Sie den vollständigen Code zur API-Integration!**\n\n## Schritt 1: Den Skill installieren\n\n<Tabs>\n  <Tab title=\"npx (Empfohlen)\">\n    Der einfachste Weg zur Installation:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Dies installiert den Skill automatisch für alle erkannten Coding-Agents (Claude Code, Cursor, Copilot usw.).\n  </Tab>\n  <Tab title=\"GitHub-URL teilen\">\n    1. Öffnen Sie **Claude Code** in Ihrem Terminal oder Ihrer IDE\n    2. Fügen Sie diesen Link in den Chat ein:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code wird ihn als Skill-Repository erkennen und Ihnen bei der Installation helfen\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Persönliche Installation** (in allen Projekten verfügbar):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Projektspezifische Installation** (wird über Git mit dem Team geteilt):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Installation überprüfen\n\nFragen Sie Claude Code:\n```\nWhat skills are available?\n```\n\nWenn Sie `lemondata-api-integration` sehen, war die Installation erfolgreich!\n\n## Schritt 2: Ihren API Key abrufen\n\nBevor Sie den Skill nutzen können, benötigen Sie einen LemonData API Key.\n\n<Tip>\n**Sie müssen den API Key nicht im Voraus konfigurieren!** Wenn Sie den Skill verwenden, wird Claude Code automatisch nach Ihrem API Key fragen und ihn in den generierten Code einfügen.\n</Tip>\n\n<Steps>\n  <Step title=\"LemonData besuchen\">\n    Gehen Sie zu [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"Anmelden\">\n    Erstellen Sie ein Konto oder loggen Sie sich ein\n  </Step>\n  <Step title=\"API Key abrufen\">\n    Navigieren Sie zu [Dashboard → API Keys](https://lemondata.cc/dashboard/api) und erstellen Sie einen neuen Key\n  </Step>\n  <Step title=\"Key kopieren\">\n    Ihr Key beginnt mit `sk-...` – bewahren Sie ihn sicher auf\n  </Step>\n</Steps>\n\n<Warning>\n**Wichtig:**\n- Alle LemonData-APIs verwenden denselben API Key\n- Teilen Sie Ihren Key nicht mit anderen\n- Übertragen (commit) Sie Ihren Key nicht nach GitHub\n</Warning>\n\n## Schritt 3: Nutzung starten\n\nDie Nutzung des Skills ist so einfach wie Chatten!\n\n### Beispiel 1: GPT-4 verwenden\n\n**Sie sagen:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code wird:**\n1. Nach Ihrem API Key fragen\n2. Nach GPT-4-bezogenen APIs suchen\n3. Verfügbare API-Optionen anzeigen\n4. Vollständigen Python-Code generieren\n5. Erklären, wie man ihn verwendet\n\n**Sie erhalten:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Beispiel 2: Bilder generieren\n\n**Sie sagen:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code wird:**\n- Nach Bildgenerierungs-APIs suchen\n- JavaScript-Code generieren\n- Code zum Herunterladen und Speichern von Bildern einfügen\n\n### Beispiel 3: Spracherkennung\n\n**Sie sagen:**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code wird:**\n- Fragen, welche Programmiersprache Sie verwenden\n- Nach Spracherkennungs-APIs suchen\n- Code in der von Ihnen gewählten Sprache generieren\n- Beispiele für die Verarbeitung von Audiodateien bereitstellen\n\n## Unterstützte Funktionen\n\nDieser Skill kann Ihnen helfen, all diese KI-Funktionen zu integrieren:\n\n| Funktionstyp | Beispiele |\n|-------------|----------|\n| 💬 Chat | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Bildgenerierung | Midjourney, Flux, Stable Diffusion |\n| 🎬 Videogenerierung | Sora, Runway, Kling, Luma AI |\n| 🎵 Musikgenerierung | Suno |\n| 🗿 3D-Modelle | Tripo3D |\n| 🎤 Audio | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Tipps zur Nutzung\n\n### 1. Nennen Sie einfach Ihre Anforderungen\n\n<Tabs>\n  <Tab title=\"❌ Nicht so\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ Sondern so\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Geben Sie Ihre Sprache an\n\nWenn Sie eine bestimmte Sprachanforderung haben, sagen Sie es einfach:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Beschreiben Sie Ihr Szenario\n\nWenn Sie spezifische Anforderungen haben, beschreiben Sie diese im Detail:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Wichtige Überlegungen\n\n### API Key Sicherheit\n\n<CardGroup cols={2}>\n  <Card title=\"❌ Niemals tun\" icon=\"xmark\" color=\"#ef4444\">\n    - API Key direkt in den Frontend-Code einfügen\n    - API Key nach GitHub übertragen (commit)\n    - API Key mit anderen teilen\n  </Card>\n  <Card title=\"✅ Best Practices\" icon=\"check\" color=\"#22c55e\">\n    - Umgebungsvariablen verwenden\n    - APIs für Websites vom Backend aus aufrufen\n    - Frameworks wie Next.js, Express verwenden\n  </Card>\n</CardGroup>\n\n### Kostenmanagement\n\n- Jeder API-Aufruf verbraucht Credits\n- Legen Sie Nutzungslimits in Ihrem [Dashboard](https://lemondata.cc/dashboard) fest\n- Verwenden Sie kleine Datenmengen zum Testen\n\n### Fehlerbehandlung\n\nDer generierte Code enthält eine grundlegende Fehlerbehandlung, aber Sie möchten eventuell:\n- Retry-Mechanismen hinzufügen\n- Fehler protokollieren\n- Benutzerfreundliche Fehlermeldungen bereitstellen\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"Skill wird nicht automatisch ausgelöst?\">\n    Versuchen Sie, „LemonData“ oder „LemonData API“ in Ihrer Anfrage zu erwähnen, oder beschreiben Sie Ihren Bedarf genauer:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"Die gewünschte API wurde nicht gefunden?\">\n    Versuchen Sie es mit anderen Schlüsselwörtern:\n    - „chat“ → „dialogue“ oder „conversation“\n    - „image“ → „picture“ oder „generation“\n    - „voice“ → „audio“ oder „speech“\n  </Accordion>\n\n  <Accordion title=\"Der generierte Code enthält Fehler?\">\n    Prüfen Sie diese Punkte:\n    1. Ist der API Key korrekt?\n    2. Sind die erforderlichen Bibliotheken installiert? (z. B. `openai` für Python)\n    3. Können Sie auf die Server von LemonData zugreifen?\n    4. Verfügt Ihr API Key über ausreichendes Guthaben?\n  </Accordion>\n\n  <Accordion title=\"Wie wird der Skill aktualisiert?\">\n    Senden Sie den GitHub-Link erneut an Claude Code:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Ressourcen\n\n<CardGroup cols={2}>\n  <Card title=\"LemonData Website\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Hauptwebsite und Dashboard\n  </Card>\n  <Card title=\"API-Dokumentation\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Vollständige API-Referenz\n  </Card>\n  <Card title=\"Modelle\" icon=\"robot\" href=\"https://lemondata.cc/de/models\">\n    Alle verfügbaren Modelle durchsuchen\n  </Card>\n  <Card title=\"Preise\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    30 % günstiger als die offiziellen Preise\n  </Card>\n</CardGroup>\n\n## Loslegen!\n\nJetzt wissen Sie, wie Sie diesen Skill verwenden. Versuchen Sie, Claude Code zu fragen:\n\n```\nI want to use GPT-4 in my project\n```\n\noder\n\n```\nHelp me integrate image generation\n```\n\nClaude Code wird diesen Skill automatisch verwenden, um die gesamte Arbeit für Sie zu erledigen!\n\n<Info>\n**Haben Sie Fragen?** Schauen Sie in unsere [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) oder kontaktieren Sie [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "fr": "---\ntitle: \"✨ Compétence Claude Code\"\ndescription: \"Guide de démarrage rapide en 5 minutes - Laissez Claude Code intégrer automatiquement n'importe quelle API LemonData pour vous\"\n---\n\n<Note>\nCette compétence vous permet d'intégrer n'importe laquelle des **centaines d'API IA** de LemonData en décrivant simplement vos besoins en langage naturel !\n</Note>\n\n## Que peut faire cette compétence ?\n\nLorsque vous souhaitez utiliser des fonctionnalités d'IA dans votre code (comme GPT-4, la génération d'images, la reconnaissance vocale, etc.), cette compétence va :\n\n1. ✅ **Rechercher automatiquement** parmi les centaines d'API de LemonData\n2. ✅ **Trouver la meilleure API** pour vos besoins\n3. ✅ **Générer du code complet et exécutable**\n4. ✅ **Configurer votre API Key**\n5. ✅ **Fournir des exemples d'utilisation** et les meilleures pratiques\n\n**En résumé : décrivez simplement vos besoins en langage naturel et obtenez un code d'intégration API complet !**\n\n## Étape 1 : Installer la compétence\n\n<Tabs>\n  <Tab title=\"npx (Recommandé)\">\n    La méthode d'installation la plus simple :\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Cela installera automatiquement la compétence sur tous les agents de codage détectés (Claude Code, Cursor, Copilot, etc.).\n  </Tab>\n  <Tab title=\"Partager l'URL GitHub\">\n    1. Ouvrez **Claude Code** dans votre terminal ou IDE\n    2. Collez ce lien dans le chat :\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code le reconnaîtra comme un dépôt de compétences et vous aidera à l'installer\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Installation personnelle** (disponible dans tous les projets) :\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Installation spécifique au projet** (partagée avec l'équipe via git) :\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Vérifier l'installation\n\nDemandez à Claude Code :\n```\nWhat skills are available?\n```\n\nSi vous voyez `lemondata-api-integration`, l'installation a réussi !\n\n## Étape 2 : Obtenir votre API Key\n\nAvant d'utiliser la compétence, vous avez besoin d'une API Key LemonData.\n\n<Tip>\n**Vous n'avez pas besoin de configurer l'API Key à l'avance !** Lorsque vous utilisez la compétence, Claude Code vous demandera automatiquement votre API Key et l'inclura dans le code généré.\n</Tip>\n\n<Steps>\n  <Step title=\"Visiter LemonData\">\n    Allez sur [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"Se connecter\">\n    Créez un compte ou connectez-vous\n  </Step>\n  <Step title=\"Obtenir l'API Key\">\n    Accédez à [Dashboard → API Keys](https://lemondata.cc/dashboard/api) et créez une nouvelle clé\n  </Step>\n  <Step title=\"Copier la clé\">\n    Votre clé commence par `sk-...` - conservez-la en toute sécurité\n  </Step>\n</Steps>\n\n<Warning>\n**Important :**\n- Toutes les API LemonData utilisent la même API Key\n- Ne partagez pas votre clé avec d'autres\n- Ne commitez pas votre clé sur GitHub\n</Warning>\n\n## Étape 3 : Commencer l'utilisation\n\nL'utilisation de la compétence est aussi simple qu'une discussion !\n\n### Exemple 1 : Utiliser GPT-4\n\n**Vous dites :**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code va :**\n1. Demander votre API Key\n2. Rechercher des API liées à GPT-4\n3. Afficher les options d'API disponibles\n4. Générer le code Python complet\n5. Expliquer comment l'utiliser\n\n**Vous obtenez :**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Exemple 2 : Générer des images\n\n**Vous dites :**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code va :**\n- Rechercher des API de génération d'images\n- Générer le code JavaScript\n- Inclure le code de téléchargement et de sauvegarde d'image\n\n### Exemple 3 : Reconnaissance vocale\n\n**Vous dites :**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code va :**\n- Demander quel langage de programmation vous utilisez\n- Rechercher des API de reconnaissance vocale\n- Générer le code dans le langage choisi\n- Fournir des exemples de manipulation de fichiers audio\n\n## Fonctionnalités supportées\n\nCette compétence peut vous aider à intégrer toutes ces capacités d'IA :\n\n| Type de fonctionnalité | Exemples |\n|-------------|----------|\n| 💬 Chat | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Génération d'images | Midjourney, Flux, Stable Diffusion |\n| 🎬 Génération de vidéos | Sora, Runway, Kling, Luma AI |\n| 🎵 Génération de musique | Suno |\n| 🗿 Modèles 3D | Tripo3D |\n| 🎤 Audio | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Conseils d'utilisation\n\n### 1. Énoncez simplement vos besoins\n\n<Tabs>\n  <Tab title=\"❌ À ne pas faire\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ À faire à la place\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Spécifiez votre langage\n\nSi vous avez une exigence de langage spécifique, dites-le simplement :\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Décrivez votre scénario\n\nSi vous avez des besoins spécifiques, décrivez-les en détail :\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Considérations importantes\n\n### Sécurité de l'API Key\n\n<CardGroup cols={2}>\n  <Card title=\"❌ À ne jamais faire\" icon=\"xmark\" color=\"#ef4444\">\n    - Mettre l'API Key directement dans le code frontend\n    - Commiter l'API Key sur GitHub\n    - Partager l'API Key avec d'autres\n  </Card>\n  <Card title=\"✅ Bonnes pratiques\" icon=\"check\" color=\"#22c55e\">\n    - Utiliser des variables d'environnement\n    - Appeler les API depuis le backend pour les sites web\n    - Utiliser des frameworks comme Next.js, Express\n  </Card>\n</CardGroup>\n\n### Gestion des coûts\n\n- Chaque appel API consomme des crédits\n- Définissez des limites d'utilisation dans votre [dashboard](https://lemondata.cc/dashboard)\n- Utilisez de petits volumes de données pour les tests\n\n### Gestion des erreurs\n\nLe code généré inclut une gestion d'erreurs de base, mais vous pourriez vouloir :\n- Ajouter des mécanismes de tentative (retry)\n- Journaliser les erreurs\n- Fournir des messages d'erreur conviviaux\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"La compétence ne se déclenche pas automatiquement ?\">\n    Essayez de mentionner \"LemonData\" ou \"LemonData API\" dans votre requête, ou décrivez votre besoin plus précisément :\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"Impossible de trouver l'API souhaitée ?\">\n    Essayez différents mots-clés :\n    - \"chat\" → \"dialogue\" ou \"conversation\"\n    - \"image\" → \"picture\" ou \"generation\"\n    - \"voice\" → \"audio\" ou \"speech\"\n  </Accordion>\n\n  <Accordion title=\"Le code généré contient des erreurs ?\">\n    Vérifiez ces points :\n    1. L'API Key est-elle correcte ?\n    2. Les bibliothèques requises sont-elles installées ? (ex: `openai` pour Python)\n    3. Pouvez-vous accéder aux serveurs de LemonData ?\n    4. Votre API Key a-t-elle suffisamment de crédits ?\n  </Accordion>\n\n  <Accordion title=\"Comment mettre à jour la compétence ?\">\n    Envoyez à nouveau le lien GitHub à Claude Code :\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Ressources\n\n<CardGroup cols={2}>\n  <Card title=\"Site Web LemonData\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Site principal et tableau de bord\n  </Card>\n  <Card title=\"Documentation API\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Référence API complète\n  </Card>\n  <Card title=\"Modèles\" icon=\"robot\" href=\"https://lemondata.cc/fr/models\">\n    Parcourir tous les modèles disponibles\n  </Card>\n  <Card title=\"Tarification\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    30 % de moins que les prix officiels\n  </Card>\n</CardGroup>\n\n## Commencez dès maintenant !\n\nVous savez maintenant comment utiliser cette compétence. Essayez de demander à Claude Code :\n\n```\nI want to use GPT-4 in my project\n```\n\nou\n\n```\nHelp me integrate image generation\n```\n\nClaude Code utilisera automatiquement cette compétence pour effectuer tout le travail pour vous !\n\n<Info>\n**Des questions ?** Consultez nos [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) ou contactez [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "es": "---\ntitle: \"✨ Claude Code Skill\"\ndescription: \"Guía de inicio rápido de 5 minutos - Deja que Claude Code integre automáticamente cualquier API de LemonData por ti\"\n---\n\n<Note>\n¡Esta skill te permite integrar cualquiera de los **cientos de APIs de IA** de LemonData con solo describir lo que necesitas en lenguaje natural!\n</Note>\n\n## ¿Qué puede hacer esta skill?\n\nCuando quieras usar funciones de IA en tu código (como GPT-4, generación de imágenes, reconocimiento de voz, etc.), esta skill:\n\n1. ✅ **Buscará automáticamente** entre los cientos de APIs de LemonData\n2. ✅ **Encontrará la mejor API** para tus necesidades\n3. ✅ **Generará código completo y ejecutable**\n4. ✅ **Configurará tu API Key**\n5. ✅ **Proporcionará ejemplos de uso** y mejores prácticas\n\n**En resumen: ¡Solo describe tus necesidades en lenguaje natural y obtén el código de integración de la API completo!**\n\n## Paso 1: Instalar la Skill\n\n<Tabs>\n  <Tab title=\"npx (Recomendado)\">\n    La forma más fácil de instalar:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Esto instalará automáticamente la skill en todos los agentes de programación detectados (Claude Code, Cursor, Copilot, etc.).\n  </Tab>\n  <Tab title=\"Compartir URL de GitHub\">\n    1. Abre **Claude Code** en tu terminal o IDE\n    2. Pega este enlace en el chat:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code lo reconocerá como un repositorio de skills y te ayudará a instalarlo\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Instalación personal** (disponible en todos los proyectos):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Instalación específica del proyecto** (compartida con el equipo a través de git):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Verificar la instalación\n\nPregunta a Claude Code:\n```\nWhat skills are available?\n```\n\nSi ves `lemondata-api-integration`, ¡la instalación fue exitosa!\n\n## Paso 2: Obtener tu API Key\n\nAntes de usar la skill, necesitas una API Key de LemonData.\n\n<Tip>\n**¡No necesitas configurar la API Key con antelación!** Cuando uses la skill, Claude Code te pedirá automáticamente tu API Key y la incluirá en el código generado.\n</Tip>\n\n<Steps>\n  <Step title=\"Visita LemonData\">\n    Ve a [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"Inicia sesión\">\n    Crea una cuenta o inicia sesión\n  </Step>\n  <Step title=\"Obtén la API Key\">\n    Navega a [Dashboard → API Keys](https://lemondata.cc/dashboard/api) y crea una nueva clave\n  </Step>\n  <Step title=\"Copia la clave\">\n    Tu clave comienza con `sk-...` - guárdala de forma segura\n  </Step>\n</Steps>\n\n<Warning>\n**Importante:**\n- Todas las APIs de LemonData usan la misma API Key\n- No compartas tu clave con otros\n- No subas tu clave a GitHub\n</Warning>\n\n## Paso 3: Empezar a usar\n\n¡Usar la skill es tan simple como chatear!\n\n### Ejemplo 1: Uso de GPT-4\n\n**Tú dices:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code hará lo siguiente:**\n1. Pedirá tu API Key\n2. Buscará APIs relacionadas con GPT-4\n3. Mostrará las opciones de API disponibles\n4. Generará código Python completo\n5. Explicará cómo usarlo\n\n**Obtendrás:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Ejemplo 2: Generar imágenes\n\n**Tú dices:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code hará lo siguiente:**\n- Buscará APIs de generación de imágenes\n- Generará código JavaScript\n- Incluirá código para descargar y guardar la imagen\n\n### Ejemplo 3: Reconocimiento de voz\n\n**Tú dices:**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code hará lo siguiente:**\n- Preguntará qué lenguaje de programación estás usando\n- Buscará APIs de reconocimiento de voz\n- Generará código en el lenguaje elegido\n- Proporcionará ejemplos de manejo de archivos de audio\n\n## Funciones compatibles\n\nEsta skill puede ayudarte a integrar todas estas capacidades de IA:\n\n| Tipo de función | Ejemplos |\n|-------------|----------|\n| 💬 Chat | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Generación de imágenes | Midjourney, Flux, Stable Diffusion |\n| 🎬 Generación de video | Sora, Runway, Kling, Luma AI |\n| 🎵 Generación de música | Suno |\n| 🗿 Modelos 3D | Tripo3D |\n| 🎤 Audio | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Consejos de uso\n\n### 1. Solo indica tus necesidades\n\n<Tabs>\n  <Tab title=\"❌ No hagas esto\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ Haz esto en su lugar\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Especifica tu lenguaje\n\nSi tienes un requisito de lenguaje específico, simplemente dilo:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Describe tu escenario\n\nSi tienes necesidades específicas, descríbelas en detalle:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Consideraciones importantes\n\n### Seguridad de la API Key\n\n<CardGroup cols={2}>\n  <Card title=\"❌ Nunca hagas esto\" icon=\"xmark\" color=\"#ef4444\">\n    - Poner la API Key directamente en el código del frontend\n    - Subir la API Key a GitHub\n    - Compartir la API Key con otros\n  </Card>\n  <Card title=\"✅ Mejores prácticas\" icon=\"check\" color=\"#22c55e\">\n    - Usar variables de entorno\n    - Llamar a las APIs desde el backend para sitios web\n    - Usar frameworks como Next.js, Express\n  </Card>\n</CardGroup>\n\n### Gestión de costos\n\n- Cada llamada a la API consume créditos\n- Establece límites de uso en tu [dashboard](https://lemondata.cc/dashboard)\n- Usa volúmenes de datos pequeños para pruebas\n\n### Manejo de errores\n\nEl código generado incluye un manejo de errores básico, pero es posible que desees:\n- Añadir mecanismos de reintento\n- Registrar errores\n- Proporcionar mensajes de error amigables para el usuario\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"¿La skill no se activa automáticamente?\">\n    Intenta mencionar \"LemonData\" o \"LemonData API\" en tu solicitud, o describe tu necesidad de forma más específica:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"¿No encuentras la API que buscas?\">\n    Prueba con diferentes palabras clave:\n    - \"chat\" → \"dialogue\" o \"conversation\"\n    - \"image\" → \"picture\" o \"generation\"\n    - \"voice\" → \"audio\" o \"speech\"\n  </Accordion>\n\n  <Accordion title=\"¿El código generado tiene errores?\">\n    Revisa estos puntos:\n    1. ¿Es correcta la API Key?\n    2. ¿Están instaladas las librerías necesarias? (por ejemplo, `openai` para Python)\n    3. ¿Puedes acceder a los servidores de LemonData?\n    4. ¿Tiene tu API Key suficientes créditos?\n  </Accordion>\n\n  <Accordion title=\"¿Cómo actualizar la skill?\">\n    Envía de nuevo el enlace de GitHub a Claude Code:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Recursos\n\n<CardGroup cols={2}>\n  <Card title=\"Sitio web de LemonData\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Sitio web principal y panel de control\n  </Card>\n  <Card title=\"Documentación de la API\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Referencia completa de la API\n  </Card>\n  <Card title=\"Modelos\" icon=\"robot\" href=\"https://lemondata.cc/es/models\">\n    Explora todos los modelos disponibles\n  </Card>\n  <Card title=\"Precios\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    30% más bajo que los precios oficiales\n  </Card>\n</CardGroup>\n\n## ¡Empieza ahora!\n\nAhora ya sabes cómo usar esta skill. Intenta preguntar a Claude Code:\n\n```\nI want to use GPT-4 in my project\n```\n\no\n\n```\nHelp me integrate image generation\n```\n\n¡Claude Code usará automáticamente esta skill para completar todo el trabajo por ti!\n\n<Info>\n**¿Tienes preguntas?** Consulta nuestros [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) o contacta con [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "pt": "---\ntitle: \"✨ Skill do Claude Code\"\ndescription: \"Guia de início rápido de 5 minutos - Deixe o Claude Code integrar automaticamente qualquer API da LemonData para você\"\n---\n\n<Note>\nEsta skill permite integrar qualquer uma das **centenas de APIs de IA** da LemonData apenas descrevendo o que você precisa em linguagem natural!\n</Note>\n\n## O que esta Skill pode fazer?\n\nQuando você quiser usar recursos de IA em seu código (como GPT-4, geração de imagens, reconhecimento de voz, etc.), esta skill irá:\n\n1. ✅ **Buscar automaticamente** as centenas de APIs da LemonData\n2. ✅ **Encontrar a melhor API** para suas necessidades\n3. ✅ **Gerar código completo e executável**\n4. ✅ **Configurar sua API Key**\n5. ✅ **Fornecer exemplos de uso** e melhores práticas\n\n**Em resumo: Basta descrever suas necessidades em linguagem natural e obter o código completo de integração da API!**\n\n## Passo 1: Instalar a Skill\n\n<Tabs>\n  <Tab title=\"npx (Recomendado)\">\n    A maneira mais fácil de instalar:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Isso instalará automaticamente a skill em todos os agentes de codificação detectados (Claude Code, Cursor, Copilot, etc.).\n  </Tab>\n  <Tab title=\"Compartilhar URL do GitHub\">\n    1. Abra o **Claude Code** no seu terminal ou IDE\n    2. Cole este link no chat:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. O Claude Code o reconhecerá como um repositório de skill e ajudará você a instalá-lo\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Instalação pessoal** (disponível em todos os projetos):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Instalação específica do projeto** (compartilhada com a equipe via git):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Verificar Instalação\n\nPergunte ao Claude Code:\n```\nWhat skills are available?\n```\n\nSe você vir `lemondata-api-integration`, a instalação foi bem-sucedida!\n\n## Passo 2: Obter sua API Key\n\nAntes de usar a skill, você precisa de uma API Key da LemonData.\n\n<Tip>\n**Você não precisa configurar a API Key antecipadamente!** Quando você usar a skill, o Claude Code solicitará automaticamente sua API Key e a incluirá no código gerado.\n</Tip>\n\n<Steps>\n  <Step title=\"Visite a LemonData\">\n    Acesse [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"Entrar\">\n    Crie uma conta ou faça login\n  </Step>\n  <Step title=\"Obter API Key\">\n    Navegue até [Dashboard → API Keys](https://lemondata.cc/dashboard/api) e crie uma nova chave\n  </Step>\n  <Step title=\"Copiar a Chave\">\n    Sua chave começa com `sk-...` - salve-a com segurança\n  </Step>\n</Steps>\n\n<Warning>\n**Importante:**\n- Todas as APIs da LemonData usam a mesma API Key\n- Não compartilhe sua chave com outras pessoas\n- Não envie sua chave para o GitHub (commit)\n</Warning>\n\n## Passo 3: Começar a Usar\n\nUsar a skill é tão simples quanto conversar no chat!\n\n### Exemplo 1: Usando o GPT-4\n\n**Você diz:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**O Claude Code irá:**\n1. Solicitar sua API Key\n2. Buscar APIs relacionadas ao GPT-4\n3. Mostrar opções de API disponíveis\n4. Gerar código Python completo\n5. Explicar como usá-lo\n\n**Você recebe:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Exemplo 2: Gerar Imagens\n\n**Você diz:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**O Claude Code irá:**\n- Buscar APIs de geração de imagem\n- Gerar código JavaScript\n- Incluir código para download e salvamento de imagem\n\n### Exemplo 3: Reconhecimento de Voz\n\n**Você diz:**\n```\nIntegrate a speech-to-text API\n```\n\n**O Claude Code irá:**\n- Perguntar qual linguagem de programação você está usando\n- Buscar APIs de reconhecimento de voz\n- Gerar código na linguagem escolhida\n- Fornecer exemplos de manipulação de arquivos de áudio\n\n## Recursos Suportados\n\nEsta skill pode ajudar você a integrar todas estas capacidades de IA:\n\n| Tipo de Recurso | Exemplos |\n|-------------|----------|\n| 💬 Chat | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Geração de Imagem | Midjourney, Flux, Stable Diffusion |\n| 🎬 Geração de Vídeo | Sora, Runway, Kling, Luma AI |\n| 🎵 Geração de Música | Suno |\n| 🗿 Modelos 3D | Tripo3D |\n| 🎤 Áudio | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Dicas de Uso\n\n### 1. Apenas declare suas necessidades\n\n<Tabs>\n  <Tab title=\"❌ Não faça isso\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ Faça isto em vez disso\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Especifique sua linguagem\n\nSe você tiver um requisito de linguagem específico, basta dizer:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Descreva seu cenário\n\nSe você tiver necessidades específicas, descreva-as em detalhes:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Considerações Importantes\n\n### Segurança da API Key\n\n<CardGroup cols={2}>\n  <Card title=\"❌ Nunca faça isso\" icon=\"xmark\" color=\"#ef4444\">\n    - Colocar a API Key diretamente no código do frontend\n    - Fazer commit da API Key no GitHub\n    - Compartilhar a API Key com outras pessoas\n  </Card>\n  <Card title=\"✅ Melhores Práticas\" icon=\"check\" color=\"#22c55e\">\n    - Usar variáveis de ambiente\n    - Chamar APIs do backend para sites\n    - Usar frameworks como Next.js, Express\n  </Card>\n</CardGroup>\n\n### Gerenciamento de Custos\n\n- Cada chamada de API consome créditos\n- Defina limites de uso no seu [dashboard](https://lemondata.cc/dashboard)\n- Use pequenos volumes de dados para testes\n\n### Tratamento de Erros\n\nO código gerado inclui tratamento de erros básico, mas você pode querer:\n- Adicionar mecanismos de tentativa (retry)\n- Registrar erros (log)\n- Fornecer mensagens de erro amigáveis ao usuário\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"A skill não está sendo ativada automaticamente?\">\n    Tente mencionar \"LemonData\" ou \"LemonData API\" em sua solicitação, ou descreva sua necessidade de forma mais específica:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"Não consegue encontrar a API que deseja?\">\n    Tente palavras-chave diferentes:\n    - \"chat\" → \"dialogue\" ou \"conversation\"\n    - \"image\" → \"picture\" ou \"generation\"\n    - \"voice\" → \"audio\" ou \"speech\"\n  </Accordion>\n\n  <Accordion title=\"O código gerado apresenta erros?\">\n    Verifique estes pontos:\n    1. A API Key está correta?\n    2. As bibliotecas necessárias estão instaladas? (ex: `openai` para Python)\n    3. Você consegue acessar os servidores da LemonData?\n    4. Sua API Key tem créditos suficientes?\n  </Accordion>\n\n  <Accordion title=\"Como atualizar a skill?\">\n    Envie o link do GitHub para o Claude Code novamente:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Recursos\n\n<CardGroup cols={2}>\n  <Card title=\"Site da LemonData\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Site principal e dashboard\n  </Card>\n  <Card title=\"Documentação da API\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Referência completa da API\n  </Card>\n  <Card title=\"Modelos\" icon=\"robot\" href=\"https://lemondata.cc/pt/models\">\n    Navegue por todos os modelos disponíveis\n  </Card>\n  <Card title=\"Preços\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    30% mais barato que os preços oficiais\n  </Card>\n</CardGroup>\n\n## Comece agora!\n\nAgora você sabe como usar esta skill. Tente perguntar ao Claude Code:\n\n```\nI want to use GPT-4 in my project\n```\n\nou\n\n```\nHelp me integrate image generation\n```\n\nO Claude Code usará automaticamente esta skill para completar todo o trabalho para você!\n\n<Info>\n**Tem dúvidas?** Verifique nossos [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) ou entre em contato com [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "ar": "---\ntitle: \"✨ مهارة Claude Code\"\ndescription: \"دليل البدء السريع في 5 دقائق - دع Claude Code يدمج لك أي API من LemonData تلقائياً\"\n---\n\n<Note>\nتتيح لك هذه المهارة دمج أي من **مئات الـ APIs للذكاء الاصطناعي** الخاصة بـ LemonData بمجرد وصف ما تحتاجه باللغة الإنجليزية البسيطة!\n</Note>\n\n## ماذا يمكن لهذه المهارة أن تفعل؟\n\nعندما تريد استخدام ميزات الذاء الاصطناعي في الكود الخاص بك (مثل GPT-4، توليد الصور، التعرف على الكلام، إلخ)، ستقوم هذه المهارة بـ:\n\n1. ✅ **البحث تلقائياً** في مئات الـ APIs الخاصة بـ LemonData\n2. ✅ **العثور على أفضل API** لاحتياجاتك\n3. ✅ **إنشاء كود كامل وقابل للتشغيل**\n4. ✅ **تهيئة الـ API Key** الخاص بك\n5. ✅ **توفير أمثلة للاستخدام** وأفضل الممارسات\n\n**باختصار: فقط صف احتياجاتك بلغة طبيعية، واحصل على كود دمج API كامل!**\n\n## الخطوة 1: تثبيت المهارة\n\n<Tabs>\n  <Tab title=\"npx (موصى به)\">\n    أسهل طريقة للتثبيت:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    سيقوم هذا تلقائياً بتثبيت المهارة في جميع وكلاء البرمجة المكتشفين (Claude Code، Cursor، Copilot، إلخ).\n  </Tab>\n  <Tab title=\"مشاركة رابط GitHub\">\n    1. افتح **Claude Code** في الـ terminal أو الـ IDE الخاص بك\n    2. الصق هذا الرابط في الدردشة:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. سيتعرف Claude Code عليه كمستودع مهارات وسيساعدك في تثبيته\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **تثبيت شخصي** (متاح في جميع المشاريع):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **تثبيت خاص بالمشروع** (يتم مشاركته مع الفريق عبر git):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### التحقق من التثبيت\n\nاسأل Claude Code:\n```\nWhat skills are available?\n```\n\nإذا رأيت `lemondata-api-integration` ، فقد تم التثبيت بنجاح!\n\n## الخطوة 2: الحصول على الـ API Key الخاص بك\n\nقبل استخدام المهارة، تحتاج إلى LemonData API Key.\n\n<Tip>\n**لا تحتاج إلى تهيئة الـ API Key مسبقاً!** عند استخدام المهارة، سيطلب Claude Code تلقائياً الـ API Key الخاص بك ويضمنه في الكود الذي تم إنشاؤه.\n</Tip>\n\n<Steps>\n  <Step title=\"زيارة LemonData\">\n    انتقل إلى [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"تسجيل الدخول\">\n    أنشئ حساباً أو سجل دخولك\n  </Step>\n  <Step title=\"الحصول على API Key\">\n    انتقل إلى [Dashboard → API Keys](https://lemondata.cc/dashboard/api) وأنشئ مفتاحاً جديداً\n  </Step>\n  <Step title=\"نسخ المفتاح\">\n    يبدأ مفتاحك بـ `sk-...` - احفظه بشكل آمن\n  </Step>\n</Steps>\n\n<Warning>\n**هام:**\n- تستخدم جميع الـ APIs في LemonData نفس الـ API Key\n- لا تشارك مفتاحك مع الآخرين\n- لا ترفع مفتاحك إلى GitHub\n</Warning>\n\n## الخطوة 3: ابدأ الاستخدام\n\nاستخدام المهارة بسيط مثل الدردشة!\n\n### مثال 1: استخدام GPT-4\n\n**أنت تقول:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**سيقوم Claude Code بـ:**\n1. طلب الـ API Key الخاص بك\n2. البحث عن الـ APIs المتعلقة بـ GPT-4\n3. عرض خيارات الـ API المتاحة\n4. إنشاء كود Python كامل\n5. شرح كيفية استخدامه\n\n**ستحصل على:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### مثال 2: توليد الصور\n\n**أنت تقول:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**سيقوم Claude Code بـ:**\n- البحث عن APIs لتوليد الصور\n- إنشاء كود JavaScript\n- تضمين كود تنزيل وحفظ الصور\n\n### مثال 3: التعرف على الكلام\n\n**أنت تقول:**\n```\nIntegrate a speech-to-text API\n```\n\n**سيقوم Claude Code بـ:**\n- السؤال عن لغة البرمجة التي تستخدمها\n- البحث عن APIs للتعرف على الكلام\n- إنشاء كود باللغة التي اخترتها\n- توفير أمثلة للتعامل مع ملفات الصوت\n\n## الميزات المدعومة\n\nيمكن لهذه المهارة مساعدتك في دمج كل قدرات الذكاء الاصطناعي هذه:\n\n| نوع الميزة | أمثلة |\n|-------------|----------|\n| 💬 الدردشة | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 توليد الصور | Midjourney, Flux, Stable Diffusion |\n| 🎬 توليد الفيديو | Sora, Runway, Kling, Luma AI |\n| 🎵 توليد الموسيقى | Suno |\n| 🗿 نماذج ثلاثية الأبعاد | Tripo3D |\n| 🎤 الصوت | Text-to-Speech, Speech-to-Text |\n| 📊 التمثيلات المتجهة (Embeddings) | text-embedding-3 |\n| 🔄 إعادة الترتيب (Rerank) | bce-reranker, qwen3-rerank |\n\n## نصائح للاستخدام\n\n### 1. فقط اذكر احتياجاتك\n\n<Tabs>\n  <Tab title=\"❌ لا تفعل هذا\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ افعل هذا بدلاً من ذلك\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. حدد لغتك\n\nإذا كان لديك متطلب لغة معين، فقط قل ذلك:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. صف السيناريو الخاص بك\n\nإذا كانت لديك احتياجات محددة، فصفها بالتفصيل:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## اعتبارات هامة\n\n### أمان الـ API Key\n\n<CardGroup cols={2}>\n  <Card title=\"❌ لا تفعل هذا أبداً\" icon=\"xmark\" color=\"#ef4444\">\n    - وضع الـ API Key مباشرة في كود الواجهة الأمامية (frontend)\n    - رفع الـ API Key إلى GitHub\n    - مشاركة الـ API Key مع الآخرين\n  </Card>\n  <Card title=\"✅ أفضل الممارسات\" icon=\"check\" color=\"#22c55e\">\n    - استخدام متغيرات البيئة (environment variables)\n    - استدعاء الـ APIs من الواجهة الخلفية (backend) للمواقع الإلكترونية\n    - استخدام أطر عمل مثل Next.js، Express\n  </Card>\n</CardGroup>\n\n### إدارة التكاليف\n\n- كل استدعاء API يستهلك رصيداً\n- تعيين حدود الاستخدام في [لوحة التحكم](https://lemondata.cc/dashboard) الخاصة بك\n- استخدام أحجام بيانات صغيرة للاختبار\n\n### معالجة الأخطاء\n\nيتضمن الكود الذي تم إنشاؤه معالجة أساسية للأخطاء، ولكن قد ترغب في:\n- إضافة آليات إعادة المحاولة\n- تسجيل الأخطاء\n- توفير رسائل خطأ سهلة الاستخدام\n\n## الأسئلة الشائعة\n\n<AccordionGroup>\n  <Accordion title=\"المهارة لا تعمل تلقائياً؟\">\n    حاول ذكر \"LemonData\" أو \"LemonData API\" في طلبك، أو صف حاجتك بشكل أكثر تحديداً:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"لا يمكن العثور على الـ API الذي أريده؟\">\n    جرب كلمات رئيسية مختلفة:\n    - \"chat\" ← \"dialogue\" أو \"conversation\"\n    - \"image\" ← \"picture\" أو \"generation\"\n    - \"voice\" ← \"audio\" أو \"speech\"\n  </Accordion>\n\n  <Accordion title=\"الكود الذي تم إنشاؤه يحتوي على أخطاء؟\">\n    تحقق من هذه النقاط:\n    1. هل الـ API Key صحيح؟\n    2. هل المكتبات المطلوبة مثبتة؟ (مثل `openai` لـ Python)\n    3. هل يمكنك الوصول إلى خوادم LemonData؟\n    4. هل يحتوي الـ API Key الخاص بك على رصيد كافٍ؟\n  </Accordion>\n\n  <Accordion title=\"كيف يتم تحديث المهارة؟\">\n    أرسل رابط GitHub إلى Claude Code مرة أخرى:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## الموارد\n\n<CardGroup cols={2}>\n  <Card title=\"موقع LemonData الإلكتروني\" icon=\"globe\" href=\"https://lemondata.cc\">\n    الموقع الرئيسي ولوحة التحكم\n  </Card>\n  <Card title=\"توثيق الـ API\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    مرجع الـ API الكامل\n  </Card>\n  <Card title=\"النماذج\" icon=\"robot\" href=\"https://lemondata.cc/ar/models\">\n    تصفح جميع النماذج المتاحة\n  </Card>\n  <Card title=\"الأسعار\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    أقل بنسبة 30% من الأسعار الرسمية\n  </Card>\n</CardGroup>\n\n## ابدأ الآن!\n\nالآن تعرف كيفية استخدام هذه المهارة. جرب سؤال Claude Code:\n\n```\nI want to use GPT-4 in my project\n```\n\nأو\n\n```\nHelp me integrate image generation\n```\n\nسيستخدم Claude Code هذه المهارة تلقائياً لإكمال جميع الأعمال نيابة عنك!\n\n<Info>\n**لديك أسئلة؟** تحقق من [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) الخاصة بنا أو تواصل مع [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "vi": "---\ntitle: \"✨ Skill Claude Code\"\ndescription: \"Hướng dẫn bắt đầu nhanh trong 5 phút - Để Claude Code tự động tích hợp bất kỳ API LemonData nào cho bạn\"\n---\n\n<Note>\nSkill này cho phép bạn tích hợp bất kỳ API nào trong số **hàng trăm AI API** của LemonData chỉ bằng cách mô tả nhu cầu của bạn bằng tiếng Anh thông thường!\n</Note>\n\n## Skill này có thể làm gì?\n\nKhi bạn muốn sử dụng các tính năng AI trong mã nguồn của mình (như GPT-4, tạo hình ảnh, nhận dạng giọng nói, v.v.), skill này sẽ:\n\n1. ✅ **Tự động tìm kiếm** hàng trăm API của LemonData\n2. ✅ **Tìm API tốt nhất** cho nhu cầu của bạn\n3. ✅ **Tạo mã nguồn hoàn chỉnh**, có thể chạy được\n4. ✅ **Cấu hình API Key** của bạn\n5. ✅ **Cung cấp các ví dụ sử dụng** và thực hành tốt nhất\n\n**Tóm lại: Chỉ cần mô tả nhu cầu của bạn bằng ngôn ngữ tự nhiên và nhận mã tích hợp API hoàn chỉnh!**\n\n## Bước 1: Cài đặt Skill\n\n<Tabs>\n  <Tab title=\"npx (Khuyên dùng)\">\n    Cách cài đặt dễ nhất:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Lệnh này sẽ tự động cài đặt skill cho tất cả các coding agent được phát hiện (Claude Code, Cursor, Copilot, v.v.).\n  </Tab>\n  <Tab title=\"Chia sẻ URL GitHub\">\n    1. Mở **Claude Code** trong terminal hoặc IDE của bạn\n    2. Dán liên kết này vào khung chat:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code sẽ nhận diện đó là một kho lưu trữ skill và hỗ trợ bạn cài đặt\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Cài đặt cá nhân** (có sẵn trong tất cả các dự án):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Cài đặt theo dự án** (chia sẻ với nhóm qua git):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Xác minh cài đặt\n\nHỏi Claude Code:\n```\nWhat skills are available?\n```\n\nNếu bạn thấy `lemondata-api-integration`, việc cài đặt đã thành công!\n\n## Bước 2: Lấy API Key của bạn\n\nTrước khi sử dụng skill, bạn cần có một LemonData API Key.\n\n<Tip>\n**Bạn không cần phải cấu hình API Key trước!** Khi bạn sử dụng skill, Claude Code sẽ tự động hỏi API Key của bạn và đưa nó vào mã nguồn được tạo.\n</Tip>\n\n<Steps>\n  <Step title=\"Truy cập LemonData\">\n    Truy cập [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"Đăng nhập\">\n    Tạo tài khoản hoặc đăng nhập\n  </Step>\n  <Step title=\"Lấy API Key\">\n    Đi tới [Dashboard → API Keys](https://lemondata.cc/dashboard/api) và tạo một key mới\n  </Step>\n  <Step title=\"Sao chép Key\">\n    Key của bạn bắt đầu bằng `sk-...` - hãy lưu trữ nó một cách an toàn\n  </Step>\n</Steps>\n\n<Warning>\n**Quan trọng:**\n- Tất cả các API của LemonData đều sử dụng cùng một API Key\n- Không chia sẻ key của bạn với người khác\n- Không commit key của bạn lên GitHub\n</Warning>\n\n## Bước 3: Bắt đầu sử dụng\n\nSử dụng skill đơn giản như đang trò chuyện!\n\n### Ví dụ 1: Sử dụng GPT-4\n\n**Bạn nói:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code sẽ:**\n1. Hỏi API Key của bạn\n2. Tìm kiếm các API liên quan đến GPT-4\n3. Hiển thị các tùy chọn API có sẵn\n4. Tạo mã Python hoàn chỉnh\n5. Giải thích cách sử dụng\n\n**Bạn nhận được:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Ví dụ 2: Tạo hình ảnh\n\n**Bạn nói:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code sẽ:**\n- Tìm kiếm các API tạo hình ảnh\n- Tạo mã JavaScript\n- Bao gồm mã tải xuống và lưu hình ảnh\n\n### Ví dụ 3: Nhận dạng giọng nói\n\n**Bạn nói:**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code sẽ:**\n- Hỏi bạn đang sử dụng ngôn ngữ lập trình nào\n- Tìm kiếm các API nhận dạng giọng nói\n- Tạo mã bằng ngôn ngữ bạn đã chọn\n- Cung cấp các ví dụ xử lý tệp âm thanh\n\n## Các tính năng được hỗ trợ\n\nSkill này có thể giúp bạn tích hợp tất cả các khả năng AI sau:\n\n| Loại tính năng | Ví dụ |\n|-------------|----------|\n| 💬 Trò chuyện | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Tạo hình ảnh | Midjourney, Flux, Stable Diffusion |\n| 🎬 Tạo video | Sora, Runway, Kling, Luma AI |\n| 🎵 Tạo nhạc | Suno |\n| 🗿 Mô hình 3D | Tripo3D |\n| 🎤 Âm thanh | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Mẹo sử dụng\n\n### 1. Chỉ cần nêu rõ nhu cầu của bạn\n\n<Tabs>\n  <Tab title=\"❌ Không nên làm thế này\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ Thay vào đó hãy làm thế này\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Chỉ định ngôn ngữ của bạn\n\nNếu bạn có yêu cầu về ngôn ngữ cụ thể, chỉ cần nói ra:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Mô tả kịch bản của bạn\n\nNếu bạn có nhu cầu cụ thể, hãy mô tả chi tiết:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Những lưu ý quan trọng\n\n### Bảo mật API Key\n\n<CardGroup cols={2}>\n  <Card title=\"❌ Không bao giờ làm điều này\" icon=\"xmark\" color=\"#ef4444\">\n    - Đặt API Key trực tiếp trong mã frontend\n    - Commit API Key lên GitHub\n    - Chia sẻ API Key với người khác\n  </Card>\n  <Card title=\"✅ Thực hành tốt nhất\" icon=\"check\" color=\"#22c55e\">\n    - Sử dụng biến môi trường\n    - Gọi API từ backend cho các trang web\n    - Sử dụng các framework như Next.js, Express\n  </Card>\n</CardGroup>\n\n### Quản lý chi phí\n\n- Mỗi lần gọi API sẽ tiêu tốn credit\n- Thiết lập giới hạn sử dụng trong [dashboard](https://lemondata.cc/dashboard) của bạn\n- Sử dụng khối lượng dữ liệu nhỏ để thử nghiệm\n\n### Xử lý lỗi\n\nMã được tạo bao gồm xử lý lỗi cơ bản, nhưng bạn có thể muốn:\n- Thêm cơ chế thử lại (retry)\n- Ghi nhật ký lỗi\n- Cung cấp thông báo lỗi thân thiện với người dùng\n\n## Câu hỏi thường gặp (FAQ)\n\n<AccordionGroup>\n  <Accordion title=\"Skill không tự động kích hoạt?\">\n    Hãy thử nhắc đến \"LemonData\" hoặc \"LemonData API\" trong yêu cầu của bạn, hoặc mô tả nhu cầu của bạn cụ thể hơn:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"Không tìm thấy API bạn muốn?\">\n    Thử các từ khóa khác nhau:\n    - \"chat\" → \"dialogue\" hoặc \"conversation\"\n    - \"image\" → \"picture\" hoặc \"generation\"\n    - \"voice\" → \"audio\" hoặc \"speech\"\n  </Accordion>\n\n  <Accordion title=\"Mã được tạo có lỗi?\">\n    Kiểm tra các điểm sau:\n    1. API Key có chính xác không?\n    2. Các thư viện bắt buộc đã được cài đặt chưa? (ví dụ: `openai` cho Python)\n    3. Bạn có thể truy cập máy chủ của LemonData không?\n    4. API Key của bạn có đủ credit không?\n  </Accordion>\n\n  <Accordion title=\"Làm thế nào để cập nhật skill?\">\n    Gửi lại liên kết GitHub cho Claude Code:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Tài nguyên\n\n<CardGroup cols={2}>\n  <Card title=\"Trang web LemonData\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Trang web chính và bảng điều khiển\n  </Card>\n  <Card title=\"Tài liệu API\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Tham chiếu API đầy đủ\n  </Card>\n  <Card title=\"Mô hình\" icon=\"robot\" href=\"https://lemondata.cc/vi/models\">\n    Duyệt qua tất cả các mô hình có sẵn\n  </Card>\n  <Card title=\"Bảng giá\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    Thấp hơn 30% so với giá chính thức\n  </Card>\n</CardGroup>\n\n## Bắt đầu ngay!\n\nBây giờ bạn đã biết cách sử dụng skill này. Hãy thử hỏi Claude Code:\n\n```\nI want to use GPT-4 in my project\n```\n\nhoặc\n\n```\nHelp me integrate image generation\n```\n\nClaude Code sẽ tự động sử dụng skill này để hoàn thành tất cả công việc cho bạn!\n\n<Info>\n**Có câu hỏi?** Kiểm tra [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) của chúng tôi hoặc liên hệ [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "id": "---\ntitle: \"✨ Claude Code Skill\"\ndescription: \"Panduan mulai cepat 5 menit - Biarkan Claude Code mengintegrasikan API LemonData apa pun secara otomatis untuk Anda\"\n---\n\n<Note>\nSkill ini memungkinkan Anda mengintegrasikan salah satu dari **ratusan API AI** LemonData hanya dengan menjelaskan apa yang Anda butuhkan dalam bahasa Inggris sederhana!\n</Note>\n\n## Apa yang Bisa Dilakukan Skill Ini?\n\nSaat Anda ingin menggunakan fitur AI dalam kode Anda (seperti GPT-4, pembuatan gambar, pengenalan suara, dll.), skill ini akan:\n\n1. ✅ **Mencari secara otomatis** ratusan API LemonData\n2. ✅ **Menemukan API terbaik** untuk kebutuhan Anda\n3. ✅ **Menghasilkan kode lengkap yang dapat dijalankan**\n4. ✅ **Mengonfigurasi API Key Anda**\n5. ✅ **Menyediakan contoh penggunaan** dan praktik terbaik\n\n**Singkatnya: Cukup jelaskan kebutuhan Anda dalam bahasa alami, dan dapatkan kode integrasi API yang lengkap!**\n\n## Langkah 1: Instal Skill\n\n<Tabs>\n  <Tab title=\"npx (Direkomendasikan)\">\n    Cara termudah untuk menginstal:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Ini akan secara otomatis menginstal skill ke semua agen pengkodean yang terdeteksi (Claude Code, Cursor, Copilot, dll.).\n  </Tab>\n  <Tab title=\"Bagikan URL GitHub\">\n    1. Buka **Claude Code** di terminal atau IDE Anda\n    2. Tempel tautan ini di chat:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code akan mengenalinya sebagai repositori skill dan membantu Anda menginstalnya\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Instalasi pribadi** (tersedia di semua proyek):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Instalasi khusus proyek** (dibagikan dengan tim melalui git):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Verifikasi Instalasi\n\nTanya Claude Code:\n```\nWhat skills are available?\n```\n\nJika Anda melihat `lemondata-api-integration`, instalasi berhasil!\n\n## Langkah 2: Dapatkan API Key Anda\n\nSebelum menggunakan skill ini, Anda memerlukan LemonData API Key.\n\n<Tip>\n**Anda tidak perlu mengonfigurasi API Key sebelumnya!** Saat Anda menggunakan skill ini, Claude Code akan secara otomatis meminta API Key Anda dan menyertakannya dalam kode yang dihasilkan.\n</Tip>\n\n<Steps>\n  <Step title=\"Kunjungi LemonData\">\n    Buka [lemondata.cc](https://lemondata.cc)\n  </Step>\n  <Step title=\"Masuk\">\n    Buat akun atau masuk\n  </Step>\n  <Step title=\"Dapatkan API Key\">\n    Navigasi ke [Dashboard → API Keys](https://lemondata.cc/dashboard/api) dan buat kunci baru\n  </Step>\n  <Step title=\"Salin Kunci\">\n    Kunci Anda dimulai dengan `sk-...` - simpan dengan aman\n  </Step>\n</Steps>\n\n<Warning>\n**Penting:**\n- Semua API LemonData menggunakan API Key yang sama\n- Jangan bagikan kunci Anda dengan orang lain\n- Jangan commit kunci Anda ke GitHub\n</Warning>\n\n## Langkah 3: Mulai Menggunakan\n\nMenggunakan skill ini semudah melakukan chat!\n\n### Contoh 1: Menggunakan GPT-4\n\n**Anda berkata:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code akan:**\n1. Meminta API Key Anda\n2. Mencari API terkait GPT-4\n3. Menampilkan opsi API yang tersedia\n4. Menghasilkan kode Python yang lengkap\n5. Menjelaskan cara menggunakannya\n\n**Anda mendapatkan:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Contoh 2: Menghasilkan Gambar\n\n**Anda berkata:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code akan:**\n- Mencari API pembuatan gambar\n- Menghasilkan kode JavaScript\n- Menyertakan kode pengunduhan dan penyimpanan gambar\n\n### Contoh 3: Pengenalan Suara\n\n**Anda berkata:**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code akan:**\n- Menanyakan bahasa pemrograman apa yang Anda gunakan\n- Mencari API pengenalan suara\n- Menghasilkan kode dalam bahasa pilihan Anda\n- Menyediakan contoh penanganan file audio\n\n## Fitur yang Didukung\n\nSkill ini dapat membantu Anda mengintegrasikan semua kemampuan AI berikut:\n\n| Jenis Fitur | Contoh |\n|-------------|----------|\n| 💬 Chat | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Pembuatan Gambar | Midjourney, Flux, Stable Diffusion |\n| 🎬 Pembuatan Video | Sora, Runway, Kling, Luma AI |\n| 🎵 Pembuatan Musik | Suno |\n| 🗿 Model 3D | Tripo3D |\n| 🎤 Audio | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Tips Penggunaan\n\n### 1. Cukup Nyatakan Kebutuhan Anda\n\n<Tabs>\n  <Tab title=\"❌ Jangan lakukan ini\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ Lakukan ini sebagai gantinya\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Tentukan Bahasa Anda\n\nJika Anda memiliki persyaratan bahasa tertentu, cukup katakan:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Jelaskan Skenario Anda\n\nJika Anda memiliki kebutuhan spesifik, jelaskan secara mendetail:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Pertimbangan Penting\n\n### Keamanan API Key\n\n<CardGroup cols={2}>\n  <Card title=\"❌ Jangan Pernah Lakukan Ini\" icon=\"xmark\" color=\"#ef4444\">\n    - Menaruh API Key langsung di kode frontend\n    - Melakukan commit API Key ke GitHub\n    - Membagikan API Key dengan orang lain\n  </Card>\n  <Card title=\"✅ Praktik Terbaik\" icon=\"check\" color=\"#22c55e\">\n    - Gunakan variabel lingkungan (environment variables)\n    - Panggil API dari backend untuk situs web\n    - Gunakan framework seperti Next.js, Express\n  </Card>\n</CardGroup>\n\n### Manajemen Biaya\n\n- Setiap panggilan API menghabiskan kredit\n- Atur batas penggunaan di [dashboard](https://lemondata.cc/dashboard) Anda\n- Gunakan volume data kecil untuk pengujian\n\n### Penanganan Kesalahan\n\nKode yang dihasilkan mencakup penanganan kesalahan dasar, tetapi Anda mungkin ingin:\n- Menambahkan mekanisme percobaan ulang (retry)\n- Mencatat log kesalahan\n- Menyediakan pesan kesalahan yang ramah pengguna\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"Skill tidak terpicu secara otomatis?\">\n    Coba sebutkan \"LemonData\" atau \"LemonData API\" dalam permintaan Anda, atau jelaskan kebutuhan Anda secara lebih spesifik:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"Tidak dapat menemukan API yang saya inginkan?\">\n    Coba kata kunci yang berbeda:\n    - \"chat\" → \"dialogue\" atau \"conversation\"\n    - \"image\" → \"picture\" atau \"generation\"\n    - \"voice\" → \"audio\" atau \"speech\"\n  </Accordion>\n\n  <Accordion title=\"Kode yang dihasilkan memiliki kesalahan?\">\n    Periksa poin-poin berikut:\n    1. Apakah API Key sudah benar?\n    2. Apakah pustaka (library) yang diperlukan sudah terinstal? (misalnya, `openai` untuk Python)\n    3. Apakah Anda dapat mengakses server LemonData?\n    4. Apakah API Key Anda memiliki kredit yang cukup?\n  </Accordion>\n\n  <Accordion title=\"Bagaimana cara memperbarui skill?\">\n    Kirim tautan GitHub ke Claude Code lagi:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Sumber Daya\n\n<CardGroup cols={2}>\n  <Card title=\"Situs Web LemonData\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Situs web utama dan dashboard\n  </Card>\n  <Card title=\"Dokumentasi API\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Referensi API lengkap\n  </Card>\n  <Card title=\"Model\" icon=\"robot\" href=\"https://lemondata.cc/id/models\">\n    Telusuri semua model yang tersedia\n  </Card>\n  <Card title=\"Harga\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    30% lebih rendah dari harga resmi\n  </Card>\n</CardGroup>\n\n## Mulai Sekarang!\n\nSekarang Anda tahu cara menggunakan skill ini. Coba tanya Claude Code:\n\n```\nI want to use GPT-4 in my project\n```\n\natau\n\n```\nHelp me integrate image generation\n```\n\nClaude Code akan secara otomatis menggunakan skill ini untuk menyelesaikan semua pekerjaan untuk Anda!\n\n<Info>\n**Punya pertanyaan?** Periksa [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) kami atau hubungi [support@lemondata.cc](mailto:support@lemondata.cc)\n</Info>",
      "tr": "---\ntitle: \"✨ Claude Code Skill\"\ndescription: \"5 dakikalık hızlı başlangıç kılavuzu - Claude Code'un herhangi bir LemonData API'sini sizin için otomatik olarak entegre etmesini sağlayın\"\n---\n\n<Note>\nBu skill, sadece neye ihtiyacınız olduğunu sade bir İngilizce ile açıklayarak LemonData'nın **yüzlerce AI API'sinden** herhangi birini entegre etmenize olanak tanır!\n</Note>\n\n## Bu Skill Neler Yapabilir?\n\nKodunuzda AI özelliklerini (GPT-4, görsel oluşturma, konuşma tanıma vb. gibi) kullanmak istediğinizde, bu skill şunları yapacaktır:\n\n1. ✅ LemonData'nın yüzlerce API'sini **otomatik olarak arar**\n2. ✅ İhtiyaçlarınız için **en iyi API'yi bulur**\n3. ✅ **Eksiksiz, çalıştırılabilir kod oluşturur**\n4. ✅ **API Key'inizi yapılandırır**\n5. ✅ **Kullanım örnekleri** ve en iyi uygulamaları sunar\n\n**Kısacası: İhtiyaçlarınızı doğal dilde tarif edin ve eksiksiz API entegrasyon kodunu alın!**\n\n## Adım 1: Skill'i Yükleyin\n\n<Tabs>\n  <Tab title=\"npx (Önerilen)\">\n    Yüklemenin en kolay yolu:\n\n    ```bash\n    npx add-skill hedging8563/lemondata-api-skill -y\n    ```\n\n    Bu, skill'i algılanan tüm kodlama ajanlarına (Claude Code, Cursor, Copilot vb.) otomatik olarak yükleyecektir.\n  </Tab>\n  <Tab title=\"GitHub URL'sini Paylaşın\">\n    1. Terminalinizde veya IDE'nizde **Claude Code**'u açın\n    2. Bu bağlantıyı sohbete yapıştırın:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n    3. Claude Code bunu bir skill deposu olarak tanıyacak ve yüklemenize yardımcı olacaktır\n  </Tab>\n  <Tab title=\"Git Clone\">\n    **Kişisel kurulum** (tüm projelerde kullanılabilir):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git ~/.claude/skills/lemondata-api-integration\n    ```\n\n    **Projeye özel kurulum** (git aracılığıyla ekiple paylaşılır):\n    ```bash\n    git clone https://github.com/hedging8563/lemondata-api-skill.git .claude/skills/lemondata-api-integration\n    ```\n  </Tab>\n</Tabs>\n\n### Kurulumu Doğrula\n\nClaude Code'a şunu sorun:\n```\nWhat skills are available?\n```\n\nEğer `lemondata-api-integration` görüyorsanız, kurulum başarılı olmuştur!\n\n## Adım 2: API Key'inizi Alın\n\nSkill'i kullanmadan önce bir LemonData API Key'e ihtiyacınız vardır.\n\n<Tip>\n**API Key'i önceden yapılandırmanıza gerek yok!** Skill'i kullandığınızda, Claude Code otomatik olarak API Key'inizi isteyecek ve oluşturulan koda dahil edecektir.\n</Tip>\n\n<Steps>\n  <Step title=\"LemonData'yı Ziyaret Edin\">\n    [lemondata.cc](https://lemondata.cc) adresine gidin\n  </Step>\n  <Step title=\"Giriş Yapın\">\n    Bir hesap oluşturun veya giriş yapın\n  </Step>\n  <Step title=\"API Key Alın\">\n    [Dashboard → API Keys](https://lemondata.cc/dashboard/api) bölümüne gidin ve yeni bir anahtar oluşturun\n  </Step>\n  <Step title=\"Anahtarı Kopyalayın\">\n    Anahtarınız `sk-...` ile başlar - güvenli bir şekilde saklayın\n  </Step>\n</Steps>\n\n<Warning>\n**Önemli:**\n- Tüm LemonData API'leri aynı API Key'i kullanır\n- Anahtarınızı başkalarıyla paylaşmayın\n- Anahtarınızı GitHub'a commit etmeyin\n</Warning>\n\n## Adım 3: Kullanmaya Başlayın\n\nSkill'i kullanmak sohbet etmek kadar basittir!\n\n### Örnek 1: GPT-4 Kullanımı\n\n**Siz dersiniz ki:**\n```\nI want to use GPT-4 in my Python project\n```\n\n**Claude Code şunları yapacaktır:**\n1. API Key'inizi isteyecek\n2. GPT-4 ile ilgili API'leri arayacak\n3. Mevcut API seçeneklerini gösterecek\n4. Eksiksiz Python kodu oluşturacak\n5. Nasıl kullanılacağını açıklayacak\n\n**Sonuç:**\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n### Örnek 2: Görsel Oluşturma\n\n**Siz dersiniz ki:**\n```\nHow do I generate images with Flux in Node.js?\n```\n\n**Claude Code şunları yapacaktır:**\n- Görsel oluşturma API'lerini arayacak\n- JavaScript kodu oluşturacak\n- Görsel indirme ve kaydetme kodunu dahil edecek\n\n### Örnek 3: Konuşma Tanıma\n\n**Siz dersiniz ki:**\n```\nIntegrate a speech-to-text API\n```\n\n**Claude Code şunları yapacaktır:**\n- Hangi programlama dilini kullandığınızı soracak\n- Konuşma tanıma API'lerini arayacak\n- Seçtiğiniz dilde kod oluşturacak\n- Ses dosyası işleme örnekleri sunacak\n\n## Desteklenen Özellikler\n\nBu skill, tüm bu AI yeteneklerini entegre etmenize yardımcı olabilir:\n\n| Özellik Türü | Örnekler |\n|-------------|----------|\n| 💬 Sohbet | GPT-4o, Claude, Gemini, DeepSeek |\n| 🎨 Görsel Oluşturma | Midjourney, Flux, Stable Diffusion |\n| 🎬 Video Oluşturma | Sora, Runway, Kling, Luma AI |\n| 🎵 Müzik Oluşturma | Suno |\n| 🗿 3D Modeller | Tripo3D |\n| 🎤 Ses | Text-to-Speech, Speech-to-Text |\n| 📊 Embeddings | text-embedding-3 |\n| 🔄 Rerank | bce-reranker, qwen3-rerank |\n\n## Kullanım İpuçları\n\n### 1. Sadece İhtiyaçlarınızı Belirtin\n\n<Tabs>\n  <Tab title=\"❌ Bunu yapmayın\">\n    ```\n    Help me check if LemonData has an image generation API\n    ```\n  </Tab>\n  <Tab title=\"✅ Bunun yerine şunu yapın\">\n    ```\n    I want to generate images in Python\n    ```\n  </Tab>\n</Tabs>\n\n### 2. Dilinizi Belirtin\n\nBelirli bir dil gereksiniminiz varsa, sadece söyleyin:\n\n```\nUse JavaScript to call GPT-4\nUse Python for speech recognition\nUse Go to generate images\n```\n\n### 3. Senaryonuzu Açıklayın\n\nÖzel ihtiyaçlarınız varsa, bunları ayrıntılı olarak açıklayın:\n\n```\nI'm building a chatbot, need to use GPT-4\nMy website needs to let users upload and transform images\nI'm making a voice notes app, need speech-to-text\n```\n\n## Önemli Hususlar\n\n### API Key Güvenliği\n\n<CardGroup cols={2}>\n  <Card title=\"❌ Asla Yapmayın\" icon=\"xmark\" color=\"#ef4444\">\n    - API Key'i doğrudan frontend koduna koymak\n    - API Key'i GitHub'a commit etmek\n    - API Key'i başkalarıyla paylaşmak\n  </Card>\n  <Card title=\"✅ En İyi Uygulamalar\" icon=\"check\" color=\"#22c55e\">\n    - Ortam değişkenlerini kullanın\n    - Web siteleri için API'leri backend'den çağırın\n    - Next.js, Express gibi framework'ler kullanın\n  </Card>\n</CardGroup>\n\n### Maliyet Yönetimi\n\n- Her API çağrısı kredi tüketir\n- [Dashboard](https://lemondata.cc/dashboard) üzerinden kullanım limitleri belirleyin\n- Test için küçük veri hacimleri kullanın\n\n### Hata Yönetimi\n\nOluşturulan kod temel hata yönetimini içerir, ancak şunları yapmak isteyebilirsiniz:\n- Yeniden deneme mekanizmaları eklemek\n- Hataları günlüğe kaydetmek (log)\n- Kullanıcı dostu hata mesajları sağlamak\n\n## SSS\n\n<AccordionGroup>\n  <Accordion title=\"Skill otomatik olarak tetiklenmiyor mu?\">\n    İsteğinizde \"LemonData\" veya \"LemonData API\" ifadesini geçirmeyi deneyin veya ihtiyacınızı daha spesifik bir şekilde açıklayın:\n    ```\n    Use LemonData to integrate GPT-4 in my project\n    ```\n  </Accordion>\n\n  <Accordion title=\"İstediğim API'yi bulamıyorum?\">\n    Farklı anahtar kelimeler deneyin:\n    - \"chat\" → \"dialogue\" veya \"conversation\"\n    - \"image\" → \"picture\" veya \"generation\"\n    - \"voice\" → \"audio\" veya \"speech\"\n  </Accordion>\n\n  <Accordion title=\"Oluşturulan kodda hatalar mı var?\">\n    Şu noktaları kontrol edin:\n    1. API Key doğru mu?\n    2. Gerekli kütüphaneler yüklü mü? (örneğin Python için `openai`)\n    3. LemonData sunucularına erişebiliyor musunuz?\n    4. API Key'inizde yeterli kredi var mı?\n  </Accordion>\n\n  <Accordion title=\"Skill nasıl güncellenir?\">\n    GitHub bağlantısını Claude Code'a tekrar gönderin:\n    ```\n    https://github.com/hedging8563/lemondata-api-skill\n    ```\n  </Accordion>\n</AccordionGroup>\n\n## Kaynaklar\n\n<CardGroup cols={2}>\n  <Card title=\"LemonData Web Sitesi\" icon=\"globe\" href=\"https://lemondata.cc\">\n    Ana web sitesi ve dashboard\n  </Card>\n  <Card title=\"API Dokümantasyonu\" icon=\"book\" href=\"https://docs.lemondata.cc\">\n    Eksiksiz API referansı\n  </Card>\n  <Card title=\"Modeller\" icon=\"robot\" href=\"https://lemondata.cc/tr/models\">\n    Tüm mevcut modellere göz atın\n  </Card>\n  <Card title=\"Fiyatlandırma\" icon=\"dollar-sign\" href=\"https://lemondata.cc/#pricing\">\n    Resmi fiyatlardan %30 daha düşük\n  </Card>\n</CardGroup>\n\n## Başlayın!\n\nArtık bu skill'i nasıl kullanacağınızı biliyorsunuz. Claude Code'a şunları sormayı deneyin:\n\n```\nI want to use GPT-4 in my project\n```\n\nveya\n\n```\nHelp me integrate image generation\n```\n\nClaude Code, tüm işi sizin yerinize tamamlamak için bu skill'i otomatik olarak kullanacaktır!\n\n<Info>\n**Sorularınız mı var?** [GitHub Issues](https://github.com/hedging8563/lemondata-api-skill/issues) sayfamızı kontrol edin veya [support@lemondata.cc](mailto:support@lemondata.cc) adresiyle iletişime geçin\n</Info>"
    },
    "updatedAt": "2026-01-26T05:35:28.248Z"
  },
  "integrations/claude-code.mdx": {
    "sourceHash": "68f9efa7b2a7f519",
    "translations": {
      "zh": "---\ntitle: \"Claude Code\"\ndescription: \"在 Anthropic 的 Claude Code CLI 中使用 LemonData\"\n---\n\n## 概览\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) 是 Anthropic 官方推出的 AI 辅助编程 CLI 工具。你可以将其配置为使用 LemonData 作为 API 提供商。\n\n## 配置\n\n### 环境变量\n\n设置以下环境变量：\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n将这些添加到你的 shell 配置文件（`~/.bashrc`、`~/.zshrc` 等）中以实现持久化。\n\n### 配置文件\n\n或者，创建或编辑 `~/.claude/settings.json`：\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## 使用方法\n\n配置完成后，即可正常使用 Claude Code：\n\n```bash\n# 启动交互式会话\nclaude\n\n# 运行单个命令\nclaude \"Explain this code\" < main.py\n\n# 聊天模式\nclaude chat\n```\n\n## 可用模型\n\nLemonData 支持所有 Claude 模型：\n\n| 模型 | 描述 |\n|-------|-------------|\n| `claude-opus-4-5` | 最强大的模型，适用于复杂推理 |\n| `claude-sonnet-4-5` | 性能均衡，非常适合编程 |\n| `claude-haiku-4-5` | 速度最快，性价比高 |\n\n## 模型选择\n\n使用 `--model` 标志指定模型：\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\n或者在配置中设置默认模型：\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## 扩展思维\n\n对于复杂的推理任务，请使用支持扩展思维（Extended Thinking）的模型：\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"连接错误\">\n    验证基础 URL 是否准确为 `https://api.lemondata.cc`（Anthropic SDK 不需要 `/v1` 后缀）。\n  </Accordion>\n\n  <Accordion title=\"无效的 API 密钥\">\n    确保你的 LemonData API 密钥以 `sk-` 开头且余额充足。\n  </Accordion>\n\n  <Accordion title=\"未找到模型\">\n    检查模型名称是否正确。请使用 `claude-sonnet-4-5` 格式。\n  </Accordion>\n</AccordionGroup>\n\n## 优势\n\n<CardGroup cols={2}>\n  <Card title=\"节省成本\" icon=\"piggy-bank\">\n    LemonData 提供具有竞争力的价格和显著的折扣。\n  </Card>\n  <Card title=\"更高的速率限制\" icon=\"gauge\">\n    对大多数用户而言，比官方 API 提供更宽松的速率限制。\n  </Card>\n  <Card title=\"多种模型\" icon=\"layer-group\">\n    除了 Claude，还可以访问 300 多种模型。\n  </Card>\n  <Card title=\"用量追踪\" icon=\"chart-line\">\n    在控制面板中查看详细的用量分析。\n  </Card>\n</CardGroup>",
      "zh-TW": "---\ntitle: \"Claude Code\"\ndescription: \"在 Anthropic 的 Claude Code CLI 中使用 LemonData\"\n---\n\n## 總覽\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) 是 Anthropic 官方推出的 AI 輔助編程 CLI 工具。您可以將其配置為使用 LemonData 作為 API 提供商。\n\n## 配置\n\n### 環境變數\n\n設置以下環境變數：\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n將這些內容添加到您的 shell 設定檔（`~/.bashrc`、`~/.zshrc` 等）中以使其持久生效。\n\n### 設定檔\n\n或者，創建或編輯 `~/.claude/settings.json`：\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## 使用方法\n\n配置完成後，即可正常使用 Claude Code：\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## 可用模型\n\nLemonData 支援所有 Claude 模型：\n\n| 模型 | 描述 |\n|-------|-------------|\n| `claude-opus-4-5` | 最強大的模型，適用於複雜推理 |\n| `claude-sonnet-4-5` | 性能平衡，非常適合編程 |\n| `claude-haiku-4-5` | 速度最快，具備成本效益 |\n\n## 模型選擇\n\n使用 `--model` 標記指定模型：\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\n或在您的配置中設置預設值：\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## 擴展思考\n\n對於複雜的推理任務，請使用具備擴展思考功能的模型：\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"連線錯誤\">\n    驗證基礎 URL 是否準確為 `https://api.lemondata.cc`（Anthropic SDK 不需要 `/v1` 後綴）。\n  </Accordion>\n\n  <Accordion title=\"無效的 API 金鑰\">\n    確保您的 LemonData API 金鑰以 `sk-` 開頭且餘額充足。\n  </Accordion>\n\n  <Accordion title=\"找不到模型\">\n    檢查模型名稱是否正確。請使用 `claude-sonnet-4-5` 格式。\n  </Accordion>\n</AccordionGroup>\n\n## 優勢\n\n<CardGroup cols={2}>\n  <Card title=\"節省成本\" icon=\"piggy-bank\">\n    LemonData 提供具競爭力的價格與顯著的折扣。\n  </Card>\n  <Card title=\"更高的速率限制\" icon=\"gauge\">\n    對大多數用戶而言，提供比官方 API 更寬鬆的速率限制。\n  </Card>\n  <Card title=\"多種模型\" icon=\"layer-group\">\n    除了 Claude 之外，還可存取 300 多種模型。\n  </Card>\n  <Card title=\"用量追蹤\" icon=\"chart-line\">\n    在您的控制面板中查看詳細的用量分析。\n  </Card>\n</CardGroup>",
      "ja": "---\ntitle: \"Claude Code\"\ndescription: \"LemonDataをAnthropicのClaude Code CLIで使用する\"\n---\n\n## 概要\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code)は、AnthropicによるAI支援コーディング用の公式CLIツールです。LemonDataをAPIプロバイダーとして使用するように設定できます。\n\n## 設定\n\n### 環境変数\n\n以下の環境変数を設定します：\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n永続化するために、これらをシェルプロファイル（`~/.bashrc`、`~/.zshrc`など）に追加してください。\n\n### 設定ファイル\n\nまたは、`~/.claude/settings.json`を作成または編集します：\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## 使い方\n\n設定後、通常通りClaude Codeを使用します：\n\n```bash\n# インタラクティブセッションを開始\nclaude\n\n# 単一のコマンドを実行\nclaude \"Explain this code\" < main.py\n\n# チャットモード\nclaude chat\n```\n\n## 利用可能なモデル\n\nLemonDataはすべてのClaudeモデルをサポートしています：\n\n| モデル | 説明 |\n|-------|-------------|\n| `claude-opus-4-5` | 最も高性能、複雑な推論 |\n| `claude-sonnet-4-5` | バランスの取れたパフォーマンス、コーディングに最適 |\n| `claude-haiku-4-5` | 最速、コスト効率が高い |\n\n## モデルの選択\n\n`--model`フラグでモデルを指定します：\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nまたは、設定でデフォルトを指定します：\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Extended Thinking\n\n複雑な推論タスクには、Extended Thinkingを備えたモデルを使用します：\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"接続エラー\">\n    ベースURLが正確に`https://api.lemondata.cc`であることを確認してください（Anthropic SDKの場合、`/v1`サフィックスは不要です）。\n  </Accordion>\n\n  <Accordion title=\"無効なAPIキー\">\n    LemonDataのAPIキーが`sk-`で始まり、十分な残高があることを確認してください。\n  </Accordion>\n\n  <Accordion title=\"モデルが見つかりません\">\n    モデル名が正しいか確認してください。`claude-sonnet-4-5`の形式を使用します。\n  </Accordion>\n</AccordionGroup>\n\n## メリット\n\n<CardGroup cols={2}>\n  <Card title=\"コスト削減\" icon=\"piggy-bank\">\n    LemonDataは、大幅な割引を伴う競争力のある価格設定を提供します。\n  </Card>\n  <Card title=\"より高いレート制限\" icon=\"gauge\">\n    ほとんどのユーザーにとって、公式APIよりも寛容なレート制限を提供します。\n  </Card>\n  <Card title=\"複数のモデル\" icon=\"layer-group\">\n    Claudeだけでなく、300以上のモデルにアクセス可能です。\n  </Card>\n  <Card title=\"使用状況の追跡\" icon=\"chart-line\">\n    ダッシュボードで詳細な使用状況分析を確認できます。\n  </Card>\n</CardGroup>",
      "ko": "---\ntitle: \"Claude Code\"\ndescription: \"Anthropic의 Claude Code CLI에서 LemonData 사용하기\"\n---\n\n## 개요\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code)는 Anthropic의 공식 AI 보조 코딩 CLI 도구입니다. LemonData를 API 제공업체로 사용하도록 설정할 수 있습니다.\n\n## 설정\n\n### 환경 변수\n\n다음 환경 변수를 설정하세요:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n지속적인 적용을 위해 이를 셸 프로필(`~/.bashrc`, `~/.zshrc` 등)에 추가하세요.\n\n### 설정 파일\n\n또는 `~/.claude/settings.json` 파일을 생성하거나 수정하세요:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## 사용법\n\n설정 후, 평소와 같이 Claude Code를 사용하세요:\n\n```bash\n# 대화형 세션 시작\nclaude\n\n# 단일 명령 실행\nclaude \"Explain this code\" < main.py\n\n# 채팅 모드\nclaude chat\n```\n\n## 사용 가능한 모델\n\nLemonData는 모든 Claude 모델을 지원합니다:\n\n| 모델 | 설명 |\n|-------|-------------|\n| `claude-opus-4-5` | 가장 강력한 성능, 복잡한 추론 |\n| `claude-sonnet-4-5` | 균형 잡힌 성능, 코딩에 탁월 |\n| `claude-haiku-4-5` | 가장 빠르고 비용 효율적 |\n\n## 모델 선택\n\n`--model` 플래그를 사용하여 모델을 지정하세요:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\n또는 설정에서 기본값을 지정할 수 있습니다:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Extended Thinking\n\n복잡한 추론 작업의 경우, extended thinking 기능이 있는 모델을 사용하세요:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"연결 오류\">\n    기본 URL이 정확히 `https://api.lemondata.cc`인지 확인하세요 (Anthropic SDK의 경우 `/v1` 접미사 없음).\n  </Accordion>\n\n  <Accordion title=\"유효하지 않은 API 키\">\n    LemonData API 키가 `sk-`로 시작하고 잔액이 충분한지 확인하세요.\n  </Accordion>\n\n  <Accordion title=\"모델을 찾을 수 없음\">\n    모델 이름이 정확한지 확인하세요. `claude-sonnet-4-5` 형식을 사용하세요.\n  </Accordion>\n</AccordionGroup>\n\n## 장점\n\n<CardGroup cols={2}>\n  <Card title=\"비용 절감\" icon=\"piggy-bank\">\n    LemonData는 상당한 할인 혜택과 함께 경쟁력 있는 가격을 제공합니다.\n  </Card>\n  <Card title=\"더 높은 속도 제한\" icon=\"gauge\">\n    대부분의 사용자에게 공식 API보다 더 넉넉한 속도 제한(rate limits)을 제공합니다.\n  </Card>\n  <Card title=\"다양한 모델\" icon=\"layer-group\">\n    Claude뿐만 아니라 300개 이상의 모델에 액세스할 수 있습니다.\n  </Card>\n  <Card title=\"사용량 추적\" icon=\"chart-line\">\n    대시보드에서 상세한 사용량 분석을 확인할 수 있습니다.\n  </Card>\n</CardGroup>",
      "de": "---\ntitle: \"Claude Code\"\ndescription: \"Nutzen Sie LemonData mit dem Claude Code CLI von Anthropic\"\n---\n\n## Übersicht\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) ist das offizielle CLI-Tool von Anthropic für KI-gestützte Programmierung. Sie können es so konfigurieren, dass LemonData als API-Anbieter verwendet wird.\n\n## Konfiguration\n\n### Umgebungsvariablen\n\nSetzen Sie die folgenden Umgebungsvariablen:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nFügen Sie diese für eine dauerhafte Speicherung zu Ihrem Shell-Profil hinzu (`~/.bashrc`, `~/.zshrc`, etc.).\n\n### Konfigurationsdatei\n\nAlternativ können Sie die Datei `~/.claude/settings.json` erstellen oder bearbeiten:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Verwendung\n\nVerwenden Sie Claude Code nach der Konfiguration ganz normal:\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## Verfügbare Modelle\n\nLemonData unterstützt alle Claude-Modelle:\n\n| Modell | Beschreibung |\n|-------|-------------|\n| `claude-opus-4-5` | Leistungsfähigstes Modell, komplexes logisches Denken |\n| `claude-sonnet-4-5` | Ausgewogene Leistung, hervorragend für die Programmierung |\n| `claude-haiku-4-5` | Schnellstes Modell, kosteneffizient |\n\n## Modellauswahl\n\nGeben Sie ein Modell mit dem `--model`-Flag an:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nOder legen Sie einen Standardwert in Ihrer Konfiguration fest:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Extended Thinking\n\nVerwenden Sie für komplexe Denkaufgaben Modelle mit Extended Thinking:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Verbindungsfehler\">\n    Überprüfen Sie, ob die Basis-URL exakt `https://api.",
      "fr": "---\ntitle: \"Claude Code\"\ndescription: \"Utilisez LemonData avec le CLI Claude Code d'Anthropic\"\n---\n\n## Aperçu\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) est l'outil CLI officiel d'Anthropic pour le codage assisté par IA. Vous pouvez le configurer pour utiliser LemonData comme fournisseur d'API.\n\n## Configuration\n\n### Variables d'environnement\n\nDéfinissez les variables d'environnement suivantes :\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nAjoutez-les à votre profil de shell (`~/.bashrc`, `~/.zshrc`, etc.) pour la persistance.\n\n### Fichier de configuration\n\nAlternativement, créez ou modifiez `~/.claude/settings.json` :\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Utilisation\n\nAprès la configuration, utilisez Claude Code normalement :\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## Modèles disponibles\n\nLemonData prend en charge tous les modèles Claude :\n\n| Modèle | Description |\n|-------|-------------|\n| `claude-opus-4-5` | Le plus performant, raisonnement complexe |\n| `claude-sonnet-4-5` | Performance équilibrée, excellent pour le codage |\n| `claude-haiku-4-5` | Le plus rapide, rentable |\n\n## Sélection du modèle\n\nSpécifiez un modèle avec le flag `--model` :\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nOu définissez une valeur par défaut dans votre configuration :\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Pensée étendue (Extended Thinking)\n\nPour les tâches de raisonnement complexes, utilisez des modèles avec la pensée étendue :\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Dépannage\n\n<AccordionGroup>\n  <Accordion title=\"Erreurs de connexion\">\n    Vérifiez que l'URL de base est exactement `https://api.lemondata.cc` (pas de suffixe `/v1` pour le SDK Anthropic).\n  </Accordion>\n\n  <Accordion title=\"Clé API invalide\">\n    Assurez-vous que votre clé API LemonData commence par `sk-` et dispose d'un solde suffisant.\n  </Accordion>\n\n  <Accordion title=\"Modèle non trouvé\">\n    Vérifiez que le nom du modèle est correct. Utilisez le format `claude-sonnet-4-5`.\n  </Accordion>\n</AccordionGroup>\n\n## Avantages\n\n<CardGroup cols={2}>\n  <Card title=\"Économies de coûts\" icon=\"piggy-bank\">\n    LemonData propose des tarifs compétitifs avec des remises importantes.\n  </Card>\n  <Card title=\"Limites de débit plus élevées\" icon=\"gauge\">\n    Des limites de débit plus généreuses que l'API officielle pour la plupart des utilisateurs.\n  </Card>\n  <Card title=\"Modèles multiples\" icon=\"layer-group\">\n    Accès à plus de 300 modèles au-delà de Claude.\n  </Card>\n  <Card title=\"Suivi de l'utilisation\" icon=\"chart-line\">\n    Analyses détaillées de l'utilisation dans votre tableau de bord.\n  </Card>\n</CardGroup>",
      "es": "---\ntitle: \"Claude Code\"\ndescription: \"Use LemonData con la CLI Claude Code de Anthropic\"\n---\n\n## Descripción general\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) es la herramienta de CLI oficial de Anthropic para la codificación asistida por IA. Puede configurarla para usar LemonData como proveedor de API.\n\n## Configuración\n\n### Variables de entorno\n\nEstablezca las siguientes variables de entorno:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nAñada estas a su perfil de shell (`~/.bashrc`, `~/.zshrc`, etc.) para que sean persistentes.\n\n### Archivo de configuración\n\nAlternativamente, cree o edite `~/.claude/settings.json`:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Uso\n\nDespués de la configuración, use Claude Code normalmente:\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## Modelos disponibles\n\nLemonData es compatible con todos los modelos de Claude:\n\n| Modelo | Descripción |\n|-------|-------------|\n| `claude-opus-4-5` | El más capaz, razonamiento complejo |\n| `claude-sonnet-4-5` | Rendimiento equilibrado, excelente para codificación |\n| `claude-haiku-4-5` | El más rápido, rentable |\n\n## Selección de modelo\n\nEspecifique un modelo con el flag `--model`:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nO establezca un valor predeterminado en su configuración:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Pensamiento extendido\n\nPara tareas de razonamiento complejo, use modelos con pensamiento extendido (extended thinking):\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Solución de problemas\n\n<AccordionGroup>\n  <Accordion title=\"Errores de conexión\">\n    Verifique que la URL base sea exactamente `https://api.lemondata.cc` (sin el sufijo `/v1` para el SDK de Anthropic).\n  </Accordion>\n\n  <Accordion title=\"Clave API no válida\">\n    Asegúrese de que su clave API de LemonData comience con `sk-` y tenga saldo suficiente.\n  </Accordion>\n\n  <Accordion title=\"Modelo no encontrado\">\n    Compruebe que el nombre del modelo sea correcto. Use el formato `claude-sonnet-4-5`.\n  </Accordion>\n</AccordionGroup>\n\n## Beneficios\n\n<CardGroup cols={2}>\n  <Card title=\"Ahorro de costes\" icon=\"piggy-bank\">\n    LemonData ofrece precios competitivos con descuentos significativos.\n  </Card>\n  <Card title=\"Límites de velocidad más altos\" icon=\"gauge\">\n    Límites de velocidad (rate limits) más generosos que la API oficial para la mayoría de los usuarios.\n  </Card>\n  <Card title=\"Múltiples modelos\" icon=\"layer-group\">\n    Acceso a más de 300 modelos más allá de Claude.\n  </Card>\n  <Card title=\"Seguimiento de uso\" icon=\"chart-line\">\n    Analíticas de uso detalladas en su panel de control (dashboard).\n  </Card>\n</CardGroup>",
      "pt": "---\ntitle: \"Claude Code\"\ndescription: \"Use o LemonData com a CLI Claude Code da Anthropic\"\n---\n\n## Visão Geral\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) é a ferramenta de CLI oficial da Anthropic para codificação assistida por IA. Você pode configurá-la para usar o LemonData como provedor de API.\n\n## Configuração\n\n### Variáveis de Ambiente\n\nDefina as seguintes variáveis de ambiente:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nAdicione estas ao seu perfil do shell (`~/.bashrc`, `~/.zshrc`, etc.) para persistência.\n\n### Arquivo de Configuração\n\nAlternativamente, crie ou edite `~/.claude/settings.json`:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Uso\n\nApós a configuração, use o Claude Code normalmente:\n\n```bash\n# Iniciar uma sessão interativa\nclaude\n\n# Executar um único comando\nclaude \"Explain this code\" < main.py\n\n# Modo chat\nclaude chat\n```\n\n## Modelos Disponíveis\n\nO LemonData suporta todos os modelos Claude:\n\n| Modelo | Descrição |\n|-------|-------------|\n| `claude-opus-4-5` | Mais capaz, raciocínio complexo |\n| `claude-sonnet-4-5` | Desempenho equilibrado, excelente para codificação |\n| `claude-haiku-4-5` | Mais rápido, custo-benefício |\n\n## Seleção de Modelo\n\nEspecifique um modelo com a flag `--model`:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nOu defina um padrão em sua configuração:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Extended Thinking\n\nPara tarefas de raciocínio complexas, use modelos com extended thinking:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Solução de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Erros de conexão\">\n    Verifique se a URL base é exatamente `https://api.lemondata.cc` (sem o sufixo `/v1` para o Anthropic SDK).\n  </Accordion>\n\n  <Accordion title=\"Chave de API inválida\">\n    Certifique-se de que sua chave de API do LemonData comece com `sk-` e tenha saldo suficiente.\n  </Accordion>\n\n  <Accordion title=\"Modelo não encontrado\">\n    Verifique se o nome do modelo está correto. Use o formato `claude-sonnet-4-5`.\n  </Accordion>\n</AccordionGroup>\n\n## Benefícios\n\n<CardGroup cols={2}>\n  <Card title=\"Economia de custos\" icon=\"piggy-bank\">\n    O LemonData oferece preços competitivos com descontos significativos.\n  </Card>\n  <Card title=\"Limites de taxa mais altos\" icon=\"gauge\">\n    Limites de taxa mais generosos do que a API oficial para a maioria dos usuários.\n  </Card>\n  <Card title=\"Múltiplos modelos\" icon=\"layer-group\">\n    Acesso a mais de 300 modelos além do Claude.\n  </Card>\n  <Card title=\"Rastreamento de uso\" icon=\"chart-line\">\n    Análises detalhadas de uso em seu painel.\n  </Card>\n</CardGroup>",
      "ar": "---\ntitle: \"Claude Code\"\ndescription: \"استخدم LemonData مع واجهة أوامر Claude Code CLI من Anthropic\"\n---\n\n## نظرة عامة\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) هي أداة CLI الرسمية من Anthropic للبرمجة بمساعدة الذكاء الاصطناعي. يمكنك تهيئتها لاستخدام LemonData كمزود API.\n\n## التهيئة\n\n### متغيرات البيئة\n\nقم بتعيين متغيرات البيئة التالية:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nأضف هذه المتغيرات إلى ملف تعريف shell الخاص بك (`~/.bashrc` أو `~/.zshrc` وما إلى ذلك) لضمان استمرارها.\n\n### ملف التهيئة\n\nبدلاً من ذلك، قم بإنشاء أو تعديل الملف `~/.claude/settings.json`:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## الاستخدام\n\nبعد التهيئة، استخدم Claude Code بشكل طبيعي:\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## النماذج المتاحة\n\nيدعم LemonData جميع نماذج Claude:\n\n| النموذج | الوصف |\n|-------|-------------|\n| `claude-opus-4-5` | الأكثر قدرة، تفكير معقد |\n| `claude-sonnet-4-5` | أداء متوازن، ممتاز للبرمجة |\n| `claude-haiku-4-5` | الأسرع، وفعال من حيث التكلفة |\n\n## اختيار النموذج\n\nحدد نموذجاً باستخدام علامة `--model`:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nأو قم بتعيين نموذج افتراضي في التهيئة الخاصة بك:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## التفكير الممتد (Extended Thinking)\n\nلمهام التفكير المعقدة، استخدم النماذج التي تدعم التفكير الممتد:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## استكشاف الأخطاء وإصلاحها\n\n<AccordionGroup>\n  <Accordion title=\"أخطاء الاتصال\">\n    تحقق من أن عنوان URL الأساسي هو `https://api.lemondata.cc` تماماً (بدون لاحقة `/v1` لـ Anthropic SDK).\n  </Accordion>\n\n  <Accordion title=\"مفتاح API غير صالح\">\n    تأكد من أن مفتاح API الخاص بـ LemonData يبدأ بـ `sk-` ولديه رصيد كافٍ.\n  </Accordion>\n\n  <Accordion title=\"النموذج غير موجود\">\n    تأكد من صحة اسم النموذج. استخدم تنسيق `claude-sonnet-4-5`.\n  </Accordion>\n</AccordionGroup>\n\n## المميزات\n\n<CardGroup cols={2}>\n  <Card title=\"توفير التكاليف\" icon=\"piggy-bank\">\n    يقدم LemonData أسعاراً تنافسية مع خصومات كبيرة.\n  </Card>\n  <Card title=\"حدود معدل أعلى\" icon=\"gauge\">\n    حدود معدل (rate limits) أكثر سخاءً من API الرسمي لمعظم المستخدمين.\n  </Card>\n  <Card title=\"نماذج متعددة\" icon=\"layer-group\">\n    الوصول إلى أكثر من 300 نموذج بالإضافة إلى Claude.\n  </Card>\n  <Card title=\"تتبع الاستخدام\" icon=\"chart-line\">\n    تحليلات استخدام مفصلة في لوحة التحكم الخاصة بك.\n  </Card>\n</CardGroup>",
      "vi": "---\ntitle: \"Claude Code\"\ndescription: \"Sử dụng LemonData với Claude Code CLI của Anthropic\"\n---\n\n## Tổng quan\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) là công cụ CLI chính thức của Anthropic dành cho việc lập trình với sự hỗ trợ của AI. Bạn có thể cấu hình công cụ này để sử dụng LemonData làm nhà cung cấp API.\n\n## Cấu hình\n\n### Biến môi trường\n\nThiết lập các biến môi trường sau:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nThêm các dòng này vào shell profile của bạn (`~/.bashrc`, `~/.zshrc`, v.v.) để duy trì cấu hình.\n\n### Tệp cấu hình\n\nNgoài ra, bạn có thể tạo hoặc chỉnh sửa tệp `~/.claude/settings.json`:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Cách sử dụng\n\nSau khi cấu hình, hãy sử dụng Claude Code như bình thường:\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## Các mô hình hiện có\n\nLemonData hỗ trợ tất cả các mô hình Claude:\n\n| Mô hình | Mô tả |\n|-------|-------------|\n| `claude-opus-4-5` | Khả năng mạnh mẽ nhất, suy luận phức tạp |\n| `claude-sonnet-4-5` | Hiệu suất cân bằng, tuyệt vời cho lập trình |\n| `claude-haiku-4-5` | Nhanh nhất, tiết kiệm chi phí |\n\n## Lựa chọn mô hình\n\nChỉ định một mô hình bằng cờ `--model`:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nHoặc thiết lập mặc định trong cấu hình của bạn:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Suy luận mở rộng\n\nĐối với các tác vụ suy luận phức tạp, hãy sử dụng các mô hình có tính năng suy luận mở rộng:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Xử lý sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Lỗi kết nối\">\n    Xác minh rằng base URL chính xác là `https://api.lemondata.cc` (không có hậu tố `/v1` cho Anthropic SDK).\n  </Accordion>\n\n  <Accordion title=\"API key không hợp lệ\">\n    Đảm bảo API key LemonData của bạn bắt đầu bằng `sk-` và có đủ số dư.\n  </Accordion>\n\n  <Accordion title=\"Không tìm thấy mô hình\">\n    Kiểm tra xem tên mô hình đã chính xác chưa. Sử dụng định dạng `claude-sonnet-4-5`.\n  </Accordion>\n</AccordionGroup>\n\n## Lợi ích\n\n<CardGroup cols={2}>\n  <Card title=\"Tiết kiệm chi phí\" icon=\"piggy-bank\">\n    LemonData cung cấp mức giá cạnh tranh với các ưu đãi giảm giá đáng kể.\n  </Card>\n  <Card title=\"Giới hạn tốc độ cao hơn\" icon=\"gauge\">\n    Giới hạn tốc độ (rate limits) hào phóng hơn so với API chính thức cho hầu hết người dùng.\n  </Card>\n  <Card title=\"Đa dạng mô hình\" icon=\"layer-group\">\n    Truy cập vào hơn 300 mô hình khác ngoài Claude.\n  </Card>\n  <Card title=\"Theo dõi sử dụng\" icon=\"chart-line\">\n    Phân tích sử dụng chi tiết trong bảng điều khiển của bạn.\n  </Card>\n</CardGroup>",
      "id": "---\ntitle: \"Claude Code\"\ndescription: \"Gunakan LemonData dengan CLI Claude Code dari Anthropic\"\n---\n\n## Ringkasan\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code) adalah alat CLI resmi Anthropic untuk pengodean berbantuan AI. Anda dapat mengonfigurasinya untuk menggunakan LemonData sebagai penyedia API.\n\n## Konfigurasi\n\n### Variabel Lingkungan\n\nAtur variabel lingkungan berikut:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nTambahkan ini ke profil shell Anda (`~/.bashrc`, `~/.zshrc`, dll.) untuk persistensi.\n\n### Berkas Konfigurasi\n\nAtau, buat atau edit `~/.claude/settings.json`:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Penggunaan\n\nSetelah konfigurasi, gunakan Claude Code secara normal:\n\n```bash\n# Mulai sesi interaktif\nclaude\n\n# Jalankan perintah tunggal\nclaude \"Explain this code\" < main.py\n\n# Mode chat\nclaude chat\n```\n\n## Model yang Tersedia\n\nLemonData mendukung semua model Claude:\n\n| Model | Deskripsi |\n|-------|-------------|\n| `claude-opus-4-5` | Paling mumpuni, penalaran kompleks |\n| `claude-sonnet-4-5` | Performa seimbang, sangat baik untuk pengodean |\n| `claude-haiku-4-5` | Tercepat, hemat biaya |\n\n## Pemilihan Model\n\nTentukan model dengan flag `--model`:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nAtau atur default dalam konfigurasi Anda:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Extended Thinking\n\nUntuk tugas penalaran yang kompleks, gunakan model dengan extended thinking:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Pemecahan Masalah\n\n<AccordionGroup>\n  <Accordion title=\"Kesalahan koneksi\">\n    Verifikasi bahwa URL dasar tepat `https://api.lemondata.cc` (tanpa akhiran `/v1` untuk SDK Anthropic).\n  </Accordion>\n\n  <Accordion title=\"Kunci API tidak valid\">\n    Pastikan kunci API LemonData Anda dimulai dengan `sk-` dan memiliki saldo yang cukup.\n  </Accordion>\n\n  <Accordion title=\"Model tidak ditemukan\">\n    Periksa apakah nama model sudah benar. Gunakan format `claude-sonnet-4-5`.\n  </Accordion>\n</AccordionGroup>\n\n## Keuntungan\n\n<CardGroup cols={2}>\n  <Card title=\"Penghematan biaya\" icon=\"piggy-bank\">\n    LemonData menawarkan harga kompetitif dengan diskon yang signifikan.\n  </Card>\n  <Card title=\"Batas laju lebih tinggi\" icon=\"gauge\">\n    Batas laju (rate limits) yang lebih besar daripada API resmi untuk sebagian besar pengguna.\n  </Card>\n  <Card title=\"Berbagai model\" icon=\"layer-group\">\n    Akses ke 300+ model selain Claude.\n  </Card>\n  <Card title=\"Pelacakan penggunaan\" icon=\"chart-line\">\n    Analisis penggunaan terperinci di dasbor Anda.\n  </Card>\n</CardGroup>",
      "tr": "---\ntitle: \"Claude Code\"\ndescription: \"LemonData'yı Anthropic'in Claude Code CLI aracıyla kullanın\"\n---\n\n## Genel Bakış\n\n[Claude Code](https://docs.anthropic.com/en/docs/claude-code), Anthropic'in yapay zeka destekli kodlama için resmi CLI aracıdır. LemonData'yı API sağlayıcısı olarak kullanacak şekilde yapılandırabilirsiniz.\n\n## Yapılandırma\n\n### Ortam Değişkenleri\n\nAşağıdaki ortam değişkenlerini ayarlayın:\n\n```bash\nexport ANTHROPIC_API_KEY=\"sk-your-lemondata-key\"\nexport ANTHROPIC_BASE_URL=\"https://api.lemondata.cc\"\n```\n\nKalıcılık sağlamak için bunları kabuk profilinize (`~/.bashrc`, `~/.zshrc`, vb.) ekleyin.\n\n### Yapılandırma Dosyası\n\nAlternatif olarak, `~/.claude/settings.json` dosyasını oluşturun veya düzenleyin:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  }\n}\n```\n\n## Kullanım\n\nYapılandırmadan sonra Claude Code'u normal şekilde kullanın:\n\n```bash\n# Start an interactive session\nclaude\n\n# Run a single command\nclaude \"Explain this code\" < main.py\n\n# Chat mode\nclaude chat\n```\n\n## Mevcut Modeller\n\nLemonData tüm Claude modellerini destekler:\n\n| Model | Açıklama |\n|-------|-------------|\n| `claude-opus-4-5` | En yetenekli, karmaşık muhakeme |\n| `claude-sonnet-4-5` | Dengeli performans, kodlama için mükemmel |\n| `claude-haiku-4-5` | En hızlı, maliyet etkin |\n\n## Model Seçimi\n\n`--model` bayrağı ile bir model belirtin:\n\n```bash\nclaude --model claude-sonnet-4-5 \"Review this PR\"\n```\n\nVeya yapılandırmanızda bir varsayılan ayarlayın:\n\n```json\n{\n  \"env\": {\n    \"ANTHROPIC_API_KEY\": \"sk-your-lemondata-key\",\n    \"ANTHROPIC_BASE_URL\": \"https://api.lemondata.cc\"\n  },\n  \"model\": \"claude-sonnet-4-5\"\n}\n```\n\n## Genişletilmiş Düşünme\n\nKarmaşık muhakeme görevleri için genişletilmiş düşünme özelliğine sahip modelleri kullanın:\n\n```bash\nclaude --model claude-opus-4-5 \"Design a microservices architecture for...\"\n```\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Bağlantı hataları\">\n    Temel URL'nin tam olarak `https://api.lemondata.cc` olduğunu doğrulayın (Anthropic SDK için `/v1` son eki yok).\n  </Accordion>\n\n  <Accordion title=\"Geçersiz API anahtarı\">\n    LemonData API anahtarınızın `sk-` ile başladığından ve yeterli bakiyeye sahip olduğundan emin olun.\n  </Accordion>\n\n  <Accordion title=\"Model bulunamadı\">\n    Model adının doğru olduğunu kontrol edin. `claude-sonnet-4-5` formatını kullanın.\n  </Accordion>\n</AccordionGroup>\n\n## Avantajlar\n\n<CardGroup cols={2}>\n  <Card title=\"Maliyet tasarrufu\" icon=\"piggy-bank\">\n    LemonData, önemli indirimlerle rekabetçi fiyatlandırma sunar.\n  </Card>\n  <Card title=\"Daha yüksek hız limitleri\" icon=\"gauge\">\n    Çoğu kullanıcı için resmi API'den daha cömert hız limitleri.\n  </Card>\n  <Card title=\"Birden fazla model\" icon=\"layer-group\">\n    Sadece Claude'un ötesinde 300'den fazla modele erişim.\n  </Card>\n  <Card title=\"Kullanım takibi\" icon=\"chart-line\">\n    Panelinizde ayrıntılı kullanım analizleri.\n  </Card>\n</CardGroup>"
    },
    "updatedAt": "2026-01-26T05:35:45.513Z"
  },
  "integrations/codex-cli.mdx": {
    "sourceHash": "47a7561be69b00cd",
    "translations": {
      "zh": "---\ntitle: \"Codex CLI\"\ndescription: \"配置 OpenAI Codex CLI 以使用 LemonData API\"\n---\n\n## 概览\n\nOpenAI Codex 是一款开源命令行工具 (CLI)，作为一个轻量级编程智能体，能够在终端中读取、修改和运行代码。它基于 GPT 模型构建，并针对代码生成进行了优化。\n\n## 系统要求\n\n- **操作系统**：macOS, Linux (官方支持), Windows (通过 WSL)\n- **Node.js**：18+ 版本\n- **npm**：10.x.x 或更高版本\n\n## 安装\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\n验证安装：\n\n```bash\ncodex --version\n```\n\n## 配置\n\n### 步骤 1：设置 API Key\n\n**临时（当前会话）：**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**永久配置：**\n\n添加到 `~/.bashrc`、`~/.zshrc` 或 `~/.bash_profile`：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n然后重新加载：\n\n```bash\nsource ~/.zshrc  # 或 source ~/.bashrc\n```\n\n### 步骤 2：配置 config.toml\n\n编辑 `~/.codex/config.toml`：\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\n如果配置文件不存在，请运行一次 `codex` 以生成该文件，然后再进行编辑。\n</Note>\n\n## 基本用法\n\n**启动交互模式：**\n\n```bash\ncodex\n```\n\n**直接命令：**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**指定模型：**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## 可用模型\n\n| 模型 | 最适用于 |\n|-------|----------|\n| `gpt-4o` | 复杂的编程任务、架构设计 |\n| `gpt-4o-mini` | 快速修复、简单任务 |\n| `claude-sonnet-4-5` | 代码审查、文档编写 |\n| `deepseek-r1` | 算法设计、推理 |\n\n## 交互式命令\n\n| 命令 | 描述 |\n|---------|-------------|\n| `/help` | 显示帮助 |\n| `/exit` 或 `Ctrl+C` | 退出 |\n| `/clear` | 清除对话 |\n| `/config` | 查看配置 |\n| `/model <name>` | 切换模型 |\n| `/tokens` | 查看 token 使用情况 |\n\n## 验证配置\n\n```bash\n# 检查环境变量\necho $OPENAI_API_KEY\n\n# 测试 API 连接\ncodex \"Hello, Codex!\"\n\n# 查看配置\ncat ~/.codex/config.toml\n```\n\n## 常见使用场景\n\n**代码审查：**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**生成提交信息：**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**修复错误：**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**解释代码：**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"连接错误\">\n    - 验证 `config.toml` 中的 `base_url` 是否准确为 `https://api.lemondata.cc/v1`\n    - 检查网络连接\n    - 确保没有代理干扰\n  </Accordion>\n\n  <Accordion title=\"身份验证失败\">\n    - 验证 `OPENAI_API_KEY` 环境变量是否已设置\n    - 检查 Key 是否以 `sk-` 开头\n    - 确保该 Key 在 LemonData 控制面板中处于激活状态\n  </Accordion>\n\n  <Accordion title=\"未找到模型\">\n    - 检查模型名称是否完全匹配\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh/models) 验证模型可用性\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"Codex CLI\"\ndescription: \"設定 OpenAI Codex CLI 以使用 LemonData API\"\n---\n\n## 概覽\n\nOpenAI Codex 是一款開源的命令列工具 (CLI)，作為輕量級的程式碼代理 (coding agent)，能夠在終端機中讀取、修改及執行程式碼。它基於 GPT 模型構建，並針對程式碼生成進行了優化。\n\n## 系統需求\n\n- **作業系統**：macOS, Linux (官方支援), Windows 透過 WSL\n- **Node.js**：版本 18+\n- **npm**：版本 10.x.x 或更高\n\n## 安裝\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\n驗證安裝：\n\n```bash\ncodex --version\n```\n\n## 配置\n\n### 步驟 1：設定 API Key\n\n**臨時（目前工作階段）：**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**永久配置：**\n\n新增至 `~/.bashrc`, `~/.zshrc`, 或 `~/.bash_profile`：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n接著重新載入：\n\n```bash\nsource ~/.zshrc  # 或 source ~/.bashrc\n```\n\n### 步驟 2：配置 config.toml\n\n編輯 `~/.codex/config.toml`：\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\n如果配置檔案不存在，請執行一次 `codex` 以生成檔案，然後再進行編輯。\n</Note>\n\n## 基本用法\n\n**啟動互動模式：**\n\n```bash\ncodex\n```\n\n**直接指令：**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**指定模型：**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## 可用模型\n\n| 模型 | 適用場景 |\n|-------|----------|\n| `gpt-4o` | 複雜的編碼任務、架構設計 |\n| `gpt-4o-mini` | 快速修復、簡單任務 |\n| `claude-sonnet-4-5` | 程式碼審查、文件撰寫 |\n| `deepseek-r1` | 演算法設計、推理 |\n\n## 互動指令\n\n| 指令 | 描述 |\n|---------|-------------|\n| `/help` | 顯示說明 |\n| `/exit` 或 `Ctrl+C` | 退出 |\n| `/clear` | 清除對話 |\n| `/config` | 查看配置 |\n| `/model <name>` | 切換模型 |\n| `/tokens` | 查看 token 使用量 |\n\n## 驗證配置\n\n```bash\n# 檢查環境變數\necho $OPENAI_API_KEY\n\n# 測試 API 連線\ncodex \"Hello, Codex!\"\n\n# 查看配置\ncat ~/.codex/config.toml\n```\n\n## 常見使用案例\n\n**程式碼審查：**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**生成 commit 訊息：**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**修復錯誤：**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**解釋程式碼：**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"連線錯誤\">\n    - 驗證 config.toml 中的 `base_url` 是否準確為 `https://api.lemondata.cc/v1`\n    - 檢查網路連線狀態\n    - 確保無代理伺服器干擾\n  </Accordion>\n\n  <Accordion title=\"身份驗證失敗\">\n    - 驗證 `OPENAI_API_KEY` 環境變數已設定\n    - 檢查金鑰是否以 `sk-` 開頭\n    - 確保金鑰在 LemonData 控制面板中處於啟用狀態\n  </Accordion>\n\n  <Accordion title=\"找不到模型\">\n    - 檢查模型名稱是否完全符合\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh-TW/models) 驗證模型可用性\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"Codex CLI\"\ndescription: \"LemonData API を使用するように OpenAI Codex CLI を設定する\"\n---\n\n## 概要\n\nOpenAI Codex は、ターミナルでコードの読み取り、修正、実行が可能な軽量コーディングエージェントとして機能するオープンソースのコマンドラインツール（CLI）です。GPT モデルに基づいて構築されており、コード生成に最適化されています。\n\n## システム要件\n\n- **OS**: macOS, Linux（公式サポート）, Windows（WSL 経由）\n- **Node.js**: バージョン 18 以上\n- **npm**: バージョン 10.x.x 以上\n\n## インストール\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nインストールの確認:\n\n```bash\ncodex --version\n```\n\n## 設定\n\n### ステップ 1: API キーの設定\n\n**一時的（現在のセッション）:**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**永続的な設定:**\n\n`~/.bashrc`、`~/.zshrc`、または `~/.bash_profile` に追加します:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nその後、再読み込みします:\n\n```bash\nsource ~/.zshrc  # または source ~/.bashrc\n```\n\n### ステップ 2: config.toml の設定\n\n`~/.codex/config.toml` を編集します:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\n設定ファイルが存在しない場合は、一度 `codex` を実行してファイルを生成してから編集してください。\n</Note>\n\n## 基本的な使い方\n\n**対話モードを開始:**\n\n```bash\ncodex\n```\n\n**直接コマンドを実行:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**モデルを指定:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## 利用可能なモデル\n\n| モデル | 最適な用途 |\n|-------|----------|\n| `gpt-4o` | 複雑なコーディングタスク、アーキテクチャ |\n| `gpt-4o-mini` | 迅速な修正、シンプルなタスク |\n| `claude-sonnet-4-5` | コードレビュー、ドキュメント作成 |\n| `deepseek-r1` | アルゴリズム設計、推論 |\n\n## 対話型コマンド\n\n| コマンド | 説明 |\n|---------|-------------|\n| `/help` | ヘルプを表示 |\n| `/exit` または `Ctrl+C` | 終了 |\n| `/clear` | 会話をクリア |\n| `/config` | 設定を表示 |\n| `/model <name>` | モデルを切り替え |\n| `/tokens` | トークン使用量を表示 |\n\n## 設定の確認\n\n```bash\n# 環境変数を確認\necho $OPENAI_API_KEY\n\n# API 接続をテスト\ncodex \"Hello, Codex!\"\n\n# 設定を表示\ncat ~/.codex/config.toml\n```\n\n## 一般的なユースケース\n\n**コードレビュー:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**コミットメッセージの生成:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**エラーの修正:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**コードの解説:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"接続エラー\">\n    - config.toml の `base_url` が正確に `https://api.lemondata.cc/v1` であることを確認してください\n    - ネットワーク接続を確認してください\n    - プロキシの干渉がないことを確認してください\n  </Accordion>\n\n  <Accordion title=\"認証失敗\">\n    - 環境変数 `OPENAI_API_KEY` が設定されていることを確認してください\n    - キーが `sk-` で始まっていることを確認してください\n    - LemonData ダッシュボードでキーが有効であることを確認してください\n  </Accordion>\n\n  <Accordion title=\"モデルが見つかりません\">\n    - モデル名が正確に一致しているか確認してください\n    - [lemondata.cc/en/models](https://lemondata.cc/ja/models) でモデルの利用可能性を確認してください\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"Codex CLI\"\ndescription: \"LemonData API를 사용하도록 OpenAI Codex CLI 설정하기\"\n---\n\n## 개요\n\nOpenAI Codex는 터미널에서 코드를 읽고, 수정하고, 실행할 수 있는 경량 코딩 에이전트 역할을 하는 오픈 소스 명령줄 도구(CLI)입니다. GPT 모델을 기반으로 구축되었으며 코드 생성에 최적화되어 있습니다.\n\n## 시스템 요구 사항\n\n- **OS**: macOS, Linux (공식 지원), Windows (WSL 사용)\n- **Node.js**: 버전 18 이상\n- **npm**: 버전 10.x.x 이상\n\n## 설치\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\n설치 확인:\n\n```bash\ncodex --version\n```\n\n## 설정\n\n### 1단계: API Key 설정\n\n**임시 (현재 세션):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**영구 설정:**\n\n`~/.bashrc`, `~/.zshrc` 또는 `~/.bash_profile`에 추가:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n이후 다시 로드:\n\n```bash\nsource ~/.zshrc  # 또는 source ~/.bashrc\n```\n\n### 2단계: config.toml 설정\n\n`~/.codex/config.toml` 수정:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\n설정 파일이 존재하지 않는 경우, `codex`를 한 번 실행하여 파일을 생성한 후 수정하세요.\n</Note>\n\n## 기본 사용법\n\n**대화형 모드 시작:**\n\n```bash\ncodex\n```\n\n**직접 명령:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**모델 지정:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## 사용 가능한 모델\n\n| 모델 | 최적 용도 |\n|-------|----------|\n| `gpt-4o` | 복잡한 코딩 작업, 아키텍처 |\n| `gpt-4o-mini` | 빠른 수정, 간단한 작업 |\n| `claude-sonnet-4-5` | 코드 리뷰, 문서화 |\n| `deepseek-r1` | 알고리즘 설계, 추론 |\n\n## 대화형 명령\n\n| 명령어 | 설명 |\n|---------|-------------|\n| `/help` | 도움말 표시 |\n| `/exit` 또는 `Ctrl+C` | 종료 |\n| `/clear` | 대화 내역 삭제 |\n| `/config` | 설정 보기 |\n| `/model <name>` | 모델 전환 |\n| `/tokens` | 토큰 사용량 확인 |\n\n## 설정 확인\n\n```bash\n# 환경 변수 확인\necho $OPENAI_API_KEY\n\n# API 연결 테스트\ncodex \"Hello, Codex!\"\n\n# 설정 보기\ncat ~/.codex/config.toml\n```\n\n## 일반적인 사용 사례\n\n**코드 리뷰:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**커밋 메시지 생성:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**오류 수정:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**코드 설명:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"연결 오류\">\n    - `config.toml`의 `base_url`이 정확히 `https://api.lemondata.cc/v1`인지 확인하세요.\n    - 네트워크 연결 상태를 확인하세요.\n    - 프록시 간섭이 없는지 확인하세요.\n  </Accordion>\n\n  <Accordion title=\"인증 실패\">\n    - `OPENAI_API_KEY` 환경 변수가 설정되어 있는지 확인하세요.\n    - 키가 `sk-`로 시작하는지 확인하세요.\n    - LemonData 대시보드에서 키가 활성 상태인지 확인하세요.\n  </Accordion>\n\n  <Accordion title=\"모델을 찾을 수 없음\">\n    - 모델 이름이 정확히 일치하는지 확인하세요.\n    - [lemondata.cc/en/models](https://lemondata.cc/ko/models)에서 모델 가용성을 확인하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Codex CLI\"\ndescription: \"Konfigurieren Sie die OpenAI Codex CLI zur Verwendung der LemonData API\"\n---\n\n## Übersicht\n\nOpenAI Codex ist ein Open-Source-Kommandozeilen-Tool (CLI), das als leichtgewichtiger Coding-Agent fungiert und in der Lage ist, Code im Terminal zu lesen, zu ändern und auszuführen. Es basiert auf GPT-Modellen und ist für die Codegenerierung optimiert.\n\n## Systemanforderungen\n\n- **Betriebssystem**: macOS, Linux (offizieller Support), Windows über WSL\n- **Node.js**: Version 18+\n- **npm**: Version 10.x.x oder höher\n\n## Installation\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nInstallation überprüfen:\n\n```bash\ncodex --version\n```\n\n## Konfiguration\n\n### Schritt 1: API-Key festlegen\n\n**Temporär (aktuelle Sitzung):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Permanente Konfiguration:**\n\nHinzufügen zu `~/.bashrc`, `~/.zshrc` oder `~/.bash_profile`:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nAnschließend neu laden:\n\n```bash\nsource ~/.zshrc  # oder source ~/.bashrc\n```\n\n### Schritt 2: config.toml konfigurieren\n\nBearbeiten Sie `~/.codex/config.toml`:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nFalls die Konfigurationsdatei nicht existiert, führen Sie `codex` einmal aus, um sie zu generieren, und bearbeiten Sie die Datei anschließend.\n</Note>\n\n## Grundlegende Nutzung\n\n**Interaktiven Modus starten:**\n\n```bash\ncodex\n```\n\n**Direkter Befehl:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Modell angeben:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Verfügbare Modelle\n\n| Modell | Bestens geeignet für |\n|-------|----------|\n| `gpt-4o` | Komplexe Coding-Aufgaben, Architektur |\n| `gpt-4o-mini` | Schnelle Fehlerbehebungen, einfache Aufgaben |\n| `claude-sonnet-4-5` | Code-Review, Dokumentation |\n| `deepseek-r1` | Algorithmus-Design, Reasoning |\n\n## Interaktive Befehle\n\n| Befehl | Beschreibung |\n|---------|-------------|\n| `/help` | Hilfe anzeigen |\n| `/exit` oder `Ctrl+C` | Beenden |\n| `/clear` | Konversation löschen |\n| `/config` | Konfiguration anzeigen |\n| `/model <name>` | Modell wechseln |\n| `/tokens` | Token-Verbrauch anzeigen |\n\n## Konfiguration überprüfen\n\n```bash\n# Umgebungsvariable prüfen\necho $OPENAI_API_KEY\n\n# API-Verbindung testen\ncodex \"Hello, Codex!\"\n\n# Konfiguration anzeigen\ncat ~/.codex/config.toml\n```\n\n## Häufige Anwendungsfälle\n\n**Code-Review:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**Commit-Nachrichten generieren:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**Fehler beheben:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**Code erklären:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Verbindungsfehler\">\n    - Überprüfen Sie, ob `base_url` in der config.toml exakt `https://api.lemondata.cc/v1` entspricht\n    - Prüfen Sie die Netzwerkverbindung\n    - Stellen Sie sicher, dass keine Proxy-Interferenzen vorliegen\n  </Accordion>\n\n  <Accordion title=\"Authentifizierung fehlgeschlagen\">\n    - Überprüfen Sie, ob die Umgebungsvariable `OPENAI_API_KEY` gesetzt ist\n    - Stellen Sie sicher, dass der Key mit `sk-` beginnt\n    - Stellen Sie sicher, dass der Key im LemonData-Dashboard aktiv ist\n  </Accordion>\n\n  <Accordion title=\"Modell nicht gefunden\">\n    - Überprüfen Sie, ob der Modellname exakt übereinstimmt\n    - Überprüfen Sie die Modellverfügbarkeit unter [lemondata.cc/en/models](https://lemondata.cc/de/models)\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"CLI Codex\"\ndescription: \"Configurer le CLI OpenAI Codex pour utiliser l'API LemonData\"\n---\n\n## Présentation\n\nOpenAI Codex est un outil en ligne de commande (CLI) open-source qui sert d'agent de codage léger, capable de lire, modifier et exécuter du code dans le terminal. Il est basé sur les modèles GPT et optimisé pour la génération de code.\n\n## Configuration requise\n\n- **Système d'exploitation** : macOS, Linux (support officiel), Windows via WSL\n- **Node.js** : Version 18+\n- **npm** : Version 10.x.x ou supérieure\n\n## Installation\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nVérifier l'installation :\n\n```bash\ncodex --version\n```\n\n## Configuration\n\n### Étape 1 : Définir la clé API\n\n**Temporaire (session actuelle) :**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Configuration permanente :**\n\nAjouter à `~/.bashrc`, `~/.zshrc` ou `~/.bash_profile` :\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nEnsuite, recharger :\n\n```bash\nsource ~/.zshrc  # or source ~/.bashrc\n```\n\n### Étape 2 : Configurer config.toml\n\nModifier `~/.codex/config.toml` :\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nSi le fichier de configuration n'existe pas, exécutez `codex` une fois pour le générer, puis modifiez le fichier.\n</Note>\n\n## Utilisation de base\n\n**Démarrer le mode interactif :**\n\n```bash\ncodex\n```\n\n**Commande directe :**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Spécifier le modèle :**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Modèles disponibles\n\n| Modèle | Idéal pour |\n|-------|----------|\n| `gpt-4o` | Tâches de codage complexes, architecture |\n| `gpt-4o-mini` | Corrections rapides, tâches simples |\n| `claude-sonnet-4-5` | Revue de code, documentation |\n| `deepseek-r1` | Conception d'algorithmes, raisonnement |\n\n## Commandes interactives\n\n| Commande | Description |\n|---------|-------------|\n| `/help",
      "es": "---\ntitle: \"Codex CLI\"\ndescription: \"Configure OpenAI Codex CLI para usar la API de LemonData\"\n---\n\n## Descripción general\n\nOpenAI Codex es una herramienta de línea de comandos (CLI) de código abierto que sirve como un agente de programación ligero, capaz de leer, modificar y ejecutar código en la terminal. Está construido sobre modelos GPT y optimizado para la generación de código.\n\n## Requisitos del sistema\n\n- **SO**: macOS, Linux (soporte oficial), Windows a través de WSL\n- **Node.js**: Versión 18+\n- **npm**: Versión 10.x.x o superior\n\n## Instalación\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nVerificar la instalación:\n\n```bash\ncodex --version\n```\n\n## Configuración\n\n### Paso 1: Configurar la API Key\n\n**Temporal (sesión actual):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Configuración permanente:**\n\nAñadir a `~/.bashrc`, `~/.zshrc` o `~/.bash_profile`:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nLuego recargar:\n\n```bash\nsource ~/.zshrc  # o source ~/.bashrc\n```\n\n### Paso 2: Configurar config.toml\n\nEditar `~/.codex/config.toml`:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nSi el archivo de configuración no existe, ejecute `codex` una vez para generarlo y luego edite el archivo.\n</Note>\n\n## Uso básico\n\n**Iniciar modo interactivo:**\n\n```bash\ncodex\n```\n\n**Comando directo:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Especificar modelo:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Modelos disponibles\n\n| Modelo | Ideal para |\n|-------|----------|\n| `gpt-4o` | Tareas de programación complejas, arquitectura |\n| `gpt-4o-mini` | Correcciones rápidas, tareas simples |\n| `claude-sonnet-4-5` | Revisión de código, documentación |\n| `deepseek-r1` | Diseño de algoritmos, razonamiento |\n\n## Comandos interactivos\n\n| Comando | Descripción |\n|---------|-------------|\n| `/help` | Mostrar ayuda |\n| `/exit` o `Ctrl+C` | Salir |\n| `/clear` | Limpiar conversación |\n| `/config` | Ver configuración |\n| `/model <name>` | Cambiar modelo |\n| `/tokens` | Ver uso de tokens |\n\n## Verificar configuración\n\n```bash\n# Comprobar variable de entorno\necho $OPENAI_API_KEY\n\n# Probar conexión de la API\ncodex \"Hello, Codex!\"\n\n# Ver configuración\ncat ~/.codex/config.toml\n```\n\n## Casos de uso comunes\n\n**Revisión de código:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**Generar mensajes de commit:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**Corregir errores:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**Explicar código:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## Solución de problemas\n\n<AccordionGroup>\n  <Accordion title=\"Error de conexión\">\n    - Verifique que `base_url` en `config.toml` sea exactamente `https://api.lemondata.cc/v1`\n    - Compruebe la conectividad de red\n    - Asegúrese de que no haya interferencia de proxy\n  </Accordion>\n\n  <Accordion title=\"Autenticación fallida\">\n    - Verifique que la variable de entorno `OPENAI_API_KEY` esté configurada\n    - Compruebe que la clave comience con `sk-`\n    - Asegúrese de que la clave esté activa en el panel de LemonData\n  </Accordion>\n\n  <Accordion title=\"Modelo no encontrado\">\n    - Compruebe que el nombre del modelo coincida exactamente\n    - Verifique la disponibilidad del modelo en [lemondata.cc/en/models](https://lemondata.cc/es/models)\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Codex CLI\"\ndescription: \"Configure a CLI do OpenAI Codex para usar a API da LemonData\"\n---\n\n## Visão Geral\n\nO OpenAI Codex é uma ferramenta de linha de comando (CLI) de código aberto que serve como um agente de codificação leve, capaz de ler, modificar e executar código no terminal. Ele é construído em modelos GPT e otimizado para geração de código.\n\n## Requisitos do Sistema\n\n- **SO**: macOS, Linux (suporte oficial), Windows via WSL\n- **Node.js**: Versão 18+\n- **npm**: Versão 10.x.x ou superior\n\n## Instalação\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nVerifique a instalação:\n\n```bash\ncodex --version\n```\n\n## Configuração\n\n### Passo 1: Configurar a Chave de API\n\n**Temporário (sessão atual):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Configuração permanente:**\n\nAdicione ao `~/.bashrc`, `~/.zshrc` ou `~/.bash_profile`:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nEm seguida, recarregue:\n\n```bash\nsource ~/.zshrc  # or source ~/.bashrc\n```\n\n### Passo 2: Configurar o config.toml\n\nEdite o `~/.codex/config.toml`:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nSe o arquivo de configuração não existir, execute o `codex` uma vez para gerá-lo e, em seguida, edite o arquivo.\n</Note>\n\n## Uso Básico\n\n**Iniciar modo interativo:**\n\n```bash\ncodex\n```\n\n**Comando direto:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Especificar modelo:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Modelos Disponíveis\n\n| Modelo | Melhor Para |\n|-------|----------|\n| `gpt-4o` | Tarefas de codificação complexas, arquitetura |\n| `gpt-4o-mini` | Correções rápidas, tarefas simples |\n| `claude-sonnet-4-5` | Revisão de código, documentação |\n| `deepseek-r1` | Design de algoritmos, raciocínio |\n\n## Comandos Interativos\n\n| Comando | Descrição |\n|---------|-------------|\n| `/help` | Exibir ajuda |",
      "ar": "---\ntitle: \"Codex CLI\"\ndescription: \"تكوين OpenAI Codex CLI لاستخدام LemonData API\"\n---\n\n## نظرة عامة\n\nتعد OpenAI Codex أداة سطر أوامر (CLI) مفتوحة المصدر تعمل كوكيل برمجة خفيف الوزن، وهي قادرة على قراءة الكود وتعديله وتشغيله في الجهاز الطرفي (terminal). تم بناؤها على نماذج GPT وهي محسنة لتوليد الكود.\n\n## متطلبات النظام\n\n- **نظام التشغيل**: macOS، Linux (دعم رسمي)، Windows عبر WSL\n- **Node.js**: الإصدار 18+\n- **npm**: الإصدار 10.x.x أو أحدث\n\n## التثبيت\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nالتحقق من التثبيت:\n\n```bash\ncodex --version\n```\n\n## التكوين\n\n### الخطوة 1: تعيين API Key\n\n**مؤقت (الجلسة الحالية):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**تكوين دائم:**\n\nأضف إلى `~/.bashrc` أو `~/.zshrc` أو `~/.bash_profile`:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nثم قم بإعادة التحميل:\n\n```bash\nsource ~/.zshrc  # or source ~/.bashrc\n```\n\n### الخطوة 2: تكوين config.toml\n\nقم بتحرير `~/.codex/config.toml`:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nإذا لم يكن ملف التكوين موجوداً، فقم بتشغيل `codex` مرة واحدة لإنشائه، ثم قم بتحرير الملف.\n</Note>\n\n## الاستخدام الأساسي\n\n**بدء الوضع التفاعلي:**\n\n```bash\ncodex\n```\n\n**أمر مباشر:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**تحديد النموذج:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## النماذج المتاحة\n\n| النموذج | الأفضل لـ |\n|-------|----------|\n| `gpt-4o` | مهام البرمجة المعقدة، الهندسة المعمارية |\n| `gpt-4o-mini` | الإصلاحات السريعة، المهام البسيطة |\n| `claude-sonnet-4-5` | مراجعة الكود، التوثيق |\n| `deepseek-r1` | تصميم الخوارزميات، الاستنتاج |\n\n## الأوامر التفاعلية\n\n| الأمر | الوصف |\n|---------|-------------|\n| `/help` | عرض المساعدة |\n| `/exit` أو `Ctrl+C` | خروج |\n| `/clear` | مسح المحادثة |\n| `/config` | عرض التكوين |\n| `/model <name>` | تبديل النموذج |\n| `/tokens` | عرض استخدام token |\n\n## التحقق من التكوين\n\n```bash\n# التحقق من متغير البيئة\necho $OPENAI_API_KEY\n\n# اختبار اتصال API\ncodex \"Hello, Codex!\"\n\n# عرض التكوين\ncat ~/.codex/config.toml\n```\n\n## حالات الاستخدام الشائعة\n\n**مراجعة الكود:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**توليد رسائل commit:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**إصلاح الأخطاء:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**شرح الكود:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## استكشاف الأخطاء وإصلاحها\n\n<AccordionGroup>\n  <Accordion title=\"خطأ في الاتصال\">\n    - تحقق من أن `base_url` في config.toml هو بالضبط `https://api.lemondata.cc/v1`\n    - تحقق من اتصال الشبكة\n    - تأكد من عدم وجود تداخل من بروكسي (proxy)\n  </Accordion>\n\n  <Accordion title=\"فشل المصادقة\">\n    - تحقق من تعيين متغير البيئة `OPENAI_API_KEY`\n    - تحقق من أن المفتاح يبدأ بـ `sk-`\n    - تأكد من أن المفتاح نشط في لوحة تحكم LemonData\n  </Accordion>\n\n  <Accordion title=\"النموذج غير موجود\">\n    - تحقق من مطابقة اسم النموذج تماماً\n    - تحقق من توفر النموذج في [lemondata.cc/en/models](https://lemondata.cc/ar/models)\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Codex CLI\"\ndescription: \"Cấu hình OpenAI Codex CLI để sử dụng LemonData API\"\n---\n\n## Tổng quan\n\nOpenAI Codex là một công cụ dòng lệnh (CLI) mã nguồn mở đóng vai trò như một tác nhân lập trình (coding agent) nhẹ, có khả năng đọc, sửa đổi và chạy mã trong terminal. Nó được xây dựng trên các mô hình GPT và được tối ưu hóa cho việc tạo mã.\n\n## Yêu cầu hệ thống\n\n- **Hệ điều hành**: macOS, Linux (hỗ trợ chính thức), Windows thông qua WSL\n- **Node.js**: Phiên bản 18+\n- **npm**: Phiên bản 10.x.x hoặc cao hơn\n\n## Cài đặt\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nXác minh cài đặt:\n\n```bash\ncodex --version\n```\n\n## Cấu hình\n\n### Bước 1: Thiết lập API Key\n\n**Tạm thời (phiên hiện tại):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Cấu hình vĩnh viễn:**\n\nThêm vào `~/.bashrc`, `~/.zshrc`, hoặc `~/.bash_profile`:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nSau đó tải lại:\n\n```bash\nsource ~/.zshrc  # hoặc source ~/.bashrc\n```\n\n### Bước 2: Cấu hình config.toml\n\nChỉnh sửa `~/.codex/config.toml`:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nNếu tệp cấu hình không tồn tại, hãy chạy `codex` một lần để tạo tệp, sau đó chỉnh sửa tệp.\n</Note>\n\n## Cách sử dụng cơ bản\n\n**Bắt đầu chế độ tương tác:**\n\n```bash\ncodex\n```\n\n**Lệnh trực tiếp:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Chỉ định mô hình:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Các mô hình hiện có\n\n| Mô hình | Tốt nhất cho |\n|-------|----------|\n| `gpt-4o` | Các tác vụ lập trình phức tạp, kiến trúc |\n| `gpt-4o-mini` | Sửa lỗi nhanh, các tác vụ đơn giản |\n| `claude-sonnet-4-5` | Đánh giá mã nguồn, tài liệu hóa |\n| `deepseek-r1` | Thiết kế thuật toán, suy luận |\n\n## Các lệnh tương tác\n\n| Lệnh | Mô tả |\n|---------|-------------|\n| `/help` | Hiển thị trợ giúp |\n| `/exit` hoặc `Ctrl+C` | Thoát |\n| `/clear` | Xóa cuộc hội thoại |\n| `/config` | Xem cấu hình |\n| `/model <name>` | Chuyển đổi mô hình |\n| `/tokens` | Xem mức sử dụng token |\n\n## Xác minh cấu hình\n\n```bash\n# Kiểm tra biến môi trường\necho $OPENAI_API_KEY\n\n# Kiểm tra kết nối API\ncodex \"Hello, Codex!\"\n\n# Xem cấu hình\ncat ~/.codex/config.toml\n```\n\n## Các trường hợp sử dụng phổ biến\n\n**Đánh giá mã nguồn:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**Tạo thông điệp commit:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**Sửa lỗi:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**Giải thích mã nguồn:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## Khắc phục sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Lỗi kết nối\">\n    - Xác minh `base_url` trong config.toml chính xác là `https://api.lemondata.cc/v1`\n    - Kiểm tra kết nối mạng\n    - Đảm bảo không có sự can thiệp của proxy\n  </Accordion>\n\n  <Accordion title=\"Xác thực thất bại\">\n    - Xác minh biến môi trường `OPENAI_API_KEY` đã được thiết lập\n    - Kiểm tra xem key có bắt đầu bằng `sk-` hay không\n    - Đảm bảo key đang hoạt động trong bảng điều khiển LemonData\n  </Accordion>\n\n  <Accordion title=\"Không tìm thấy mô hình\">\n    - Kiểm tra tên mô hình khớp chính xác\n    - Xác minh tính khả dụng của mô hình tại [lemondata.cc/en/models](https://lemondata.cc/vi/models)\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Codex CLI\"\ndescription: \"Konfigurasi OpenAI Codex CLI untuk menggunakan LemonData API\"\n---\n\n## Ringkasan\n\nOpenAI Codex adalah alat baris perintah (CLI) sumber terbuka yang berfungsi sebagai agen pengodean ringan, yang mampu membaca, memodifikasi, dan menjalankan kode di terminal. Alat ini dibangun di atas model GPT dan dioptimalkan untuk pembuatan kode.\n\n## Persyaratan Sistem\n\n- **OS**: macOS, Linux (dukungan resmi), Windows melalui WSL\n- **Node.js**: Versi 18+\n- **npm**: Versi 10.x.x atau lebih tinggi\n\n## Instalasi\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nVerifikasi instalasi:\n\n```bash\ncodex --version\n```\n\n## Konfigurasi\n\n### Langkah 1: Atur API Key\n\n**Sementara (sesi saat ini):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Konfigurasi permanen:**\n\nTambahkan ke `~/.bashrc`, `~/.zshrc`, atau `~/.bash_profile`:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nKemudian muat ulang:\n\n```bash\nsource ~/.zshrc  # atau source ~/.bashrc\n```\n\n### Langkah 2: Konfigurasi config.toml\n\nEdit `~/.codex/config.toml`:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nJika file konfigurasi tidak ada, jalankan `codex` sekali untuk membuatnya, lalu edit file tersebut.\n</Note>\n\n## Penggunaan Dasar\n\n**Mulai mode interaktif:**\n\n```bash\ncodex\n```\n\n**Perintah langsung:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Tentukan model:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Model yang Tersedia\n\n| Model | Terbaik Untuk |\n|-------|----------|\n| `gpt-4o` | Tugas pengodean kompleks, arsitektur |\n| `gpt-4o-mini` | Perbaikan cepat, tugas sederhana |\n| `claude-sonnet-4-5` | Tinjauan kode, dokumentasi |\n| `deepseek-r1` | Desain algoritma, penalaran |\n\n## Perintah Interaktif\n\n| Perintah | Deskripsi |\n|---------|-------------|\n| `/help` | Tampilkan bantuan |\n| `/exit` atau `Ctrl+C` | Keluar |\n| `/clear` | Bersihkan percakapan |\n| `/config` | Lihat konfigurasi |\n| `/model <name>` | Ganti model |\n| `/tokens` | Lihat penggunaan token |\n\n## Verifikasi Konfigurasi\n\n```bash\n# Periksa variabel lingkungan\necho $OPENAI_API_KEY\n\n# Uji koneksi API\ncodex \"Hello, Codex!\"\n\n# Lihat konfigurasi\ncat ~/.codex/config.toml\n```\n\n## Kasus Penggunaan Umum\n\n**Tinjauan kode:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**Buat pesan commit:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**Perbaiki kesalahan:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**Jelaskan kode:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## Pemecahan Masalah\n\n<AccordionGroup>\n  <Accordion title=\"Kesalahan Koneksi\">\n    - Verifikasi `base_url` di config.toml tepat `https://api.lemondata.cc/v1`\n    - Periksa konektivitas jaringan\n    - Pastikan tidak ada gangguan proksi\n  </Accordion>\n\n  <Accordion title=\"Autentikasi Gagal\">\n    - Verifikasi variabel lingkungan `OPENAI_API_KEY` telah diatur\n    - Pastikan kunci dimulai dengan `sk-`\n    - Pastikan kunci aktif di dasbor LemonData\n  </Accordion>\n\n  <Accordion title=\"Model Tidak Ditemukan\">\n    - Pastikan nama model cocok secara tepat\n    - Verifikasi ketersediaan model di [lemondata.cc/en/models](https://lemondata.cc/id/models)\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"Codex CLI\"\ndescription: \"OpenAI Codex CLI'yı LemonData API kullanacak şekilde yapılandırın\"\n---\n\n## Genel Bakış\n\nOpenAI Codex, terminalde kod okuyabilen, değiştirebilen ve çalıştırabilen hafif bir kodlama aracısı olarak hizmet veren açık kaynaklı bir komut satırı aracıdır (CLI). GPT modelleri üzerine inşa edilmiştir ve kod üretimi için optimize edilmiştir.\n\n## Sistem Gereksinimleri\n\n- **İşletim Sistemi**: macOS, Linux (resmi destek), WSL aracılığıyla Windows\n- **Node.js**: Versiyon 18+\n- **npm**: Versiyon 10.x.x veya daha yüksek\n\n## Kurulum\n\n```bash\nsudo npm install -g @openai/codex@latest\n```\n\nKurulumu doğrulayın:\n\n```bash\ncodex --version\n```\n\n## Yapılandırma\n\n### Adım 1: API Anahtarını Ayarlayın\n\n**Geçici (mevcut oturum):**\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\n**Kalıcı yapılandırma:**\n\n`~/.bashrc`, `~/.zshrc` veya `~/.bash_profile` dosyasına ekleyin:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\n```\n\nArdından yeniden yükleyin:\n\n```bash\nsource ~/.zshrc  # veya source ~/.bashrc\n```\n\n### Adım 2: config.toml Dosyasını Yapılandırın\n\n`~/.codex/config.toml` dosyasını düzenleyin:\n\n```toml\nmodel = \"gpt-4o\"\nmodel_provider = \"lemondata\"\n\n[model_providers.lemondata]\nname = \"LemonData\"\nbase_url = \"https://api.lemondata.cc/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n```\n\n<Note>\nYapılandırma dosyası mevcut değilse, dosyayı oluşturmak için `codex` komutunu bir kez çalıştırın ve ardından dosyayı düzenleyin.\n</Note>\n\n## Temel Kullanım\n\n**Etkileşimli modu başlatın:**\n\n```bash\ncodex\n```\n\n**Doğrudan komut:**\n\n```bash\ncodex \"Fix the bug in main.py line 42\"\n```\n\n**Modeli belirtin:**\n\n```bash\ncodex -m gpt-4o \"Build a REST API server\"\n```\n\n## Mevcut Modeller\n\n| Model | En İyi Kullanım Alanı |\n|-------|----------|\n| `gpt-4o` | Karmaşık kodlama görevleri, mimari |\n| `gpt-4o-mini` | Hızlı düzeltmeler, basit görevler |\n| `claude-sonnet-4-5` | Kod incelemesi, dokümantasyon |\n| `deepseek-r1` | Algoritma tasarımı, akıl yürütme |\n\n## Etkileşimli Komutlar\n\n| Komut | Açıklama |\n|---------|-------------|\n| `/help` | Yardımı görüntüle |\n| `/exit` veya `Ctrl+C` | Çıkış |\n| `/clear` | Konuşmayı temizle |\n| `/config` | Yapılandırmayı görüntüle |\n| `/model <name>` | Model değiştir |\n| `/tokens` | Token kullanımını görüntüle |\n\n## Yapılandırmayı Doğrulayın\n\n```bash\n# Ortam değişkenini kontrol edin\necho $OPENAI_API_KEY\n\n# API bağlantısını test edin\ncodex \"Hello, Codex!\"\n\n# Yapılandırmayı görüntüleyin\ncat ~/.codex/config.toml\n```\n\n## Yaygın Kullanım Örnekleri\n\n**Kod incelemesi:**\n\n```bash\ngit diff | codex \"Review these code changes\"\n```\n\n**Commit mesajları oluşturun:**\n\n```bash\ngit diff --staged | codex \"Generate a commit message for these changes\"\n```\n\n**Hataları düzeltin:**\n\n```bash\ncodex \"Fix the TypeScript errors in src/components/\"\n```\n\n**Kodu açıklayın:**\n\n```bash\ncat main.py | codex \"Explain what this code does\"\n```\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Bağlantı Hatası\">\n    - config.toml içindeki `base_url` değerinin tam olarak `https://api.lemondata.cc/v1` olduğunu doğrulayın\n    - Ağ bağlantısını kontrol edin\n    - Proxy müdahalesi olmadığından emin olun\n  </Accordion>\n\n  <Accordion title=\"Kimlik Doğrulama Başarısız\">\n    - `OPENAI_API_KEY` ortam değişkeninin ayarlandığını doğrulayın\n    - Anahtarın `sk-` ile başladığını kontrol edin\n    - Anahtarın LemonData panelinde aktif olduğundan emin olun\n  </Accordion>\n\n  <Accordion title=\"Model Bulunamadı\">\n    - Model adının tam olarak eşleştiğini kontrol edin\n    - [lemondata.cc/en/models](https://lemondata.cc/tr/models) adresinden modelin kullanılabilirliğini doğrulayın\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:36:07.924Z"
  },
  "integrations/cursor.mdx": {
    "sourceHash": "b71cc565ae728239",
    "translations": {
      "zh": "---\ntitle: \"Cursor\"\ndescription: \"在 Cursor AI 代码编辑器中使用 LemonData\"\n---\n\n## 概览\n\n[Cursor](https://cursor.sh) 是一款 AI 驱动的代码编辑器。您可以将 LemonData 作为自定义 API 提供商，以访问 300 多个模型。\n\n## 配置\n\n1. 打开 Cursor 设置 (`Cmd/Ctrl + ,`)\n2. 导航至左侧边栏的 **Models**\n3. 向下滚动到 **OpenAI API Key** 部分\n4. 输入您的 LemonData API 密钥\n5. 启用 **Override OpenAI Base URL**\n6. 将 Base URL 设置为：\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## 模型选择\n\n配置完成后，您可以在 Cursor 中选择任何 LemonData 模型：\n\n### 推荐的编程模型\n\n| 模型 | 最适合 |\n|-------|----------|\n| `claude-opus-4-5` | 复杂编程、架构 |\n| `claude-sonnet-4-5` | 通用编程、调试 |\n| `gpt-4o` | 代码生成、解释 |\n| `deepseek-r1` | 重推理任务 |\n| `gpt-4o-mini` | 快速补全、简单任务 |\n\n## 功能特性\n\n### Chat\n\n配合任何 LemonData 模型使用 AI 聊天面板 (`Cmd/Ctrl + L`)：\n\n- 代码解释\n- Bug 修复\n- 重构建议\n- 文档生成\n\n### Composer\n\nComposer 功能 (`Cmd/Ctrl + I`) 可配合 LemonData 模型用于：\n\n- 多文件编辑\n- 功能实现\n- 代码迁移\n\n### Tab 补全\n\n对于 Tab 补全，我们建议使用快速模型：\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"模型未显示\">\n    Cursor 会缓存模型列表。配置 API 后请尝试重启 Cursor。\n  </Accordion>\n\n  <Accordion title=\"响应缓慢\">\n    检查您的模型选择。对于快速任务，请使用像 `gpt-4o-mini` 这样更快的模型。\n  </Accordion>\n\n  <Accordion title=\"身份验证错误\">\n    请验证您的 API 密钥是否正确且余额充足。\n  </Accordion>\n</AccordionGroup>\n\n## 提示\n\n<CardGroup cols={2}>\n  <Card title=\"使用合适的模型\" icon=\"gauge-high\">\n    使用 `gpt-4o-mini` 进行 Tab 补全，使用较大的模型进行复杂推理。\n  </Card>\n  <Card title=\"检查余额\" icon=\"wallet\">\n    在您的 LemonData 仪表板中监控使用情况。\n  </Card>\n</CardGroup>",
      "zh-TW": "---\ntitle: \"Cursor\"\ndescription: \"在 Cursor AI 程式碼編輯器中使用 LemonData\"\n---\n\n## 概覽\n\n[Cursor](https://cursor.sh) 是一款 AI 驅動的程式碼編輯器。您可以將 LemonData 作為自定義 API 提供商，以存取 300 多種模型。\n\n## 配置\n\n1. 打開 Cursor 設定 (`Cmd/Ctrl + ,`)\n2. 導覽至左側側邊欄的 **Models**\n3. 向下捲動至 **OpenAI API Key** 區塊\n4. 輸入您的 LemonData API key\n5. 啟用 **Override OpenAI Base URL**\n6. 將 Base URL 設定為：\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## 模型選擇\n\n配置完成後，您可以在 Cursor 中選擇任何 LemonData 模型：\n\n### 推薦的程式碼編寫模型\n\n| 模型 | 最適合 |\n|-------|----------|\n| `claude-opus-4-5` | 複雜編碼、架構設計 |\n| `claude-sonnet-4-5` | 一般編碼、除錯 |\n| `gpt-4o` | 程式碼生成、解釋 |\n| `deepseek-r1` | 重推理任務 |\n| `gpt-4o-mini` | 快速補全、簡單任務 |\n\n## 功能\n\n### Chat\n\n配合任何 LemonData 模型使用 AI 對話面板 (`Cmd/Ctrl + L`)：\n\n- 程式碼解釋\n- 修復 Bug\n- 重構建議\n- 生成文件\n\n### Composer\n\nComposer 功能 (`Cmd/Ctrl + I`) 可配合 LemonData 模型用於：\n\n- 多檔案編輯\n- 功能實現\n- 程式碼遷移\n\n### Tab 補全\n\n對於 Tab 補全，我們建議使用快速模型：\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## 疑難排解\n\n<AccordionGroup>\n  <Accordion title=\"模型未顯示\">\n    Cursor 會快取模型列表。請在配置 API 後嘗試重新啟動 Cursor。\n  </Accordion>\n\n  <Accordion title=\"回應緩慢\">\n    檢查您的模型選擇。對於快速任務，請使用如 `gpt-4o-mini` 等較快的模型。\n  </Accordion>\n\n  <Accordion title=\"身分驗證錯誤\">\n    請確認您的 API key 正確且餘額充足。\n  </Accordion>\n</AccordionGroup>\n\n## 提示\n\n<CardGroup cols={2}>\n  <Card title=\"使用合適的模型\" icon=\"gauge-high\">\n    使用 `gpt-4o-mini` 進行 Tab 補全，使用較大的模型進行複雜推理。\n  </Card>\n  <Card title=\"檢查您的餘額\" icon=\"wallet\">\n    在您的 LemonData 控制面板中監控使用情況。\n  </Card>\n</CardGroup>",
      "ja": "---\ntitle: \"Cursor\"\ndescription: \"LemonDataをCursor AIコードエディタで使用する\"\n---\n\n## 概要\n\n[Cursor](https://cursor.sh)は、AIを搭載したコードエディタです。LemonDataをカスタムAPIプロバイダーとして使用することで、300以上のモデルにアクセスできます。\n\n## 設定\n\n1. Cursorの設定を開く (`Cmd/Ctrl + ,`)\n2. 左サイドバーの **Models** に移動する\n3. **OpenAI API Key** セクションまでスクロールする\n4. LemonDataのAPIキーを入力する\n5. **Override OpenAI Base URL** を有効にする\n6. Base URLを以下に設定する:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## モデルの選択\n\n設定後、Cursor内で任意のLemonDataモデルを選択できるようになります:\n\n### コーディングに推奨されるモデル\n\n| モデル | 最適な用途 |\n|-------|----------|\n| `claude-opus-4-5` | 複雑なコーディング、アーキテクチャ |\n| `claude-sonnet-4-5` | 一般的なコーディング、デバッグ |\n| `gpt-4o` | コード生成、解説 |\n| `deepseek-r1` | 高度な推論が必要なタスク |\n| `gpt-4o-mini` | 高速な補完、単純なタスク |\n\n## 機能\n\n### チャット\n\n任意のLemonDataモデルを使用して、AIチャットパネル (`Cmd/Ctrl + L`) を利用できます:\n\n- コードの解説\n- バグ修正\n- リファクタリングの提案\n- ドキュメント生成\n\n### Composer\n\nComposer機能 (`Cmd/Ctrl + I`) は、LemonDataモデルを使用して以下のことが可能です:\n\n- 複数ファイルの編集\n- 機能の実装\n- コードの移行\n\n### タブ補完\n\nタブ補完には、高速なモデルの使用を推奨します:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"モデルが表示されない\">\n    Cursorはモデルリストをキャッシュします。API設定後にCursorを再起動してみてください。\n  </Accordion>\n\n  <Accordion title=\"レスポンスが遅い\">\n    選択しているモデルを確認してください。素早いタスクには `gpt-4o-mini` のような高速なモデルを使用してください。\n  </Accordion>\n\n  <Accordion title=\"認証エラー\">\n    APIキーが正しく、残高が十分であることを確認してください。\n  </Accordion>\n</AccordionGroup>\n\n## ヒント\n\n<CardGroup cols={2}>\n  <Card title=\"適切なモデルの使用\" icon=\"gauge-high\">\n    タブ補完には `gpt-4o-mini` を、複雑な推論にはより大規模なモデルを使用してください。\n  </Card>\n  <Card title=\"残高の確認\" icon=\"wallet\">\n    LemonDataダッシュボードで使用状況を確認してください。\n  </Card>\n</CardGroup>",
      "ko": "---\ntitle: \"Cursor\"\ndescription: \"LemonData를 Cursor AI 코드 에디터와 함께 사용하세요\"\n---\n\n## 개요\n\n[Cursor](https://cursor.sh)는 AI 기반 코드 에디터입니다. LemonData를 커스텀 API 제공업체로 사용하여 300개 이상의 모델을 사용할 수 있습니다.\n\n## 설정\n\n1. Cursor 설정 열기 (`Cmd/Ctrl + ,`)\n2. 왼쪽 사이드바에서 **Models**로 이동\n3. **OpenAI API Key** 섹션으로 스크롤 다운\n4. LemonData API key 입력\n5. **Override OpenAI Base URL** 활성화\n6. Base URL을 다음과 같이 설정:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## 모델 선택\n\n설정 후 Cursor에서 모든 LemonData 모델을 선택할 수 있습니다:\n\n### 코딩 추천 모델\n\n| 모델 | 용도 |\n|-------|----------|\n| `claude-opus-4-5` | 복잡한 코딩, 아키텍처 |\n| `claude-sonnet-4-5` | 일반적인 코딩, 디버깅 |\n| `gpt-4o` | 코드 생성, 설명 |\n| `deepseek-r1` | 추론 중심 작업 |\n| `gpt-4o-mini` | 빠른 완성, 단순 작업 |\n\n## 기능\n\n### Chat\n\n모든 LemonData 모델과 함께 AI 채팅 패널(`Cmd/Ctrl + L`)을 사용하세요:\n\n- 코드 설명\n- 버그 수정\n- 리팩토링 제안\n- 문서 생성\n\n### Composer\n\nComposer 기능(`Cmd/Ctrl + I`)은 LemonData 모델과 함께 다음 작업에 사용할 수 있습니다:\n\n- 다중 파일 편집\n- 기능 구현\n- 코드 마이그레이션\n\n### Tab Completion\n\n탭 완성의 경우, 빠른 모델 사용을 권장합니다:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"모델이 표시되지 않음\">\n    Cursor는 모델 목록을 캐시합니다. API 설정 후 Cursor를 재시작해 보세요.\n  </Accordion>\n\n  <Accordion title=\"응답 속도 느림\">\n    선택한 모델을 확인하세요. 빠른 작업에는 `gpt-4o-mini`와 같은 더 빠른 모델을 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"인증 오류\">\n    API key가 정확하고 잔액이 충분한지 확인하세요.\n  </Accordion>\n</AccordionGroup>\n\n## 팁\n\n<CardGroup cols={2}>\n  <Card title=\"적절한 모델 사용\" icon=\"gauge-high\">\n    탭 완성에는 `gpt-4o-mini`를, 복잡한 추론에는 더 큰 모델을 사용하세요.\n  </Card>\n  <Card title=\"잔액 확인\" icon=\"wallet\">\n    LemonData 대시보드에서 사용량을 모니터링하세요.\n  </Card>\n</CardGroup>",
      "de": "---\ntitle: \"Cursor\"\ndescription: \"Verwenden Sie LemonData mit dem Cursor AI Code-Editor\"\n---\n\n## Übersicht\n\n[Cursor](https://cursor.sh) ist ein KI-gestützter Code-Editor. Sie können LemonData als benutzerdefinierten API-Anbieter verwenden, um auf über 300 Modelle zuzugreifen.\n\n## Konfiguration\n\n1. Öffnen Sie die Cursor-Einstellungen (`Cmd/Ctrl + ,`)\n2. Navigieren Sie zu **Models** in der linken Seitenleiste\n3. Scrollen Sie nach unten zum Abschnitt **OpenAI API Key**\n4. Geben Sie Ihren LemonData API-Key ein\n5. Aktivieren Sie **Override OpenAI Base URL**\n6. Setzen Sie die Base URL auf:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Modellauswahl\n\nNach der Konfiguration können Sie jedes LemonData-Modell in Cursor auswählen:\n\n### Empfohlene Modelle für das Coding\n\n| Modell | Bestens geeignet für |\n|-------|----------|\n| `claude-opus-4-5` | Komplexes Coding, Architektur |\n| `claude-sonnet-4-5` | Allgemeines Coding, Debugging |\n| `gpt-4o` | Code-Generierung, Erklärungen |\n| `deepseek-r1` | Aufgaben mit hohem logischem Anspruch |\n| `gpt-4o-mini` | Schnelle Vervollständigungen, einfache Aufgaben |\n\n## Funktionen\n\n### Chat\n\nVerwenden Sie das KI-Chat-Panel (`Cmd/Ctrl + L`) mit jedem LemonData-Modell:\n\n- Code-Erklärungen\n- Fehlerbehebung\n- Refactoring-Vorschläge\n- Dokumentationserstellung\n\n### Composer\n\nDie Composer-Funktion (`Cmd/Ctrl + I`) funktioniert mit LemonData-Modellen für:\n\n- Bearbeitungen über mehrere Dateien hinweg\n- Feature-Implementierung\n- Code-Migrationen\n\n### Tab-Vervollständigung\n\nFür Tab-Vervollständigungen empfehlen wir die Verwendung schneller Modelle:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Modelle werden nicht angezeigt\">\n    Cursor speichert Modelllisten im Cache. Versuchen Sie, Cursor nach der Konfiguration der API neu zu starten.\n  </Accordion>\n\n  <Accordion title=\"Langsame Antworten\">\n    Überprüfen Sie Ihre Modellauswahl. Verwenden Sie schnellere Modelle wie `gpt-4o-mini` für schnelle Aufgaben.\n  </Accordion>\n\n  <Accordion title=\"Authentifizierungsfehler\">\n    Überprüfen Sie, ob Ihr API-Key korrekt ist und über ausreichendes Guthaben verfügt.\n  </Accordion>\n</AccordionGroup>\n\n## Tipps\n\n<CardGroup cols={2}>\n  <Card title=\"Verwenden Sie geeignete Modelle\" icon=\"gauge-high\">\n    Verwenden Sie `gpt-4o-mini` für Tab-Vervollständigungen und größere Modelle für komplexe Logik.\n  </Card>\n  <Card title=\"Überprüfen Sie Ihr Guthaben\" icon=\"wallet\">\n    Überwachen Sie die Nutzung in Ihrem LemonData-Dashboard.\n  </Card>\n</CardGroup>",
      "fr": "---\ntitle: \"Cursor\"\ndescription: \"Utilisez LemonData avec l'éditeur de code Cursor AI\"\n---\n\n## Aperçu\n\n[Cursor](https://cursor.sh) est un éditeur de code propulsé par l'IA. Vous pouvez utiliser LemonData comme fournisseur d'API personnalisé pour accéder à plus de 300 modèles.\n\n## Configuration\n\n1. Ouvrez les paramètres de Cursor (`Cmd/Ctrl + ,`)\n2. Accédez à **Models** dans la barre latérale gauche\n3. Faites défiler jusqu'à la section **OpenAI API Key**\n4. Saisissez votre clé API LemonData\n5. Activez **Override OpenAI Base URL**\n6. Définissez l'URL de base sur :\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Sélection de modèles\n\nAprès la configuration, vous pouvez sélectionner n'importe quel modèle LemonData dans Cursor :\n\n### Modèles recommandés pour le codage\n\n| Modèle | Idéal pour |\n|-------|----------|\n| `claude-opus-4-5` | Codage complexe, architecture |\n| `claude-sonnet-4-5` | Codage général, débogage |\n| `gpt-4o` | Génération de code, explications |\n| `deepseek-r1` | Tâches nécessitant un raisonnement poussé |\n| `gpt-4o-mini` | Complétions rapides, tâches simples |\n\n## Fonctionnalités\n\n### Chat\n\nUtilisez le panneau de chat IA (`Cmd/Ctrl + L`) avec n'importe quel modèle LemonData :\n\n- Explications de code\n- Correction de bugs\n- Suggestions de refactorisation\n- Génération de documentation\n\n### Composer\n\nLa fonctionnalité Composer (`Cmd/Ctrl + I`) fonctionne avec les modèles LemonData pour :\n\n- Modifications multi-fichiers\n- Implémentation de fonctionnalités\n- Migrations de code\n\n### Complétion par tabulation\n\nPour les complétions par tabulation, nous recommandons d'utiliser des modèles rapides :\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Dépannage\n\n<AccordionGroup>\n  <Accordion title=\"Les modèles ne s'affichent pas\">\n    Cursor met en cache les listes de modèles. Essayez de redémarrer Cursor après avoir configuré l'API.\n  </Accordion>\n\n  <Accordion title=\"Réponses lentes\">\n    Vérifiez votre sélection de modèles. Utilisez des modèles plus rapides comme `gpt-4o-mini` pour les tâches rapides.\n  </Accordion>\n\n  <Accordion title=\"Erreurs d'authentification\">\n    Vérifiez que votre clé API est correcte et que votre solde est suffisant.\n  </Accordion>\n</AccordionGroup>\n\n## Conseils\n\n<CardGroup cols={2}>\n  <Card title=\"Utilisez des modèles appropriés\" icon=\"gauge-high\">\n    Utilisez `gpt-4o-mini` pour les complétions par tabulation, et des modèles plus grands pour le raisonnement complexe.\n  </Card>\n  <Card title=\"Vérifiez votre solde\" icon=\"wallet\">\n    Surveillez votre consommation dans votre tableau de bord LemonData.\n  </Card>\n</CardGroup>",
      "es": "---\ntitle: \"Cursor\"\ndescription: \"Usa LemonData con el editor de código Cursor AI\"\n---\n\n## Resumen\n\n[Cursor](https://cursor.sh) es un editor de código impulsado por IA. Puedes usar LemonData como un proveedor de API personalizado para acceder a más de 300 modelos.\n\n## Configuración\n\n1. Abre los Ajustes de Cursor (`Cmd/Ctrl + ,`)\n2. Navega a **Models** en la barra lateral izquierda\n3. Desplázate hacia abajo hasta la sección **OpenAI API Key**\n4. Introduce tu clave API de LemonData\n5. Activa **Override OpenAI Base URL**\n6. Establece la Base URL a:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Selección de Modelos\n\nDespués de la configuración, puedes seleccionar cualquier modelo de LemonData en Cursor:\n\n### Modelos Recomendados para Programación\n\n| Modelo | Ideal para |\n|-------|----------|\n| `claude-opus-4-5` | Programación compleja, arquitectura |\n| `claude-sonnet-4-5` | Programación general, depuración |\n| `gpt-4o` | Generación de código, explicaciones |\n| `deepseek-r1` | Tareas con alta carga de razonamiento |\n| `gpt-4o-mini` | Completados rápidos, tareas simples |\n\n## Características\n\n### Chat\n\nUsa el panel de chat de IA (`Cmd/Ctrl + L`) con cualquier modelo de LemonData:\n\n- Explicaciones de código\n- Corrección de errores\n- Sugerencias de refactorización\n- Generación de documentación\n\n### Composer\n\nLa función Composer (`Cmd/Ctrl + I`) funciona con los modelos de LemonData para:\n\n- Ediciones de múltiples archivos\n- Implementación de funciones\n- Migraciones de código\n\n### Completado con tabulador\n\nPara los completados con tabulador, recomendamos usar modelos rápidos:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Solución de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Los modelos no aparecen\">\n    Cursor almacena en caché las listas de modelos. Intenta reiniciar Cursor después de configurar la API.\n  </Accordion>\n\n  <Accordion title=\"Respuestas lentas\">\n    Verifica tu selección de modelos. Usa modelos más rápidos como `gpt-4o-mini` para tareas rápidas.\n  </Accordion>\n\n  <Accordion title=\"Errores de autenticación\">\n    Verifica que tu clave API sea correcta y tenga saldo suficiente.\n  </Accordion>\n</AccordionGroup>\n\n## Consejos\n\n<CardGroup cols={2}>\n  <Card title=\"Usa los modelos adecuados\" icon=\"gauge-high\">\n    Usa `gpt-4o-mini` para completados con tabulador y modelos más grandes para razonamiento complejo.\n  </Card>\n  <Card title=\"Consulta tu saldo\" icon=\"wallet\">\n    Monitorea el uso en tu panel de control de LemonData.\n  </Card>\n</CardGroup>",
      "pt": "---\ntitle: \"Cursor\"\ndescription: \"Use o LemonData com o editor de código Cursor AI\"\n---\n\n## Visão Geral\n\nO [Cursor](https://cursor.sh) é um editor de código baseado em IA. Você pode usar o LemonData como um provedor de API personalizado para acessar mais de 300 modelos.\n\n## Configuração\n\n1. Abra as Configurações do Cursor (`Cmd/Ctrl + ,`)\n2. Navegue até **Models** na barra lateral esquerda\n3. Role para baixo até a seção **OpenAI API Key**\n4. Insira sua chave de API do LemonData\n5. Ative **Override OpenAI Base URL**\n6. Defina a Base URL para:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Seleção de Modelos\n\nApós a configuração, você pode selecionar qualquer modelo do LemonData no Cursor:\n\n### Modelos Recomendados para Codificação\n\n| Modelo | Melhor Para |\n|-------|----------|\n| `claude-opus-4-5` | Codificação complexa, arquitetura |\n| `claude-sonnet-4-5` | Codificação geral, depuração |\n| `gpt-4o` | Geração de código, explicações |\n| `deepseek-r1` | Tarefas com uso intensivo de raciocínio |\n| `gpt-4o-mini` | Preenchimentos rápidos, tarefas simples |\n\n## Recursos\n\n### Chat\n\nUse o painel de chat de IA (`Cmd/Ctrl + L`) com qualquer modelo do LemonData:\n\n- Explicações de código\n- Correção de bugs\n- Sugestões de refatoração\n- Geração de documentação\n\n### Composer\n\nO recurso Composer (`Cmd/Ctrl + I`) funciona com modelos do LemonData para:\n\n- Edições em múltiplos arquivos\n- Implementação de recursos\n- Migrações de código\n\n### Tab Completion\n\nPara preenchimentos com tab, recomendamos o uso de modelos rápidos:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Solução de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Modelos não aparecem\">\n    O Cursor armazena listas de modelos em cache. Tente reiniciar o Cursor após configurar a API.\n  </Accordion>\n\n  <Accordion title=\"Respostas lentas\">\n    Verifique sua seleção de modelos. Use modelos mais rápidos como `gpt-4o-mini` para tarefas rápidas.\n  </Accordion>\n\n  <Accordion title=\"Erros de autenticação\">\n    Verifique se sua chave de API está correta e se possui saldo suficiente.\n  </Accordion>\n</AccordionGroup>\n\n## Dicas\n\n<CardGroup cols={2}>\n  <Card title=\"Use modelos apropriados\" icon=\"gauge-high\">\n    Use `gpt-4o-mini` para preenchimentos com tab e modelos maiores para raciocínio complexo.\n  </Card>\n  <Card title=\"Verifique seu saldo\" icon=\"wallet\">\n    Monitore o uso em seu painel do LemonData.\n  </Card>\n</CardGroup>",
      "ar": "---\ntitle: \"Cursor\"\ndescription: \"استخدم LemonData مع محرر الأكواد Cursor AI\"\n---\n\n## نظرة عامة\n\n[Cursor](https://cursor.sh) هو محرر أكواد مدعوم بالذكاء الاصطناعي. يمكنك استخدام LemonData كمزود API مخصص للوصول إلى أكثر من 300 نموذج.\n\n## الإعداد\n\n1. افتح إعدادات Cursor (`Cmd/Ctrl + ,`)\n2. انتقل إلى **Models** في الشريط الجانبي الأيسر\n3. قم بالتمرير لأسفل إلى قسم **OpenAI API Key**\n4. أدخل مفتاح API الخاص بـ LemonData\n5. قم بتفعيل **Override OpenAI Base URL**\n6. قم بتعيين Base URL إلى:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## اختيار النموذج\n\nبعد الإعداد، يمكنك اختيار أي نموذج من LemonData في Cursor:\n\n### النماذج الموصى بها للبرمجة\n\n| النموذج | الأفضل لـ |\n|-------|----------|\n| `claude-opus-4-5` | البرمجة المعقدة، الهندسة المعمارية |\n| `claude-sonnet-4-5` | البرمجة العامة، تصحيح الأخطاء |\n| `gpt-4o` | توليد الأكواد، الشروحات |\n| `deepseek-r1` | المهام التي تتطلب تفكيراً منطقياً مكثفاً |\n| `gpt-4o-mini` | الإكمال السريع، المهام البسيطة |\n\n## الميزات\n\n### الدردشة (Chat)\n\nاستخدم لوحة دردشة الذكاء الاصطناعي (`Cmd/Ctrl + L`) مع أي نموذج من LemonData:\n\n- شروحات الأكواد\n- إصلاح الأخطاء\n- اقتراحات إعادة هيكلة الكود (Refactoring)\n- توليد التوثيق\n\n### الملحن (Composer)\n\nتعمل ميزة Composer (`Cmd/Ctrl + I`) مع نماذج LemonData من أجل:\n\n- تعديلات على ملفات متعددة\n- تنفيذ الميزات\n- ترحيل الأكواد (Code migrations)\n\n### الإكمال التلقائي (Tab Completion)\n\nبالنسبة للإكمال التلقائي، نوصي باستخدام النماذج السريعة:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## استكشاف الأخطاء وإصلاحها\n\n<AccordionGroup>\n  <Accordion title=\"النماذج لا تظهر\">\n    يقوم Cursor بتخزين قوائم النماذج مؤقتاً. حاول إعادة تشغيل Cursor بعد إعداد API.\n  </Accordion>\n\n  <Accordion title=\"استجابات بطيئة\">\n    تحقق من اختيارك للنموذج. استخدم نماذج أسرع مثل `gpt-4o-mini` للمهام السريعة.\n  </Accordion>\n\n  <Accordion title=\"أخطاء في المصادقة\">\n    تحقق من أن مفتاح API الخاص بك صحيح وأن لديك رصيداً كافياً.\n  </Accordion>\n</AccordionGroup>\n\n## نصائح\n\n<CardGroup cols={2}>\n  <Card title=\"استخدم النماذج المناسبة\" icon=\"gauge-high\">\n    استخدم `gpt-4o-mini` للإكمال التلقائي، والنماذج الأكبر للتفكير المنطقي المعقد.\n  </Card>\n  <Card title=\"تحقق من رصيدك\" icon=\"wallet\">\n    راقب الاستخدام في لوحة تحكم LemonData الخاصة بك.\n  </Card>\n</CardGroup>",
      "vi": "---\ntitle: \"Cursor\"\ndescription: \"Sử dụng LemonData với trình soạn thảo mã nguồn AI Cursor\"\n---\n\n## Tổng quan\n\n[Cursor](https://cursor.sh) là một trình soạn thảo mã nguồn được hỗ trợ bởi AI. Bạn có thể sử dụng LemonData như một nhà cung cấp API tùy chỉnh để truy cập hơn 300 mô hình.\n\n## Cấu hình\n\n1. Mở Cài đặt Cursor (`Cmd/Ctrl + ,`)\n2. Đi đến mục **Models** ở thanh bên trái\n3. Cuộn xuống phần **OpenAI API Key**\n4. Nhập API key của LemonData\n5. Bật tùy chọn **Override OpenAI Base URL**\n6. Thiết lập Base URL thành:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Lựa chọn mô hình\n\nSau khi cấu hình, bạn có thể chọn bất kỳ mô hình LemonData nào trong Cursor:\n\n### Các mô hình được đề xuất cho lập trình\n\n| Mô hình | Tốt nhất cho |\n|-------|----------|\n| `claude-opus-4-5` | Lập trình phức tạp, kiến trúc |\n| `claude-sonnet-4-5` | Lập trình tổng quát, gỡ lỗi |\n| `gpt-4o` | Tạo mã, giải thích mã |\n| `deepseek-r1` | Các tác vụ yêu cầu suy luận cao |\n| `gpt-4o-mini` | Hoàn thành nhanh, các tác vụ đơn giản |\n\n## Tính năng\n\n### Chat\n\nSử dụng bảng chat AI (`Cmd/Ctrl + L`) với bất kỳ mô hình LemonData nào:\n\n- Giải thích mã\n- Sửa lỗi\n- Gợi ý tái cấu trúc mã\n- Tạo tài liệu\n\n### Composer\n\nTính năng Composer (`Cmd/Ctrl + I`) hoạt động với các mô hình LemonData cho:\n\n- Chỉnh sửa nhiều tệp\n- Triển khai tính năng\n- Di chuyển mã nguồn\n\n### Tab Completion\n\nĐối với tính năng tự động hoàn thành (tab completion), chúng tôi khuyên dùng các mô hình tốc độ nhanh:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Khắc phục sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Mô hình không hiển thị\">\n    Cursor lưu bộ nhớ đệm danh sách mô hình. Hãy thử khởi động lại Cursor sau khi cấu hình API.\n  </Accordion>\n\n  <Accordion title=\"Phản hồi chậm\">\n    Kiểm tra lựa chọn mô hình của bạn. Sử dụng các mô hình nhanh hơn như `gpt-4o-mini` cho các tác vụ nhanh.\n  </Accordion>\n\n  <Accordion title=\"Lỗi xác thực\">\n    Xác minh API key của bạn là chính xác và có đủ số dư.\n  </Accordion>\n</AccordionGroup>\n\n## Mẹo\n\n<CardGroup cols={2}>\n  <Card title=\"Sử dụng mô hình phù hợp\" icon=\"gauge-high\">\n    Sử dụng `gpt-4o-mini` cho tự động hoàn thành, các mô hình lớn hơn cho suy luận phức tạp.\n  </Card>\n  <Card title=\"Kiểm tra số dư\" icon=\"wallet\">\n    Theo dõi mức độ sử dụng trong bảng điều khiển LemonData của bạn.\n  </Card>\n</CardGroup>",
      "id": "---\ntitle: \"Cursor\"\ndescription: \"Gunakan LemonData dengan editor kode AI Cursor\"\n---\n\n## Ringkasan\n\n[Cursor](https://cursor.sh) adalah editor kode bertenaga AI. Anda dapat menggunakan LemonData sebagai penyedia API kustom untuk mengakses 300+ model.\n\n## Konfigurasi\n\n1. Buka Pengaturan Cursor (`Cmd/Ctrl + ,`)\n2. Navigasikan ke **Models** di bilah sisi kiri\n3. Gulir ke bawah ke bagian **OpenAI API Key**\n4. Masukkan kunci API LemonData Anda\n5. Aktifkan **Override OpenAI Base URL**\n6. Atur Base URL ke:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Pemilihan Model\n\nSetelah konfigurasi, Anda dapat memilih model LemonData apa pun di Cursor:\n\n### Model yang Direkomendasikan untuk Coding\n\n| Model | Terbaik Untuk |\n|-------|----------|\n| `claude-opus-4-5` | Coding kompleks, arsitektur |\n| `claude-sonnet-4-5` | Coding umum, debugging |\n| `gpt-4o` | Pembuatan kode, penjelasan |\n| `deepseek-r1` | Tugas berat penalaran |\n| `gpt-4o-mini` | Penyelesaian cepat, tugas sederhana |\n\n## Fitur\n\n### Chat\n\nGunakan panel chat AI (`Cmd/Ctrl + L`) dengan model LemonData apa pun:\n\n- Penjelasan kode\n- Perbaikan bug\n- Saran refactoring\n- Pembuatan dokumentasi\n\n### Composer\n\nFitur Composer (`Cmd/Ctrl + I`) berfungsi dengan model LemonData untuk:\n\n- Pengeditan multi-file\n- Implementasi fitur\n- Migrasi kode\n\n### Tab Completion\n\nUntuk tab completion, kami merekomendasikan penggunaan model yang cepat:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Pemecahan Masalah\n\n<AccordionGroup>\n  <Accordion title=\"Model tidak muncul\">\n    Cursor menyimpan cache daftar model. Coba mulai ulang Cursor setelah mengonfigurasi API.\n  </Accordion>\n\n  <Accordion title=\"Respons lambat\">\n    Periksa pemilihan model Anda. Gunakan model yang lebih cepat seperti `gpt-4o-mini` untuk tugas-tugas cepat.\n  </Accordion>\n\n  <Accordion title=\"Kesalahan autentikasi\">\n    Verifikasi bahwa kunci API Anda benar dan memiliki saldo yang cukup.\n  </Accordion>\n</AccordionGroup>\n\n## Tips\n\n<CardGroup cols={2}>\n  <Card title=\"Gunakan model yang sesuai\" icon=\"gauge-high\">\n    Gunakan `gpt-4o-mini` untuk tab completion, model yang lebih besar untuk penalaran kompleks.\n  </Card>\n  <Card title=\"Periksa saldo Anda\" icon=\"wallet\">\n    Pantau penggunaan di dasbor LemonData Anda.\n  </Card>\n</CardGroup>",
      "tr": "---\ntitle: \"Cursor\"\ndescription: \"LemonData'yı Cursor AI kod düzenleyicisi ile kullanın\"\n---\n\n## Genel Bakış\n\n[Cursor](https://cursor.sh) yapay zeka destekli bir kod düzenleyicidir. 300'den fazla modele erişmek için LemonData'yı özel bir API sağlayıcısı olarak kullanabilirsiniz.\n\n## Yapılandırma\n\n1. Cursor Ayarlarını açın (`Cmd/Ctrl + ,`)\n2. Sol kenar çubuğundaki **Models** bölümüne gidin\n3. Aşağıdaki **OpenAI API Key** bölümüne kaydırın\n4. LemonData API anahtarınızı girin\n5. **Override OpenAI Base URL** seçeneğini etkinleştirin\n6. Base URL değerini şu şekilde ayarlayın:\n\n```\nhttps://api.lemondata.cc/v1\n```\n\n## Model Seçimi\n\nYapılandırmadan sonra, Cursor içinde herhangi bir LemonData modelini seçebilirsiniz:\n\n### Kodlama İçin Önerilen Modeller\n\n| Model | En İyi Kullanım Alanı |\n|-------|----------|\n| `claude-opus-4-5` | Karmaşık kodlama, mimari |\n| `claude-sonnet-4-5` | Genel kodlama, hata ayıklama |\n| `gpt-4o` | Kod oluşturma, açıklamalar |\n| `deepseek-r1` | Yoğun muhakeme gerektiren görevler |\n| `gpt-4o-mini` | Hızlı tamamlamalar, basit görevler |\n\n## Özellikler\n\n### Chat\n\nHerhangi bir LemonData modeli ile yapay zeka sohbet panelini (`Cmd/Ctrl + L`) kullanın:\n\n- Kod açıklamaları\n- Hata düzeltme\n- Yeniden yapılandırma (refactoring) önerileri\n- Dokümantasyon oluşturma\n\n### Composer\n\nComposer özelliği (`Cmd/Ctrl + I`), şu işlemler için LemonData modelleriyle çalışır:\n\n- Çoklu dosya düzenlemeleri\n- Özellik uygulama\n- Kod taşıma (migration)\n\n### Tab Completion\n\nSekme tamamlamaları için hızlı modeller kullanmanızı öneririz:\n\n- `gpt-4o-mini`\n- `claude-haiku-4-5`\n- `gemini-2.5-flash`\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Modeller görünmüyor\">\n    Cursor model listelerini önbelleğe alır. API'yi yapılandırdıktan sonra Cursor'ı yeniden başlatmayı deneyin.\n  </Accordion>\n\n  <Accordion title=\"Yavaş yanıtlar\">\n    Model seçiminizi kontrol edin. Hızlı görevler için `gpt-4o-mini` gibi daha hızlı modeller kullanın.\n  </Accordion>\n\n  <Accordion title=\"Kimlik doğrulama hataları\">\n    API anahtarınızın doğru olduğunu ve yeterli bakiyeniz olduğunu doğrulayın.\n  </Accordion>\n</AccordionGroup>\n\n## İpuçları\n\n<CardGroup cols={2}>\n  <Card title=\"Uygun modelleri kullanın\" icon=\"gauge-high\">\n    Sekme tamamlamaları için `gpt-4o-mini`, karmaşık muhakeme için daha büyük modeller kullanın.\n  </Card>\n  <Card title=\"Bakiyenizi kontrol edin\" icon=\"wallet\">\n    Kullanımı LemonData panelinizden takip edin.\n  </Card>\n</CardGroup>"
    },
    "updatedAt": "2026-01-26T05:36:24.607Z"
  },
  "integrations/dify.mdx": {
    "sourceHash": "7a35104bd7a07803",
    "translations": {
      "zh": "---\ntitle: \"Dify\"\ndescription: \"将 LemonData 与 Dify 集成以构建 LLM 应用程序\"\n---\n\n## 概览\n\nDify 是一个开源的 LLM 应用程序开发平台。它提供可视化提示词编排、RAG 管道、Agent 框架和 LLMOps 能力。LemonData 可以配置为 Dify 中的自定义模型供应商。\n\n## 优势\n\n- 通过一个接口访问 300 多个 AI 模型\n- 在不更改应用程序逻辑的情况下切换模型\n- 通过为每个任务选择最佳模型来优化成本\n- 统一的账单和用量追踪\n\n## 前提条件\n\n- 具有 API 访问权限的 LemonData 账户\n- Dify 安装（云端或私有化部署）\n\n## 配置步骤\n\n### 步骤 1：获取您的 API Key\n\n1. 登录 [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. 导航至 [API Keys](https://lemondata.cc/dashboard/api)\n3. 创建并复制您的 API Key（格式：`sk-...`）\n\n### 步骤 2：添加自定义模型供应商\n\n<Steps>\n  <Step title=\"打开设置\">\n    在 Dify 中，前往 **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"添加 OpenAI 兼容供应商\">\n    点击 **Add Model Provider** 并选择 **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"配置供应商\">\n    输入以下设置：\n\n    | 字段 | 值 |\n    |-------|-------|\n    | 供应商名称 | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"添加模型\">\n    添加您想要使用的模型：\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### 步骤 3：测试连接\n\n1. 选择一个模型（例如 `gpt-4o-mini`）\n2. 发送一条测试消息\n3. 确认您收到了回复\n\n## 在应用程序中使用\n\n### 聊天机器人\n\n1. 创建一个新的 Chatbot 应用程序\n2. 选择 LemonData 作为模型供应商\n3. 选择您偏好的模型\n4. 配置系统提示词和参数\n\n### Agent\n\n1. 创建一个 Agent 应用程序\n2. 选择一个能力强的模型（GPT-4o, Claude）\n3. 添加工具和知识库\n4. 配置 Agent 行为\n\n### 工作流\n\n1. 创建一个 Workflow\n2. 添加 LLM 节点\n3. 为每个节点选择 LemonData 模型\n4. 连接节点并配置数据流\n\n## 可用模型\n\n| 类别 | 模型 |\n|----------|--------|\n| 对话 | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| 嵌入 | text-embedding-3-small, text-embedding-3-large |\n| 视觉 | GPT-4o (with images), Claude Sonnet (with images) |\n\n## RAG 配置\n\n对于 RAG 应用程序，请配置嵌入模型：\n\n1. 前往 **Settings** → **Model Provider**\n2. 添加嵌入模型：`text-embedding-3-small`\n3. 在知识库设置中将其设为默认嵌入模型\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"从经济型模型开始\">\n    在测试和开发阶段使用 GPT-4o-mini，在生产环境切换到更强大的模型。\n  </Accordion>\n\n  <Accordion title=\"使用模型路由\">\n    为不同的任务配置不同的模型 —— 简单查询使用快速模型，复杂推理使用强大模型。\n  </Accordion>\n\n  <Accordion title=\"监控用量\">\n    结合使用 Dify 的 LLMOps 功能和 LemonData 控制台来追踪成本和性能。\n  </Accordion>\n</AccordionGroup>\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"连接错误\">\n    - 确认 API Base URL 准确为 `https://api.lemondata.cc/v1`\n    - 检查末尾是否有斜杠\n    - 验证网络连接性\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - 仔细检查 API Key\n    - 确保该 Key 在 LemonData 控制台中处于激活状态\n  </Accordion>\n\n  <Accordion title=\"未找到模型\">\n    - 确认模型名称完全匹配\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh/models) 查看模型可用性\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"Dify\"\ndescription: \"將 LemonData 與 Dify 整合以構建 LLM 應用程式\"\n---\n\n## 概覽\n\nDify 是一個開源的 LLM 應用程式開發平台。它提供視覺化的 Prompt 編排、RAG 流水線、Agent 框架以及 LLMOps 能力。LemonData 可以被配置為 Dify 中的自定義模型供應商。\n\n## 優勢\n\n- 透過單一介面存取 300+ AI 模型\n- 無需更改應用程式邏輯即可切換模型\n- 為每項任務選擇最佳模型以優化成本\n- 統一的計費與用量追蹤\n\n## 先決條件\n\n- 具備 API 存取權限的 LemonData 帳戶\n- Dify 安裝（雲端版或私有化部署）\n\n## 配置步驟\n\n### 步驟 1：獲取您的 API Key\n\n1. 登入 [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. 導航至 [API Keys](https://lemondata.cc/dashboard/api)\n3. 建立並複製您的 API Key（格式：`sk-...`）\n\n### 步驟 2：新增自定義模型供應商\n\n<Steps>\n  <Step title=\"開啟設定\">\n    在 Dify 中，前往 **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"新增 OpenAI 相容供應商\">\n    點擊 **Add Model Provider** 並選擇 **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"配置供應商\">\n    輸入以下設定：\n\n    | 欄位 | 值 |\n    |-------|-------|\n    | 供應商名稱 | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"新增模型\">\n    新增您想要使用的模型：\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### 步驟 3：測試連線\n\n1. 選擇一個模型（例如 `gpt-4o-mini`）\n2. 發送一條測試訊息\n3. 確認您收到回覆\n\n## 在應用程式中使用\n\n### 聊天機器人 (Chatbot)\n\n1. 建立一個新的 Chatbot 應用程式\n2. 選擇 LemonData 作為模型供應商\n3. 選擇您偏好的模型\n4. 配置系統提示詞 (System Prompt) 與參數\n\n### 代理 (Agent)\n\n1. 建立一個 Agent 應用程式\n2. 選擇一個能力強大的模型 (GPT-4o, Claude)\n3. 新增工具與知識庫\n4. 配置 Agent 行為\n\n### 工作流 (Workflow)\n\n1. 建立一個工作流\n2. 新增 LLM 節點\n3. 為每個節點選擇 LemonData 模型\n4. 連接節點並配置數據流\n\n## 可用模型\n\n| 類別 | 模型 |\n|----------|--------|\n| 對話 (Chat) | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| 嵌入 (Embeddings) | text-embedding-3-small, text-embedding-3-large |\n| 視覺 (Vision) | GPT-4o (支援圖片), Claude Sonnet (支援圖片) |\n\n## RAG 配置\n\n對於 RAG 應用程式，請配置嵌入模型：\n\n1. 前往 **Settings** → **Model Provider**\n2. 新增嵌入模型：`text-embedding-3-small`\n3. 在知識庫設定中將其設為預設嵌入模型\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"從經濟型模型開始\">\n    在測試和開發階段使用 GPT-4o-mini，在正式環境切換到更強大的模型。\n  </Accordion>\n\n  <Accordion title=\"使用模型路由\">\n    為不同任務配置不同模型 — 簡單查詢使用快速模型，複雜推理使用強大模型。\n  </Accordion>\n\n  <Accordion title=\"監控用量\">\n    結合使用 Dify 的 LLMOps 功能與 LemonData 控制台來追蹤成本與效能。\n  </Accordion>\n</AccordionGroup>\n\n## 疑難排解\n\n<AccordionGroup>\n  <Accordion title=\"連線錯誤\">\n    - 確認 API Base URL 準確為 `https://api.lemondata.cc/v1`\n    - 檢查結尾是否有斜線\n    - 檢查網路連線\n  </Accordion>\n\n  <Accordion title=\"401 未授權 (Unauthorized)\">\n    - 再次檢查 API Key\n    - 確保該 Key 在 LemonData 控制台中處於啟用狀態\n  </Accordion>\n\n  <Accordion title=\"找不到模型\">\n    - 確認模型名稱完全匹配\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh-TW/models) 檢查模型可用性\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"Dify\"\ndescription: \"LLMアプリケーション構築のためにLemonDataをDifyと統合する\"\n---\n\n## 概要\n\nDifyは、オープンソースのLLMアプリケーション開発プラットフォームです。ビジュアルなプロンプトオーケストレーション、RAGパイプライン、エージェントフレームワーク、およびLLMOps機能を提供します。LemonDataは、Difyのカスタムモデルプロバイダーとして設定できます。\n\n## メリット\n\n- 1つのインターフェースで300以上のAIモデルにアクセス\n- アプリケーションロジックを変更せずにモデルを切り替え\n- 各タスクに最適なモデルを選択することでコストを最適化\n- 一元化された請求と使用状況の追跡\n\n## 前提条件\n\n- APIアクセス権を持つLemonDataアカウント\n- Difyのインストール（クラウドまたはセルフホスト）\n\n## 設定手順\n\n### ステップ 1: APIキーの取得\n\n1. [LemonData Dashboard](https://lemondata.cc/dashboard)にログインします\n2. [API Keys](https://lemondata.cc/dashboard/api)に移動します\n3. APIキーを作成してコピーします（形式: `sk-...`）\n\n### ステップ 2: カスタムモデルプロバイダーの追加\n\n<Steps>\n  <Step title=\"設定を開く\">\n    Difyで、**Settings** → **Model Provider**に移動します\n  </Step>\n  <Step title=\"OpenAI互換プロバイダーの追加\">\n    **Add Model Provider**をクリックし、**OpenAI-API-compatible**を選択します\n  </Step>\n  <Step title=\"プロバイダーの設定\">\n    以下の設定を入力します：\n\n    | フィールド | 値 |\n    |-------|-------|\n    | プロバイダー名 | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"モデルの追加\">\n    使用したいモデルを追加します：\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### ステップ 3: 接続テスト\n\n1. モデルを選択します（例: `gpt-4o-mini`）\n2. テストメッセージを送信します\n3. レスポンスを受信することを確認します\n\n## アプリケーションでの使用\n\n### チャットボット\n\n1. 新しいチャットボットアプリケーションを作成します\n2. モデルプロバイダーとしてLemonDataを選択します\n3. 好みのモデルを選択します\n4. システムプロンプトとパラメータを設定します\n\n### エージェント\n\n1. 新しいエージェントアプリケーションを作成します\n2. 能力の高いモデル（GPT-4o、Claude）を選択します\n3. ツールとナレッジベースを追加します\n4. エージェントの動作を設定します\n\n### ワークフロー\n\n1. ワークフローを作成します\n2. LLMノードを追加します\n3. 各ノードにLemonDataモデルを選択します\n4. ノードを接続し、データフローを設定します\n\n## 利用可能なモデル\n\n| カテゴリ | モデル |\n|----------|--------|\n| チャット | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| 埋め込み | text-embedding-3-small, text-embedding-3-large |\n| ビジョン | GPT-4o (画像対応), Claude Sonnet (画像対応) |\n\n## RAGの設定\n\nRAGアプリケーションの場合は、埋め込みを設定します：\n\n1. **Settings** → **Model Provider**に移動します\n2. 埋め込みモデルを追加します: `text-embedding-3-small`\n3. ナレッジベースの設定でデフォルトの埋め込みモデルとして設定します\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"経済的なモデルから始める\">\n    テストと開発にはGPT-4o-miniを使用し、本番環境ではより強力なモデルに切り替えます。\n  </Accordion>\n\n  <Accordion title=\"モデルルーティングの使用\">\n    タスクごとに異なるモデルを設定します。単純なクエリには高速なモデルを、複雑な推論には強力なモデルを使用します。\n  </Accordion>\n\n  <Accordion title=\"使用状況の監視\">\n    DifyのLLMOps機能とLemonDataダッシュボードを併用して、コストとパフォーマンスを追跡します。\n  </Accordion>\n</AccordionGroup>\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"接続エラー\">\n    - API Base URLが正確に `https://api.lemondata.cc/v1` であることを確認してください\n    - 末尾のスラッシュを確認してください\n    - ネットワーク接続を確認してください\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized（認証エラー）\">\n    - APIキーを再確認してください\n    - LemonDataダッシュボードでキーが有効であることを確認してください\n  </Accordion>\n\n  <Accordion title=\"モデルが見つかりません\">\n    - モデル名が正確に一致しているか確認してください\n    - [lemondata.cc/en/models](https://lemondata.cc/ja/models) でモデルの利用可能性を確認してください\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"Dify\"\ndescription: \"LLM 애플리케이션 구축을 위해 LemonData를 Dify와 통합하세요\"\n---\n\n## 개요\n\nDify는 오픈 소스 LLM 애플리케이션 개발 플랫폼입니다. 시각적 프롬프트 오케스트레이션, RAG 파이프라인, 에이전트 프레임워크 및 LLMOps 기능을 제공합니다. LemonData는 Dify에서 커스텀 모델 제공업체로 설정할 수 있습니다.\n\n## 주요 이점\n\n- 하나의 인터페이스를 통해 300개 이상의 AI 모델에 액세스\n- 애플리케이션 로직을 변경하지 않고 모델 간 전환\n- 각 작업에 가장 적합한 모델을 선택하여 비용 최적화\n- 통합된 빌링 및 사용량 추적\n\n## 사전 요구 사항\n\n- API 액세스 권한이 있는 LemonData 계정\n- Dify 설치 (클라우드 또는 셀프 호스팅)\n\n## 설정 단계\n\n### 1단계: API Key 받기\n\n1. [LemonData Dashboard](https://lemondata.cc/dashboard)에 로그인합니다\n2. [API Keys](https://lemondata.cc/dashboard/api)로 이동합니다\n3. API Key를 생성하고 복사합니다 (형식: `sk-...`)\n\n### 2단계: 커스텀 모델 제공업체 추가\n\n<Steps>\n  <Step title=\"설정 열기\">\n    Dify에서 **Settings** → **Model Provider**로 이동합니다\n  </Step>\n  <Step title=\"OpenAI 호환 제공업체 추가\">\n    **Add Model Provider**를 클릭하고 **OpenAI-API-compatible**을 선택합니다\n  </Step>\n  <Step title=\"제공업체 설정\">\n    다음 설정을 입력합니다:\n\n    | 필드 | 값 |\n    |-------|-------|\n    | 제공업체 이름 | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"모델 추가\">\n    사용하려는 모델을 추가합니다:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### 3단계: 연결 테스트\n\n1. 모델을 선택합니다 (예: `gpt-4o-mini`)\n2. 테스트 메시지를 보냅니다\n3. 응답을 받는지 확인합니다\n\n## 애플리케이션에서 사용하기\n\n### 챗봇\n\n1. 새로운 챗봇 애플리케이션을 생성합니다\n2. 모델 제공업체로 LemonData를 선택합니다\n3. 선호하는 모델을 선택합니다\n4. 시스템 프롬프트 및 파라미터를 설정합니다\n\n### 에이전트\n\n1. 에이전트 애플리케이션을 생성합니다\n2. 성능이 뛰어난 모델(GPT-4o, Claude)을 선택합니다\n3. 도구 및 지식 베이스를 추가합니다\n4. 에이전트 동작을 설정합니다\n\n### 워크플로우\n\n1. 워크플로우를 생성합니다\n2. LLM 노드를 추가합니다\n3. 각 노드에 대해 LemonData 모델을 선택합니다\n4. 노드를 연결하고 데이터 흐름을 설정합니다\n\n## 사용 가능한 모델\n\n| 카테고리 | 모델 |\n|----------|--------|\n| 채팅 | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| 임베딩 | text-embedding-3-small, text-embedding-3-large |\n| 비전 | GPT-4o (이미지 포함), Claude Sonnet (이미지 포함) |\n\n## RAG 설정\n\nRAG 애플리케이션의 경우 임베딩을 설정하세요:\n\n1. **Settings** → **Model Provider**로 이동합니다\n2. 임베딩 모델 추가: `text-embedding-3-small`\n3. 지식 베이스 설정에서 기본 임베딩 모델로 설정합니다\n\n## 모범 사례\n\n<AccordionGroup>\n  <Accordion title=\"경제적인 모델로 시작하기\">\n    테스트 및 개발에는 GPT-4o-mini를 사용하고, 프로덕션 환경에서는 더 강력한 모델로 전환하세요.\n  </Accordion>\n\n  <Accordion title=\"모델 라우팅 사용\">\n    단순한 쿼리에는 빠른 모델을, 복잡한 추론에는 강력한 모델을 사용하는 등 작업에 따라 다른 모델을 구성하세요.\n  </Accordion>\n\n  <Accordion title=\"사용량 모니터링\">\n    Dify의 LLMOps 기능과 LemonData 대시보드를 함께 사용하여 비용과 성능을 추적하세요.\n  </Accordion>\n</AccordionGroup>\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"연결 오류\">\n    - API Base URL이 정확히 `https://api.lemondata.cc/v1`인지 확인하세요\n    - 끝에 슬래시(/)가 있는지 확인하세요\n    - 네트워크 연결 상태를 확인하세요\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized (권한 없음)\">\n    - API Key를 다시 확인하세요\n    - LemonData 대시보드에서 키가 활성 상태인지 확인하세요\n  </Accordion>\n\n  <Accordion title=\"모델을 찾을 수 없음\">\n    - 모델 이름이 정확히 일치하는지 확인하세요\n    - [lemondata.cc/en/models](https://lemondata.cc/ko/models)에서 모델 가용성을 확인하세요\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Dify\"\ndescription: \"Integrieren Sie LemonData mit Dify zur Erstellung von LLM-Anwendungen\"\n---\n\n## Übersicht\n\nDify ist eine Open-Source-Plattform zur Entwicklung von LLM-Anwendungen. Sie bietet visuelle Prompt-Orchestrierung, RAG-Pipelines, Agent-Frameworks und LLMOps-Funktionen. LemonData kann als benutzerdefinierter Modellanbieter in Dify konfiguriert werden.\n\n## Vorteile\n\n- Zugriff auf über 300 KI-Modelle über eine einzige Schnittstelle\n- Wechseln zwischen Modellen ohne Änderung der Anwendungslogik\n- Kostenoptimierung durch Auswahl des besten Modells für jede Aufgabe\n- Einheitliche Abrechnung und Nutzungsverfolgung\n\n## Voraussetzungen\n\n- LemonData-Konto mit API-Zugriff\n- Dify-Installation (Cloud oder selbst gehostet)\n\n## Konfigurationsschritte\n\n### Schritt 1: API-Key abrufen\n\n1. Melden Sie sich im [LemonData Dashboard](https://lemondata.cc/dashboard) an\n2. Navigieren Sie zu [API Keys](https://lemondata.cc/dashboard/api)\n3. Erstellen und kopieren Sie Ihren API-Key (Format: `sk-...`)\n\n### Schritt 2: Benutzerdefinierten Modellanbieter hinzufügen\n\n<Steps>\n  <Step title=\"Einstellungen öffnen\">\n    Gehen Sie in Dify zu **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"OpenAI-kompatiblen Anbieter hinzufügen\">\n    Klicken Sie auf **Add Model Provider** und wählen Sie **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"Anbieter konfigurieren\">\n    Geben Sie die folgenden Einstellungen ein:\n\n    | Feld | Wert |\n    |-------|-------|\n    | Anbietername | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Modelle hinzufügen\">\n    Fügen Sie die Modelle hinzu, die Sie verwenden möchten:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Schritt 3: Verbindung testen\n\n1. Wählen Sie ein Modell aus (z. B. `gpt-4o-mini`)\n2. Senden Sie eine Testnachricht\n3. Überprüfen Sie, ob Sie eine Antwort erhalten\n\n## Verwendung in Anwendungen\n\n### Chatbot\n\n1. Erstellen Sie eine neue Chatbot-Anwendung\n2. Wählen Sie LemonData als Modellanbieter aus\n3. Wählen Sie Ihr bevorzugtes Modell\n4. Konfigurieren Sie System-Prompt und Parameter\n\n### Agent\n\n1. Erstellen Sie eine Agent-Anwendung\n2. Wählen Sie ein leistungsfähiges Modell (GPT-4o, Claude)\n3. Fügen Sie Tools und Wissensdatenbanken hinzu\n4. Konfigurieren Sie das Verhalten des Agents\n\n### Workflow\n\n1. Erstellen Sie einen Workflow\n2. Fügen Sie LLM-Knoten hinzu\n3. Wählen Sie LemonData-Modelle für jeden Knoten aus\n4. Verbinden Sie die Knoten und konfigurieren Sie den Datenfluss\n\n## Verfügbare Modelle\n\n| Kategorie | Modelle |\n|----------|--------|\n| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embeddings | `text-embedding-3-small`, `text-embedding-3-large` |\n| Vision | GPT-4o (mit Bildern), Claude Sonnet (mit Bildern) |\n\n## RAG-Konfiguration\n\nKonfigurieren Sie für RAG-Anwendungen die Embeddings:\n\n1. Gehen Sie zu **Settings** → **Model Provider**\n2. Fügen Sie das Embedding-Modell hinzu: `text-embedding-3-small`\n3. Als Standard-Embedding-Modell in den Einstellungen der Wissensdatenbank festlegen\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Mit kostengünstigen Modellen beginnen\">\n    Verwenden Sie GPT-4o-mini für Tests und Entwicklung, wechseln Sie für die Produktion zu leistungsstärkeren Modellen.\n  </Accordion>\n\n  <Accordion title=\"Modell-Routing verwenden\">\n    Konfigurieren Sie verschiedene Modelle für unterschiedliche Aufgaben – schnelle Modelle für einfache Abfragen, leistungsstarke Modelle für komplexe logische Schlussfolgerungen.\n  </Accordion>\n\n  <Accordion title=\"Nutzung überwachen\">\n    Nutzen Sie die LLMOps-Funktionen von Dify zusammen mit dem LemonData-Dashboard, um Kosten und Leistung zu verfolgen.\n  </Accordion>\n</AccordionGroup>\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Verbindungsfehler\">\n    - Überprüfen Sie, ob die API Base URL exakt `https://api.lemondata.cc/v1` entspricht\n    - Prüfen Sie auf abschließende Schrägstriche\n    - Überprüfen Sie die Netzwerkkonnektivität\n  </Accordion>\n\n  <Accordion title=\"401 Nicht autorisiert\">\n    - Überprüfen Sie den API-Key erneut\n    - Stellen Sie sicher, dass der Key im LemonData-Dashboard aktiv ist\n  </Accordion>\n\n  <Accordion title=\"Modell nicht gefunden\">\n    - Überprüfen Sie, ob der Modellname exakt übereinstimmt\n    - Prüfen Sie die Modellverfügbarkeit unter [lemondata.cc/en/models](https://lemondata.cc/de/models)\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"Dify\"\ndescription: \"Intégrez LemonData avec Dify pour créer des applications LLM\"\n---\n\n## Présentation\n\nDify est une plateforme de développement d'applications LLM open-source. Elle propose l'orchestration visuelle de prompts, des pipelines RAG, des frameworks d'agents et des fonctionnalités LLMOps. LemonData peut être configuré comme un fournisseur de modèles personnalisé dans Dify.\n\n## Avantages\n\n- Accédez à plus de 300 modèles d'IA via une interface unique\n- Basculez entre les modèles sans modifier la logique de l'application\n- Optimisez les coûts en sélectionnant le meilleur modèle pour chaque tâche\n- Facturation unifiée et suivi de l'utilisation\n\n## Prérequis\n\n- Compte LemonData avec accès API\n- Installation de Dify (cloud ou auto-hébergée)\n\n## Étapes de configuration\n\n### Étape 1 : Obtenir votre clé API\n\n1. Connectez-vous au [Tableau de bord LemonData](https://lemondata.cc/dashboard)\n2. Accédez aux [Clés API](https://lemondata.cc/dashboard/api)\n3. Créez et copiez votre clé API (format : `sk-...`)\n\n### Étape 2 : Ajouter un fournisseur de modèles personnalisé\n\n<Steps>\n  <Step title=\"Ouvrir les paramètres\">\n    Dans Dify, allez dans **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"Ajouter un fournisseur compatible OpenAI\">\n    Cliquez sur **Add Model Provider** et sélectionnez **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"Configurer le fournisseur\">\n    Saisissez les paramètres suivants :\n\n    | Champ | Valeur |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Ajouter des modèles\">\n    Ajoutez les modèles que vous souhaitez utiliser :\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Étape 3 : Tester la connexion\n\n1. Sélectionnez un modèle (par ex., `gpt-4o-mini`)\n2. Envoyez un message de test\n3. Vérifiez que vous recevez une réponse\n\n## Utilisation dans les applications\n\n### Chatbot\n\n1. Créez une nouvelle application Chatbot\n2. Sélectionnez LemonData comme fournisseur de modèles\n3. Choisissez votre modèle préféré\n4. Configurez le prompt système et les paramètres\n\n### Agent\n\n1. Créez une application Agent\n2. Sélectionnez un modèle performant (GPT-4o, Claude)\n3. Ajoutez des outils et des bases de connaissances\n4. Configurez le comportement de l'agent\n\n### Workflow\n\n1. Créez un Workflow\n2. Ajoutez des nœuds LLM\n3. Sélectionnez des modèles LemonData pour chaque nœud\n4. Connectez les nœuds et configurez le flux de données\n\n## Modèles disponibles\n\n| Catégorie | Modèles |\n|----------|--------|\n| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embeddings | text-embedding-3-small, text-embedding-3-large |\n| Vision | GPT-4o (avec images), Claude Sonnet (avec images) |\n\n## Configuration RAG\n\nPour les applications RAG, configurez les embeddings :\n\n1. Allez dans **Settings** → **Model Provider**\n2. Ajoutez le modèle d'embedding : `text-embedding-3-small`\n3. Définissez-le comme modèle d'embedding par défaut dans les paramètres de la base de connaissances\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Commencez par des modèles économiques\">\n    Utilisez GPT-4o-mini pour les tests et le développement, puis passez à des modèles plus puissants pour la production.\n  </Accordion>\n\n  <Accordion title=\"Utilisez le routage de modèles\">\n    Configurez différents modèles pour différentes tâches : des modèles rapides pour les requêtes simples, des modèles puissants pour le raisonnement complexe.\n  </Accordion>\n\n  <Accordion title=\"Surveillez l'utilisation\">\n    Utilisez les fonctionnalités LLMOps de Dify parallèlement au tableau de bord LemonData pour suivre les coûts et les performances.\n  </Accordion>\n</AccordionGroup>\n\n## Dépannage\n\n<AccordionGroup>\n  <Accordion title=\"Erreur de connexion\">\n    - Vérifiez que l'URL de base de l'API est exactement `https://api.lemondata.cc/v1`\n    - Vérifiez l'absence de barres obliques (slashes) à la fin\n    - Vérifiez la connectivité réseau\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Vérifiez à nouveau la clé API\n    - Assurez-vous que la clé est active dans le tableau de bord LemonData\n  </Accordion>\n\n  <Accordion title=\"Modèle non trouvé\">\n    - Vérifiez que le nom du modèle correspond exactement\n    - Vérifiez la disponibilité des modèles sur [lemondata.cc/en/models](https://lemondata.cc/fr/models)\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Dify\"\ndescription: \"Integra LemonData con Dify para construir aplicaciones de LLM\"\n---\n\n## Resumen\n\nDify es una plataforma de desarrollo de aplicaciones de LLM de código abierto. Proporciona orquestación visual de prompts, pipelines de RAG, frameworks de agentes y capacidades de LLMOps. LemonData puede configurarse como un proveedor de modelos personalizado en Dify.\n\n## Beneficios\n\n- Accede a más de 300 modelos de IA a través de una sola interfaz\n- Cambia entre modelos sin modificar la lógica de la aplicación\n- Optimiza costos seleccionando el mejor modelo para cada tarea\n- Facturación unificada y seguimiento de uso\n\n## Requisitos previos\n\n- Cuenta de LemonData con acceso a la API\n- Instalación de Dify (en la nube o auto-hospedado)\n\n## Pasos de configuración\n\n### Paso 1: Obtén tu API Key\n\n1. Inicia sesión en el [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navega a [API Keys](https://lemondata.cc/dashboard/api)\n3. Crea y copia tu API key (formato: `sk-...`)\n\n### Paso 2: Agrega el proveedor de modelos personalizado\n\n<Steps>\n  <Step title=\"Abre la configuración\">\n    En Dify, ve a **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"Agrega un proveedor compatible con OpenAI\">\n    Haz clic en **Add Model Provider** y selecciona **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"Configura el proveedor\">\n    Ingresa la siguiente configuración:\n\n    | Campo | Valor |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Agrega modelos\">\n    Agrega los modelos que desees utilizar:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Paso 3: Prueba la conexión\n\n1. Selecciona un modelo (por ejemplo, `gpt-4o-mini`)\n2. Envía un mensaje de prueba\n3. Verifica que recibas una respuesta\n\n## Uso en aplicaciones\n\n### Chatbot\n\n1. Crea una nueva aplicación de Chatbot\n2. Selecciona LemonData como el proveedor de modelos\n3. Elige tu modelo preferido\n4. Configura el prompt de sistema y los parámetros\n\n### Agente\n\n1. Crea una aplicación de Agente\n2. Selecciona un modelo capaz (GPT-4o, Claude)\n3. Agrega herramientas y bases de conocimiento\n4. Configura el comportamiento del agente\n\n### Workflow\n\n1. Crea un Workflow\n2. Agrega nodos de LLM\n3. Selecciona modelos de LemonData para cada nodo\n4. Conecta los nodos y configura el flujo de datos\n\n## Modelos disponibles\n\n| Categoría | Modelos |\n|----------|--------|\n| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embeddings | text-embedding-3-small, text-embedding-3-large |\n| Visión | GPT-4o (con imágenes), Claude Sonnet (con imágenes) |\n\n## Configuración de RAG\n\nPara aplicaciones de RAG, configura los embeddings:\n\n1. Ve a **Settings** → **Model Provider**\n2. Agrega el modelo de embedding: `text-embedding-3-small`\n3. Establécelo como el modelo de embedding predeterminado en la configuración de la base de conocimiento\n\n## Mejores prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Comienza con modelos económicos\">\n    Usa GPT-4o-mini para pruebas y desarrollo, y cambia a modelos más potentes para producción.\n  </Accordion>\n\n  <Accordion title=\"Usa enrutamiento de modelos\">\n    Configura diferentes modelos para diferentes tareas: modelos rápidos para consultas simples y modelos potentes para razonamiento complejo.\n  </Accordion>\n\n  <Accordion title=\"Monitorea el uso\">\n    Utiliza las funciones de LLMOps de Dify junto con el dashboard de LemonData para realizar un seguimiento de los costos y el rendimiento.\n  </Accordion>\n</AccordionGroup>\n\n## Solución de problemas\n\n<AccordionGroup>\n  <Accordion title=\"Error de conexión\">\n    - Verifica que la API Base URL sea exactamente `https://api.lemondata.cc/v1`\n    - Comprueba si hay barras diagonales al final\n    - Verifica la conectividad de red\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Verifica nuevamente la API key\n    - Asegúrate de que la clave esté activa en el dashboard de LemonData\n  </Accordion>\n\n  <Accordion title=\"Modelo no encontrado\">\n    - Verifica que el nombre del modelo coincida exactamente\n    - Consulta la disponibilidad de los modelos en [lemondata.cc/en/models](https://lemondata.cc/es/models)\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Dify\"\ndescription: \"Integre o LemonData com o Dify para construir aplicações de LLM\"\n---\n\n## Visão Geral\n\nO Dify é uma plataforma de desenvolvimento de aplicações de LLM de código aberto. Ele fornece orquestração visual de prompts, pipelines de RAG, frameworks de agentes e recursos de LLMOps. O LemonData pode ser configurado como um provedor de modelo personalizado no Dify.\n\n## Benefícios\n\n- Acesse mais de 300 modelos de IA através de uma única interface\n- Alterne entre modelos sem alterar a lógica da aplicação\n- Otimize custos selecionando o melhor modelo para cada tarefa\n- Faturamento unificado e rastreamento de uso\n\n## Pré-requisitos\n\n- Conta LemonData com acesso à API\n- Instalação do Dify (nuvem ou self-hosted)\n\n## Passos de Configuração\n\n### Passo 1: Obtenha sua API Key\n\n1. Faça login no [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navegue até [API Keys](https://lemondata.cc/dashboard/api)\n3. Crie e copie sua API key (formato: `sk-...`)\n\n### Passo 2: Adicione um Provedor de Modelo Personalizado\n\n<Steps>\n  <Step title=\"Abra as Configurações\">\n    No Dify, vá em **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"Adicione um Provedor Compatível com OpenAI\">\n    Clique em **Add Model Provider** e selecione **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"Configure o Provedor\">\n    Insira as seguintes configurações:\n\n    | Campo | Valor |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Adicione Modelos\">\n    Adicione os modelos que deseja usar:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Passo 3: Teste a Conexão\n\n1. Selecione um modelo (ex: `gpt-4o-mini`)\n2. Envie uma mensagem de teste\n3. Verifique se você recebeu uma resposta\n\n## Uso em Aplicações\n\n### Chatbot\n\n1. Crie uma nova aplicação de Chatbot\n2. Selecione o LemonData como o provedor de modelo\n3. Escolha seu modelo preferido\n4. Configure o prompt de sistema e os parâmetros\n\n### Agente\n\n1. Crie uma aplicação de Agente\n2. Selecione um modelo capaz (GPT-4o, Claude)\n3. Adicione ferramentas e bases de conhecimento\n4. Configure o comportamento do agente\n\n### Workflow\n\n1. Crie um Workflow\n2. Adicione nós de LLM\n3. Selecione modelos do LemonData para cada nó\n4. Conecte os nós e configure o fluxo de dados\n\n## Modelos Disponíveis\n\n| Categoria | Modelos |\n|----------|--------|\n| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embeddings | text-embedding-3-small, text-embedding-3-large |\n| Visão | GPT-4o (com imagens), Claude Sonnet (com imagens) |\n\n## Configuração de RAG\n\nPara aplicações de RAG, configure os embeddings:\n\n1. Vá em **Settings** → **Model Provider**\n2. Adicione o modelo de embedding: `text-embedding-3-small`\n3. Defina como o modelo de embedding padrão nas configurações da base de conhecimento\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Comece com modelos econômicos\">\n    Use o GPT-4o-mini para testes e desenvolvimento, e mude para modelos mais potentes para produção.\n  </Accordion>\n\n  <Accordion title=\"Use roteamento de modelos\">\n    Configure diferentes modelos para diferentes tarefas - modelos rápidos para consultas simples, modelos potentes para raciocínio complexo.\n  </Accordion>\n\n  <Accordion title=\"Monitore o uso\">\n    Use os recursos de LLMOps do Dify junto com o dashboard do LemonData para rastrear custos e desempenho.\n  </Accordion>\n</AccordionGroup>\n\n## Solução de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Erro de Conexão\">\n    - Verifique se a API Base URL é exatamente `https://api.lemondata.cc/v1`\n    - Verifique se há barras no final (trailing slashes)\n    - Verifique a conectividade da rede\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Verifique novamente a API key\n    - Certifique-se de que a chave está ativa no dashboard do LemonData\n  </Accordion>\n\n  <Accordion title=\"Modelo Não Encontrado\">\n    - Verifique se o nome do modelo corresponde exatamente\n    - Verifique a disponibilidade do modelo em [lemondata.cc/en/models](https://lemondata.cc/pt/models)\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"Dify\"\ndescription: \"قم بدمج LemonData مع Dify لبناء تطبيقات LLM\"\n---\n\n## نظرة عامة\n\nDify هي منصة مفتوحة المصدر لتطوير تطبيقات LLM. توفر المنصة تنسيقاً مرئياً للمطالبات (prompts)، ومسارات RAG، وأطر عمل الوكلاء (agents)، وقدرات LLMOps. يمكن تكوين LemonData كمزود نماذج مخصص في Dify.\n\n## الفوائد\n\n- الوصول إلى أكثر من 300 نموذج ذكاء اصطناعي من خلال واجهة واحدة\n- التبديل بين النماذج دون تغيير منطق التطبيق\n- تحسين التكاليف عن طريق اختيار النموذج الأفضل لكل مهمة\n- فوترة موحدة وتتبع الاستخدام\n\n## المتطلبات المسبقة\n\n- حساب LemonData مع وصول إلى API\n- تثبيت Dify (سحابي أو استضافة ذاتية)\n\n## خطوات التكوين\n\n### الخطوة 1: الحصول على مفتاح API الخاص بك\n\n1. قم بتسجيل الدخول إلى [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. انتقل إلى [API Keys](https://lemondata.cc/dashboard/api)\n3. قم بإنشاء ونسخ مفتاح API الخاص بك (بتنسيق: `sk-...`)\n\n### الخطوة 2: إضافة مزود نموذج مخصص\n\n<Steps>\n  <Step title=\"فتح الإعدادات\">\n    في Dify، انتقل إلى **Settings** ← **Model Provider**\n  </Step>\n  <Step title=\"إضافة مزود متوافق مع OpenAI\">\n    انقر فوق **Add Model Provider** وحدد **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"تكوين المزود\">\n    أدخل الإعدادات التالية:\n\n    | الحقل | القيمة |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"إضافة النماذج\">\n    أضف النماذج التي تريد استخدامها:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### الخطوة 3: اختبار الاتصال\n\n1. اختر نموذجاً (مثل `gpt-4o-mini`)\n2. أرسل رسالة اختبار\n3. تحقق من تلقي استجابة\n\n## الاستخدام في التطبيقات\n\n### روبوت الدردشة (Chatbot)\n\n1. أنشئ تطبيق Chatbot جديد\n2. اختر LemonData كمزود للنماذج\n3. اختر نموذجك المفضل\n4. قم بتكوين مطالبات النظام (system prompt) والمعلمات\n\n### الوكيل (Agent)\n\n1. أنشئ تطبيق Agent\n2. اختر نموذجاً قادراً (GPT-4o, Claude)\n3. أضف الأدوات وقواعد المعرفة\n4. تكوين سلوك الوكيل\n\n### سير العمل (Workflow)\n\n1. أنشئ سير عمل (Workflow)\n2. أضف عقد LLM\n3. اختر نماذج LemonData لكل عقدة\n4. قم بتوصيل العقد وتكوين تدفق البيانات\n\n## النماذج المتاحة\n\n| الفئة | النماذج |\n|----------|--------|\n| الدردشة | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| التضمينات (Embeddings) | text-embedding-3-small, text-embedding-3-large |\n| الرؤية (Vision) | GPT-4o (with images), Claude Sonnet (with images) |\n\n## تكوين RAG\n\nلتطبيقات RAG، قم بتكوين التضمينات (embeddings):\n\n1. انتقل إلى **Settings** ← **Model Provider**\n2. أضف نموذج التضمين: `text-embedding-3-small`\n3. تعيينه كنموذج تضمين افتراضي في إعدادات قاعدة المعرفة\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"ابدأ بالنماذج الاقتصادية\">\n    استخدم GPT-4o-mini للاختبار والتطوير، وانتقل إلى نماذج أكثر قوة للإنتاج.\n  </Accordion>\n\n  <Accordion title=\"استخدم توجيه النماذج\">\n    قم بتكوين نماذج مختلفة لمهام مختلفة - نماذج سريعة للاستعلامات البسيطة، ونماذج قوية للاستنتاج المعقد.\n  </Accordion>\n\n  <Accordion title=\"مراقبة الاستخدام\">\n    استخدم ميزات LLMOps في Dify جنباً إلى جنب مع لوحة تحكم LemonData لتتبع التكاليف والأداء.\n  </Accordion>\n</AccordionGroup>\n\n## استكشاف الأخطاء وإصلاحها\n\n<AccordionGroup>\n  <Accordion title=\"خطأ في الاتصال\">\n    - تحقق من أن عنوان URL الأساسي لـ API هو بالضبط `https://api.lemondata.cc/v1`\n    - تحقق من عدم وجود شرطات مائلة زائدة في النهاية\n    - تحقق من اتصال الشبكة\n  </Accordion>\n\n  <Accordion title=\"401 غير مصرح به\">\n    - تحقق جيداً من مفتاح API\n    - تأكد من أن المفتاح نشط في لوحة تحكم LemonData\n  </Accordion>\n\n  <Accordion title=\"النموذج غير موجود\">\n    - تحقق من مطابقة اسم النموذج تماماً\n    - تحقق من توفر النموذج في [lemondata.cc/en/models](https://lemondata.cc/ar/models)\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Dify\"\ndescription: \"Tích hợp LemonData với Dify để xây dựng các ứng dụng LLM\"\n---\n\n## Tổng quan\n\nDify là một nền tảng phát triển ứng dụng LLM mã nguồn mở. Nó cung cấp khả năng điều phối prompt trực quan, pipeline RAG, agent framework và các tính năng LLMOps. LemonData có thể được cấu hình như một nhà cung cấp mô hình tùy chỉnh trong Dify.\n\n## Lợi ích\n\n- Truy cập hơn 300 mô hình AI thông qua một giao diện duy nhất\n- Chuyển đổi giữa các mô hình mà không cần thay đổi logic ứng dụng\n- Tối ưu hóa chi phí bằng cách chọn mô hình tốt nhất cho từng tác vụ\n- Thanh toán và theo dõi mức độ sử dụng tập trung\n\n## Điều kiện tiên quyết\n\n- Tài khoản LemonData có quyền truy cập API\n- Cài đặt Dify (bản cloud hoặc tự lưu trữ)\n\n## Các bước cấu hình\n\n### Bước 1: Lấy API Key của bạn\n\n1. Đăng nhập vào [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Đi tới [API Keys](https://lemondata.cc/dashboard/api)\n3. Tạo và sao chép API key của bạn (định dạng: `sk-...`)\n\n### Bước 2: Thêm nhà cung cấp mô hình tùy chỉnh\n\n<Steps>\n  <Step title=\"Mở Cài đặt\">\n    Trong Dify, đi tới **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"Thêm nhà cung cấp tương thích OpenAI\">\n    Nhấp vào **Add Model Provider** và chọn **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"Cấu hình nhà cung cấp\">\n    Nhập các cài đặt sau:\n\n    | Trường | Giá trị |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Thêm mô hình\">\n    Thêm các mô hình bạn muốn sử dụng:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Bước 3: Kiểm tra kết nối\n\n1. Chọn một mô hình (ví dụ: `gpt-4o-mini`)\n2. Gửi một tin nhắn thử nghiệm\n3. Xác nhận bạn nhận được phản hồi\n\n## Sử dụng trong ứng dụng\n\n### Chatbot\n\n1. Tạo một ứng dụng Chatbot mới\n2. Chọn LemonData làm nhà cung cấp mô hình\n3. Chọn mô hình ưu tiên của bạn\n4. Cấu hình system prompt và các tham số\n\n### Agent\n\n1. Tạo một ứng dụng Agent\n2. Chọn một mô hình có năng lực (GPT-4o, Claude)\n3. Thêm các công cụ và cơ sở tri thức\n4. Cấu hình hành vi của agent\n\n### Workflow\n\n1. Tạo một Workflow\n2. Thêm các node LLM\n3. Chọn các mô hình LemonData cho từng node\n4. Kết nối các node và cấu hình luồng dữ liệu\n\n## Các mô hình hiện có\n\n| Danh mục | Mô hình |\n|----------|--------|\n| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embeddings | text-embedding-3-small, text-embedding-3-large |\n| Vision | GPT-4o (với hình ảnh), Claude Sonnet (với hình ảnh) |\n\n## Cấu hình RAG\n\nĐối với các ứng dụng RAG, hãy cấu hình embeddings:\n\n1. Đi tới **Settings** → **Model Provider**\n2. Thêm mô hình embedding: `text-embedding-3-small`\n3. Đặt làm mô hình embedding mặc định trong cài đặt cơ sở tri thức\n\n## Thực hành tốt nhất\n\n<AccordionGroup>\n  <Accordion title=\"Bắt đầu với các mô hình tiết kiệm\">\n    Sử dụng GPT-4o-mini để thử nghiệm và phát triển, chuyển sang các mô hình mạnh mẽ hơn cho môi trường production.\n  </Accordion>\n\n  <Accordion title=\"Sử dụng điều hướng mô hình\">\n    Cấu hình các mô hình khác nhau cho các tác vụ khác nhau - mô hình nhanh cho các truy vấn đơn giản, mô hình mạnh mẽ cho suy luận phức tạp.\n  </Accordion>\n\n  <Accordion title=\"Theo dõi mức độ sử dụng\">\n    Sử dụng các tính năng LLMOps của Dify cùng với dashboard của LemonData để theo dõi chi phí và hiệu suất.\n  </Accordion>\n</AccordionGroup>\n\n## Xử lý sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Lỗi kết nối\">\n    - Xác minh API Base URL chính xác là `https://api.lemondata.cc/v1`\n    - Kiểm tra các dấu gạch chéo ở cuối\n    - Kiểm tra kết nối mạng\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Kiểm tra lại API key\n    - Đảm bảo key đang hoạt động trong dashboard của LemonData\n  </Accordion>\n\n  <Accordion title=\"Không tìm thấy mô hình\">\n    - Xác minh tên mô hình khớp chính xác\n    - Kiểm tra tính khả dụng của mô hình tại [lemondata.cc/en/models](https://lemondata.cc/vi/models)\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Dify\"\ndescription: \"Integrasikan LemonData dengan Dify untuk membangun aplikasi LLM\"\n---\n\n## Gambaran Umum\n\nDify adalah platform pengembangan aplikasi LLM sumber terbuka. Dify menyediakan orkestrasi prompt visual, pipeline RAG, framework agen, dan kapabilitas LLMOps. LemonData dapat dikonfigurasi sebagai penyedia model kustom di Dify.\n\n## Manfaat\n\n- Akses 300+ model AI melalui satu antarmuka\n- Beralih antar model tanpa mengubah logika aplikasi\n- Optimalkan biaya dengan memilih model terbaik untuk setiap tugas\n- Penagihan dan pelacakan penggunaan yang terpadu\n\n## Prasyarat\n\n- Akun LemonData dengan akses API\n- Instalasi Dify (cloud atau self-hosted)\n\n## Langkah Konfigurasi\n\n### Langkah 1: Dapatkan API Key Anda\n\n1. Masuk ke [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navigasi ke [API Keys](https://lemondata.cc/dashboard/api)\n3. Buat dan salin API key Anda (format: `sk-...`)\n\n### Langkah 2: Tambahkan Penyedia Model Kustom\n\n<Steps>\n  <Step title=\"Buka Pengaturan\">\n    Di Dify, buka **Settings** → **Model Provider**\n  </Step>\n  <Step title=\"Tambahkan Penyedia Kompatibel OpenAI\">\n    Klik **Add Model Provider** dan pilih **OpenAI-API-compatible**\n  </Step>\n  <Step title=\"Konfigurasi Penyedia\">\n    Masukkan pengaturan berikut:\n\n    | Bidang | Nilai |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Tambahkan Model\">\n    Tambahkan model yang ingin Anda gunakan:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Langkah 3: Uji Koneksi\n\n1. Pilih sebuah model (misalnya, `gpt-4o-mini`)\n2. Kirim pesan uji coba\n3. Verifikasi bahwa Anda menerima respons\n\n## Penggunaan dalam Aplikasi\n\n### Chatbot\n\n1. Buat aplikasi Chatbot baru\n2. Pilih LemonData sebagai penyedia model\n3. Pilih model pilihan Anda\n4. Konfigurasi prompt sistem dan parameter\n\n### Agen\n\n1. Buat aplikasi Agen\n2. Pilih model yang mumpuni (GPT-4o, Claude)\n3. Tambahkan alat dan basis pengetahuan\n4. Konfigurasi perilaku agen\n\n### Alur Kerja\n\n1. Buat Alur Kerja (Workflow)\n2. Tambahkan node LLM\n3. Pilih model LemonData untuk setiap node\n4. Hubungkan node dan konfigurasi aliran data\n\n## Model yang Tersedia\n\n| Kategori | Model |\n|----------|--------|\n| Chat | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embedding | text-embedding-3-small, text-embedding-3-large |\n| Visi | GPT-4o (dengan gambar), Claude Sonnet (dengan gambar) |\n\n## Konfigurasi RAG\n\nUntuk aplikasi RAG, konfigurasi embedding:\n\n1. Buka **Settings** → **Model Provider**\n2. Tambahkan model embedding: `text-embedding-3-small`\n3. Atur sebagai model embedding default dalam pengaturan basis pengetahuan\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Mulai dengan model ekonomis\">\n    Gunakan GPT-4o-mini untuk pengujian dan pengembangan, beralih ke model yang lebih kuat untuk produksi.\n  </Accordion>\n\n  <Accordion title=\"Gunakan perutean model\">\n    Konfigurasi model yang berbeda untuk tugas yang berbeda - model cepat untuk kueri sederhana, model kuat untuk penalaran kompleks.\n  </Accordion>\n\n  <Accordion title=\"Pantau penggunaan\">\n    Gunakan fitur LLMOps Dify bersama dengan dashboard LemonData untuk melacak biaya dan performa.\n  </Accordion>\n</AccordionGroup>\n\n## Penyelesaian Masalah\n\n<AccordionGroup>\n  <Accordion title=\"Kesalahan Koneksi\">\n    - Verifikasi API Base URL tepat `https://api.lemondata.cc/v1`\n    - Periksa adanya garis miring di akhir (trailing slashes)\n    - Verifikasi konektivitas jaringan\n  </Accordion>\n\n  <Accordion title=\"401 Unauthorized\">\n    - Periksa kembali API key\n    - Pastikan kunci aktif di dashboard LemonData\n  </Accordion>\n\n  <Accordion title=\"Model Tidak Ditemukan\">\n    - Verifikasi nama model cocok secara tepat\n    - Periksa ketersediaan model di [lemondata.cc/en/models](https://lemondata.cc/id/models)\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"Dify\"\ndescription: \"LLM uygulamaları oluşturmak için LemonData'yı Dify ile entegre edin\"\n---\n\n## Genel Bakış\n\nDify, açık kaynaklı bir LLM uygulama geliştirme platformudur. Görsel prompt orkestrasyonu, RAG boru hatları (pipelines), ajan çerçeveleri ve LLMOps yetenekleri sunar. LemonData, Dify'da özel bir model sağlayıcısı olarak yapılandırılabilir.\n\n## Avantajlar\n\n- Tek bir arayüz üzerinden 300'den fazla yapay zeka modeline erişin\n- Uygulama mantığını değiştirmeden modeller arasında geçiş yapın\n- Her görev için en iyi modeli seçerek maliyetleri optimize edin\n- Birleştirilmiş faturalandırma ve kullanım takibi\n\n## Ön Koşullar\n\n- API erişimi olan LemonData hesabı\n- Dify kurulumu (bulut veya self-hosted)\n\n## Yapılandırma Adımları\n\n### Adım 1: API Anahtarınızı Alın\n\n1. [LemonData Dashboard](https://lemondata.cc/dashboard) adresine giriş yapın\n2. [API Keys](https://lemondata.cc/dashboard/api) bölümüne gidin\n3. API anahtarınızı oluşturun ve kopyalayın (format: `sk-...`)\n\n### Adım 2: Özel Model Sağlayıcısı Ekleme\n\n<Steps>\n  <Step title=\"Ayarları Açın\">\n    Dify'da **Settings** → **Model Provider** bölümüne gidin\n  </Step>\n  <Step title=\"OpenAI Uyumlu Sağlayıcı Ekle\">\n    **Add Model Provider**'a tıklayın ve **OpenAI-API-compatible** seçeneğini seçin\n  </Step>\n  <Step title=\"Sağlayıcıyı Yapılandır\">\n    Aşağıdaki ayarları girin:\n\n    | Alan | Değer |\n    |-------|-------|\n    | Provider Name | LemonData |\n    | API Key | `sk-your-lemondata-key` |\n    | API Base URL | `https://api.lemondata.cc/v1` |\n  </Step>\n  <Step title=\"Modelleri Ekle\">\n    Kullanmak istediğiniz modelleri ekleyin:\n    - `gpt-4o`\n    - `gpt-4o-mini`\n    - `claude-sonnet-4-5`\n    - `claude-opus-4-5`\n    - `gemini-2.5-flash`\n    - `gemini-2.5-pro`\n    - `deepseek-r1`\n  </Step>\n</Steps>\n\n### Adım 3: Bağlantıyı Test Edin\n\n1. Bir model seçin (örneğin, `gpt-4o-mini`)\n2. Bir test mesajı gönderin\n3. Yanıt aldığınızı doğrulayın\n\n## Uygulamalarda Kullanım\n\n### Chatbot\n\n1. Yeni bir Chatbot uygulaması oluşturun\n2. Model sağlayıcısı olarak LemonData'yı seçin\n3. Tercih ettiğiniz modeli seçin\n4. Sistem prompt'unu ve parametreleri yapılandırın\n\n### Agent\n\n1. Bir Agent uygulaması oluşturun\n2. Yetenekli bir model seçin (GPT-4o, Claude)\n3. Araçlar ve bilgi tabanları (knowledge bases) ekleyin\n4. Agent davranışını yapılandırın\n\n### Workflow\n\n1. Bir Workflow oluşturun\n2. LLM düğümleri (nodes) ekleyin\n3. Her düğüm için LemonData modellerini seçin\n4. Düğümleri bağlayın ve veri akışını yapılandırın\n\n## Mevcut Modeller\n\n| Kategori | Modeller |\n|----------|--------|\n| Sohbet | GPT-4o, GPT-4o-mini, Claude Sonnet/Opus, Gemini, DeepSeek |\n| Embeddings | text-embedding-3-small, text-embedding-3-large |\n| Görüntü İşleme (Vision) | GPT-4o (görsellerle), Claude Sonnet (görsellerle) |\n\n## RAG Yapılandırması\n\nRAG uygulamaları için embedding'leri yapılandırın:\n\n1. **Settings** → **Model Provider** bölümüne gidin\n2. Embedding modeli ekleyin: `text-embedding-3-small`\n3. Bilgi tabanı ayarlarında varsayılan embedding modeli olarak ayarlayın\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Ekonomik modellerle başlayın\">\n    Test ve geliştirme için GPT-4o-mini kullanın, üretim aşamasında daha güçlü modellere geçin.\n  </Accordion>\n\n  <Accordion title=\"Model yönlendirmeyi kullanın\">\n    Farklı görevler için farklı modeller yapılandırın - basit sorgular için hızlı modeller, karmaşık akıl yürütme için güçlü modeller.\n  </Accordion>\n\n  <Accordion title=\"Kullanımı izleyin\">\n    Maliyetleri ve performansı takip etmek için LemonData paneliyle birlikte Dify'ın LLMOps özelliklerini kullanın.\n  </Accordion>\n</AccordionGroup>\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Bağlantı Hatası\">\n    - API Base URL'nin tam olarak `https://api.lemondata.cc/v1` olduğunu doğrulayın\n    - Sondaki eğik çizgilere (slash) dikkat edin\n    - Ağ bağlantısını doğrulayın\n  </Accordion>\n\n  <Accordion title=\"401 Yetkisiz Erişim (Unauthorized)\">\n    - API anahtarını tekrar kontrol edin\n    - Anahtarın LemonData panelinde aktif olduğundan emin olun\n  </Accordion>\n\n  <Accordion title=\"Model Bulunamadı\">\n    - Model adının tam olarak eşleştiğini doğrulayın\n    - Modelin kullanılabilirliğini [lemondata.cc/en/models](https://lemondata.cc/tr/models) adresinden kontrol edin\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:36:48.887Z"
  },
  "integrations/gemini-cli.mdx": {
    "sourceHash": "e41cd0bbcd2bb607",
    "translations": {
      "zh": "---\ntitle: \"Gemini CLI\"\ndescription: \"配置 Google Gemini CLI 以使用 LemonData API\"\n---\n\n## 概览\n\nGoogle Gemini CLI 是一个用于与 Gemini 模型交互的命令行工具。LemonData 提供了一个兼容的端点，允许您使用 Gemini CLI 访问 300 多个模型。\n\n## 系统要求\n\n- **Node.js**: 20.0+ 版本\n- **OS**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+ 或 Debian 10+\n\n## 安装\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\n验证安装：\n\n```bash\ngemini --version\n```\n\n## 配置\n\n### 第 1 步：获取您的 API Key\n\n1. 登录 [LemonData 控制台](https://lemondata.cc/dashboard)\n2. 导航至 [API Keys](https://lemondata.cc/dashboard/api)\n3. 创建并复制您的 API key（格式：`sk-...`）\n\n### 第 2 步：设置环境变量\n\n**临时（当前会话）：**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**永久配置：**\n\n添加到您的 shell 配置文件中：\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\n配置完成后请重启终端以使更改生效。\n</Warning>\n\n<Note>\n**已知限制**：在某些情况下，如果存在缓存的 Google 会话，Gemini CLI 可能不会遵循 `GOOGLE_GEMINI_BASE_URL`。如果您遇到连接问题，请尝试清除 CLI 缓存或启动新的终端会话。\n</Note>\n\n## 基本用法\n\n从您的项目目录启动 Gemini CLI：\n\n```bash\ncd your-project\ngemini\n```\n\n首次运行时，您需要：\n1. 选择主题\n2. 确认安全声明\n3. 信任工作目录\n\n## 可用模型\n\n| 模型 | 描述 |\n|-------|-------------|\n| `gemini-2.5-pro` | 功能最强大的 Gemini 模型 |\n| `gemini-2.5-flash` | 快速、高效，适用于大多数任务 |\n| `gemini-2.0-flash` | 上一代版本，稳定 |\n\n## 常用命令\n\n**提问：**\n\n```\n> What is the best way to structure a React app?\n```\n\n**分析代码：**\n\n```\n> Explain the code in src/main.ts\n```\n\n**生成代码：**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**审查更改：**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## 验证配置\n\n```bash\n# 检查环境变量\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# 测试连接\ngemini\n```\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"连接错误\">\n    - 验证 `GOOGLE_GEMINI_BASE_URL` 已设置为 `https://api.lemondata.cc`\n    - 注意：Gemini 端点不需要 `/v1` 后缀\n    - 检查网络连接\n  </Accordion>\n\n  <Accordion title=\"身份验证失败\">\n    - 验证 `GEMINI_API_KEY` 环境变量已设置\n    - 检查 Key 是否以 `sk-` 开头\n    - 确保 Key 在 LemonData 控制台中处于激活状态\n  </Accordion>\n\n  <Accordion title=\"模型不可用\">\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh/models) 查看 Gemini 模型的可用性\n    - 尝试不同的 Gemini 模型变体\n  </Accordion>\n</AccordionGroup>\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"在项目目录中使用\">\n    始终从项目根目录运行 Gemini CLI，以便更好地理解上下文。\n  </Accordion>\n\n  <Accordion title=\"谨慎信任目录\">\n    仅信任您拥有的目录。Gemini CLI 可以读取和修改文件。\n  </Accordion>\n\n  <Accordion title=\"审查生成的代码\">\n    在提交到项目之前，务必审查 AI 生成的代码。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"Gemini CLI\"\ndescription: \"配置 Google Gemini CLI 以使用 LemonData API\"\n---\n\n## 概覽\n\nGoogle Gemini CLI 是一個用於與 Gemini 模型互動的命令列工具。LemonData 提供了一個相容的端點，讓您可以使用 Gemini CLI 並存取 300 多個模型。\n\n## 系統需求\n\n- **Node.js**: 版本 20.0+\n- **作業系統**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+, 或 Debian 10+\n\n## 安裝\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\n驗證安裝：\n\n```bash\ngemini --version\n```\n\n## 配置\n\n### 步驟 1：獲取您的 API Key\n\n1. 登入 [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. 導覽至 [API Keys](https://lemondata.cc/dashboard/api)\n3. 建立並複製您的 API Key（格式：`sk-...`）\n\n### 步驟 2：設定環境變數\n\n**臨時（目前工作階段）：**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**永久配置：**\n\n新增至您的 shell 配置檔案：\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\n配置完成後請重啟終端機，以使變更生效。\n</Warning>\n\n<Note>\n**已知限制**：在某些情況下，如果存在快取的 Google 工作階段，Gemini CLI 可能不會遵循 `GOOGLE_GEMINI_BASE_URL`。如果您遇到連線問題，請嘗試清除 CLI 快取或啟動新的終端機工作階段。\n</Note>\n\n## 基本用法\n\n從您的專案目錄啟動 Gemini CLI：\n\n```bash\ncd your-project\ngemini\n```\n\n首次執行時，您需要：\n1. 選擇主題\n2. 確認安全須知\n3. 信任工作目錄\n\n## 可用模型\n\n| 模型 | 描述 |\n|-------|-------------|\n| `gemini-2.5-pro` | 功能最強大的 Gemini 模型 |\n| `gemini-2.5-flash` | 快速且高效，適用於大多數任務 |\n| `gemini-2.0-flash` | 前一代版本，穩定 |\n\n## 常見指令\n\n**提問：**\n\n```\n> What is the best way to structure a React app?\n```\n\n**分析程式碼：**\n\n```\n> Explain the code in src/main.ts\n```\n\n**生成程式碼：**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**審查變更：**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## 驗證配置\n\n```bash\n# 檢查環境變數\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# 測試連線\ngemini\n```\n\n## 疑難排解\n\n<AccordionGroup>\n  <Accordion title=\"連線錯誤\">\n    - 驗證 `GOOGLE_GEMINI_BASE_URL` 已設定為 `https://api.lemondata.cc`\n    - 注意：Gemini 端點不需要 `/v1` 字尾\n    - 檢查網路連線\n  </Accordion>\n\n  <Accordion title=\"身份驗證失敗\">\n    - 驗證 `GEMINI_API_KEY` 環境變數已設定\n    - 檢查 Key 是否以 `sk-` 開頭\n    - 確保該 Key 在 LemonData 控制台（dashboard）中處於啟用狀態\n  </Accordion>\n\n  <Accordion title=\"模型不可用\">\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh-TW/models) 檢查 Gemini 模型的可用性\n    - 嘗試不同的 Gemini 模型變體\n  </Accordion>\n</AccordionGroup>\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"在專案目錄中使用\">\n    始終從專案根目錄執行 Gemini CLI，以便更好地理解上下文。\n  </Accordion>\n\n  <Accordion title=\"謹慎信任目錄\">\n    僅信任您擁有的目錄。Gemini CLI 可以讀取和修改檔案。\n  </Accordion>\n\n  <Accordion title=\"審查生成的程式碼\">\n    在提交到專案之前，務必審查 AI 生成的程式碼。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"Gemini CLI\"\ndescription: \"LemonData API を使用するように Google Gemini CLI を設定する\"\n---\n\n## 概要\n\nGoogle Gemini CLI は、Gemini モデルとやり取りするためのコマンドラインツールです。LemonData は互換性のあるエンドポイントを提供しており、Gemini CLI を使用して 300 以上のモデルにアクセスできます。\n\n## システム要件\n\n- **Node.js**: バージョン 20.0 以上\n- **OS**: Windows 10/11、macOS 10.15 以上、Ubuntu 20.04 以上、または Debian 10 以上\n\n## インストール\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nインストールの確認:\n\n```bash\ngemini --version\n```\n\n## 設定\n\n### ステップ 1: API キーの取得\n\n1. [LemonData ダッシュボード](https://lemondata.cc/dashboard)にログインします\n2. [API キー](https://lemondata.cc/dashboard/api)に移動します\n3. API キーを作成してコピーします（形式: `sk-...`）\n\n### ステップ 2: 環境変数の設定\n\n**一時的（現在のセッション）:**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**永続的な設定:**\n\nシェル設定ファイルに追加します:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\n設定を反映させるために、設定後にターミナルを再起動してください。\n</Warning>\n\n<Note>\n**既知の制限事項**: キャッシュされた Google セッションがある場合、Gemini CLI が `GOOGLE_GEMINI_BASE_URL` を無視することがあります。接続の問題が発生した場合は、CLI キャッシュをクリアするか、新しいターミナルセッションを開始してください。\n</Note>\n\n## 基本的な使い方\n\nプロジェクトディレクトリから Gemini CLI を起動します:\n\n```bash\ncd your-project\ngemini\n```\n\n初回実行時には、以下の操作を行います:\n1. テーマの選択\n2. 安全に関する通知の確認\n3. 作業ディレクトリの信頼設定\n\n## 利用可能なモデル\n\n| モデル | 説明 |\n|-------|-------------|\n| `gemini-2.5-pro` | 最も高性能な Gemini モデル |\n| `gemini-2.5-flash` | 高速で、ほとんどのタスクに効率的 |\n| `gemini-2.0-flash` | 前世代、安定版 |\n\n## よく使うコマンド\n\n**質問する:**\n\n```\n> What is the best way to structure a React app?\n```\n\n**コードを分析する:**\n\n```\n> Explain the code in src/main.ts\n```\n\n**コードを生成する:**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**変更をレビューする:**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## 設定の確認\n\n```bash\n# 環境変数の確認\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# 接続テスト\ngemini\n```\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"接続エラー\">\n    - `GOOGLE_GEMINI_BASE_URL` が `https://api.lemondata.cc` に設定されていることを確認してください\n    - 注意: Gemini エンドポイントには `/v1` サフィックスは不要です\n    - ネットワーク接続を確認してください\n  </Accordion>\n\n  <Accordion title=\"認証失敗\">\n    - `GEMINI_API_KEY` 環境変数が設定されていることを確認してください\n    - キーが `sk-` で始まっていることを確認してください\n    - LemonData ダッシュボードでキーが有効であることを確認してください\n  </Accordion>\n\n  <Accordion title=\"モデルが利用不可\">\n    - [lemondata.cc/en/models](https://lemondata.cc/ja/models) で Gemini モデルの利用可能性を確認してください\n    - 別の Gemini モデルバリアントを試してください\n  </Accordion>\n</AccordionGroup>\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"プロジェクトディレクトリで使用する\">\n    コンテキストをより正確に理解させるため、常にプロジェクトのルートから Gemini CLI を実行してください。\n  </Accordion>\n\n  <Accordion title=\"ディレクトリの信頼設定は慎重に行う\">\n    自身が所有するディレクトリのみを信頼してください。Gemini CLI はファイルを読み取り、変更する可能性があります。\n  </Accordion>\n\n  <Accordion title=\"生成されたコードをレビューする\">\n    プロジェクトにコミットする前に、AI が生成したコードを必ずレビューしてください。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"Gemini CLI\"\ndescription: \"LemonData API를 사용하도록 Google Gemini CLI 설정하기\"\n---\n\n## 개요\n\nGoogle Gemini CLI는 Gemini 모델과 상호작용하기 위한 명령줄 도구입니다. LemonData는 300개 이상의 모델에 액세스하여 Gemini CLI를 사용할 수 있도록 호환 가능한 엔드포인트를 제공합니다.\n\n## 시스템 요구 사항\n\n- **Node.js**: 버전 20.0+\n- **OS**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+, 또는 Debian 10+\n\n## 설치\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\n설치 확인:\n\n```bash\ngemini --version\n```\n\n## 설정\n\n### 1단계: API 키 가져오기\n\n1. [LemonData 대시보드](https://lemondata.cc/dashboard)에 로그인합니다.\n2. [API 키](https://lemondata.cc/dashboard/api)로 이동합니다.\n3. API 키를 생성하고 복사합니다 (형식: `sk-...`).\n\n### 2단계: 환경 변수 설정\n\n**임시 (현재 세션):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**영구 설정:**\n\n쉘 설정 파일에 추가합니다:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\n설정 후 변경 사항을 적용하려면 터미널을 재시작하세요.\n</Warning>\n\n<Note>\n**알려진 제한 사항**: 경우에 따라 캐시된 Google 세션이 있으면 Gemini CLI가 `GOOGLE_GEMINI_BASE_URL`을 무시할 수 있습니다. 연결 문제가 발생하면 CLI 캐시를 지우거나 새 터미널 세션을 시작해 보세요.\n</Note>\n\n## 기본 사용법\n\n프로젝트 디렉토리에서 Gemini CLI를 시작합니다:\n\n```bash\ncd your-project\ngemini\n```\n\n처음 실행 시 다음을 수행합니다:\n1. 테마 선택\n2. 안전 고지 확인\n3. 작업 디렉토리 신뢰\n\n## 사용 가능한 모델\n\n| 모델 | 설명 |\n|-------|-------------|\n| `gemini-2.5-pro` | 가장 성능이 뛰어난 Gemini 모델 |\n| `gemini-2.5-flash` | 대부분의 작업에 빠르고 효율적임 |\n| `gemini-2.0-flash` | 이전 세대, 안정적임 |\n\n## 주요 명령\n\n**질문하기:**\n\n```\n> What is the best way to structure a React app?\n```\n\n**코드 분석:**\n\n```\n> Explain the code in src/main.ts\n```\n\n**코드 생성:**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**변경 사항 검토:**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## 설정 확인\n\n```bash\n# 환경 변수 확인\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# 연결 테스트\ngemini\n```\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"연결 오류\">\n    - `GOOGLE_GEMINI_BASE_URL`이 `https://api.lemondata.cc`로 설정되어 있는지 확인하세요.\n    - 참고: Gemini 엔드포인트에는 `/v1` 접미사가 없습니다.\n    - 네트워크 연결을 확인하세요.\n  </Accordion>\n\n  <Accordion title=\"인증 실패\">\n    - `GEMINI_API_KEY` 환경 변수가 설정되어 있는지 확인하세요.\n    - 키가 `sk-`로 시작하는지 확인하세요.\n    - LemonData 대시보드에서 키가 활성 상태인지 확인하세요.\n  </Accordion>\n\n  <Accordion title=\"모델을 사용할 수 없음\">\n    - [lemondata.cc/en/models](https://lemondata.cc/ko/models)에서 Gemini 모델 가용성을 확인하세요.\n    - 다른 Gemini 모델 변형을 시도해 보세요.\n  </Accordion>\n</AccordionGroup>\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"프로젝트 디렉토리에서 사용\">\n    더 나은 컨텍스트 이해를 위해 항상 프로젝트 루트에서 Gemini CLI를 실행하세요.\n  </Accordion>\n\n  <Accordion title=\"디렉토리 신뢰 시 주의\">\n    본인이 소유한 디렉토리만 신뢰하세요. Gemini CLI는 파일을 읽고 수정할 수 있습니다.\n  </Accordion>\n\n  <Accordion title=\"생성된 코드 검토\">\n    프로젝트에 커밋하기 전에 항상 AI가 생성한 코드를 검토하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Gemini CLI\"\ndescription: \"Konfigurieren Sie das Google Gemini CLI zur Verwendung der LemonData API\"\n---\n\n## Übersicht\n\nDas Google Gemini CLI ist ein Befehlszeilen-Tool zur Interaktion mit Gemini-Modellen. LemonData bietet einen kompatiblen Endpunkt, der es Ihnen ermöglicht, das Gemini CLI mit Zugriff auf über 300 Modelle zu nutzen.\n\n## Systemanforderungen\n\n- **Node.js**: Version 20.0+\n- **Betriebssystem**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+ oder Debian 10+\n\n## Installation\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nInstallation überprüfen:\n\n```bash\ngemini --version\n```\n\n## Konfiguration\n\n### Schritt 1: API-Key abrufen\n\n1. Melden Sie sich im [LemonData Dashboard](https://lemondata.cc/dashboard) an\n2. Navigieren Sie zu [API Keys](https://lemondata.cc/dashboard/api)\n3. Erstellen und kopieren Sie Ihren API-Key (Format: `sk-...`)\n\n### Schritt 2: Umgebungsvariablen festlegen\n\n**Temporär (aktuelle Sitzung):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Permanente Konfiguration:**\n\nFügen Sie dies zu Ihrer Shell-Konfigurationsdatei hinzu:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nStarten Sie Ihr Terminal nach der Konfiguration neu, damit die Änderungen wirksam werden.\n</Warning>\n\n<Note>\n**Bekannte Einschränkung**: In einigen Fällen ignoriert das Gemini CLI möglicherweise `GOOGLE_GEMINI_BASE_URL`, wenn eine Google-Sitzung im Cache gespeichert ist. Wenn Verbindungsprobleme auftreten, versuchen Sie, den CLI-Cache zu leeren oder eine neue Terminal-Sitzung zu starten.\n</Note>\n\n## Grundlegende Nutzung\n\nStarten Sie das Gemini CLI aus Ihrem Projektverzeichnis:\n\n```bash\ncd your-project\ngemini\n```\n\nBeim ersten Ausführen werden Sie:\n1. Ein Theme auswählen\n2. Den Sicherheitshinweis bestätigen\n3. Dem Arbeitsverzeichnis vertrauen\n\n## Verfügbare Modelle\n\n| Modell | Beschreibung |\n|-------|-------------|\n| `gemini-2.5-pro` | Leistungsfähigstes Gemini-Modell |\n| `gemini-2.5-flash` | Schnell, effizient für die meisten Aufgaben |\n| `gemini-2.0-flash` | Vorherige Generation, stabil |\n\n## Häufige Befehle\n\n**Eine Frage stellen:**\n\n```\n> What is the best way to structure a React app?\n```\n\n**Code analysieren:**\n\n```\n> Explain the code in src/main.ts\n```\n\n**Code generieren:**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**Änderungen überprüfen:**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## Konfiguration überprüfen\n\n```bash\n# Umgebungsvariablen prüfen\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# Verbindung testen\ngemini\n```\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Verbindungsfehler\">\n    - Überprüfen Sie, ob `GOOGLE_GEMINI_BASE_URL` auf `https://api.lemondata.cc` gesetzt ist\n    - Hinweis: Kein `/v1`-Suffix für den Gemini-Endpunkt\n    - Netzwerkverbindung prüfen\n  </Accordion>\n\n  <Accordion title=\"Authentifizierung fehlgeschlagen\">\n    - Überprüfen Sie, ob die Umgebungsvariable `GEMINI_API_KEY` gesetzt ist\n    - Prüfen Sie, ob der Key mit `sk-` beginnt\n    - Stellen Sie sicher, dass der Key im LemonData-Dashboard aktiv ist\n  </Accordion>\n\n  <Accordion title=\"Modell nicht verfügbar\">\n    - Prüfen Sie die Verfügbarkeit der Gemini-Modelle unter [lemondata.cc/en/models](https://lemondata.cc/de/models)\n    - Versuchen Sie eine andere Gemini-Modellvariante\n  </Accordion>\n</AccordionGroup>\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Im Projektverzeichnis verwenden\">\n    Führen Sie das Gemini CLI immer aus dem Projekt-Root-Verzeichnis aus, um ein besseres Kontextverständnis zu ermöglichen.\n  </Accordion>\n\n  <Accordion title=\"Verzeichnissen vorsichtig vertrauen\">\n    Vertrauen Sie nur Verzeichnissen, die Ihnen gehören. Das Gemini CLI kann Dateien lesen und ändern.\n  </Accordion>\n\n  <Accordion title=\"Generierten Code überprüfen\">\n    Überprüfen Sie KI-generierten Code immer, bevor Sie ihn in Ihr Projekt übernehmen.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"Gemini CLI\"\ndescription: \"Configurez Google Gemini CLI pour utiliser l'API LemonData\"\n---\n\n## Présentation\n\nGoogle Gemini CLI est un outil en ligne de commande pour interagir avec les modèles Gemini. LemonData fournit un endpoint compatible qui vous permet d'utiliser Gemini CLI avec un accès à plus de 300 modèles.\n\n## Configuration requise\n\n- **Node.js** : Version 20.0+\n- **OS** : Windows 10/11, macOS 10.15+, Ubuntu 20.04+, ou Debian 10+\n\n## Installation\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nVérifiez l'installation :\n\n```bash\ngemini --version\n```\n\n## Configuration\n\n### Étape 1 : Obtenez votre clé API\n\n1. Connectez-vous au [Tableau de bord LemonData](https://lemondata.cc/dashboard)\n2. Accédez aux [Clés API](https://lemondata.cc/dashboard/api)\n3. Créez et copiez votre clé API (format : `sk-...`)\n\n### Étape 2 : Définissez les variables d'environnement\n\n**Temporaire (session actuelle) :**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Configuration permanente :**\n\nAjoutez à votre fichier de configuration shell :\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nRedémarrez votre terminal après la configuration pour que les modifications prennent effet.\n</Warning>\n\n<Note>\n**Limitation connue** : Dans certains cas, Gemini CLI peut ne pas respecter `GOOGLE_GEMINI_BASE_URL` s'il existe une session Google en cache. Si vous rencontrez des problèmes de connexion, essayez de vider le cache du CLI ou de démarrer une nouvelle session de terminal.\n</Note>\n\n## Utilisation de base\n\nLancez Gemini CLI depuis le répertoire de votre projet :\n\n```bash\ncd your-project\ngemini\n```\n\nLors de la première exécution, vous devrez :\n1. Choisir un thème\n2. Confirmer l'avis de sécurité\n3. Faire confiance au répertoire de travail\n\n## Modèles disponibles\n\n| Modèle | Description |\n|-------|-------------|\n| `gemini-2.5-pro` | Le modèle Gemini le plus performant |\n| `gemini-2.5-flash` | Rapide, efficace pour la plupart des tâches |\n| `gemini-2.0-flash` | Génération précédente, stable |\n\n## Commandes courantes\n\n**Poser une question :**\n\n```\n> What is the best way to structure a React app?\n```\n\n**Analyser du code :**\n\n```\n> Explain the code in src/main.ts\n```\n\n**Générer du code :**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**Réviser les modifications :**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## Vérifier la configuration\n\n```bash\n# Vérifier les variables d'environnement\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# Tester la connexion\ngemini\n```\n\n## Dépannage\n\n<AccordionGroup>\n  <Accordion title=\"Erreur de connexion\">\n    - Vérifiez que `GOOGLE_GEMINI_BASE_URL` est défini sur `https://api.lemondata.cc`\n    - Note : Pas de suffixe `/v1` pour l'endpoint Gemini\n    - Vérifiez la connectivité réseau\n  </Accordion>\n\n  <Accordion title=\"Échec de l'authentification\">\n    - Vérifiez que la variable d'environnement `GEMINI_API_KEY` est définie\n    - Vérifiez que la clé commence par `sk-`\n    - Assurez-vous que la clé est active dans le tableau de bord LemonData\n  </Accordion>\n\n  <Accordion title=\"Modèle non disponible\">\n    - Vérifiez la disponibilité des modèles Gemini sur [lemondata.cc/en/models](https://lemondata.cc/fr/models)\n    - Essayez une autre variante du modèle Gemini\n  </Accordion>\n</AccordionGroup>\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Utiliser dans le répertoire du projet\">\n    Exécutez toujours Gemini CLI depuis la racine de votre projet pour une meilleure compréhension du contexte.\n  </Accordion>\n\n  <Accordion title=\"Faire confiance aux répertoires avec prudence\">\n    Ne faites confiance qu'aux répertoires qui vous appartiennent. Gemini CLI peut lire et modifier des fichiers.\n  </Accordion>\n\n  <Accordion title=\"Réviser le code généré\">\n    Révisez toujours le code généré par l'IA avant de le commiter dans votre projet.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Gemini CLI\"\ndescription: \"Configura Google Gemini CLI para usar la API de LemonData\"\n---\n\n## Resumen\n\nGoogle Gemini CLI es una herramienta de línea de comandos para interactuar con los modelos de Gemini. LemonData proporciona un endpoint compatible que te permite usar Gemini CLI con acceso a más de 300 modelos.\n\n## Requisitos del sistema\n\n- **Node.js**: Versión 20.0+\n- **SO**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+ o Debian 10+\n\n## Instalación\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nVerifica la instalación:\n\n```bash\ngemini --version\n```\n\n## Configuración\n\n### Paso 1: Obtén tu API Key\n\n1. Inicia sesión en el [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navega a [API Keys](https://lemondata.cc/dashboard/api)\n3. Crea y copia tu API key (formato: `sk-...`)\n\n### Paso 2: Configura las variables de entorno\n\n**Temporal (sesión actual):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Configuración permanente:**\n\nAñade a tu archivo de configuración de shell:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nReinicia tu terminal después de la configuración para que los cambios surtan efecto.\n</Warning>\n\n<Note>\n**Limitación conocida**: En algunos casos, Gemini CLI puede no respetar `GOOGLE_GEMINI_BASE_URL` si existe una sesión de Google en caché. Si experimentas problemas de conexión, intenta limpiar la caché de la CLI o iniciar una nueva sesión de terminal.\n</Note>\n\n## Uso",
      "pt": "---\ntitle: \"Gemini CLI\"\ndescription: \"Configure o Google Gemini CLI para usar a API da LemonData\"\n---\n\n## Visão Geral\n\nO Google Gemini CLI é uma ferramenta de linha de comando para interagir com os modelos Gemini. A LemonData fornece um endpoint compatível que permite usar o Gemini CLI com acesso a mais de 300 modelos.\n\n## Requisitos do Sistema\n\n- **Node.js**: Versão 20.0+\n- **SO**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+, ou Debian 10+\n\n## Instalação\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nVerifique a instalação:\n\n```bash\ngemini --version\n```\n\n## Configuração\n\n### Passo 1: Obtenha sua Chave de API\n\n1. Faça login no [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navegue até [API Keys](https://lemondata.cc/dashboard/api)\n3. Crie e copie sua chave de API (formato: `sk-...`)\n\n### Passo 2: Defina as Variáveis de Ambiente\n\n**Temporário (sessão atual):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Configuração permanente:**\n\nAdicione ao seu arquivo de configuração do shell:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nReinicie seu terminal após a configuração para que as alterações entrem em vigor.\n</Warning>\n\n<Note>\n**Limitação conhecida**: Em alguns casos, o Gemini CLI pode não respeitar a `GOOGLE_GEMINI_BASE_URL` se houver uma sessão do Google em cache. Se você tiver problemas de conexão, tente limpar o cache do CLI ou iniciar uma nova sessão de terminal.\n</Note>\n\n## Uso Básico\n\nInicie o Gemini CLI a partir do diretório do seu projeto:\n\n```bash\ncd your-project\ngemini\n```\n\nNa primeira execução, você irá:\n1. Escolher um tema\n2. Confirmar o aviso de segurança\n3. Confiar no diretório de trabalho\n\n## Modelos Disponíveis\n\n| Modelo | Descrição |\n|-------|-------------|\n| `gemini-2.5-pro` | O modelo Gemini mais capaz |\n| `gemini-2.5-flash` | Rápido e eficiente para a maioria das tarefas |\n| `gemini-2.0-flash` | Geração anterior, estável |\n\n## Comandos Comuns\n\n**Faça uma pergunta:**\n\n```\n> What is the best way to structure a React app?\n```\n\n**Analise código:**\n\n```\n> Explain the code in src/main.ts\n```\n\n**Gere código:**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**Revise alterações:**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## Verificar Configuração\n\n```bash\n# Verificar variáveis de ambiente\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# Testar conexão\ngemini\n```\n\n## Solução de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Erro de Conexão\">\n    - Verifique se `GOOGLE_GEMINI_BASE_URL` está definida como `https://api.lemondata.cc`\n    - Nota: Sem o sufixo `/v1` para o endpoint do Gemini\n    - Verifique a conectividade de rede\n  </Accordion>\n\n  <Accordion title=\"Falha na Autenticação\">\n    - Verifique se a variável de ambiente `GEMINI_API_KEY` está definida\n    - Verifique se a chave começa com `sk-`\n    - Certifique-se de que a chave está ativa no dashboard da LemonData\n  </Accordion>\n\n  <Accordion title=\"Modelo Não Disponível\">\n    - Verifique a disponibilidade dos modelos Gemini em [lemondata.cc/en/models](https://lemondata.cc/pt/models)\n    - Tente uma variante de modelo Gemini diferente\n  </Accordion>\n</AccordionGroup>\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Use no diretório do projeto\">\n    Sempre execute o Gemini CLI a partir da raiz do seu projeto para uma melhor compreensão do contexto.\n  </Accordion>\n\n  <Accordion title=\"Confie em diretórios com cuidado\">\n    Confie apenas em diretórios de sua propriedade. O Gemini CLI pode ler e modificar arquivos.\n  </Accordion>\n\n  <Accordion title=\"Revise o código gerado\">\n    Sempre revise o código gerado por IA antes de fazer o commit no seu projeto.\n  </Accordion>\n</AccordionGroup>",
      "ar": "\".\n\n    *   *Specific check on \"review generated code\":* \"مراجعة الكود المولد\".\n\n    *   *Specific check on \"project root\":* \"جذر المشروع\".\n\n    *   *Specific check on \"context understanding\":* \"فهم السياق\".\n\n    *   *Specific check on \"own\":* \"تملكها\".\n\n    *   *Specific check on \"read and modify\":* \"قراءة وتعديل\".\n\n    *   *Specific check on \"AI-generated\":* \"المولد بواسطة الذكاء الاصطناعي\".\n\n    *   *Specific check on \"committing\":* \"اعتماده\".\n\n    *   *Specific check on \"v1 suffix\":* \"لاحقة /v1\".\n\n    *   *Specific check on \"Gemini endpoint\":* \"نقطة نهاية Gemini\".\n\n    *   *Specific check on \"LemonData Dashboard\":* \"لوحة تحكم LemonData\".\n\n    *   *Specific check on \"API Keys\":* \"مفاتيح API\".\n\n    *   *Specific check on \"format\":* \"التنسيق\".\n\n    *   *Specific check on \"shell configuration file\":* \"ملف تكوين الصدفة (shell)\".\n\n    *   *Specific check on \"restart your terminal\":* \"أعد تشغيل الجهاز الطرفي (terminal)\".\n\n    *   *Specific check on \"changes to take effect\":* \"لتفعيل التغييرات\".\n\n    *   *Specific check on \"known limitation\":* \"قيد معروف\".\n\n    *   *Specific check on \"cached Google session\":* \"جلسة Google مخزنة مؤقتاً\".\n\n    *   *Specific check on \"connection issues\":* \"مشكلات في الاتصال\".\n\n    *   *Specific check on \"clearing the CLI cache\":* \"مسح ذاكرة التخزين المؤقت لـ CLI\".\n\n    *   *Specific check on \"fresh terminal session\":* \"جلسة terminal جديدة\".\n\n    *   *Specific check on \"start Gemini CLI\":* \"ابدأ Gemini CLI\".\n\n    *   *Specific check on \"project directory\":* \"دليل المشروع\".\n\n    *   *Specific check on \"first run\":* \"التشغيل الأول\".\n\n    *   *Specific check on \"choose a theme\":* \"اختيار سمة (theme)\".\n\n    *   *Specific check on \"confirm safety notice\":* \"تأكيد إشعار السلامة\".\n\n    *   *Specific check on \"trust the working directory\":* \"الوثوق بدليل العمل\".\n\n    *   *Specific check on \"most capable\":* \"الأكثر قدرة\".\n\n    *   *Specific check on \"fast, efficient\":* \"سريع وفعال\".\n\n    *   *Specific check on \"previous generation\":* \"الجيل السابق\".\n\n    *   *Specific check on \"stable\":* \"مستقر\".\n\n    *   *Specific check on \"ask a question\":* \"طرح سؤال\".\n\n    *   *Specific check",
      "vi": "---\ntitle: \"Gemini CLI\"\ndescription: \"Cấu hình Google Gemini CLI để sử dụng LemonData API\"\n---\n\n## Tổng quan\n\nGoogle Gemini CLI là một công cụ dòng lệnh để tương tác với các mô hình Gemini. LemonData cung cấp một endpoint tương thích cho phép bạn sử dụng Gemini CLI với quyền truy cập vào hơn 300 mô hình.\n\n## Yêu cầu hệ thống\n\n- **Node.js**: Phiên bản 20.0+\n- **Hệ điều hành**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+, hoặc Debian 10+\n\n## Cài đặt\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nXác minh cài đặt:\n\n```bash\ngemini --version\n```\n\n## Cấu hình\n\n### Bước 1: Lấy API Key của bạn\n\n1. Đăng nhập vào [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Đi tới [API Keys](https://lemondata.cc/dashboard/api)\n3. Tạo và sao chép API key của bạn (định dạng: `sk-...`)\n\n### Bước 2: Thiết lập biến môi trường\n\n**Tạm thời (phiên hiện tại):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Cấu hình vĩnh viễn:**\n\nThêm vào tệp cấu hình shell của bạn:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nKhởi động lại terminal của bạn sau khi cấu hình để các thay đổi có hiệu lực.\n</Warning>\n\n<Note>\n**Hạn chế đã biết**: Trong một số trường hợp, Gemini CLI có thể không tuân thủ `GOOGLE_GEMINI_BASE_URL` nếu có một phiên Google được lưu trong bộ nhớ cache. Nếu bạn gặp vấn đề về kết nối, hãy thử xóa bộ nhớ cache của CLI hoặc bắt đầu một phiên terminal mới.\n</Note>\n\n## Cách sử dụng cơ bản\n\nKhởi động Gemini CLI từ thư mục dự án của bạn:\n\n```bash\ncd your-project\ngemini\n```\n\nTrong lần chạy đầu tiên, bạn sẽ:\n1. Chọn một chủ đề (theme)\n2. Xác nhận thông báo an toàn\n3. Tin cậy thư mục làm việc\n\n## Các mô hình hiện có\n\n| Mô hình | Mô tả |\n|-------|-------------|\n| `gemini-2.5-pro` | Mô hình Gemini mạnh mẽ nhất |\n| `gemini-2.5-flash` | Nhanh, hiệu quả cho hầu hết các tác vụ |\n| `gemini-2.0-flash` | Thế hệ trước, ổn định |\n\n## Các lệnh phổ biến\n\n**Đặt câu hỏi:**\n\n```\n> What is the best way to structure a React app?\n```\n\n**Phân tích mã nguồn:**\n\n```\n> Explain the code in src/main.ts\n```\n\n**Tạo mã nguồn:**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**Xem lại các thay đổi:**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## Xác minh cấu hình\n\n```bash\n# Kiểm tra các biến môi trường\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# Kiểm tra kết nối\ngemini\n```\n\n## Xử lý sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Lỗi kết nối\">\n    - Xác minh `GOOGLE_GEMINI_BASE_URL` đã được đặt thành `https://api.lemondata.cc`\n    - Lưu ý: Không có hậu tố `/v1` cho Gemini endpoint\n    - Kiểm tra kết nối mạng\n  </Accordion>\n\n  <Accordion title=\"Xác thực thất bại\">\n    - Xác minh biến môi trường `GEMINI_API_KEY` đã được thiết lập\n    - Kiểm tra xem key có bắt đầu bằng `sk-` không\n    - Đảm bảo key đang hoạt động trong LemonData dashboard\n  </Accordion>\n\n  <Accordion title=\"Mô hình không khả dụng\">\n    - Kiểm tra tính khả dụng của các mô hình Gemini tại [lemondata.cc/en/models](https://lemondata.cc/vi/models)\n    - Thử một biến thể mô hình Gemini khác\n  </Accordion>\n</AccordionGroup>\n\n## Thực hành tốt nhất\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng trong thư mục dự án\">\n    Luôn chạy Gemini CLI từ thư mục gốc của dự án để hiểu ngữ cảnh tốt hơn.\n  </Accordion>\n\n  <Accordion title=\"Tin cậy thư mục một cách cẩn thận\">\n    Chỉ tin cậy các thư mục bạn sở hữu. Gemini CLI có thể đọc và sửa đổi các tệp.\n  </Accordion>\n\n  <Accordion title=\"Xem lại mã được tạo\">\n    Luôn xem lại mã do AI tạo trước khi commit vào dự án của bạn.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Gemini CLI\"\ndescription: \"Konfigurasikan Google Gemini CLI untuk menggunakan LemonData API\"\n---\n\n## Ringkasan\n\nGoogle Gemini CLI adalah alat baris perintah (command-line tool) untuk berinteraksi dengan model Gemini. LemonData menyediakan endpoint yang kompatibel yang memungkinkan Anda menggunakan Gemini CLI dengan akses ke 300+ model.\n\n## Persyaratan Sistem\n\n- **Node.js**: Versi 20.0+\n- **OS**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+, atau Debian 10+\n\n## Instalasi\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nVerifikasi instalasi:\n\n```bash\ngemini --version\n```\n\n## Konfigurasi\n\n### Langkah 1: Dapatkan API Key Anda\n\n1. Masuk ke [LemonData Dashboard](https://lemondata.cc/dashboard)\n2. Navigasi ke [API Keys](https://lemondata.cc/dashboard/api)\n3. Buat dan salin API key Anda (format: `sk-...`)\n\n### Langkah 2: Atur Environment Variables\n\n**Sementara (sesi saat ini):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Konfigurasi permanen:**\n\nTambahkan ke file konfigurasi shell Anda:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nMulai ulang terminal Anda setelah konfigurasi agar perubahan dapat diterapkan.\n</Warning>\n\n<Note>\n**Batasan yang diketahui**: Dalam beberapa kasus, Gemini CLI mungkin tidak mematuhi `GOOGLE_GEMINI_BASE_URL` jika terdapat sesi Google yang tersimpan di cache. Jika Anda mengalami masalah koneksi, coba hapus cache CLI atau mulai sesi terminal baru.\n</Note>\n\n## Penggunaan Dasar\n\n",
      "tr": "---\ntitle: \"Gemini CLI\"\ndescription: \"Google Gemini CLI'yı LemonData API kullanacak şekilde yapılandırın\"\n---\n\n## Genel Bakış\n\nGoogle Gemini CLI, Gemini modelleriyle etkileşim kurmak için kullanılan bir komut satırı aracıdır. LemonData, Gemini CLI'yı 300'den fazla modele erişimle kullanmanıza olanak tanıyan uyumlu bir uç nokta (endpoint) sağlar.\n\n## Sistem Gereksinimleri\n\n- **Node.js**: Sürüm 20.0+\n- **OS**: Windows 10/11, macOS 10.15+, Ubuntu 20.04+ veya Debian 10+\n\n## Kurulum\n\n```bash\nnpm install -g @google/gemini-cli\n```\n\nKurulumu doğrulayın:\n\n```bash\ngemini --version\n```\n\n## Yapılandırma\n\n### Adım 1: API Anahtarınızı Alın\n\n1. [LemonData Dashboard](https://lemondata.cc/dashboard) sayfasına giriş yapın\n2. [API Keys](https://lemondata.cc/dashboard/api) bölümüne gidin\n3. API anahtarınızı oluşturun ve kopyalayın (format: `sk-...`)\n\n### Adım 2: Ortam Değişkenlerini Ayarlayın\n\n**Geçici (mevcut oturum):**\n\n```bash\nexport GEMINI_API_KEY=\"sk-your-lemondata-key\"\nexport GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"\n```\n\n**Kalıcı yapılandırma:**\n\nKabuk (shell) yapılandırma dosyanıza ekleyin:\n\n<Tabs>\n  <Tab title=\"Bash\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.bashrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.bashrc\n    source ~/.bashrc\n    ```\n  </Tab>\n  <Tab title=\"Zsh\">\n    ```bash\n    echo 'export GEMINI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\n    echo 'export GOOGLE_GEMINI_BASE_URL=\"https://api.lemondata.cc\"' >> ~/.zshrc\n    source ~/.zshrc\n    ```\n  </Tab>\n  <Tab title=\"Fish\">\n    ```bash\n    set -Ux GEMINI_API_KEY \"sk-your-lemondata-key\"\n    set -Ux GOOGLE_GEMINI_BASE_URL \"https://api.lemondata.cc\"\n    ```\n  </Tab>\n</Tabs>\n\n<Warning>\nDeğişikliklerin etkili olması için yapılandırmadan sonra terminalinizi yeniden başlatın.\n</Warning>\n\n<Note>\n**Bilinen kısıtlama**: Bazı durumlarda, önbelleğe alınmış bir Google oturumu varsa Gemini CLI, `GOOGLE_GEMINI_BASE_URL` değişkenini dikkate almayabilir. Bağlantı sorunları yaşıyorsanız, CLI önbelleğini temizlemeyi veya yeni bir terminal oturumu başlatmayı deneyin.\n</Note>\n\n## Temel Kullanım\n\nGemini CLI'yı proje dizininizden başlatın:\n\n```bash\ncd your-project\ngemini\n```\n\nİlk çalıştırmada şunları yapacaksınız:\n1. Bir tema seçin\n2. Güvenlik bildirimini onaylayın\n3. Çalışma dizinine güvenin\n\n## Mevcut Modeller\n\n| Model | Açıklama |\n|-------|-------------|\n| `gemini-2.5-pro` | En yetenekli Gemini modeli |\n| `gemini-2.5-flash` | Çoğu görev için hızlı ve verimli |\n| `gemini-2.0-flash` | Önceki nesil, kararlı |\n\n## Yaygın Komutlar\n\n**Bir soru sorun:**\n\n```\n> What is the best way to structure a React app?\n```\n\n**Kodu analiz edin:**\n\n```\n> Explain the code in src/main.ts\n```\n\n**Kod oluşturun:**\n\n```\n> Create a Python function to parse JSON files\n```\n\n**Değişiklikleri gözden geçirin:**\n\n```\n> Review the recent git changes and suggest improvements\n```\n\n## Yapılandırmayı Doğrulayın\n\n```bash\n# Ortam değişkenlerini kontrol edin\necho $GEMINI_API_KEY\necho $GOOGLE_GEMINI_BASE_URL\n\n# Bağlantıyı test edin\ngemini\n```\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Bağlantı Hatası\">\n    - `GOOGLE_GEMINI_BASE_URL` değişkeninin `https://api.lemondata.cc` olarak ayarlandığını doğrulayın\n    - Not: Gemini uç noktası için `/v1` son eki yoktur\n    - Ağ bağlantısını kontrol edin\n  </Accordion>\n\n  <Accordion title=\"Kimlik Doğrulama Başarısız\">\n    - `GEMINI_API_KEY` ortam değişkeninin ayarlandığını doğrulayın\n    - Anahtarın `sk-` ile başladığını kontrol edin\n    - Anahtarın LemonData panelinde aktif olduğundan emin olun\n  </Accordion>\n\n  <Accordion title=\"Model Mevcut Değil\">\n    - Gemini modellerinin uygunluğunu [lemondata.cc/en/models](https://lemondata.cc/tr/models) adresinden kontrol edin\n    - Farklı bir Gemini model varyantı deneyin\n  </Accordion>\n</AccordionGroup>\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Proje dizininde kullanın\">\n    Daha iyi bağlam anlayışı için Gemini CLI'yı her zaman proje kök dizininizden çalıştırın.\n  </Accordion>\n\n  <Accordion title=\"Dizinlere dikkatle güvenin\">\n    Yalnızca size ait olan dizinlere güvenin. Gemini CLI dosyaları okuyabilir ve değiştirebilir.\n  </Accordion>\n\n  <Accordion title=\"Üretilen kodu gözden geçirin\">\n    Yapay zeka tarafından oluşturulan kodu projenize işlemeden (commit) önce her zaman gözden geçirin.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:38:20.986Z"
  },
  "integrations/langchain.mdx": {
    "sourceHash": "97344857bc9cdf88",
    "translations": {
      "zh": "---\ntitle: \"LangChain\"\ndescription: \"将 LemonData 与 LangChain 集成\"\n---\n\n## 概览\n\nLangChain 是一个用于构建 LLM 应用程序的流行框架。LemonData 与 LangChain 的 OpenAI 集成无缝协作。\n\n## 安装\n\n```bash\npip install langchain langchain-openai\n```\n\n## 基础配置\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## 使用不同模型\n\n访问任何 LemonData 模型：\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## 带有消息历史的对话\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## 流式传输\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## 异步用法\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## 链 (Chains)\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (检索增强生成)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## 智能体 (Agents)\n\n<Note>\n  LangChain 中的智能体 API 正在不断演进。对于新项目，请考虑使用 [LangGraph](https://python.langchain.com/docs/langgraph) 以获得更灵活的智能体架构。\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## 环境变量\n\n为了使代码更整洁，请使用环境变量：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Will automatically use environment variables\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## 回调与追踪\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"根据成本选择合适的模型\">\n    在链中的简单任务使用更便宜的模型 (GPT-4o-mini)。\n  </Accordion>\n\n  <Accordion title=\"实现重试机制\">\n    LangChain 针对瞬时错误内置了重试逻辑。\n  </Accordion>\n\n  <Accordion title=\"监控 token 使用情况\">\n    使用回调来追踪 token 消耗。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"LangChain\"\ndescription: \"將 LemonData 與 LangChain 整合\"\n---\n\n## 概覽\n\nLangChain 是一個用於構建 LLM 應用程式的熱門框架。LemonData 可與 LangChain 的 OpenAI 整合無縫協作。\n\n## 安裝\n\n```bash\npip install langchain langchain-openai\n```\n\n## 基礎配置\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## 使用不同的模型\n\n存取任何 LemonData 模型：\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## 使用訊息紀錄進行對話\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## 串流 (Streaming)\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## 非同步用法\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## 鏈 (Chains)\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (檢索增強生成)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## 代理 (Agents)\n\n<Note>\n  LangChain 中的 agent API 正在不斷演進。對於新專案，請考慮使用 [LangGraph](https://python.langchain.com/docs/langgraph) 以獲得更靈活的代理架構。\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## 環境變數\n\n為了使程式碼更簡潔，請使用環境變數：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Will automatically use environment variables\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## 回呼 (Callbacks) 與追蹤\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"根據成本選擇合適的模型\">\n    在鏈中使用較便宜的模型 (GPT-4o-mini) 處理簡單任務。\n  </Accordion>\n\n  <Accordion title=\"實作重試機制\">\n    LangChain 針對暫時性錯誤內建了重試邏輯。\n  </Accordion>\n\n  <Accordion title=\"監控 token 使用量\">\n    使用回呼 (callbacks) 來追蹤 token 消耗。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"LangChain\"\ndescription: \"LemonDataをLangChainと統合する\"\n---\n\n## 概要\n\nLangChainは、LLMアプリケーションを構築するための人気のあるフレームワークです。LemonDataは、LangChainのOpenAI統合とシームレスに動作します。\n\n## インストール\n\n```bash\npip install langchain langchain-openai\n```\n\n## 基本設定\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## 異なるモデルの使用\n\n任意のLemonDataモデルにアクセスします：\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## メッセージ履歴を使用したチャット\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## ストリーミング\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## 非同期の使用\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## チェーン\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (検索拡張生成)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# 埋め込み\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# ベクトルストアの作成\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAGチェーン\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## エージェント\n\n<Note>\n  LangChainのエージェントAPIは進化を続けています。新しいプロジェクトでは、より柔軟なエージェントアーキテクチャのために[LangGraph](https://python.langchain.com/docs/langgraph)の使用を検討してください。\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## 環境変数\n\nコードをよりクリーンにするために、環境変数を使用します：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# 自動的に環境変数が使用されます\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## コールバックとトレース\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"コストに適したモデルの使用\">\n    チェーン内の単純なタスクには、より安価なモデル（GPT-4o-mini）を使用してください。\n  </Accordion>\n\n  <Accordion title=\"リトライの実装\">\n    LangChainには、一時的なエラーのためのリトライロジックが組み込まれています。\n  </Accordion>\n\n  <Accordion title=\"トークン使用量の監視\">\n    コールバックを使用してトークンの消費を追跡します。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"LangChain\"\ndescription: \"LemonData를 LangChain과 통합하기\"\n---\n\n## 개요\n\nLangChain은 LLM 애플리케이션 구축을 위한 인기 있는 프레임워크입니다. LemonData는 LangChain의 OpenAI 통합과 원활하게 작동합니다.\n\n## 설치\n\n```bash\npip install langchain langchain-openai\n```\n\n## 기본 설정\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## 다양한 모델 사용하기\n\n모든 LemonData 모델에 액세스할 수 있습니다:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## 메시지 기록을 포함한 채팅\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## 스트리밍\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## 비동기 사용\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## 체인(Chains)\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (검색 증강 생성)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## 에이전트(Agents)\n\n<Note>\n  LangChain의 에이전트 API는 계속 발전하고 있습니다. 새로운 프로젝트의 경우, 더 유연한 에이전트 아키텍처를 위해 [LangGraph](https://python.langchain.com/docs/langgraph) 사용을 고려해 보세요.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## 환경 변수\n\n더 깔끔한 코드를 위해 환경 변수를 사용하세요:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# 환경 변수를 자동으로 사용합니다\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## 콜백 및 트레이싱\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## 모범 사례\n\n<AccordionGroup>\n  <Accordion title=\"비용에 적합한 모델 사용\">\n    체인의 단순한 작업에는 더 저렴한 모델(GPT-4o-mini)을 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"재시도 구현\">\n    LangChain에는 일시적인 오류에 대한 재시도 로직이 내장되어 있습니다.\n  </Accordion>\n\n  <Accordion title=\"토큰 사용량 모니터링\">\n    콜백을 사용하여 토큰 소비를 추적하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"LangChain\"\ndescription: \"Integrieren Sie LemonData mit LangChain\"\n---\n\n## Übersicht\n\nLangChain ist ein beliebtes Framework für die Erstellung von LLM-Anwendungen. LemonData arbeitet nahtlos mit der OpenAI-Integration von LangChain zusammen.\n\n## Installation\n\n```bash\npip install langchain langchain-openai\n```\n\n## Basiskonfiguration\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Verwendung verschiedener Modelle\n\nGreifen Sie auf jedes LemonData-Modell zu:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat mit Nachrichtenverlauf\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Streaming\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Asynchrone Nutzung\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Chains\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (Retrieval Augmented Generation)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## Agents\n\n<Note>\n  Die Agent-APIs in LangChain entwickeln sich ständig weiter. Erwägen Sie für neue Projekte die Verwendung von [LangGraph](https://python.langchain.com/docs/langgraph) für flexiblere Agent-Architekturen.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## Umgebungsvariablen\n\nVerwenden Sie Umgebungsvariablen für saubereren Code:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Will automatically use environment variables\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## Callbacks und Tracing\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Verwenden Sie kosteneffiziente Modelle\">\n    Verwenden Sie günstigere Modelle (GPT-4o-mini) für einfache Aufgaben in Chains.\n  </Accordion>\n\n  <Accordion title=\"Retries implementieren\">\n    LangChain verfügt über eine integrierte Retry-Logik für vorübergehende Fehler.\n  </Accordion>\n\n  <Accordion title=\"Token-Verbrauch überwachen\">\n    Verwenden Sie Callbacks, um den Token-Verbrauch zu verfolgen.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"LangChain\"\ndescription: \"Intégrer LemonData avec LangChain\"\n---\n\n## Aperçu\n\nLangChain est un framework populaire pour la création d'applications LLM. LemonData fonctionne parfaitement avec l'intégration OpenAI de LangChain.\n\n## Installation\n\n```bash\npip install langchain langchain-openai\n```\n\n## Configuration de base\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Utilisation de différents modèles\n\nAccédez à n'importe quel modèle LemonData :\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat avec historique des messages\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Streaming\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Utilisation asynchrone\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Chaînes\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (Retrieval Augmented Generation)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## Agents\n\n<Note>\n  Les API d'agent dans LangChain évoluent. Pour les nouveaux projets, envisagez d'utiliser [LangGraph](https://python.langchain.com/docs/langgraph) pour des architectures d'agent plus flexibles.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## Variables d'environnement\n\nPour un code plus propre, utilisez des variables d'environnement :\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Utilisera automatiquement les variables d'environnement\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## Callbacks et traçage\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Utiliser des modèles appropriés pour le coût\">\n    Utilisez des modèles moins chers (GPT-4o-mini) pour les tâches simples dans les chaînes.\n  </Accordion>\n\n  <Accordion title=\"Implémenter des retries\">\n    LangChain dispose d'une logique de retry intégrée pour les erreurs transitoires.\n  </Accordion>\n\n  <Accordion title=\"Surveiller l'utilisation des tokens\">\n    Utilisez des callbacks pour suivre la consommation de tokens.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"LangChain\"\ndescription: \"Integra LemonData con LangChain\"\n---\n\n## Descripción general\n\nLangChain es un framework popular para construir aplicaciones de LLM. LemonData funciona a la perfección con la integración de OpenAI de LangChain.\n\n## Instalación\n\n```bash\npip install langchain langchain-openai\n```\n\n## Configuración básica\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Uso de diferentes modelos\n\nAccede a cualquier modelo de LemonData:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat con historial de mensajes\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Streaming\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Uso asíncrono\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Cadenas (Chains)\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (Generación Aumentada por Recuperación)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## Agentes\n\n<Note>\n  Las API de agentes en LangChain están evolucionando. Para proyectos nuevos, considera usar [LangGraph](https://python.langchain.com/docs/langgraph) para arquitecturas de agentes más flexibles.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## Variables de entorno\n\nPara un código más limpio, utiliza variables de entorno:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Utilizará automáticamente las variables de entorno\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## Callbacks y Tracing\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## Mejores prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Usa modelos adecuados según el costo\">\n    Utiliza modelos más económicos (GPT-4o-mini) para tareas sencillas en cadenas.\n  </Accordion>\n\n  <Accordion title=\"Implementa reintentos\">\n    LangChain tiene lógica de reintento integrada para errores transitorios.\n  </Accordion>\n\n  <Accordion title=\"Monitorea el uso de tokens\">\n    Utiliza callbacks para rastrear el consumo de tokens.\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"LangChain\"\ndescription: \"Integre o LemonData com o LangChain\"\n---\n\n## Visão Geral\n\nO LangChain é um framework popular para a construção de aplicações de LLM. O LemonData funciona perfeitamente com a integração OpenAI do LangChain.\n\n## Instalação\n\n```bash\npip install langchain langchain-openai\n```\n\n## Configuração Básica\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Usando Diferentes Modelos\n\nAcesse qualquer modelo do LemonData:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat com Histórico de Mensagens\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Streaming\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Uso Assíncrono\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Chains\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"",
      "ar": "---\ntitle: \"LangChain\"\ndescription: \"دمج LemonData مع LangChain\"\n---\n\n## نظرة عامة\n\nتعد LangChain إطار عمل شائع لبناء تطبيقات LLM. تعمل LemonData بسلاسة مع تكامل OpenAI الخاص بـ LangChain.\n\n## التثبيت\n\n```bash\npip install langchain langchain-openai\n```\n\n## الإعداد الأساسي\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## استخدام نماذج مختلفة\n\nالوصول إلى أي نموذج من نماذج LemonData:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## الدردشة مع سجل الرسائل\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## البث (Streaming)\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## الاستخدام غير المتزامن (Async)\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## السلاسل (Chains)\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (التوليد المعزز بالاسترجاع)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## الوكلاء (Agents)\n\n<Note>\n  تتطور واجهات برمجة تطبيقات الوكلاء (agent APIs) في LangChain. بالنسبة للمشاريع الجديدة، فكر في استخدام [LangGraph](https://python.langchain.com/docs/langgraph) للحصول على بنيات وكلاء أكثر مرونة.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## متغيرات البيئة\n\nللحصول على كود أكثر نظافة، استخدم متغيرات البيئة:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Will automatically use environment variables\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## عمليات الاستدعاء (Callbacks) والتتبع\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"استخدم النماذج المناسبة للتكلفة\">\n    استخدم نماذج أرخص (GPT-4o-mini) للمهام البسيطة في السلاسل.\n  </Accordion>\n\n  <Accordion title=\"تنفيذ عمليات إعادة المحاولة\">\n    تمتلك LangChain منطق إعادة محاولة مدمج للأخطاء العابرة.\n  </Accordion>\n\n  <Accordion title=\"مراقبة استخدام الـ tokens\">\n    استخدم الـ callbacks لتتبع استهلاك الـ tokens.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"LangChain\"\ndescription: \"Tích hợp LemonData với LangChain\"\n---\n\n## Tổng quan\n\nLangChain là một framework phổ biến để xây dựng các ứng dụng LLM. LemonData hoạt động mượt mà với tích hợp OpenAI của LangChain.\n\n## Cài đặt\n\n```bash\npip install langchain langchain-openai\n```\n\n## Cấu hình cơ bản\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Sử dụng các mô hình khác nhau\n\nTruy cập bất kỳ mô hình LemonData nào:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat với lịch sử tin nhắn\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Streaming\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Sử dụng bất đồng bộ (Async)\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Chains\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (Retrieval Augmented Generation)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Create vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## Agents\n\n<Note>\n  Các API agent trong LangChain đang phát triển. Đối với các dự án mới, hãy cân nhắc sử dụng [LangGraph](https://python.langchain.com/docs/langgraph) để có kiến trúc agent linh hoạt hơn.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## Biến môi trường\n\nĐể mã nguồn sạch hơn, hãy sử dụng các biến môi trường:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Sẽ tự động sử dụng các biến môi trường\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## Callbacks và Tracing\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## Các thực hành tốt nhất\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng các mô hình phù hợp với chi phí\">\n    Sử dụng các mô hình rẻ hơn (GPT-4o-mini) cho các tác vụ đơn giản trong chains.\n  </Accordion>\n\n  <Accordion title=\"Triển khai cơ chế thử lại (retries)\">\n    LangChain có sẵn logic thử lại cho các lỗi tạm thời.\n  </Accordion>\n\n  <Accordion title=\"Theo dõi việc sử dụng token\">\n    Sử dụng callbacks để theo dõi mức tiêu thụ token.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"LangChain\"\ndescription: \"Integrasikan LemonData dengan LangChain\"\n---\n\n## Ikhtisar\n\nLangChain adalah framework populer untuk membangun aplikasi LLM. LemonData bekerja secara mulus dengan integrasi OpenAI milik LangChain.\n\n## Instalasi\n\n```bash\npip install langchain langchain-openai\n```\n\n## Konfigurasi Dasar\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Menggunakan Berbagai Model\n\nAkses model LemonData apa pun:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat dengan Riwayat Pesan\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Streaming\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Penggunaan Async\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Chains\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (Retrieval Augmented Generation)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Buat vector store\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG chain\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## Agents\n\n<Note>\n  API agent di LangChain terus berkembang. Untuk proyek baru, pertimbangkan untuk menggunakan [LangGraph](https://python.langchain.com/docs/langgraph) untuk arsitektur agent yang lebih fleksibel.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## Variabel Lingkungan\n\nUntuk kode yang lebih bersih, gunakan variabel lingkungan:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Akan secara otomatis menggunakan variabel lingkungan\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## Callback dan Tracing\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Gunakan model yang sesuai untuk biaya\">\n    Gunakan model yang lebih murah (GPT-4o-mini) untuk tugas-tugas sederhana dalam chains.\n  </Accordion>\n\n  <Accordion title=\"Implementasikan percobaan ulang (retries)\">\n    LangChain memiliki logika retry bawaan untuk kesalahan sementara.\n  </Accordion>\n\n  <Accordion title=\"Pantau penggunaan token\">\n    Gunakan callback untuk melacak konsumsi token.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"LangChain\"\ndescription: \"LemonData'yı LangChain ile entegre edin\"\n---\n\n## Genel Bakış\n\nLangChain, LLM uygulamaları oluşturmak için popüler bir çerçevedir (framework). LemonData, LangChain'in OpenAI entegrasyonu ile sorunsuz bir şekilde çalışır.\n\n## Kurulum\n\n```bash\npip install langchain langchain-openai\n```\n\n## Temel Yapılandırma\n\n```python\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = llm.invoke(\"Hello, how are you?\")\nprint(response.content)\n```\n\n## Farklı Modelleri Kullanma\n\nHerhangi bir LemonData modeline erişin:\n\n```python\n# OpenAI GPT-4o\ngpt4 = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude\nclaude = ChatOpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = ChatOpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# DeepSeek\ndeepseek = ChatOpenAI(\n    model=\"deepseek-r1\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Mesaj Geçmişi ile Sohbet\n\n```python\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\")\n]\n\nresponse = llm.invoke(messages)\nprint(response.content)\n```\n\n## Akış (Streaming)\n\n```python\nfor chunk in llm.stream(\"Write a poem about coding\"):\n    print(chunk.content, end=\"\", flush=True)\n```\n\n## Asenkron Kullanım\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.ainvoke(\"Hello!\")\n    print(response.content)\n\nasyncio.run(main())\n```\n\n## Zincirler (Chains)\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n    (\"human\", \"{text}\")\n])\n\nchain = prompt | llm | StrOutputParser()\n\nresult = chain.invoke({\n    \"input_language\": \"English\",\n    \"output_language\": \"French\",\n    \"text\": \"Hello, how are you?\"\n})\nprint(result)\n```\n\n## RAG (Geri Getirme ile Artırılmış Üretim)\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Embeddings\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\n# Vektör deposu oluştur\ntexts = [\"LemonData supports 300+ AI models\", \"API is OpenAI compatible\"]\nvectorstore = FAISS.from_texts(texts, embeddings)\nretriever = vectorstore.as_retriever()\n\n# RAG zinciri\ntemplate = \"\"\"Answer based on context:\n{context}\n\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\nresponse = rag_chain.invoke(\"How many models does LemonData support?\")\nprint(response.content)\n```\n\n## Ajanlar (Agents)\n\n<Note>\n  LangChain'deki ajan API'ları gelişmeye devam ediyor. Yeni projeler için daha esnek ajan mimarileri sunan [LangGraph](https://python.langchain.com/docs/langgraph) kullanmayı düşünebilirsiniz.\n</Note>\n\n```python\nfrom langchain.agents import create_openai_tools_agent, AgentExecutor\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Search results for: {query}\"\n\ntools = [search]\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant with access to tools.\"),\n    (\"human\", \"{input}\"),\n    (\"placeholder\", \"{agent_scratchpad}\")\n])\n\nagent = create_openai_tools_agent(llm, tools, prompt)\nexecutor = AgentExecutor(agent=agent, tools=tools)\n\nresult = executor.invoke({\"input\": \"Search for LemonData pricing\"})\nprint(result[\"output\"])\n```\n\n## Ortam Değişkenleri\n\nDaha temiz bir kod için ortam değişkenlerini kullanın:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Otomatik olarak ortam değişkenlerini kullanacaktır\nllm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n## Geri Çağırmalar (Callbacks) ve İzleme\n\n```python\nfrom langchain_core.callbacks import StdOutCallbackHandler\n\nllm = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    base_url=\"https://api.lemondata.cc/v1\",\n    callbacks=[StdOutCallbackHandler()]\n)\n```\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Maliyet için uygun modelleri kullanın\">\n    Zincirlerdeki basit görevler için daha ucuz modeller (GPT-4o-mini) kullanın.\n  </Accordion>\n\n  <Accordion title=\"Yeniden denemeleri (retries) uygulayın\">\n    LangChain, geçici hatalar için yerleşik yeniden deneme mantığına sahiptir.\n  </Accordion>\n\n  <Accordion title=\"Token kullanımını izleyin\">\n    Token tüketimini takip etmek için geri çağırmaları (callbacks) kullanın.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:38:39.006Z"
  },
  "integrations/llamaindex.mdx": {
    "sourceHash": "a3780f6ff6fe39fe",
    "translations": {
      "zh": "---\ntitle: \"LlamaIndex\"\ndescription: \"将 LemonData 与 LlamaIndex 集成以构建 RAG 应用\"\n---\n\n## 概览\n\nLlamaIndex 是一个用于 LLM 应用的数据框架，在构建 RAG（检索增强生成）系统方面功能尤为强大。LemonData 可以与 LlamaIndex 的 OpenAI 集成无缝协作。\n\n## 安装\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## 基础配置\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## 使用不同模型\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## 对话接口\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## 流式传输\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## 使用文档构建 RAG\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## 对话引擎\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## 异步用法\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## 环境变量\n\n为了使代码更整洁，请使用环境变量：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"选择合适的模型\">\n    对于嵌入和摘要任务，请使用速度更快的模型 (GPT-4o-mini)；对于最终回答，请保留性能更强大的模型 (GPT-4o, Claude)。\n  </Accordion>\n\n  <Accordion title=\"优化分块大小\">\n    根据您的文档类型调整分块大小。对于密集的技术文档，使用较小的分块；对于叙述性内容，使用较大的分块。\n  </Accordion>\n\n  <Accordion title=\"使用缓存\">\n    启用 LlamaIndex 缓存，以避免在开发过程中产生冗余的 API 调用。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"LlamaIndex\"\ndescription: \"將 LemonData 與 LlamaIndex 整合以用於 RAG 應用程式\"\n---\n\n## 概覽\n\nLlamaIndex 是一個用於 LLM 應用程式的資料框架，在構建 RAG（Retrieval Augmented Generation）系統方面特別強大。LemonData 可與 LlamaIndex 的 OpenAI 整合無縫運作。\n\n## 安裝\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## 基本配置\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## 使用不同模型\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## 對話介面\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## 串流\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## 使用文件的 RAG\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## 對話引擎\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## 非同步用法\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## 環境變數\n\n為了使程式碼更簡潔，請使用環境變數：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"選擇合適的模型\">\n    將較快的模型 (GPT-4o-mini) 用於 Embedding 和摘要任務，將強大的模型 (GPT-4o, Claude) 留給最終回應。\n  </Accordion>\n\n  <Accordion title=\"優化區塊大小\">\n    根據您的文件類型調整區塊大小 (chunk size)。對於密集的技術文件使用較小的區塊，對於敘述性內容則使用較大的區塊。\n  </Accordion>\n\n  <Accordion title=\"使用快取\">\n    啟用 LlamaIndex 快取功能，以避免在開發過程中產生多餘的 API 呼叫。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"LlamaIndex\"\ndescription: \"LemonDataをLlamaIndexと統合してRAGアプリケーションを構築する\"\n---\n\n## 概要\n\nLlamaIndexはLLMアプリケーションのためのデータフレームワークであり、特にRAG（検索拡張生成）システムの構築に強力です。LemonDataはLlamaIndexのOpenAI統合とシームレスに動作します。\n\n## インストール\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## 基本設定\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## 異なるモデルの使用\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## チャットインターフェース\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## ストリーミング\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## エンベディング\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## ドキュメントを使用したRAG\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## チャットエンジン\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## 非同期利用\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## 環境変数\n\nコードをよりクリーンにするために、環境変数を使用してください：\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"適切なモデルの選択\">\n    エンベディングや要約タスクには高速なモデル（`gpt-4o-mini`）を使用し、最終的な回答には強力なモデル（`gpt-4o`、`claude`）を予約してください。\n  </Accordion>\n\n  <Accordion title=\"チャンクサイズの最適化\">\n    ドキュメントの種類に基づいてチャンクサイズを調整してください。密度の高い技術文書には小さなチャンクを、物語的なコンテンツには大きなチャンクを使用します。\n  </Accordion>\n\n  <Accordion title=\"キャッシュの使用\">\n    開発中の冗長なAPI呼び出しを避けるために、LlamaIndexのキャッシュを有効にしてください。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"LlamaIndex\"\ndescription: \"RAG 애플리케이션을 위해 LemonData를 LlamaIndex와 통합하기\"\n---\n\n## 개요\n\nLlamaIndex는 LLM 애플리케이션을 위한 데이터 프레임워크로, 특히 RAG(Retrieval Augmented Generation) 시스템 구축에 강력합니다. LemonData는 LlamaIndex의 OpenAI 통합과 원활하게 작동합니다.\n\n## 설치\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## 기본 설정\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## 다양한 모델 사용하기\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## 채팅 인터페이스\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## 스트리밍\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## 임베딩\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## 문서를 활용한 RAG\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## 채팅 엔진\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## 비동기 사용\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## 환경 변수\n\n더 깔끔한 코드를 위해 환경 변수를 사용하세요:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"적절한 모델 선택하기\">\n    임베딩 및 요약 작업에는 더 빠른 모델(GPT-4o-mini)을 사용하고, 최종 응답에는 강력한 모델(GPT-4o, Claude)을 할당하세요.\n  </Accordion>\n\n  <Accordion title=\"청크 크기 최적화\">\n    문서 유형에 따라 청크 크기를 조정하세요. 조밀한 기술 문서에는 작은 청크를, 서사적인 콘텐츠에는 큰 청크를 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"캐싱 사용\">\n    개발 중 불필요한 API 호출을 방지하려면 LlamaIndex 캐싱을 활성화하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"LlamaIndex\"\ndescription: \"Integrieren Sie LemonData mit LlamaIndex für RAG-Anwendungen\"\n---\n\n## Übersicht\n\nLlamaIndex ist ein Daten-Framework für LLM-Anwendungen, das besonders leistungsstark für die Erstellung von RAG-Systemen (Retrieval Augmented Generation) ist. LemonData arbeitet nahtlos mit der OpenAI-Integration von LlamaIndex zusammen.\n\n## Installation\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Basiskonfiguration\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Verwendung verschiedener Modelle\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Chat-Schnittstelle\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## RAG mit Dokumenten\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## Chat-Engine\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## Asynchrone Nutzung\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## Umgebungsvariablen\n\nFür saubereren Code verwenden Sie Umgebungsvariablen:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Wählen Sie das richtige Modell\">\n    Verwenden Sie schnellere Modelle (GPT-4o-mini) für Embedding- und Zusammenfassungsaufgaben, reservieren Sie leistungsstarke Modelle (GPT-4o, Claude) für die finalen Antworten.\n  </Accordion>\n\n  <Accordion title=\"Chunk-Größe optimieren\">\n    Passen Sie die Chunk-Größe basierend auf Ihren Dokumenttypen an. Kleinere Chunks für dichte technische Dokumente, größere für erzählende Inhalte.\n  </Accordion>\n\n  <Accordion title=\"Caching verwenden\">\n    Aktivieren Sie das LlamaIndex-Caching, um redundante API-Aufrufe während der Entwicklung zu vermeiden.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"LlamaIndex\"\ndescription: \"Intégrez LemonData avec LlamaIndex pour les applications RAG\"\n---\n\n## Aperçu\n\nLlamaIndex est un framework de données pour les applications LLM, particulièrement puissant pour la construction de systèmes RAG (Retrieval Augmented Generation). LemonData fonctionne de manière fluide avec l'intégration OpenAI de LlamaIndex.\n\n## Installation\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Configuration de base\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Utilisation de différents modèles\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Interface de chat\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## RAG avec des documents\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## Moteur de chat\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## Utilisation asynchrone\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## Variables d'environnement\n\nPour un code plus propre, utilisez des variables d'environnement :\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Choisir le bon modèle\">\n    Utilisez des modèles plus rapides (GPT-4o-mini) pour les tâches d'embedding et de résumé, réservez les modèles puissants (GPT-4o, Claude) pour les réponses finales.\n  </Accordion>\n\n  <Accordion title=\"Optimiser la taille des segments\">\n    Ajustez la taille des segments (chunk size) en fonction de vos types de documents. Des segments plus petits pour les documents techniques denses, plus grands pour le contenu narratif.\n  </Accordion>\n\n  <Accordion title=\"Utiliser la mise en cache\">\n    Activez la mise en cache de LlamaIndex pour éviter les appels API redondants pendant le développement.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"LlamaIndex\"\ndescription: \"Integra LemonData con LlamaIndex para aplicaciones RAG\"\n---\n\n## Descripción general\n\nLlamaIndex es un framework de datos para aplicaciones de LLM, especialmente potente para construir sistemas RAG (Retrieval Augmented Generation). LemonData funciona a la perfección con la integración de OpenAI de LlamaIndex.\n\n## Instalación\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Configuración básica\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Uso de diferentes modelos\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Interfaz de chat\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model =",
      "pt": "---\ntitle: \"LlamaIndex\"\ndescription: \"Integre o LemonData com o LlamaIndex para aplicações RAG\"\n---\n\n## Visão Geral\n\nO LlamaIndex é um framework de dados para aplicações de LLM, especialmente poderoso para a construção de sistemas RAG (Retrieval Augmented Generation). O LemonData funciona perfeitamente com a integração OpenAI do LlamaIndex.\n\n## Instalação\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Configuração Básica\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Usando Diferentes Modelos\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Interface de Chat\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## RAG com Documentos\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## Chat Engine\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## Uso Assíncrono\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## Variáveis de Ambiente\n\nPara um código mais limpo, use variáveis de ambiente:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Escolha o modelo certo\">\n    Use modelos mais rápidos (GPT-4o-mini) para tarefas de embedding e sumarização, reserve modelos poderosos (GPT-4o, Claude) para as respostas finais.\n  </Accordion>\n\n  <Accordion title=\"Otimize o tamanho do chunk\">\n    Ajuste o tamanho do chunk com base nos seus tipos de documento. Chunks menores para documentos técnicos densos, maiores para conteúdo narrativo.\n  </Accordion>\n\n  <Accordion title=\"Use cache\">\n    Habilite o cache do LlamaIndex para evitar chamadas de API redundantes durante o desenvolvimento.\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"LlamaIndex\"\ndescription: \"دمج LemonData مع LlamaIndex لتطبيقات RAG\"\n---\n\n## نظرة عامة\n\nتعد LlamaIndex إطار عمل للبيانات لتطبيقات LLM، وهي قوية بشكل خاص لبناء أنظمة RAG (Retrieval Augmented Generation). تعمل LemonData بسلاسة مع تكامل OpenAI الخاص بـ LlamaIndex.\n\n## التثبيت\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## التكوين الأساسي\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## استخدام نماذج مختلفة\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## واجهة الدردشة\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## البث (Streaming)\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## التضمينات (Embeddings)\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## RAG مع المستندات\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## محرك الدردشة (Chat Engine)\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## الاستخدام غير المتزامن (Async)\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## متغيرات البيئة\n\nللحصول على كود أكثر تنظيماً، استخدم متغيرات البيئة:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"اختر النموذج المناسب\">\n    استخدم النماذج الأسرع (GPT-4o-mini) لمهام التضمين والتلخيص، واحجز النماذج القوية (GPT-4o، Claude) للاستجابات النهائية.\n  </Accordion>\n\n  <Accordion title=\"تحسين حجم الأجزاء (Chunk size)\">\n    اضبط حجم الأجزاء بناءً على أنواع مستنداتك. استخدم أجزاء أصغر للمستندات التقنية الكثيفة، وأجزاء أكبر للمحتوى السردي.\n  </Accordion>\n\n  <Accordion title=\"استخدام التخزين المؤقت (Caching)\">\n    قم بتمكين التخزين المؤقت في LlamaIndex لتجنب استدعاءات API المتكررة أثناء التطوير.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"LlamaIndex\"\ndescription: \"Tích hợp LemonData với LlamaIndex cho các ứng dụng RAG\"\n---\n\n## Tổng quan\n\nLlamaIndex là một framework dữ liệu cho các ứng dụng LLM, đặc biệt mạnh mẽ trong việc xây dựng các hệ thống RAG (Retrieval Augmented Generation). LemonData hoạt động mượt mà với tích hợp OpenAI của LlamaIndex.\n\n## Cài đặt\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Cấu hình cơ bản\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Sử dụng các mô hình khác nhau\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Giao diện Chat\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## RAG với Tài liệu\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## Chat Engine\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## Sử dụng Bất đồng bộ (Async)\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## Biến môi trường\n\nĐể mã nguồn sạch hơn, hãy sử dụng các biến môi trường:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## Các phương pháp hay nhất (Best Practices)\n\n<AccordionGroup>\n  <Accordion title=\"Chọn mô hình phù hợp\">\n    Sử dụng các mô hình nhanh hơn (GPT-4o-mini) cho các tác vụ embedding và tóm tắt, dành các mô hình mạnh mẽ (GPT-4o, Claude) cho các phản hồi cuối cùng.\n  </Accordion>\n\n  <Accordion title=\"Tối ưu hóa kích thước đoạn (chunk size)\">\n    Điều chỉnh kích thước đoạn dựa trên loại tài liệu của bạn. Các đoạn nhỏ hơn cho tài liệu kỹ thuật dày đặc, các đoạn lớn hơn cho nội dung tự sự.\n  </Accordion>\n\n  <Accordion title=\"Sử dụng bộ nhớ đệm (caching)\">\n    Bật tính năng caching của LlamaIndex để tránh các lệnh gọi API dư thừa trong quá trình phát triển.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"LlamaIndex\"\ndescription: \"Integrasikan LemonData dengan LlamaIndex untuk aplikasi RAG\"\n---\n\n## Ikhtisar\n\nLlamaIndex adalah framework data untuk aplikasi LLM, yang sangat kuat untuk membangun sistem RAG (Retrieval Augmented Generation). LemonData bekerja secara mulus dengan integrasi OpenAI dari LlamaIndex.\n\n## Instalasi\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Konfigurasi Dasar\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Menggunakan Berbagai Model\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Antarmuka Chat\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## RAG dengan Dokumen\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## Chat Engine\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## Penggunaan Async\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## Variabel Lingkungan\n\nUntuk kode yang lebih bersih, gunakan variabel lingkungan:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Pilih model yang tepat\">\n    Gunakan model yang lebih cepat (GPT-4o-mini) untuk tugas embedding dan peringkasan, simpan model yang kuat (GPT-4o, Claude) untuk respons akhir.\n  </Accordion>\n\n  <Accordion title=\"Optimalkan ukuran chunk\">\n    Sesuaikan ukuran chunk berdasarkan jenis dokumen Anda. Chunk yang lebih kecil untuk dokumen teknis yang padat, lebih besar untuk konten naratif.\n  </Accordion>\n\n  <Accordion title=\"Gunakan caching\">\n    Aktifkan caching LlamaIndex untuk menghindari panggilan API yang redundan selama pengembangan.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"LlamaIndex\"\ndescription: \"RAG uygulamaları için LemonData'yı LlamaIndex ile entegre edin\"\n---\n\n## Genel Bakış\n\nLlamaIndex, LLM uygulamaları için bir veri çerçevesidir ve özellikle RAG (Retrieval Augmented Generation) sistemleri oluşturmak için oldukça güçlüdür. LemonData, LlamaIndex'in OpenAI entegrasyonu ile sorunsuz bir şekilde çalışır.\n\n## Kurulum\n\n```bash\npip install llama-index llama-index-llms-openai llama-index-embeddings-openai\n```\n\n## Temel Yapılandırma\n\n```python\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure LLM\nllm = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.llm = llm\n\n# Simple query\nresponse = llm.complete(\"What is LemonData?\")\nprint(response.text)\n```\n\n## Farklı Modelleri Kullanma\n\n```python\n# OpenAI GPT-4o\ngpt4 = OpenAI(\n    model=\"gpt-4o\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Anthropic Claude (via OpenAI-compatible endpoint)\nclaude = OpenAI(\n    model=\"claude-sonnet-4-5\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Google Gemini\ngemini = OpenAI(\n    model=\"gemini-2.5-flash\",\n    api_key=\"sk-your-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n```\n\n## Sohbet Arayüzü\n\n```python\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n    ChatMessage(role=\"user\", content=\"What is the capital of France?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n```\n\n## Streaming\n\n```python\n# Streaming completion\nfor chunk in llm.stream_complete(\"Write a poem about AI\"):\n    print(chunk.delta, end=\"\", flush=True)\n\n# Streaming chat\nfor chunk in llm.stream_chat(messages):\n    print(chunk.delta, end=\"\", flush=True)\n```\n\n## Embeddings\n\n```python\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=\"sk-your-lemondata-key\",\n    api_base=\"https://api.lemondata.cc/v1\"\n)\n\n# Set as default\nSettings.embed_model = embed_model\n\n# Get embeddings\nembeddings = embed_model.get_text_embedding(\"Hello, world!\")\nprint(f\"Embedding dimension: {len(embeddings)}\")\n```\n\n## Belgelerle RAG\n\n```python\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Configure settings\nSettings.llm = llm\nSettings.embed_model = embed_model\n\n# Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# Query\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is in my documents?\")\nprint(response)\n```\n\n## Sohbet Motoru (Chat Engine)\n\n```python\n# Create chat engine with memory\nchat_engine = index.as_chat_engine(chat_mode=\"condense_question\")\n\n# Multi-turn conversation\nresponse = chat_engine.chat(\"What is LemonData?\")\nprint(response)\n\nresponse = chat_engine.chat(\"How many models does it support?\")\nprint(response)\n```\n\n## Asenkron Kullanım\n\n```python\nimport asyncio\n\nasync def main():\n    response = await llm.acomplete(\"Hello!\")\n    print(response.text)\n\nasyncio.run(main())\n```\n\n## Ortam Değişkenleri\n\nDaha temiz bir kod için ortam değişkenlerini kullanın:\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport OPENAI_API_BASE=\"https://api.lemondata.cc/v1\"\n```\n\n```python\nfrom llama_index.llms.openai import OpenAI\n\n# Will automatically use environment variables\nllm = OpenAI(model=\"gpt-4o\")\n```\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Doğru modeli seçin\">\n    Embedding ve özetleme görevleri için daha hızlı modelleri (GPT-4o-mini) kullanın, güçlü modelleri (GPT-4o, Claude) nihai yanıtlar için ayırın.\n  </Accordion>\n\n  <Accordion title=\"Parça (chunk) boyutunu optimize edin\">\n    Parça boyutunu belge türlerinize göre ayarlayın. Yoğun teknik belgeler için daha küçük parçalar, anlatı içerikleri için daha büyük parçalar kullanın.\n  </Accordion>\n\n  <Accordion title=\"Önbelleğe almayı kullanın\">\n    Geliştirme sırasında gereksiz API çağrılarından kaçınmak için LlamaIndex önbelleğe alma (caching) özelliğini etkinleştirin.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:39:58.362Z"
  },
  "integrations/openai-sdk.mdx": {
    "sourceHash": "ff7895be4bcc9fd9",
    "translations": {
      "zh": "---\ntitle: \"OpenAI SDK\"\ndescription: \"通过官方 OpenAI SDK 使用 LemonData\"\n---\n\n## 概览\n\nLemonData 完全兼容 OpenAI SDK。只需更改 `base URL`，即可访问 300 多个模型。\n\n## 安装\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 配置\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## 聊天补全 (Chat Completions)\n\n工作方式与 OpenAI API 完全一致：\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## 流式传输 (Streaming)\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## 函数调用 / 工具 (Function Calling / Tools)\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## 视觉 (Vision)\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## 图像生成 (Image Generation)\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## 嵌入 (Embeddings)\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## 音频 - 文本转语音 (Text to Speech)\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## 音频 - 语音转文字 (Transcription)\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## 使用不同的模型\n\nLemonData 的核心优势在于可以访问多个提供商：\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## 错误处理\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "zh-TW": "---\ntitle: \"OpenAI SDK\"\ndescription: \"透過官方 OpenAI SDK 使用 LemonData\"\n---\n\n## 概覽\n\nLemonData 完全相容於 OpenAI SDK。只需更改 base URL，即可存取 300 多種模型。\n\n## 安裝\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 配置\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\n運作方式與 OpenAI API 完全相同：\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Tools\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## 圖片生成 (Image Generation)\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## 音訊 - 文字轉語音 (Text to Speech)\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## 音訊 - 語音轉文字 (Transcription)\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## 使用不同的模型\n\nLemonData 的核心優勢在於可以存取多個供應商：\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## 錯誤處理\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "ja": "---\ntitle: \"OpenAI SDK\"\ndescription: \"公式の OpenAI SDK で LemonData を使用する\"\n---\n\n## 概要\n\nLemonData は OpenAI SDK と完全に互換性があります。ベース URL を変更するだけで、300 以上のモデルにアクセスできます。\n\n## インストール\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 設定\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## チャットコンプリーション\n\nOpenAI API とまったく同じように動作します：\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # または任意の LemonData モデル\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## ストリーミング\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## 関数呼び出し / ツール\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# モデルが関数を呼び出したいか確認\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # または claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## 画像生成\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## エンベディング\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # 最初の 5 次元\n```\n\n## オーディオ - テキスト読み上げ\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## オーディオ - 文字起こし\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## 異なるモデルの使用\n\nLemonData の主な利点は、複数のプロバイダーにアクセスできることです：\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## エラーハンドリング\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "ko": "---\ntitle: \"OpenAI SDK\"\ndescription: \"공식 OpenAI SDK와 함께 LemonData를 사용하세요\"\n---\n\n## 개요\n\nLemonData는 OpenAI SDK와 완벽하게 호환됩니다. `base URL`만 변경하면 300개 이상의 모델에 액세스할 수 있습니다.\n\n## 설치\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 설정\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nOpenAI API와 동일하게 작동합니다:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # 또는 모든 LemonData 모델\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## 스트리밍\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Tools\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# 모델이 함수 호출을 원하는지 확인\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # 또는 claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## 이미지 생성\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # 처음 5개 차원\n```\n\n## 오디오 - Text to Speech\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## 오디오 - Transcription\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## 다양한 모델 사용하기\n\nLemonData의 핵심 장점은 여러 제공업체의 모델에 액세스할 수 있다는 것입니다:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## 오류 처리\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "de": "---\ntitle: \"OpenAI SDK\"\ndescription: \"Nutzen Sie LemonData mit dem offiziellen OpenAI SDK\"\n---\n\n## Übersicht\n\nLemonData ist vollständig kompatibel mit dem OpenAI SDK. Ändern Sie einfach die Basis-URL und Sie können auf über 300 Modelle zugreifen.\n\n## Installation\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Konfiguration\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nFunktioniert genau wie die OpenAI API:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Oder jedes andere LemonData-Modell\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Tools\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Prüfen, ob das Modell eine Funktion aufrufen möchte\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Oder claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## Bildgenerierung\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # Erste 5 Dimensionen\n```\n\n## Audio - Text to Speech\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## Audio - Transkription\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## Verwendung verschiedener Modelle\n\nDer entscheidende Vorteil von LemonData ist der Zugriff auf mehrere Anbieter:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Fehlerbehandlung\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Ungültiger API-Key\")\nexcept RateLimitError:\n    print(\"Rate-Limit überschritten, bitte warten\")\nexcept APIError as e:\n    print(f\"API-Fehler: {e.status_code} - {e.message}\")\n```",
      "fr": "---\ntitle: \"SDK OpenAI\"\ndescription: \"Utilisez LemonData avec le SDK OpenAI officiel\"\n---\n\n## Aperçu\n\nLemonData est entièrement compatible avec le SDK OpenAI. Il suffit de modifier l'URL de base pour accéder à plus de 300 modèles.\n\n## Installation\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Configuration\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nFonctionne exactement comme l'API OpenAI :\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\",",
      "es": "---\ntitle: \"OpenAI SDK\"\ndescription: \"Utilice LemonData con el SDK oficial de OpenAI\"\n---\n\n## Descripción general\n\nLemonData es totalmente compatible con el SDK de OpenAI. Simplemente cambie la URL base y podrá acceder a más de 300 modelos.\n\n## Instalación\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Configuración\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nFunciona exactamente igual que la API de OpenAI:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Tools\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Visión\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## Generación de imágenes\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## Audio - Texto a voz\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## Audio - Transcripción\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## Uso de diferentes modelos\n\nLa ventaja clave de LemonData es el acceso a múltiples proveedores:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Manejo de errores\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "pt": "---\ntitle: \"OpenAI SDK\"\ndescription: \"Use o LemonData com o OpenAI SDK oficial\"\n---\n\n## Visão Geral\n\nO LemonData é totalmente compatível com o OpenAI SDK. Basta alterar a URL base e você poderá acessar mais de 300 modelos.\n\n## Instalação\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Configuração\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nFunciona exatamente como a API da OpenAI:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Ferramentas\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Visão\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## Geração de Imagens\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## Áudio - Text to Speech\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## Áudio - Transcrição\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## Usando Diferentes Modelos\n\nA principal vantagem do LemonData é o acesso a múltiplos provedores:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Tratamento de Erros\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "ar": "---\ntitle: \"OpenAI SDK\"\ndescription: \"استخدم LemonData مع OpenAI SDK الرسمي\"\n---\n\n## نظرة عامة\n\nإن LemonData متوافق تماماً مع OpenAI SDK. ما عليك سوى تغيير `base URL` لتتمكن من الوصول إلى أكثر من 300 نموذج.\n\n## التثبيت\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## الإعداد\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## إكمال الدردشة (Chat Completions)\n\nيعمل تماماً مثل OpenAI API:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## البث (Streaming)\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## استدعاء الوظائف / الأدوات (Function Calling / Tools)\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## الرؤية (Vision)\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## توليد الصور (Image Generation)\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## تضمينات المتجهات (Embeddings)\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## الصوت - تحويل النص إلى كلام (Text to Speech)\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## الصوت - النسخ الصوتي (Transcription)\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## استخدام نماذج مختلفة\n\nالميزة الأساسية لـ LemonData هي الوصول إلى مزودين متعددين:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## معالجة الأخطاء (Error Handling)\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "vi": "---\ntitle: \"OpenAI SDK\"\ndescription: \"Sử dụng LemonData với OpenAI SDK chính thức\"\n---\n\n## Tổng quan\n\nLemonData hoàn toàn tương thích với OpenAI SDK. Chỉ cần thay đổi base URL và bạn có thể truy cập hơn 300 mô hình.\n\n## Cài đặt\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Cấu hình\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nHoạt động hoàn toàn giống với OpenAI API:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Tools\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## Image Generation\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## Audio - Text to Speech\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## Audio - Transcription\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## Sử dụng các mô hình khác nhau\n\nƯu điểm then chốt của LemonData là khả năng truy cập nhiều nhà cung cấp khác nhau:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Xử lý lỗi\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "id": "---\ntitle: \"OpenAI SDK\"\ndescription: \"Gunakan LemonData dengan OpenAI SDK resmi\"\n---\n\n## Ringkasan\n\nLemonData sepenuhnya kompatibel dengan OpenAI SDK. Cukup ubah base URL dan Anda dapat mengakses 300+ model.\n\n## Instalasi\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Konfigurasi\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nBekerja persis seperti OpenAI API:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Tools\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## Pembuatan Gambar\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## Audio - Text to Speech\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## Audio - Transkripsi\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## Menggunakan Berbagai Model\n\nKeunggulan utama LemonData adalah akses ke berbagai penyedia:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Penanganan Error\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```",
      "tr": "---\ntitle: \"OpenAI SDK\"\ndescription: \"LemonData'yı resmi OpenAI SDK ile kullanın\"\n---\n\n## Genel Bakış\n\nLemonData, OpenAI SDK ile tam uyumludur. Sadece `base URL` bilgisini değiştirerek 300'den fazla modele erişebilirsiniz.\n\n## Kurulum\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Yapılandırma\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-lemondata-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-lemondata-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n```\n\n```go Go\npackage main\n\nimport \"github.com/sashabaranov/go-openai\"\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-lemondata-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n    client := openai.NewClientWithConfig(config)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-lemondata-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [['role' => 'user', 'content' => 'Hello!']]\n    ])\n]);\n```\n\n</CodeGroup>\n\n## Chat Completions\n\nTam olarak OpenAI API gibi çalışır:\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or any LemonData model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    temperature=0.7,\n    max_tokens=1000\n)\n\nprint(response.choices[0].message.content)\n```\n\n## Streaming\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Function Calling / Araçlar\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }]\n)\n\n# Check if model wants to call a function\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Function: {tool_call.function.name}\")\n    print(f\"Arguments: {tool_call.function.arguments}\")\n```\n\n## Vision\n\n```python\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",  # Or claude-sonnet-4-5\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n        ]\n    }]\n)\n```\n\n## Görsel Oluşturma\n\n```python\nresponse = client.images.generate(\n    model=\"dall-e-3\",\n    prompt=\"A white siamese cat\",\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1\n)\n\nprint(response.data[0].url)\n```\n\n## Embeddings\n\n```python\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=\"Hello world\"\n)\n\nprint(response.data[0].embedding[:5])  # First 5 dimensions\n```\n\n## Ses - Metinden Sese (Text to Speech)\n\n```python\nresponse = client.audio.speech.create(\n    model=\"tts-1\",\n    voice=\"alloy\",\n    input=\"Hello, welcome to LemonData!\"\n)\n\nresponse.stream_to_file(\"output.mp3\")\n```\n\n## Ses - Transkripsiyon\n\n```python\nwith open(\"audio.mp3\", \"rb\") as audio_file:\n    response = client.audio.transcriptions.create(\n        model=\"whisper-1\",\n        file=audio_file\n    )\n\nprint(response.text)\n```\n\n## Farklı Modelleri Kullanma\n\nLemonData'nın temel avantajı, birden fazla sağlayıcıya erişim sağlamasıdır:\n\n```python\n# OpenAI\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Hata Yönetimi\n\n```python\nfrom openai import APIError, RateLimitError, AuthenticationError\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept AuthenticationError:\n    print(\"Invalid API key\")\nexcept RateLimitError:\n    print(\"Rate limit exceeded, please wait\")\nexcept APIError as e:\n    print(f\"API error: {e.status_code} - {e.message}\")\n```"
    },
    "updatedAt": "2026-01-26T05:40:24.742Z"
  },
  "integrations/opencode.mdx": {
    "sourceHash": "af3809b7b7e3bed2",
    "translations": {
      "zh": "---\ntitle: \"OpenCode\"\ndescription: \"配置 OpenCode 以使用 LemonData API\"\n---\n\n## 概览\n\nOpenCode 是一款在终端运行的开源 AI 编程助手。它支持多个 LLM 提供商，并可配置使用 LemonData 的 API 以访问 300 多种模型。\n\n## 安装\n\n<Tabs>\n  <Tab title=\"安装脚本\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    或者使用社区 tap（更新更频繁）：\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\n验证安装：\n\n```bash\nopencode --version\n```\n\n## 配置\n\n### 步骤 1：设置环境变量\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\n如需永久配置，请添加到 `~/.bashrc` 或 `~/.zshrc`：\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### 步骤 2：配置 OpenCode\n\n在项目根目录下创建或编辑 `~/.config/opencode/opencode.json`（全局）或 `opencode.json`：\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## 基本用法\n\n**启动交互模式：**\n\n```bash\nopencode\n```\n\n**使用提示词运行：**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**指定模型：**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## 可用模型\n\n| 模型 | 最适用于 |\n|-------|----------|\n| `gpt-4o` | 复杂任务、代码架构 |\n| `gpt-4o-mini` | 快速修复、简单查询 |\n| `claude-sonnet-4-5` | 代码审查、文档编写 |\n| `claude-opus-4-5` | 复杂推理 |\n| `gemini-2.5-flash` | 快速响应 |\n| `deepseek-r1` | 算法设计 |\n\n## 常用命令\n\n**分析代码：**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**生成代码：**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**审查更改：**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**修复错误：**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## 交互式命令\n\n| 命令 | 描述 |\n|---------|-------------|\n| `/help` | 显示可用命令 |\n| `/model <name>` | 切换到不同的模型 |\n| `/clear` | 清除对话历史 |\n| `/exit` | 退出 OpenCode |\n\n## 故障排除\n\n<AccordionGroup>\n  <Accordion title=\"连接错误\">\n    - 验证 `LOCAL_ENDPOINT` 已设置为 `https://api.lemondata.cc/v1`\n    - 检查网络连接\n    - 尝试使用 `curl https://api.lemondata.cc/v1/models` 进行测试\n  </Accordion>\n\n  <Accordion title=\"身份验证失败\">\n    - 验证是否已设置 `OPENAI_API_KEY` 环境变量\n    - 检查密钥是否以 `sk-` 开头\n    - 确保密钥在 LemonData 控制面板中处于激活状态\n  </Accordion>\n\n  <Accordion title=\"未找到模型\">\n    - 检查配置中的模型名称是否完全匹配\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh/models) 验证模型的可用性\n  </Accordion>\n</AccordionGroup>\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"使用项目上下文\">\n    从项目根目录运行 OpenCode，以便更好地理解您的代码库。\n  </Accordion>\n\n  <Accordion title=\"选择合适的模型\">\n    对于简单任务使用更快的模型 (`gpt-4o-mini`)，对于复杂任务使用功能强大的模型 (`gpt-4o`, `claude`)。\n  </Accordion>\n\n  <Accordion title=\"审查生成的代码\">\n    在将更改应用到项目之前，务必审查 AI 生成的代码。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"OpenCode\"\ndescription: \"配置 OpenCode 以使用 LemonData API\"\n---\n\n## 概覽\n\nOpenCode 是一款在終端機中運行的開源 AI 編碼助手。它支援多個 LLM 提供商，並可配置為使用 LemonData 的 API 以存取 300 多個模型。\n\n## 安裝\n\n<Tabs>\n  <Tab title=\"安裝腳本\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    或使用社群 tap（更新更頻繁）：\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\n驗證安裝：\n\n```bash\nopencode --version\n```\n\n## 配置\n\n### 步驟 1：設置環境變數\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\n如需永久配置，請將其添加到 `~/.bashrc` 或 `~/.zshrc`：\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### 步驟 2：配置 OpenCode\n\n在您的專案根目錄中建立或編輯 `~/.config/opencode/opencode.json`（全域）或 `opencode.json`：\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## 基本用法\n\n**啟動互動模式：**\n\n```bash\nopencode\n```\n\n**使用提示詞運行：**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**指定模型：**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## 可用模型\n\n| 模型 | 最適用於 |\n|-------|----------|\n| `gpt-4o` | 複雜任務、程式碼架構 |\n| `gpt-4o-mini` | 快速修復、簡單查詢 |\n| `claude-sonnet-4-5` | 程式碼審查、文件編寫 |\n| `claude-opus-4-5` | 複雜推理 |\n| `gemini-2.5-flash` | 快速響應 |\n| `deepseek-r1` | 演算法設計 |\n\n## 常見指令\n\n**分析程式碼：**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**生成程式碼：**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**審查變更：**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**修復錯誤：**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## 互動指令\n\n| 指令 | 描述 |\n|---------|-------------|\n| `/help` | 顯示可用指令 |\n| `/model <name>` | 切換到不同的模型 |\n| `/clear` | 清除對話歷史 |\n| `/exit` | 退出 OpenCode |\n\n## 疑難排解\n\n<AccordionGroup>\n  <Accordion title=\"連線錯誤\">\n    - 驗證 `LOCAL_ENDPOINT` 已設置為 `https://api.lemondata.cc/v1`\n    - 檢查網路連線\n    - 嘗試使用 `curl https://api.lemondata.cc/v1/models` 進行測試\n  </Accordion>\n\n  <Accordion title=\"身分驗證失敗\">\n    - 驗證 `OPENAI_API_KEY` 環境變數已設置\n    - 檢查金鑰是否以 `sk-` 開頭\n    - 確保金鑰在 LemonData 控制面板中處於啟用狀態\n  </Accordion>\n\n  <Accordion title=\"找不到模型\">\n    - 檢查配置中的模型名稱是否完全匹配\n    - 在 [lemondata.cc/en/models](https://lemondata.cc/zh-TW/models) 驗證模型可用性\n  </Accordion>\n</AccordionGroup>\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"使用專案上下文\">\n    從您的專案根目錄運行 OpenCode，以便更好地理解您的程式碼庫。\n  </Accordion>\n\n  <Accordion title=\"選擇合適的模型\">\n    對於簡單任務使用更快的模型 (gpt-4o-mini)，對於複雜任務使用強大的模型 (gpt-4o, claude)。\n  </Accordion>\n\n  <Accordion title=\"審查生成的程式碼\">\n    在將變更應用到您的專案之前，請務必審查 AI 生成的程式碼。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"OpenCode\"\ndescription: \"LemonData APIを使用するようにOpenCodeを設定する\"\n---\n\n## 概要\n\nOpenCodeは、ターミナルで動作するオープンソースのAIコーディングアシスタントです。複数のLLMプロバイダーをサポートしており、LemonDataのAPIを使用して300以上のモデルにアクセスするように設定できます。\n\n## インストール\n\n<Tabs>\n  <Tab title=\"インストールスクリプト\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    または、コミュニティタップ（より頻繁な更新）を使用してください：\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nインストールの確認：\n\n```bash\nopencode --version\n```\n\n## 設定\n\n### ステップ 1: 環境変数の設定\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\n永続的な設定を行うには、`~/.bashrc` または `~/.zshrc` に追加してください：\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### ステップ 2: OpenCodeの設定\n\nプロジェクトのルートディレクトリにある `~/.config/opencode/opencode.json`（グローバル）または `opencode.json` を作成または編集します：\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## 基本的な使い方\n\n**対話モードを開始する:**\n\n```bash\nopencode\n```\n\n**プロンプトを指定して実行する:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**モデルを指定する:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## 利用可能なモデル\n\n| モデル | 最適な用途 |\n|-------|----------|\n| `gpt-4o` | 複雑なタスク、コードアーキテクチャ |\n| `gpt-4o-mini` | 迅速な修正、単純なクエリ |\n| `claude-sonnet-4-5` | コードレビュー、ドキュメント作成 |\n| `claude-opus-4-5` | 複雑な推論 |\n| `gemini-2.5-flash` | 高速なレスポンス |\n| `deepseek-r1` | アルゴリズム設計 |\n\n## 一般的なコマンド\n\n**コードを分析する:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**コードを生成する:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**変更をレビューする:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**エラーを修正する:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## 対話型コマンド\n\n| コマンド | 説明 |\n|---------|-------------|\n| `/help` | 利用可能なコマンドを表示する |\n| `/model <name>` | 別のモデルに切り替える |\n| `/clear` | 会話履歴をクリアする |\n| `/exit` | OpenCodeを終了する |\n\n## トラブルシューティング\n\n<AccordionGroup>\n  <Accordion title=\"接続エラー\">\n    - `LOCAL_ENDPOINT` が `https://api.lemondata.cc/v1` に設定されていることを確認してください\n    - ネットワーク接続を確認してください\n    - テストのために `curl https://api.lemondata.cc/v1/models` を試してください\n  </Accordion>\n\n  <Accordion title=\"認証に失敗しました\">\n    - `OPENAI_API_KEY` 環境変数が設定されていることを確認してください\n    - キーが `sk-` で始まっていることを確認してください\n    - LemonData ダッシュボードでキーが有効であることを確認してください\n  </Accordion>\n\n  <Accordion title=\"モデルが見つかりません\">\n    - 設定内のモデル名が正確に一致しているか確認してください\n    - [lemondata.cc/en/models](https://lemondata.cc/ja/models) でモデルの利用可能性を確認してください\n  </Accordion>\n</AccordionGroup>\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"プロジェクトのコンテキストを使用する\">\n    コードベースをより正確に理解させるために、プロジェクトのルートから OpenCode を実行してください。\n  </Accordion>\n\n  <Accordion title=\"適切なモデルを選択する\">\n    単純なタスクには高速なモデル（gpt-4o-mini）を、複雑なタスクには強力なモデル（gpt-4o, claude）を使用してください。\n  </Accordion>\n\n  <Accordion title=\"生成されたコードをレビューする\">\n    プロジェクトに変更を適用する前に、常にAIが生成したコードをレビューしてください。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"OpenCode\"\ndescription: \"LemonData API를 사용하도록 OpenCode 설정하기\"\n---\n\n## 개요\n\nOpenCode는 터미널에서 실행되는 오픈 소스 AI 코딩 어시스턴트입니다. 여러 LLM 제공업체를 지원하며, 300개 이상의 모델에 액세스하기 위해 LemonData의 API를 사용하도록 설정할 수 있습니다.\n\n## 설치\n\n<Tabs>\n  <Tab title=\"설치 스크립트\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    또는 커뮤니티 탭을 사용하세요 (더 빈번한 업데이트):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\n설치 확인:\n\n```bash\nopencode --version\n```\n\n## 설정\n\n### 1단계: 환경 변수 설정\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\n영구적인 설정을 위해 `~/.bashrc` 또는 `~/.zshrc`에 추가하세요:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### 2단계: OpenCode 설정\n\n프로젝트 루트에 `~/.config/opencode/opencode.json` (전역) 또는 `opencode.json`을 생성하거나 편집하세요:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## 기본 사용법\n\n**대화형 모드 시작:**\n\n```bash\nopencode\n```\n\n**프롬프트와 함께 실행:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**모델 지정:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## 사용 가능한 모델\n\n| 모델 | 최적 용도 |\n|-------|----------|\n| `gpt-4o` | 복잡한 작업, 코드 아키텍처 |\n| `gpt-4o-mini` | 빠른 수정, 단순 쿼리 |\n| `claude-sonnet-4-5` | 코드 리뷰, 문서화 |\n| `claude-opus-4-5` | 복잡한 추론 |\n| `gemini-2.5-flash` | 빠른 응답 |\n| `deepseek-r1` | 알고리즘 설계 |\n\n## 일반 명령어\n\n**코드 분석:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**코드 생성:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**변경 사항 검토:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**오류 수정:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## 대화형 명령어\n\n| 명령어 | 설명 |\n|---------|-------------|\n| `/help` | 사용 가능한 명령어 표시 |\n| `/model <name>` | 다른 모델로 전환 |\n| `/clear` | 대화 기록 삭제 |\n| `/exit` | OpenCode 종료 |\n\n## 문제 해결\n\n<AccordionGroup>\n  <Accordion title=\"연결 오류\">\n    - `LOCAL_ENDPOINT`가 `https://api.lemondata.cc/v1`으로 설정되어 있는지 확인하세요\n    - 네트워크 연결 상태를 확인하세요\n    - 테스트를 위해 `curl https://api.lemondata.cc/v1/models`를 실행해 보세요\n  </Accordion>\n\n  <Accordion title=\"인증 실패\">\n    - `OPENAI_API_KEY` 환경 변수가 설정되어 있는지 확인하세요\n    - 키가 `sk-`로 시작하는지 확인하세요\n    - LemonData 대시보드에서 키가 활성 상태인지 확인하세요\n  </Accordion>\n\n  <Accordion title=\"모델을 찾을 수 없음\">\n    - 설정의 모델 이름이 정확히 일치하는지 확인하세요\n    - [lemondata.cc/en/models](https://lemondata.cc/ko/models)에서 모델 가용성을 확인하세요\n  </Accordion>\n</AccordionGroup>\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"프로젝트 컨텍스트 사용\">\n    코드베이스를 더 잘 이해할 수 있도록 프로젝트 루트에서 OpenCode를 실행하세요.\n  </Accordion>\n\n  <Accordion title=\"적절한 모델 선택\">\n    단순한 작업에는 더 빠른 모델(gpt-4o-mini)을, 복잡한 작업에는 강력한 모델(gpt-4o, claude)을 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"생성된 코드 검토\">\n    프로젝트에 변경 사항을 적용하기 전에 항상 AI가 생성한 코드를 검토하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"OpenCode\"\ndescription: \"Konfigurieren Sie OpenCode für die Nutzung der LemonData API\"\n---\n\n## Übersicht\n\nOpenCode ist ein Open-Source-KI-Coding-Assistent, der in Ihrem Terminal läuft. Er unterstützt mehrere LLM-Provider und kann so konfiguriert werden, dass er die LemonData API für den Zugriff auf über 300 Modelle nutzt.\n\n## Installation\n\n<Tabs>\n  <Tab title=\"Installations-Skript\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    Oder nutzen Sie den Community-Tap (häufigere Updates):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nInstallation überprüfen:\n\n```bash\nopencode --version\n```\n\n## Konfiguration\n\n### Schritt 1: Umgebungsvariablen setzen\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nFür eine dauerhafte Konfiguration fügen Sie dies zu `~/.bashrc` oder `~/.zshrc` hinzu:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Schritt 2: OpenCode konfigurieren\n\nErstellen oder bearbeiten Sie `~/.config/opencode/opencode.json` (global) oder `opencode.json` in Ihrem Projektverzeichnis:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Grundlegende Nutzung\n\n**Interaktiven Modus starten:**\n\n```bash\nopencode\n```\n\n**Mit einem Prompt ausführen:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Modell angeben:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Verfügbare Modelle\n\n| Modell | Bestens geeignet für |\n|-------|----------|\n| `gpt-4o` | Komplexe Aufgaben, Code-Architektur |\n| `gpt-4o-mini` | Schnelle Fehlerbehebungen, einfache Abfragen |\n| `claude-sonnet-4-5` | Code-Review, Dokumentation |\n| `claude-opus-4-5` | Komplexes logisches Schlussfolgern |\n| `gemini-2.5-flash` | Schnelle Antworten |\n| `deepseek-r1` | Algorithmus-Design |\n\n## Gängige Befehle\n\n**Code analysieren:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Code generieren:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Änderungen überprüfen:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Fehler beheben:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Interaktive Befehle\n\n| Befehl | Beschreibung |\n|---------|-------------|\n| `/help` | Verfügbare Befehle anzeigen |\n| `/model <name>` | Zu einem anderen Modell wechseln |\n| `/clear` | Konversationsverlauf löschen |\n| `/exit` | OpenCode beenden |\n\n## Fehlerbehebung\n\n<AccordionGroup>\n  <Accordion title=\"Verbindungsfehler\">\n    - Überprüfen Sie, ob `LOCAL_ENDPOINT` auf `https://api.lemondata.cc/v1` gesetzt ist\n    - Prüfen Sie die Netzwerkverbindung\n    - Testen Sie mit `curl https://api.lemondata.cc/v1/models`\n  </Accordion>\n\n  <Accordion title=\"Authentifizierung fehlgeschlagen\">\n    - Überprüfen Sie, ob die Umgebungsvariable `OPENAI_API_KEY` gesetzt ist\n    - Stellen Sie sicher, dass der Key mit `sk-` beginnt\n    - Stellen Sie sicher, dass der Key im LemonData-Dashboard aktiv ist\n  </Accordion>\n\n  <Accordion title=\"Modell nicht gefunden\">\n    - Überprüfen Sie, ob der Modellname in der Konfiguration exakt übereinstimmt\n    - Überprüfen Sie die Modellverfügbarkeit unter [lemondata.cc/en/models](https://lemondata.cc/de/models)\n  </Accordion>\n</AccordionGroup>\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Projektkontext nutzen\">\n    Führen Sie OpenCode aus Ihrem Projektverzeichnis aus, um ein besseres Verständnis Ihrer Codebasis zu ermöglichen.\n  </Accordion>\n\n  <Accordion title=\"Passende Modelle wählen\">\n    Verwenden Sie schnellere Modelle (gpt-4o-mini) für einfache Aufgaben und leistungsstarke Modelle (gpt-4o, claude) für komplexe Aufgaben.\n  </Accordion>\n\n  <Accordion title=\"Generierten Code überprüfen\">\n    Überprüfen Sie KI-generierten Code immer, bevor Sie Änderungen an Ihrem Projekt vornehmen.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"OpenCode\"\ndescription: \"Configurez OpenCode pour utiliser l'API LemonData\"\n---\n\n## Présentation\n\nOpenCode est un assistant de codage IA open-source qui s'exécute dans votre terminal. Il prend en charge plusieurs fournisseurs de LLM et peut être configuré pour utiliser l'API de LemonData afin d'accéder à plus de 300 modèles.\n\n## Installation\n\n<Tabs>\n  <Tab title=\"Script d'installation\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    Ou utilisez le tap de la communauté (mises à jour plus fréquentes) :\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nVérifiez l'installation :\n\n```bash\nopencode --version\n```\n\n## Configuration\n\n### Étape 1 : Définir les variables d'environnement\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nPour une configuration permanente, ajoutez à `~/.bashrc` ou `~/.zshrc` :\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Étape 2 : Configurer OpenCode\n\nCréez ou modifiez `~/.config/opencode/opencode.json` (global) ou `opencode.json` à la racine de votre projet :\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Utilisation de base\n\n**Démarrer le mode interactif :**\n\n```bash\nopencode\n```\n\n**Exécuter avec un prompt :**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Spécifier le modèle :**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Modèles disponibles\n\n| Modèle | Idéal pour |\n|-------|----------|\n| `gpt-4o` | Tâches complexes, architecture de code |\n| `gpt-4o-mini` | Corrections rapides, requêtes simples |\n| `claude-sonnet-4-5` | Revue de code, documentation |\n| `claude-opus-4-5` | Raisonnement complexe |\n| `gemini-2.5-flash` | Réponses rapides |\n| `deepseek-r1` | Conception d'algorithmes |\n\n## Commandes courantes\n\n**Analyser le code :**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Générer du code :**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Réviser les modifications :**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Corriger les erreurs :**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Commandes interactives\n\n| Commande | Description |\n|---------|-------------|\n| `/help` | Afficher les commandes disponibles |\n| `/model <name>` | Passer à un modèle différent |\n| `/clear` | Effacer l'historique de la conversation |\n| `/exit` | Quitter OpenCode |\n\n## Dépannage\n\n<AccordionGroup>\n  <Accordion title=\"Erreur de connexion\">\n    - Vérifiez que `LOCAL_ENDPOINT` est défini sur `https://api.lemondata.cc/v1`\n    - Vérifiez la connectivité réseau\n    - Essayez `curl https://api.lemondata.cc/v1/models` pour tester\n  </Accordion>\n\n  <Accordion title=\"Échec de l'authentification\">\n    - Vérifiez que la variable d'environnement `OPENAI_API_KEY` est définie\n    - Vérifiez que la clé commence par `sk-`\n    - Assurez-vous que la clé est active dans le tableau de bord LemonData\n  </Accordion>\n\n  <Accordion title=\"Modèle non trouvé\">\n    - Vérifiez que le nom du modèle dans la configuration correspond exactement\n    - Vérifiez la disponibilité du modèle sur [lemondata.cc/en/models](https://lemondata.cc/fr/models)\n  </Accordion>\n</AccordionGroup>\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Utiliser le contexte du projet\">\n    Exécutez OpenCode depuis la racine de votre projet pour une meilleure compréhension de votre base de code.\n  </Accordion>\n\n  <Accordion title=\"Choisir les modèles appropriés\">\n    Utilisez des modèles plus rapides (gpt-4o-mini) pour les tâches simples, et des modèles puissants (gpt-4o, claude) pour les tâches complexes.\n  </Accordion>\n\n  <Accordion title=\"Réviser le code généré\">\n    Révisez toujours le code généré par l'IA avant d'appliquer les modifications à votre projet.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"OpenCode\"\ndescription: \"Configure OpenCode para usar la API de LemonData\"\n---\n\n## Descripción general\n\nOpenCode es un asistente de codificación de IA de código abierto que se ejecuta en su terminal. Admite múltiples proveedores de LLM y puede configurarse para usar la API de LemonData para acceder a más de 300 modelos.\n\n## Instalación\n\n<Tabs>\n  <Tab title=\"Script de instalación\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    O use el tap de la comunidad (actualizaciones más frecuentes):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nVerificar la instalación:\n\n```bash\nopencode --version\n```\n\n## Configuración\n\n### Paso 1: Establecer variables de entorno\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nPara una configuración permanente, añádalo a `~/.bashrc` o `~/.zshrc`:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Paso 2: Configurar OpenCode\n\nCree o edite `~/.config/opencode/opencode.json` (global) o `opencode.json` en la raíz de su proyecto:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Uso básico\n\n**Iniciar modo interactivo:**\n\n```bash\nopencode\n```\n\n**Ejecutar con un prompt:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Especificar modelo:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Modelos disponibles\n\n| Modelo | Ideal para |\n|-------|----------|\n| `gpt-4o` | Tareas complejas, arquitectura de código |\n| `gpt-4o-mini` | Correcciones rápidas, consultas simples |\n| `claude-sonnet-4-5` | Revisión de código, documentación |\n| `claude-opus-4-5` | Razonamiento complejo |\n| `gemini-2.5-flash` | Respuestas rápidas |\n| `deepseek-r1` | Diseño de algoritmos |\n\n## Comandos comunes\n\n**Analizar código:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Generar código:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Revisar cambios:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Corregir errores:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Comandos interactivos\n\n| Comando | Descripción |\n|---------|-------------|\n| `/help` | Mostrar comandos disponibles |\n| `/model <name>` | Cambiar a un modelo diferente |\n| `/clear` | Limpiar el historial de conversación |\n| `/exit` | Salir de OpenCode |\n\n## Solución de problemas\n\n<AccordionGroup>\n  <Accordion title=\"Error de conexión\">\n    - Verifique que `LOCAL_ENDPOINT` esté configurado en `https://api.lemondata.cc/v1`\n    - Compruebe la conectividad de red\n    - Pruebe `curl https://api.lemondata.cc/v1/models` para testear\n  </Accordion>\n\n  <Accordion title=\"Fallo de autenticación\">\n    - Verifique que la variable de entorno `OPENAI_API_KEY` esté establecida\n    - Compruebe que la clave comience con `sk-`\n    - Asegúrese de que la clave esté activa en el dashboard de LemonData\n  </Accordion>\n\n  <Accordion title=\"Modelo no encontrado\">\n    - Compruebe que el nombre del modelo en la configuración coincida exactamente\n    - Verifique la disponibilidad del modelo en [lemondata.cc/en/models](https://lemondata.cc/es/models)\n  </Accordion>\n</AccordionGroup>\n\n## Mejores prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Usar el contexto del proyecto\">\n    Ejecute OpenCode desde la raíz de su proyecto para una mejor comprensión de su base de código.\n  </Accordion>\n\n  <Accordion title=\"Elegir modelos adecuados\">\n    Use modelos más rápidos (gpt-4o-mini) para tareas simples, y modelos potentes (gpt-4o, claude) para tareas complejas.\n  </Accordion>\n\n  <Accordion title=\"Revisar el código generado\">\n    Revise siempre el código generado por IA antes de aplicar cambios a su proyecto.\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"OpenCode\"\ndescription: \"Configure o OpenCode para usar a API da LemonData\"\n---\n\n## Visão Geral\n\nO OpenCode é um assistente de codificação de IA de código aberto que roda no seu terminal. Ele suporta múltiplos provedores de LLM e pode ser configurado para usar a API da LemonData para acessar mais de 300 modelos.\n\n## Instalação\n\n<Tabs>\n  <Tab title=\"Script de Instalação\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    Ou use o tap da comunidade (atualizações mais frequentes):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nVerifique a instalação:\n\n```bash\nopencode --version\n```\n\n## Configuração\n\n### Passo 1: Definir Variáveis de Ambiente\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nPara configuração permanente, adicione ao `~/.bashrc` ou `~/.zshrc`:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Passo 2: Configurar o OpenCode\n\nCrie ou edite `~/.config/opencode/opencode.json` (global) ou `opencode.json` na raiz do seu projeto:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Uso Básico\n\n**Iniciar modo interativo:**\n\n```bash\nopencode\n```\n\n**Executar com um prompt:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Especificar modelo:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Modelos Disponíveis\n\n| Modelo | Ideal Para |\n|-------|----------|\n| `gpt-4o` | Tarefas complexas, arquitetura de código |\n| `gpt-4o-mini` | Correções rápidas, consultas simples |\n| `claude-sonnet-4-5` | Revisão de código, documentação |\n| `claude-opus-4-5` | Raciocínio complexo |\n| `gemini-2.5-flash` | Respostas rápidas |\n| `deepseek-r1` | Design de algoritmos |\n\n## Comandos Comuns\n\n**Analisar código:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Gerar código:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Revisar alterações:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Corrigir erros:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Comandos Interativos\n\n| Comando | Descrição |\n|---------|-------------|\n| `/help` | Mostrar comandos disponíveis |\n| `/model <name>` | Alternar para um modelo diferente |\n| `/clear` | Limpar histórico da conversa |\n| `/exit` | Sair do OpenCode |\n\n## Solução de Problemas\n\n<AccordionGroup>\n  <Accordion title=\"Erro de Conexão\">\n    - Verifique se `LOCAL_ENDPOINT` está definido como `https://api.lemondata.cc/v1`\n    - Verifique a conectividade de rede\n    - Tente `curl https://api.lemondata.cc/v1/models` para testar\n  </Accordion>\n\n  <Accordion title=\"Falha na Autenticação\">\n    - Verifique se a variável de ambiente `OPENAI_API_KEY` está definida\n    - Verifique se a chave começa com `sk-`\n    - Certifique-se de que a chave está ativa no painel da LemonData\n  </Accordion>\n\n  <Accordion title=\"Modelo Não Encontrado\">\n    - Verifique se o nome do modelo na configuração corresponde exatamente\n    - Verifique a disponibilidade do modelo em [lemondata.cc/en/models](https://lemondata.cc/pt/models)\n  </Accordion>\n</AccordionGroup>\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Use o contexto do projeto\">\n    Execute o OpenCode a partir da raiz do seu projeto para uma melhor compreensão da sua base de código.\n  </Accordion>\n\n  <Accordion title=\"Escolha modelos apropriados\">\n    Use modelos mais rápidos (gpt-4o-mini) para tarefas simples e modelos potentes (gpt-4o, claude) para tarefas complexas.\n  </Accordion>\n\n  <Accordion title=\"Revise o código gerado\">\n    Sempre revise o código gerado por IA antes de aplicar as alterações ao seu projeto.\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"OpenCode\"\ndescription: \"قم بتهيئة OpenCode لاستخدام LemonData API\"\n---\n\n## نظرة عامة\n\nOpenCode هو مساعد برمجة بالذكاء الاصطناعي مفتوح المصدر يعمل في واجهة الأوامر (terminal). يدعم العديد من مزودي نماذج اللغة الكبيرة (LLM) ويمكن تهيئته لاستخدام LemonData API للوصول إلى أكثر من 300 نموذج.\n\n## التثبيت\n\n<Tabs>\n  <Tab title=\"سكربت التثبيت\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    أو استخدم الـ tap الخاص بالمجتمع (تحديثات أكثر تكراراً):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nالتحقق من التثبيت:\n\n```bash\nopencode --version\n```\n\n## التهيئة\n\n### الخطوة 1: تعيين متغيرات البيئة\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nللتهيئة الدائمة، أضفها إلى `~/.bashrc` أو `~/.zshrc`:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### الخطوة 2: تهيئة OpenCode\n\nقم بإنشاء أو تعديل `~/.config/opencode/opencode.json` (عام) أو `opencode.json` في جذر مشروعك:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## الاستخدام الأساسي\n\n**بدء الوضع التفاعلي:**\n\n```bash\nopencode\n```\n\n**التشغيل باستخدام مطالبة (prompt):**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**تحديد النموذج:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## النماذج المتاحة\n\n| النموذج | الأفضل لـ |\n|-------|----------|\n| `gpt-4o` | المهام المعقدة، بنية الكود |\n| `gpt-4o-mini` | الإصلاحات السريعة، الاستفسارات البسيطة |\n| `claude-sonnet-4-5` | مراجعة الكود، التوثيق |\n| `claude-opus-4-5` | الاستنتاج المعقد |\n| `gemini-2.5-flash` | الاستجابات السريعة |\n| `deepseek-r1` | تصميم الخوارزميات |\n\n## الأوامر الشائعة\n\n**تحليل الكود:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**تولد الكود:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**مراجعة التغييرات:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**إصلاح الأخطاء:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## الأوامر التفاعلية\n\n| الأمر | الوصف |\n|---------|-------------|\n| `/help` | عرض الأوامر المتاحة |\n| `/model <name>` | التبديل إلى نموذج مختلف |\n| `/clear` | مسح سجل المحادثة |\n| `/exit` | الخروج من OpenCode |\n\n## استكشاف الأخطاء وإصلاحها\n\n<AccordionGroup>\n  <Accordion title=\"خطأ في الاتصال\">\n    - تحقق من تعيين `LOCAL_ENDPOINT` إلى `https://api.lemondata.cc/v1`\n    - تحقق من اتصال الشبكة\n    - جرب `curl https://api.lemondata.cc/v1/models` للاختبار\n  </Accordion>\n\n  <Accordion title=\"فشل المصادقة\">\n    - تحقق من تعيين متغير البيئة `OPENAI_API_KEY`\n    - تأكد من أن المفتاح يبدأ بـ `sk-`\n    - تأكد من أن المفتاح نشط في لوحة تحكم LemonData\n  </Accordion>\n\n  <Accordion title=\"النموذج غير موجود\">\n    - تحقق من أن اسم النموذج في التهيئة مطابق تماماً\n    - تحقق من توفر النموذج في [lemondata.cc/en/models](https://lemondata.cc/ar/models)\n  </Accordion>\n</AccordionGroup>\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"استخدام سياق المشروع\">\n    قم بتشغيل OpenCode من جذر مشروعك لفهم أفضل للكود الخاص بك.\n  </Accordion>\n\n  <Accordion title=\"اختيار النماذج المناسبة\">\n    استخدم نماذج أسرع (gpt-4o-mini) للمهام البسيطة، ونماذج قوية (gpt-4o, claude) للمهام المعقدة.\n  </Accordion>\n\n  <Accordion title=\"مراجعة الكود المولد\">\n    قم دائماً بمراجعة الكود المولد بواسطة الذكاء الاصطناعي قبل تطبيق التغييرات على مشروعك.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"OpenCode\"\ndescription: \"Cấu hình OpenCode để sử dụng LemonData API\"\n---\n\n## Tổng quan\n\nOpenCode là một trợ lý lập trình AI mã nguồn mở chạy trong terminal của bạn. Nó hỗ trợ nhiều nhà cung cấp LLM và có thể được cấu hình để sử dụng LemonData API nhằm truy cập hơn 300 mô hình.\n\n## Cài đặt\n\n<Tabs>\n  <Tab title=\"Script cài đặt\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    Hoặc sử dụng community tap (cập nhật thường xuyên hơn):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nXác minh cài đặt:\n\n```bash\nopencode --version\n```\n\n## Cấu hình\n\n### Bước 1: Thiết lập biến môi trường\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nĐể cấu hình vĩnh viễn, hãy thêm vào `~/.bashrc` hoặc `~/.zshrc`:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Bước 2: Cấu hình OpenCode\n\nTạo hoặc chỉnh sửa `~/.config/opencode/opencode.json` (toàn cục) hoặc `opencode.json` trong thư mục gốc của dự án:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Cách sử dụng cơ bản\n\n**Bắt đầu chế độ tương tác:**\n\n```bash\nopencode\n```\n\n**Chạy với một prompt:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Chỉ định mô hình:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Các mô hình hiện có\n\n| Mô hình | Phù hợp nhất cho |\n|-------|----------|\n| `gpt-4o` | Các tác vụ phức tạp, kiến trúc mã nguồn |\n| `gpt-4o-mini` | Sửa lỗi nhanh, các truy vấn đơn giản |\n| `claude-sonnet-4-5` | Review mã nguồn, tài liệu |\n| `claude-opus-4-5` | Suy luận phức tạp |\n| `gemini-2.5-flash` | Phản hồi nhanh |\n| `deepseek-r1` | Thiết kế thuật toán |\n\n## Các lệnh phổ biến\n\n**Phân tích mã nguồn:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Tạo mã nguồn:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Xem lại các thay đổi:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Sửa lỗi:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Các lệnh tương tác\n\n| Lệnh | Mô tả |\n|---------|-------------|\n| `/help` | Hiển thị các lệnh hiện có |\n| `/model <name>` | Chuyển sang một mô hình khác |\n| `/clear` | Xóa lịch sử hội thoại |\n| `/exit` | Thoát OpenCode |\n\n## Xử lý sự cố\n\n<AccordionGroup>\n  <Accordion title=\"Lỗi kết nối\">\n    - Xác minh `LOCAL_ENDPOINT` đã được thiết lập thành `https://api.lemondata.cc/v1`\n    - Kiểm tra kết nối mạng\n    - Thử `curl https://api.lemondata.cc/v1/models` để kiểm tra\n  </Accordion>\n\n  <Accordion title=\"Xác thực thất bại\">\n    - Xác minh biến môi trường `OPENAI_API_KEY` đã được thiết lập\n    - Kiểm tra xem key có bắt đầu bằng `sk-` hay không\n    - Đảm bảo key đang hoạt động trong dashboard của LemonData\n  </Accordion>\n\n  <Accordion title=\"Không tìm thấy mô hình\">\n    - Kiểm tra tên mô hình trong cấu hình có khớp chính xác hay không\n    - Xác minh tính khả dụng của mô hình tại [lemondata.cc/en/models](https://lemondata.cc/vi/models)\n  </Accordion>\n</AccordionGroup>\n\n## Các thực hành tốt nhất\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng ngữ cảnh dự án\">\n    Chạy OpenCode từ thư mục gốc của dự án để hiểu rõ hơn về mã nguồn của bạn.\n  </Accordion>\n\n  <Accordion title=\"Chọn mô hình phù hợp\">\n    Sử dụng các mô hình nhanh hơn (gpt-4o-mini) cho các tác vụ đơn giản, các mô hình mạnh mẽ (gpt-4o, claude) cho các tác vụ phức tạp.\n  </Accordion>\n\n  <Accordion title=\"Xem lại mã nguồn được tạo\">\n    Luôn xem lại mã nguồn do AI tạo ra trước khi áp dụng các thay đổi vào dự án của bạn.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"OpenCode\"\ndescription: \"Konfigurasi OpenCode untuk menggunakan LemonData API\"\n---\n\n## Ringkasan\n\nOpenCode adalah asisten coding AI sumber terbuka (open-source) yang berjalan di terminal Anda. Ini mendukung berbagai penyedia LLM dan dapat dikonfigurasi untuk menggunakan LemonData API guna mengakses lebih dari 300 model.\n\n## Instalasi\n\n<Tabs>\n  <Tab title=\"Skrip Instalasi\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    Atau gunakan community tap (pembaruan lebih sering):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nVerifikasi instalasi:\n\n```bash\nopencode --version\n```\n\n## Konfigurasi\n\n### Langkah 1: Atur Environment Variables\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nUntuk konfigurasi permanen, tambahkan ke `~/.bashrc` atau `~/.zshrc`:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Langkah 2: Konfigurasi OpenCode\n\nBuat atau edit `~/.config/opencode/opencode.json` (global) atau `opencode.json` di root proyek Anda:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Penggunaan Dasar\n\n**Mulai mode interaktif:**\n\n```bash\nopencode\n```\n\n**Jalankan dengan prompt:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Tentukan model:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Model yang Tersedia\n\n| Model | Terbaik Untuk |\n|-------|----------|\n| `gpt-4o` | Tugas kompleks, arsitektur kode |\n| `gpt-4o-mini` | Perbaikan cepat, kueri sederhana |\n| `claude-sonnet-4-5` | Tinjauan kode, dokumentasi |\n| `claude-opus-4-5` | Penalaran kompleks |\n| `gemini-2.5-flash` | Respons cepat |\n| `deepseek-r1` | Desain algoritma |\n\n## Perintah Umum\n\n**Analisis kode:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Hasilkan kode:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Tinjau perubahan:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Perbaiki kesalahan:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Perintah Interaktif\n\n| Perintah | Deskripsi |\n|---------|-------------|\n| `/help` | Tampilkan perintah yang tersedia |\n| `/model <name>` | Beralih ke model yang berbeda |\n| `/clear` | Hapus riwayat percakapan |\n| `/exit` | Keluar dari OpenCode |\n\n## Pemecahan Masalah\n\n<AccordionGroup>\n  <Accordion title=\"Kesalahan Koneksi\">\n    - Verifikasi `LOCAL_ENDPOINT` telah diatur ke `https://api.lemondata.cc/v1`\n    - Periksa konektivitas jaringan\n    - Coba `curl https://api.lemondata.cc/v1/models` untuk menguji\n  </Accordion>\n\n  <Accordion title=\"Autentikasi Gagal\">\n    - Verifikasi environment variable `OPENAI_API_KEY` telah diatur\n    - Pastikan kunci dimulai dengan `sk-`\n    - Pastikan kunci aktif di dashboard LemonData\n  </Accordion>\n\n  <Accordion title=\"Model Tidak Ditemukan\">\n    - Periksa apakah nama model di konfigurasi sudah sesuai\n    - Verifikasi ketersediaan model di [lemondata.cc/en/models](https://lemondata.cc/id/models)\n  </Accordion>\n</AccordionGroup>\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Gunakan konteks proyek\">\n    Jalankan OpenCode dari root proyek Anda untuk pemahaman yang lebih baik tentang basis kode Anda.\n  </Accordion>\n\n  <Accordion title=\"Pilih model yang sesuai\">\n    Gunakan model yang lebih cepat (gpt-4o-mini) untuk tugas sederhana, model yang kuat (gpt-4o, claude) untuk tugas yang kompleks.\n  </Accordion>\n\n  <Accordion title=\"Tinjau kode yang dihasilkan\">\n    Selalu tinjau kode yang dihasilkan AI sebelum menerapkan perubahan pada proyek Anda.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"OpenCode\"\ndescription: \"OpenCode'u LemonData API kullanacak şekilde yapılandırın\"\n---\n\n## Genel Bakış\n\nOpenCode, terminalinizde çalışan açık kaynaklı bir yapay zeka kodlama asistanıdır. Birden fazla LLM sağlayıcısını destekler ve 300'den fazla modele erişim için LemonData API'sini kullanacak şekilde yapılandırılabilir.\n\n## Kurulum\n\n<Tabs>\n  <Tab title=\"Kurulum Betiği\">\n    ```bash\n    curl -fsSL https://opencode.ai/install | bash\n    ```\n  </Tab>\n  <Tab title=\"Homebrew\">\n    ```bash\n    brew install opencode-ai/tap/opencode\n    ```\n\n    Veya topluluk tap'ini kullanın (daha sık güncellemeler):\n    ```bash\n    brew install anomalyco/tap/opencode\n    ```\n  </Tab>\n  <Tab title=\"Go\">\n    ```bash\n    go install github.com/opencode-ai/opencode@latest\n    ```\n  </Tab>\n</Tabs>\n\nKurulumu doğrulayın:\n\n```bash\nopencode --version\n```\n\n## Yapılandırma\n\n### Adım 1: Ortam Değişkenlerini Ayarlayın\n\n```bash\nexport OPENAI_API_KEY=\"sk-your-lemondata-key\"\nexport LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"\n```\n\nKalıcı yapılandırma için `~/.bashrc` veya `~/.zshrc` dosyasına ekleyin:\n\n```bash\necho 'export OPENAI_API_KEY=\"sk-your-lemondata-key\"' >> ~/.zshrc\necho 'export LOCAL_ENDPOINT=\"https://api.lemondata.cc/v1\"' >> ~/.zshrc\nsource ~/.zshrc\n```\n\n### Adım 2: OpenCode'u Yapılandırın\n\nProje kök dizininizde `~/.config/opencode/opencode.json` (global) veya `opencode.json` dosyasını oluşturun veya düzenleyin:\n\n```json\n{\n  \"provider\": {\n    \"openai\": {\n      \"options\": {\n        \"apiKey\": \"sk-your-lemondata-key\",\n        \"baseURL\": \"https://api.lemondata.cc/v1\"\n      }\n    }\n  },\n  \"model\": \"gpt-4o\",\n  \"agent\": {\n    \"coder\": {\n      \"model\": \"gpt-4o\"\n    }\n  }\n}\n```\n\n## Temel Kullanım\n\n**Etkileşimli modu başlatın:**\n\n```bash\nopencode\n```\n\n**Bir istem (prompt) ile çalıştırın:**\n\n```bash\nopencode \"Explain this codebase\"\n```\n\n**Model belirtin:**\n\n```bash\nopencode --model gpt-4o \"Fix the bugs in main.py\"\n```\n\n## Mevcut Modeller\n\n| Model | En İyi Kullanım Alanı |\n|-------|----------|\n| `gpt-4o` | Karmaşık görevler, kod mimarisi |\n| `gpt-4o-mini` | Hızlı düzeltmeler, basit sorgular |\n| `claude-sonnet-4-5` | Kod incelemesi, dokümantasyon |\n| `claude-opus-4-5` | Karmaşık akıl yürütme |\n| `gemini-2.5-flash` | Hızlı yanıtlar |\n| `deepseek-r1` | Algoritma tasarımı |\n\n## Yaygın Komutlar\n\n**Kodu analiz edin:**\n\n```bash\nopencode \"What does this function do?\" < src/utils.ts\n```\n\n**Kod oluşturun:**\n\n```bash\nopencode \"Create a REST API with Express\"\n```\n\n**Değişiklikleri inceleyin:**\n\n```bash\ngit diff | opencode \"Review these changes\"\n```\n\n**Hataları düzeltin:**\n\n```bash\nopencode \"Fix the TypeScript errors in this project\"\n```\n\n## Etkileşimli Komutlar\n\n| Komut | Açıklama |\n|---------|-------------|\n| `/help` | Mevcut komutları göster |\n| `/model <name>` | Farklı bir modele geç |\n| `/clear` | Konuşma geçmişini temizle |\n| `/exit` | OpenCode'dan çık |\n\n## Sorun Giderme\n\n<AccordionGroup>\n  <Accordion title=\"Bağlantı Hatası\">\n    - `LOCAL_ENDPOINT`'in `https://api.lemondata.cc/v1` olarak ayarlandığını doğrulayın\n    - Ağ bağlantısını kontrol edin\n    - Test etmek için `curl https://api.lemondata.cc/v1/models` komutunu deneyin\n  </Accordion>\n\n  <Accordion title=\"Kimlik Doğrulama Başarısız\">\n    - `OPENAI_API_KEY` ortam değişkeninin ayarlandığını doğrulayın\n    - Anahtarın `sk-` ile başladığını kontrol edin\n    - Anahtarın LemonData panelinde aktif olduğundan emin olun\n  </Accordion>\n\n  <Accordion title=\"Model Bulunamadı\">\n    - Yapılandırmadaki model adının tam olarak eşleştiğini kontrol edin\n    - [lemondata.cc/en/models](https://lemondata.cc/tr/models) adresinden modelin kullanılabilirliğini doğrulayın\n  </Accordion>\n</AccordionGroup>\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Proje bağlamını kullanın\">\n    Kod tabanınızın daha iyi anlaşılması için OpenCode'u proje kök dizininizden çalıştırın.\n  </Accordion>\n\n  <Accordion title=\"Uygun modelleri seçin\">\n    Basit görevler için daha hızlı modelleri (gpt-4o-mini), karmaşık görevler için güçlü modelleri (gpt-4o, claude) kullanın.\n  </Accordion>\n\n  <Accordion title=\"Oluşturulan kodu inceleyin\">\n    Projenize değişiklikleri uygulamadan önce yapay zeka tarafından oluşturulan kodu her zaman inceleyin.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:40:48.467Z"
  },
  "integrations/vercel-ai-sdk.mdx": {
    "sourceHash": "1d20f0e049d70a71",
    "translations": {
      "zh": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"将 LemonData 与适用于 React 和 Next.js 应用程序的 Vercel AI SDK 集成\"\n---\n\n## 概览\n\nVercel AI SDK 是一个用于使用 React、Next.js、Vue、Svelte 等构建 AI 驱动的流式应用程序的 TypeScript 工具包。LemonData 通过兼容 OpenAI 的提供程序实现无缝协作。\n\n## 安装\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## 基础配置\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## 生成文本\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## 流式传输文本\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## 聊天消息\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## 使用不同的模型\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js API 路由\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## React 聊天组件\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## 工具调用\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## 结构化输出\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## 环境变量\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## 最佳实践\n\n<AccordionGroup>\n  <Accordion title=\"使用流式传输以获得更好的用户体验\">\n    始终在聊天应用程序中使用流式传输，以提供实时反馈。\n  </Accordion>\n\n  <Accordion title=\"优雅地处理错误\">\n    在 React 组件中实现适当的错误边界。\n  </Accordion>\n\n  <Accordion title=\"保护您的 API 密钥安全\">\n    切勿在客户端代码中暴露您的 API 密钥。请始终使用服务器端 API 路由。\n  </Accordion>\n</AccordionGroup>",
      "zh-TW": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"將 LemonData 與 Vercel AI SDK 整合，用於 React 和 Next.js 應用程式\"\n---\n\n## 概覽\n\nVercel AI SDK 是一個 TypeScript 工具包，用於使用 React、Next.js、Vue、Svelte 等框架構建 AI 驅動的串流應用程式。LemonData 可透過與 OpenAI 相容的 provider 無縫運作。\n\n## 安裝\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## 基本配置\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## 生成文字\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## 串流文字\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## 對話訊息\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## 使用不同的模型\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js API 路由\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## React 對話組件\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## 工具調用\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## 結構化輸出\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## 環境變數\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## 最佳實踐\n\n<AccordionGroup>\n  <Accordion title=\"使用串流以獲得更好的使用者體驗 (UX)\">\n    在對話應用程式中始終使用串流，以提供即時回饋。\n  </Accordion>\n\n  <Accordion title=\"優雅地處理錯誤\">\n    在 React 組件中實作適當的錯誤邊界 (Error Boundaries)。\n  </Accordion>\n\n  <Accordion title=\"保護您的 API key\">\n    切勿在用戶端程式碼中洩露您的 API key。請務必使用伺服器端 API 路由。\n  </Accordion>\n</AccordionGroup>",
      "ja": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"LemonDataをReactおよびNext.jsアプリケーション用のVercel AI SDKと統合する\"\n---\n\n## 概要\n\nVercel AI SDKは、React、Next.js、Vue、Svelteなどを使用してAI駆動のストリーミングアプリケーションを構築するためのTypeScriptツールキットです。LemonDataは、OpenAI互換プロバイダーを通じてシームレスに動作します。\n\n## インストール\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## 基本設定\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## テキスト生成\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## テキストのストリーミング\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## チャットメッセージ\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## 異なるモデルの使用\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js APIルート\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## Reactチャットコンポーネント\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## ツール呼び出し\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## 構造化出力\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## 環境変数\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## ベストプラクティス\n\n<AccordionGroup>\n  <Accordion title=\"優れたUXのためにストリーミングを使用する\">\n    リアルタイムのフィードバックを提供するために、チャットアプリケーションでは常にストリーミングを使用してください。\n  </Accordion>\n\n  <Accordion title=\"エラーを適切に処理する\">\n    Reactコンポーネントに適切なエラー境界（error boundaries）を実装してください。\n  </Accordion>\n\n  <Accordion title=\"APIキーを保護する\">\n    クライアント側のコードでAPIキーを公開しないでください。常にサーバー側のAPIルートを使用してください。\n  </Accordion>\n</AccordionGroup>",
      "ko": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"React 및 Next.js 애플리케이션을 위해 LemonData를 Vercel AI SDK와 통합하세요\"\n---\n\n## 개요\n\nVercel AI SDK는 React, Next.js, Vue, Svelte 등을 사용하여 AI 기반 스트리밍 애플리케이션을 구축하기 위한 TypeScript 툴킷입니다. LemonData는 OpenAI 호환 제공업체를 통해 원활하게 작동합니다.\n\n## 설치\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## 기본 설정\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## 텍스트 생성\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## 스트리밍 텍스트\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## 채팅 메시지\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## 다양한 모델 사용하기\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js API 라우트\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## React 채팅 컴포넌트\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## 도구 호출 (Tool Calling)\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## 구조화된 출력\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## 환경 변수\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## 권장 사항\n\n<AccordionGroup>\n  <Accordion title=\"더 나은 UX를 위해 스트리밍 사용\">\n    실시간 피드백을 제공하기 위해 채팅 애플리케이션에는 항상 스트리밍을 사용하세요.\n  </Accordion>\n\n  <Accordion title=\"정교한 에러 처리\">\n    React 컴포넌트에서 적절한 에러 경계(error boundaries)를 구현하세요.\n  </Accordion>\n\n  <Accordion title=\"API 키 보안\">\n    클라이언트 측 코드에 API 키를 노출하지 마세요. 항상 서버 측 API 라우트를 사용하세요.\n  </Accordion>\n</AccordionGroup>",
      "de": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"Integrieren Sie LemonData mit dem Vercel AI SDK für React- und Next.js-Anwendungen\"\n---\n\n## Übersicht\n\nDas Vercel AI SDK ist ein TypeScript-Toolkit zur Erstellung von KI-gestützten Streaming-Anwendungen mit React, Next.js, Vue, Svelte und mehr. LemonData funktioniert nahtlos über den OpenAI-kompatiblen Provider.\n\n## Installation\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Basiskonfiguration\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Text generieren\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Text-Streaming\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Chat-Nachrichten\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Verwendung verschiedener Modelle\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js API-Route\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## React Chat-Komponente\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Tool-Aufrufe\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Strukturierte Ausgabe\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Umgebungsvariablen\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Best Practices\n\n<AccordionGroup>\n  <Accordion title=\"Streaming für bessere UX nutzen\">\n    Verwenden Sie für Chat-Anwendungen immer Streaming, um Echtzeit-Feedback zu geben.\n  </Accordion>\n\n  <Accordion title=\"Fehler elegant behandeln\">\n    Implementieren Sie geeignete Error Boundaries in React-Komponenten.\n  </Accordion>\n\n  <Accordion title=\"Sichern Sie Ihren API-Key\">\n    Geben Sie Ihren API-Key niemals im clientseitigen Code preis. Verwenden Sie immer serverseitige API-Routen.\n  </Accordion>\n</AccordionGroup>",
      "fr": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"Intégrez LemonData avec le Vercel AI SDK pour les applications React et Next.js\"\n---\n\n## Présentation\n\nLe Vercel AI SDK est un kit d'outils TypeScript pour créer des applications de streaming basées sur l'IA avec React, Next.js, Vue, Svelte, et plus encore. LemonData fonctionne de manière fluide via le fournisseur compatible OpenAI.\n\n## Installation\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Configuration de base\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Générer du texte\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Streaming de texte\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Messages de chat\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Utilisation de différents modèles\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Route API Next.js\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## Composant de chat React\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Appel d'outils (Tool Calling)\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Sortie structurée\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Variables d'environnement\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Bonnes pratiques\n\n<AccordionGroup>\n  <Accordion title=\"Utilisez le streaming pour une meilleure UX\">\n    Utilisez toujours le streaming pour les applications de chat afin de fournir un retour en temps réel.\n  </Accordion>\n\n  <Accordion title=\"Gérez les erreurs avec élégance\">\n    Implémentez des limites d'erreur (`error boundaries`) appropriées dans les composants React.\n  </Accordion>\n\n  <Accordion title=\"Sécurisez votre clé API\">\n    N'exposez jamais votre clé API dans le code côté client. Utilisez toujours des routes API côté serveur.\n  </Accordion>\n</AccordionGroup>",
      "es": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"Integra LemonData con Vercel AI SDK para aplicaciones React y Next.js\"\n---\n\n## Visión general\n\nEl Vercel AI SDK es un conjunto de herramientas de TypeScript para construir aplicaciones de streaming impulsadas por IA con React, Next.js, Vue, Svelte y más. LemonData funciona sin problemas a través del proveedor compatible con OpenAI.\n\n## Instalación\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Configuración básica\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Generar texto\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Streaming de texto\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Mensajes de chat\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Uso de diferentes modelos\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Ruta de API de Next.js\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## Componente de chat de React\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Tool Calling\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Salida estructurada\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Variables de entorno\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Mejores prácticas\n\n<AccordionGroup>\n  <Accordion title=\"Usa streaming para una mejor UX\">\n    Usa siempre streaming para aplicaciones de chat para proporcionar retroalimentación en tiempo real.\n  </Accordion>\n\n  <Accordion title=\"Maneja los errores con elegancia\">\n    Implementa límites de error (`error boundaries`) adecuados en los componentes de React.\n  </Accordion>\n\n  <Accordion title=\"Asegura tu API key\">\n    Nunca expongas tu `API key` en el código del lado del cliente. Usa siempre rutas de API del lado del servidor.\n  </Accordion>\n</AccordionGroup>",
      "pt": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"Integre o LemonData com o Vercel AI SDK para aplicações React e Next.js\"\n---\n\n## Visão Geral\n\nO Vercel AI SDK é um toolkit TypeScript para construir aplicações de streaming baseadas em IA com React, Next.js, Vue, Svelte e mais. O LemonData funciona perfeitamente através do provedor compatível com OpenAI.\n\n## Instalação\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Configuração Básica\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Gerar Texto\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Streaming de Texto\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Mensagens de Chat\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Usando Diferentes Modelos\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Rota de API Next.js\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## Componente de Chat React\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Chamada de Ferramentas (Tool Calling)\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Saída Estruturada\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Variáveis de Ambiente\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Melhores Práticas\n\n<AccordionGroup>\n  <Accordion title=\"Use streaming para uma melhor UX\">\n    Sempre use streaming para aplicações de chat para fornecer feedback em tempo real.\n  </Accordion>\n\n  <Accordion title=\"Trate erros de forma adequada\">\n    Implemente error boundaries adequados em componentes React.\n  </Accordion>\n\n  <Accordion title=\"Proteja sua chave de API\">\n    Nunca exponha sua chave de API no código do lado do cliente. Sempre use rotas de API no lado do servidor.\n  </Accordion>\n</AccordionGroup>",
      "ar": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"قم بدمج LemonData مع Vercel AI SDK لتطبيقات React و Next.js\"\n---\n\n## نظرة عامة\n\nتُعد Vercel AI SDK مجموعة أدوات TypeScript لبناء تطبيقات البث (streaming) المدعومة بالذكاء الاصطناعي باستخدام React و Next.js و Vue و Svelte وغيرها. تعمل LemonData بسلاسة من خلال المزود المتوافق مع OpenAI.\n\n## التثبيت\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## الإعداد الأساسي\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## توليد النصوص\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## بث النصوص (Streaming)\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## رسائل الدردشة\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## استخدام نماذج مختلفة\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## مسار API في Next.js\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## مكون الدردشة في React\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## استدعاء الأدوات (Tool Calling)\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## المخرجات المهيكلة (Structured Output)\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## متغيرات البيئة\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## أفضل الممارسات\n\n<AccordionGroup>\n  <Accordion title=\"استخدم البث (streaming) لتجربة مستخدم أفضل\">\n    استخدم البث دائمًا لتطبيقات الدردشة لتقديم استجابة فورية في الوقت الفعلي.\n  </Accordion>\n\n  <Accordion title=\"معالجة الأخطاء بشكل سليم\">\n    قم بتنفيذ حدود الأخطاء (error boundaries) المناسبة في مكونات React.\n  </Accordion>\n\n  <Accordion title=\"تأمين مفتاح API الخاص بك\">\n    لا تقم أبدًا بالكشف عن مفتاح API الخاص بك في كود جانب العميل. استخدم دائمًا مسارات API من جانب الخادم.\n  </Accordion>\n</AccordionGroup>",
      "vi": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"Tích hợp LemonData với Vercel AI SDK cho các ứng dụng React và Next.js\"\n---\n\n## Tổng quan\n\nVercel AI SDK là một bộ công cụ TypeScript để xây dựng các ứng dụng streaming hỗ trợ AI với React, Next.js, Vue, Svelte và nhiều nền tảng khác. LemonData hoạt động mượt mà thông qua provider tương thích với OpenAI.\n\n## Cài đặt\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Cấu hình cơ bản\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Tạo văn bản\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Streaming văn bản\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Tin nhắn Chat\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Sử dụng các Model khác nhau\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js API Route\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## React Chat Component\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Tool Calling\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Kết quả có cấu trúc\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Biến môi trường\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Thực hành tốt nhất\n\n<AccordionGroup>\n  <Accordion title=\"Sử dụng streaming để có UX tốt hơn\">\n    Luôn sử dụng streaming cho các ứng dụng chat để cung cấp phản hồi theo thời gian thực.\n  </Accordion>\n\n  <Accordion title=\"Xử lý lỗi một cách khéo léo\">\n    Triển khai các error boundary phù hợp trong các component React.\n  </Accordion>\n\n  <Accordion title=\"Bảo mật API key của bạn\">\n    Không bao giờ để lộ API key trong mã nguồn phía client. Luôn sử dụng các API route phía server.\n  </Accordion>\n</AccordionGroup>",
      "id": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"Integrasikan LemonData dengan Vercel AI SDK untuk aplikasi React dan Next.js\"\n---\n\n## Gambaran Umum\n\nVercel AI SDK adalah toolkit TypeScript untuk membangun aplikasi streaming bertenaga AI dengan React, Next.js, Vue, Svelte, dan lainnya. LemonData bekerja secara mulus melalui penyedia yang kompatibel dengan OpenAI.\n\n## Instalasi\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Konfigurasi Dasar\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Menghasilkan Teks\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Streaming Teks\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Pesan Chat\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Menggunakan Berbagai Model\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Rute API Next.js\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## Komponen Chat React\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Pemanggilan Tool\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Output Terstruktur\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Variabel Lingkungan\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Praktik Terbaik\n\n<AccordionGroup>\n  <Accordion title=\"Gunakan streaming untuk UX yang lebih baik\">\n    Selalu gunakan streaming untuk aplikasi chat guna memberikan umpan balik secara real-time.\n  </Accordion>\n\n  <Accordion title=\"Tangani kesalahan dengan baik\">\n    Terapkan error boundaries yang tepat dalam komponen React.\n  </Accordion>\n\n  <Accordion title=\"Amankan API key Anda\">\n    Jangan pernah mengekspos API key Anda dalam kode sisi klien. Selalu gunakan rute API sisi server.\n  </Accordion>\n</AccordionGroup>",
      "tr": "---\ntitle: \"Vercel AI SDK\"\ndescription: \"React ve Next.js uygulamaları için LemonData'yı Vercel AI SDK ile entegre edin\"\n---\n\n## Genel Bakış\n\nVercel AI SDK; React, Next.js, Vue, Svelte ve daha fazlasıyla yapay zeka destekli akış (streaming) uygulamaları oluşturmak için tasarlanmış bir TypeScript araç setidir. LemonData, OpenAI uyumlu sağlayıcı aracılığıyla sorunsuz bir şekilde çalışır.\n\n## Kurulum\n\n```bash\nnpm install ai @ai-sdk/openai\n```\n\n## Temel Yapılandırma\n\n```typescript\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst lemondata = createOpenAI({\n  apiKey: 'sk-your-lemondata-key',\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## Metin Oluşturma\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'What is LemonData?',\n});\n\nconsole.log(text);\n```\n\n## Metin Akışı (Streaming)\n\n```typescript\nimport { streamText } from 'ai';\n\nconst result = await streamText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Write a poem about AI.',\n});\n\nfor await (const textPart of result.textStream) {\n  process.stdout.write(textPart);\n}\n```\n\n## Sohbet Mesajları\n\n```typescript\nimport { generateText } from 'ai';\n\nconst { text } = await generateText({\n  model: lemondata('gpt-4o'),\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' },\n  ],\n});\n```\n\n## Farklı Modelleri Kullanma\n\n```typescript\n// OpenAI GPT-4o\nconst gpt4Response = await generateText({\n  model: lemondata('gpt-4o'),\n  prompt: 'Hello!',\n});\n\n// Anthropic Claude\nconst claudeResponse = await generateText({\n  model: lemondata('claude-sonnet-4-5'),\n  prompt: 'Hello!',\n});\n\n// Google Gemini\nconst geminiResponse = await generateText({\n  model: lemondata('gemini-2.5-flash'),\n  prompt: 'Hello!',\n});\n\n// DeepSeek\nconst deepseekResponse = await generateText({\n  model: lemondata('deepseek-r1'),\n  prompt: 'Hello!',\n});\n```\n\n## Next.js API Rotası\n\n```typescript\n// app/api/chat/route.ts\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = streamText({\n    model: lemondata('gpt-4o'),\n    messages,\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\n## React Sohbet Bileşeni\n\n```tsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n  });\n\n  return (\n    <div>\n      {messages.map((m) => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n```\n\n## Araç Çağırma (Tool Calling)\n\n```typescript\nimport { generateText, tool } from 'ai';\nimport { z } from 'zod';\n\nconst { text, toolCalls } = await generateText({\n  model: lemondata('gpt-4o'),\n  tools: {\n    weather: tool({\n      description: 'Get the weather in a location',\n      parameters: z.object({\n        location: z.string().describe('The location to get weather for'),\n      }),\n      execute: async ({ location }) => {\n        return { temperature: 72, condition: 'sunny' };\n      },\n    }),\n  },\n  prompt: 'What is the weather in San Francisco?',\n});\n```\n\n## Yapılandırılmış Çıktı\n\n```typescript\nimport { generateObject } from 'ai';\nimport { z } from 'zod';\n\nconst { object } = await generateObject({\n  model: lemondata('gpt-4o'),\n  schema: z.object({\n    name: z.string(),\n    age: z.number(),\n    email: z.string().email(),\n  }),\n  prompt: 'Generate a fake user profile.',\n});\n\nconsole.log(object);\n```\n\n## Ortam Değişkenleri\n\n```bash\n# .env.local\nLEMONDATA_API_KEY=sk-your-lemondata-key\n```\n\n```typescript\nconst lemondata = createOpenAI({\n  apiKey: process.env.LEMONDATA_API_KEY,\n  baseURL: 'https://api.lemondata.cc/v1',\n});\n```\n\n## En İyi Uygulamalar\n\n<AccordionGroup>\n  <Accordion title=\"Daha iyi kullanıcı deneyimi (UX) için akış kullanın\">\n    Gerçek zamanlı geri bildirim sağlamak için sohbet uygulamalarında her zaman akış (streaming) kullanın.\n  </Accordion>\n\n  <Accordion title=\"Hataları düzgün bir şekilde yönetin\">\n    React bileşenlerinde uygun hata sınırları (error boundaries) uygulayın.\n  </Accordion>\n\n  <Accordion title=\"API anahtarınızı güvenceye alın\">\n    API anahtarınızı asla istemci tarafı kodunda açıkta bırakmayın. Her zaman sunucu tarafı API rotalarını kullanın.\n  </Accordion>\n</AccordionGroup>"
    },
    "updatedAt": "2026-01-26T05:41:04.619Z"
  },
  "introduction.mdx": {
    "sourceHash": "1e356021da191387",
    "translations": {
      "zh": "---\ntitle: \"简介\"\ndescription: \"通过统一的 API 访问 300 多个 AI 模型\"\n---\n\n## 什么是 LemonData？\n\nLemonData 是一个 AI API 聚合平台，提供对来自 OpenAI、Anthropic、Google、DeepSeek 等领先供应商的 **300 多个 AI 模型**的统一访问。\n\n<CardGroup cols={2}>\n  <Card title=\"兼容 OpenAI\" icon=\"plug\">\n    OpenAI API 的即插即用替代方案。只需更改基础 URL。\n  </Card>\n  <Card title=\"300+ 模型\" icon=\"robot\">\n    访问 GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora 等。\n  </Card>\n  <Card title=\"按需付费\" icon=\"coins\">\n    无需订阅。仅为您使用的部分付费，价格极具竞争力。\n  </Card>\n  <Card title=\"高可用性\" icon=\"shield-check\">\n    自动故障转移和负载均衡，确保服务可靠。\n  </Card>\n</CardGroup>\n\n## 支持的能力\n\n| 能力 | 描述 | 示例模型 |\n|------------|-------------|----------------|\n| **对话 (Chat)** | 文本生成与对话 | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **视觉 (Vision)** | 图像理解与分析 | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **图像生成** | 根据文本创建图像 | DALL-E 3, Midjourney, Flux, Ideogram |\n| **视频生成** | 根据文本/图像创建视频 | Sora, Runway Gen-3, Kling |\n| **音频** | 文本转语音与转录 | Whisper, TTS-1, MiniMax |\n| **嵌入 (Embeddings)** | 文本向量化 | text-embedding-3-small |\n| **音乐** | AI 音乐生成 | Suno |\n| **3D 生成** | 创建 3D 模型 | Tripo3D |\n\n## 核心特性\n\n<CardGroup cols={2}>\n  <Card title=\"多格式 API\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    使用单个 API key 即可调用 OpenAI、Anthropic 或 Gemini 的原生格式。无需切换 SDK。\n  </Card>\n  <Card title=\"语义缓存\" icon=\"bolt\" href=\"/guides/caching\">\n    智能缓存可降低成本和延迟。相似的查询将自动命中缓存。\n  </Card>\n  <Card title=\"智能路由\" icon=\"route\">\n    供应商之间的自动故障转移确保了高可用性和最优价格。\n  </Card>\n  <Card title=\"深度思考 (Extended Thinking)\" icon=\"brain\">\n    通过原生 Anthropic 格式支持访问 Claude 的深度思考模式。\n  </Card>\n</CardGroup>\n\n## 快速示例\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## 后续步骤\n\n<CardGroup cols={2}>\n  <Card title=\"快速入门\" icon=\"rocket\" href=\"/quickstart\">\n    获取您的 API key，并在 2 分钟内发出您的第一个请求。\n  </Card>\n  <Card title=\"API 参考\" icon=\"code\" href=\"/api-reference/introduction\">\n    探索完整的 API 文档。\n  </Card>\n  <Card title=\"模型\" icon=\"robot\" href=\"https://lemondata.cc/zh/models\">\n    浏览所有 300 多个可用模型。\n  </Card>\n  <Card title=\"价格\" icon=\"dollar-sign\" href=\"https://lemondata.cc/zh/models\">\n    查看所有模型的透明定价。\n  </Card>\n</CardGroup>",
      "zh-TW": "---\ntitle: \"簡介\"\ndescription: \"透過統一的 API 存取 300 多種 AI 模型\"\n---\n\n## 什麼是 LemonData？\n\nLemonData 是一個 AI API 聚合平台，提供對來自 OpenAI、Anthropic、Google、DeepSeek 等領先供應商的 **300 多種 AI 模型** 的統一存取。\n\n<CardGroup cols={2}>\n  <Card title=\"相容 OpenAI\" icon=\"plug\">\n    可直接替換 OpenAI API。只需更改 base URL。\n  </Card>\n  <Card title=\"300 多種模型\" icon=\"robot\">\n    存取 GPT-4o、Claude、Gemini、DeepSeek、Midjourney、Sora 等。\n  </Card>\n  <Card title=\"按需付費\" icon=\"coins\">\n    無需訂閱。僅為您使用的部分付費，價格極具競爭力。\n  </Card>\n  <Card title=\"高可用性\" icon=\"shield-check\">\n    自動故障轉移和負載平衡，確保服務可靠。\n  </Card>\n</CardGroup>\n\n## 支援的功能\n\n| 功能 | 描述 | 範例模型 |\n|------------|-------------|----------------|\n| **對話** | 文字生成與對話 | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **視覺** | 圖片理解與分析 | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **圖片生成** | 根據文字建立圖片 | DALL-E 3, Midjourney, Flux, Ideogram |\n| **影片生成** | 根據文字/圖片建立影片 | Sora, Runway Gen-3, Kling |\n| **音訊** | 文字轉語音與逐字稿 | Whisper, TTS-1, MiniMax |\n| **Embeddings** | 文字向量化 | text-embedding-3-small |\n| **音樂** | AI 音樂生成 | Suno |\n| **3D 生成** | 建立 3D 模型 | Tripo3D |\n\n## 核心特性\n\n<CardGroup cols={2}>\n  <Card title=\"多格式 API\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    使用單一 API key 即可使用 OpenAI、Anthropic 或 Gemini 的原生格式。無需切換 SDK。\n  </Card>\n  <Card title=\"語義快取\" icon=\"bolt\" href=\"/guides/caching\">\n    智慧快取可降低成本和延遲。相似的查詢會自動命中快取。\n  </Card>\n  <Card title=\"智慧路由\" icon=\"route\">\n    供應商之間的自動故障轉移可確保高可用性和最佳價格。\n  </Card>\n  <Card title=\"延伸思考\" icon=\"brain\">\n    透過原生 Anthropic 格式支援存取 Claude 的延伸思考模式。\n  </Card>\n</CardGroup>\n\n## 快速範例\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## 後續步驟\n\n<CardGroup cols={2}>\n  <Card title=\"快速入門\" icon=\"rocket\" href=\"/quickstart\">\n    在 2 分鐘內獲取您的 API key 並發送您的第一個請求。\n  </Card>\n  <Card title=\"API 參考\" icon=\"code\" href=\"/api-reference/introduction\">\n    探索完整的 API 文件。\n  </Card>\n  <Card title=\"模型\" icon=\"robot\" href=\"https://lemondata.cc/zh-TW/models\">\n    瀏覽所有 300 多種可用模型。\n  </Card>\n  <Card title=\"定價\" icon=\"dollar-sign\" href=\"https://lemondata.cc/zh-TW/models\">\n    查看所有模型的透明定價。\n  </Card>\n</CardGroup>",
      "ja": "---\ntitle: \"はじめに\"\ndescription: \"統合されたAPIを通じて300以上のAIモデルにアクセス\"\n---\n\n## LemonDataとは？\n\nLemonDataは、OpenAI、Anthropic、Google、DeepSeekなどの主要プロバイダーによる**300以上のAIモデル**への統合アクセスを提供するAI APIアグリゲーションプラットフォームです。\n\n<CardGroup cols={2}>\n  <Card title=\"OpenAI互換\" icon=\"plug\">\n    OpenAI APIのドロップインリプレイスメントとして利用可能です。ベースURLを変更するだけで使用できます。\n  </Card>\n  <Card title=\"300以上のモデル\" icon=\"robot\">\n    GPT-4o、Claude、Gemini、DeepSeek、Midjourney、Soraなどにアクセスできます。\n  </Card>\n  <Card title=\"従量課金制\" icon=\"coins\">\n    サブスクリプションは不要です。競争力のある価格設定で、使用した分だけお支払いいただけます。\n  </Card>\n  <Card title=\"高可用性\" icon=\"shield-check\">\n    信頼性の高いサービスを提供するため、自動フェイルオーバーとロードバランシングを備えています。\n  </Card>\n</CardGroup>\n\n## サポートされている機能\n\n| 機能 | 説明 | モデル例 |\n|------------|-------------|----------------|\n| **チャット** | テキスト生成および対話 | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **ビジョン** | 画像理解および分析 | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **画像生成** | テキストからの画像作成 | DALL-E 3, Midjourney, Flux, Ideogram |\n| **動画生成** | テキストや画像からの動画作成 | Sora, Runway Gen-3, Kling |\n| **オーディオ** | テキスト読み上げおよび文字起こし | Whisper, TTS-1, MiniMax |\n| **埋め込み** | テキストのベクトル化 | text-embedding-3-small |\n| **音楽** | AIによる音楽生成 | Suno |\n| **3D生成** | 3Dモデルの作成 | Tripo3D |\n\n## 主な特徴\n\n<CardGroup cols={2}>\n  <Card title=\"マルチフォーマットAPI\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    単一のAPIキーで、OpenAI、Anthropic、またはGeminiのネイティブフォーマットを使用できます。SDKを切り替える必要はありません。\n  </Card>\n  <Card title=\"セマンティックキャッシュ\" icon=\"bolt\" href=\"/guides/caching\">\n    インテリジェントなキャッシュにより、コストとレイテンシを削減します。類似のクエリは自動的にキャッシュにヒットします。\n  </Card>\n  <Card title=\"スマートルーティング\" icon=\"route\">\n    プロバイダー間の自動フェイルオーバーにより、高可用性と最適な価格設定を保証します。\n  </Card>\n  <Card title=\"思考拡張（Extended Thinking）\" icon=\"brain\">\n    ネイティブのAnthropicフォーマットサポートを通じて、Claudeの思考拡張モードにアクセスできます。\n  </Card>\n</CardGroup>\n\n## クイック例\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## 次のステップ\n\n<CardGroup cols={2}>\n  <Card title=\"クイックスタート\" icon=\"rocket\" href=\"/quickstart\">\n    APIキーを取得して、2分以内に最初のリクエストを実行しましょう。\n  </Card>\n  <Card title=\"APIリファレンス\" icon=\"code\" href=\"/api-reference/introduction\">\n    完全なAPIドキュメントを確認してください。\n  </Card>\n  <Card title=\"モデル\" icon=\"robot\" href=\"https://lemondata.cc/ja/models\">\n    利用可能な300以上の全モデルを閲覧できます。\n  </Card>\n  <Card title=\"料金\" icon=\"dollar-sign\" href=\"https://lemondata.cc/ja/models\">\n    すべてのモデルの透明性の高い料金体系を確認できます。\n  </Card>\n</CardGroup>",
      "ko": "---\ntitle: \"소개\"\ndescription: \"통합 API를 통해 300개 이상의 AI 모델에 액세스하세요\"\n---\n\n## LemonData란 무엇인가요?\n\nLemonData는 OpenAI, Anthropic, Google, DeepSeek 등 주요 제공업체의 **300개 이상의 AI 모델**에 대한 통합 액세스를 제공하는 AI API 애그리게이션 플랫폼입니다.\n\n<CardGroup cols={2}>\n  <Card title=\"OpenAI 호환성\" icon=\"plug\">\n    OpenAI API를 즉시 대체할 수 있습니다. base URL만 변경하면 됩니다.\n  </Card>\n  <Card title=\"300개 이상의 모델\" icon=\"robot\">\n    GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora 등을 이용할 수 있습니다.\n  </Card>\n  <Card title=\"종량제 요금제\" icon=\"coins\">\n    구독료가 없습니다. 경쟁력 있는 가격으로 사용한 만큼만 지불하세요.\n  </Card>\n  <Card title=\"높은 가용성\" icon=\"shield-check\">\n    신뢰할 수 있는 서비스를 위한 자동 장애 조치(failover) 및 로드 밸런싱을 제공합니다.\n  </Card>\n</CardGroup>\n\n## 지원되는 기능\n\n| 기능 | 설명 | 예시 모델 |\n|------------|-------------|----------------|\n| **채팅** | 텍스트 생성 및 대화 | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **비전** | 이미지 이해 및 분석 | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **이미지 생성** | 텍스트로 이미지 생성 | DALL-E 3, Midjourney, Flux, Ideogram |\n| **비디오 생성** | 텍스트/이미지로 비디오 생성 | Sora, Runway Gen-3, Kling |\n| **오디오** | 텍스트 음성 변환 및 전사 | Whisper, TTS-1, MiniMax |\n| **임베딩** | 텍스트 벡터화 | text-embedding-3-small |\n| **음악** | AI 음악 생성 | Suno |\n| **3D 생성** | 3D 모델 생성 | Tripo3D |\n\n## 주요 기능\n\n<CardGroup cols={2}>\n  <Card title=\"멀티 포맷 API\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    단일 API key로 OpenAI, Anthropic 또는 Gemini 네이티브 포맷을 사용하세요. SDK를 교체할 필요가 없습니다.\n  </Card>\n  <Card title=\"시맨틱 캐싱\" icon=\"bolt\" href=\"/guides/caching\">\n    지능형 캐싱으로 비용과 지연 시간을 줄입니다. 유사한 쿼리는 자동으로 캐시를 사용합니다.\n  </Card>\n  <Card title=\"스마트 라우팅\" icon=\"route\">\n    제공업체 간의 자동 장애 조치를 통해 높은 가용성과 최적의 가격을 보장합니다.\n  </Card>\n  <Card title=\"확장 사고(Extended Thinking)\" icon=\"brain\">\n    네이티브 Anthropic 포맷 지원을 통해 Claude의 확장 사고 모드에 액세스하세요.\n  </Card>\n</CardGroup>\n\n## 빠른 예시\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## 다음 단계\n\n<CardGroup cols={2}>\n  <Card title=\"퀵스타트\" icon=\"rocket\" href=\"/quickstart\">\n    API key를 발급받고 2분 안에 첫 번째 요청을 보내보세요.\n  </Card>\n  <Card title=\"API 레퍼런스\" icon=\"code\" href=\"/api-reference/introduction\">\n    전체 API 문서를 살펴보세요.\n  </Card>\n  <Card title=\"모델\" icon=\"robot\" href=\"https://lemondata.cc/ko/models\">\n    사용 가능한 300개 이상의 모든 모델을 찾아보세요.\n  </Card>\n  <Card title=\"요금 안내\" icon=\"dollar-sign\" href=\"https://lemondata.cc/ko/models\">\n    모든 모델에 대한 투명한 요금을 확인하세요.\n  </Card>\n</CardGroup>",
      "de": "---\ntitle: \"Einführung\"\ndescription: \"Greifen Sie über eine einheitliche API auf über 300 KI-Modelle zu\"\n---\n\n## Was ist LemonData?\n\nLemonData ist eine KI-API-Aggregationsplattform, die einen einheitlichen Zugriff auf **über 300 KI-Modelle** von führenden Anbietern wie OpenAI, Anthropic, Google, DeepSeek und weiteren bietet.\n\n<CardGroup cols={2}>\n  <Card title=\"OpenAI-kompatibel\" icon=\"plug\">\n    Drop-in-Ersatz für die OpenAI API. Ändern Sie einfach die Basis-URL.\n  </Card>\n  <Card title=\"Über 300 Modelle\" icon=\"robot\">\n    Greifen Sie auf GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora und mehr zu.\n  </Card>\n  <Card title=\"Pay-As-You-Go\" icon=\"coins\">\n    Keine Abonnements. Zahlen Sie nur für das, was Sie nutzen, zu wettbewerbsfähigen Preisen.\n  </Card>\n  <Card title=\"Hohe Verfügbarkeit\" icon=\"shield-check\">\n    Automatisches Failover und Load Balancing für einen zuverlässigen Service.\n  </Card>\n</CardGroup>\n\n## Unterstützte Funktionen\n\n| Funktion | Beschreibung | Beispielmodelle |\n|------------|-------------|----------------|\n| **Chat** | Texterstellung und Konversation | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Vision** | Bildverständnis und -analyse | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Bilderzeugung** | Bilder aus Text erstellen | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Videoerzeugung** | Videos aus Text/Bildern erstellen | Sora, Runway Gen-3, Kling |\n| **Audio** | Text-to-Speech und Transkription | Whisper, TTS-1, MiniMax |\n| **Embeddings** | Textvektorisierung | text-embedding-3-small |\n| **Musik** | KI-Musikerzeugung | Suno |\n| **3D-Erzeugung** | 3D-Modelle erstellen | Tripo3D |\n\n## Hauptmerkmale\n\n<CardGroup cols={2}>\n  <Card title=\"Multi-Format-API\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Nutzen Sie native Formate von OpenAI, Anthropic oder Gemini mit einem einzigen API-Key. Kein Wechsel des SDK erforderlich.\n  </Card>\n  <Card title=\"Semantisches Caching\" icon=\"bolt\" href=\"/guides/caching\">\n    Intelligentes Caching reduziert Kosten und Latenz. Ähnliche Abfragen nutzen automatisch den Cache.\n  </Card>\n  <Card title=\"Smart Routing\" icon=\"route\">\n    Automatisches Failover zwischen Anbietern gewährleistet hohe Verfügbarkeit und optimale Preise.\n  </Card>\n  <Card title=\"Extended Thinking\" icon=\"brain\">\n    Greifen Sie über die native Unterstützung des Anthropic-Formats auf den Extended Thinking Modus von Claude zu.\n  </Card>\n</CardGroup>\n\n## Schnelles Beispiel\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Nächste Schritte\n\n<CardGroup cols={2}>\n  <Card title=\"Schnellstart\" icon=\"rocket\" href=\"/quickstart\">\n    Holen Sie sich Ihren API-Key und senden Sie Ihre erste Anfrage in nur 2 Minuten.\n  </Card>\n  <Card title=\"API-Referenz\" icon=\"code\" href=\"/api-reference/introduction\">\n    Erkunden Sie die vollständige API-Dokumentation.\n  </Card>\n  <Card title=\"Modelle\" icon=\"robot\" href=\"https://lemondata.cc/de/models\">\n    Durchsuchen Sie alle über 300 verfügbaren Modelle.\n  </Card>\n  <Card title=\"Preise\" icon=\"dollar-sign\" href=\"https://lemondata.cc/de/models\">\n    Sehen Sie sich die transparenten Preise für alle Modelle an.\n  </Card>\n</CardGroup>",
      "fr": "---\ntitle: \"Introduction\"\ndescription: \"Accédez à plus de 300 modèles d'IA via une API unifiée\"\n---\n\n## Qu'est-ce que LemonData ?\n\nLemonData est une plateforme d'agrégation d'API d'IA qui offre un accès unifié à **plus de 300 modèles d'IA** provenant des principaux fournisseurs, notamment OpenAI, Anthropic, Google, DeepSeek, et plus encore.\n\n<CardGroup cols={2}>\n  <Card title=\"Compatible OpenAI\" icon=\"plug\">\n    Remplacement direct pour l'API OpenAI. Il suffit de changer l'URL de base.\n  </Card>\n  <Card title=\"Plus de 300 modèles\" icon=\"robot\">\n    Accédez à GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora, et plus encore.\n  </Card>\n  <Card title=\"Paiement à l'usage\" icon=\"coins\">\n    Pas d'abonnement. Ne payez que ce que vous utilisez avec des tarifs compétitifs.\n  </Card>\n  <Card title=\"Haute disponibilité\" icon=\"shield-check\">\n    Basculement automatique et équilibrage de charge pour un service fiable.\n  </Card>\n</CardGroup>\n\n## Capacités prises en charge\n\n| Capacité | Description | Exemples de modèles |\n|------------|-------------|----------------|\n| **Chat** | Génération de texte et conversation | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Vision** | Compréhension et analyse d'images | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Génération d'images** | Création d'images à partir de texte | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Génération de vidéos** | Création de vidéos à partir de texte/images | Sora, Runway Gen-3, Kling |\n| **Audio** | Synthèse vocale et transcription | Whisper, TTS-1, MiniMax |\n| **Embeddings** | Vectorisation de texte | text-embedding-3-small |\n| **Musique** | Génération de musique par IA | Suno |\n| **Génération 3D** | Création de modèles 3D | Tripo3D |\n\n## Fonctionnalités clés\n\n<CardGroup cols={2}>\n  <Card title=\"API multi-format\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Utilisez les formats natifs d'OpenAI, Anthropic ou Gemini avec une seule clé API. Aucun changement de SDK n'est nécessaire.\n  </Card>\n  <Card title=\"Mise en cache sémantique\" icon=\"bolt\" href=\"/guides/caching\">\n    La mise en cache intelligente réduit les coûts et la latence. Les requêtes similaires utilisent automatiquement le cache.\n  </Card>\n  <Card title=\"Routage intelligent\" icon=\"route\">\n    Le basculement automatique entre les fournisseurs garantit une haute disponibilité et une tarification optimale.\n  </Card>\n  <Card title=\"Réflexion étendue\" icon=\"brain\">\n    Accédez au mode de réflexion étendue de Claude via le support natif du format Anthropic.\n  </Card>\n</CardGroup>\n\n## Exemple rapide\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Prochaines étapes\n\n<CardGroup cols={2}>\n  <Card title=\"Démarrage rapide\" icon=\"rocket\" href=\"/quickstart\">\n    Obtenez votre clé API et effectuez votre première requête en 2 minutes.\n  </Card>\n  <Card title=\"Référence API\" icon=\"code\" href=\"/api-reference/introduction\">\n    Explorez la documentation complète de l'API.\n  </Card>\n  <Card title=\"Modèles\" icon=\"robot\" href=\"https://lemondata.cc/fr/models\">\n    Parcourez l'ensemble des plus de 300 modèles disponibles.\n  </Card>\n  <Card title=\"Tarification\" icon=\"dollar-sign\" href=\"https://lemondata.cc/fr/models\">\n    Consultez une tarification transparente pour tous les modèles.\n  </Card>\n</CardGroup>",
      "es": "---\ntitle: \"Introducción\"\ndescription: \"Acceda a más de 300 modelos de IA a través de una API unificada\"\n---\n\n## ¿Qué es LemonData?\n\nLemonData es una plataforma de agregación de API de IA que proporciona acceso unificado a **más de 300 modelos de IA** de proveedores líderes, incluidos OpenAI, Anthropic, Google, DeepSeek y más.\n\n<CardGroup cols={2}>\n  <Card title=\"Compatible con OpenAI\" icon=\"plug\">\n    Reemplazo directo para la API de OpenAI. Solo cambie la URL base.\n  </Card>\n  <Card title=\"Más de 300 modelos\" icon=\"robot\">\n    Acceda a GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora y más.\n  </Card>\n  <Card title=\"Pago por uso\" icon=\"coins\">\n    Sin suscripciones. Solo pague por lo que usa con precios competitivos.\n  </Card>\n  <Card title=\"Alta disponibilidad\" icon=\"shield-check\">\n    Conmutación por error automática y equilibrio de carga para un servicio confiable.\n  </Card>\n</CardGroup>\n\n## Capacidades compatibles\n\n| Capacidad | Descripción | Modelos de ejemplo |\n|------------|-------------|----------------|\n| **Chat** | Generación de texto y conversación | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Visión** | Comprensión y análisis de imágenes | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Generación de imágenes** | Cree imágenes a partir de texto | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Generación de video** | Cree videos a partir de texto/imágenes | Sora, Runway Gen-3, Kling |\n| **Audio** | Texto a voz y transcripción | Whisper, TTS-1, MiniMax |\n| **Embeddings** | Vectorización de texto | text-embedding-3-small |\n| **Música** | Generación de música por IA | Suno |\n| **Generación 3D** | Cree modelos 3D | Tripo3D |\n\n## Características principales\n\n<CardGroup cols={2}>\n  <Card title=\"API multiformato\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Use los formatos nativos de OpenAI, Anthropic o Gemini con una sola API key. Sin necesidad de cambiar de SDK.\n  </Card>\n  <Card title=\"Caché semántica\" icon=\"bolt\" href=\"/guides/caching\">\n    El almacenamiento en caché inteligente reduce los costos y la latencia. Las consultas similares utilizan el caché automáticamente.\n  </Card>\n  <Card title=\"Enrutamiento inteligente\" icon=\"route\">\n    La conmutación por error automática entre proveedores garantiza una alta disponibilidad y precios óptimos.\n  </Card>\n  <Card title=\"Pensamiento extendido\" icon=\"brain\">\n    Acceda al modo de pensamiento extendido de Claude a través del soporte de formato nativo de Anthropic.\n  </Card>\n</CardGroup>\n\n## Ejemplo rápido\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Próximos pasos\n\n<CardGroup cols={2}>\n  <Card title=\"Inicio rápido\" icon=\"rocket\" href=\"/quickstart\">\n    Obtenga su API key y realice su primera solicitud en 2 minutos.\n  </Card>\n  <Card title=\"Referencia de la API\" icon=\"code\" href=\"/api-reference/introduction\">\n    Explore la documentación completa de la API.\n  </Card>\n  <Card title=\"Modelos\" icon=\"robot\" href=\"https://lemondata.cc/es/models\">\n    Explore los más de 300 modelos disponibles.\n  </Card>\n  <Card title=\"Precios\" icon=\"dollar-sign\" href=\"https://lemondata.cc/es/models\">\n    Consulte precios transparentes para todos los modelos.\n  </Card>\n</CardGroup>",
      "pt": "---\ntitle: \"Introdução\"\ndescription: \"Acesse mais de 300 modelos de IA através de uma API unificada\"\n---\n\n## O que é LemonData?\n\nLemonData é uma plataforma de agregação de APIs de IA que fornece acesso unificado a **mais de 300 modelos de IA** dos principais provedores, incluindo OpenAI, Anthropic, Google, DeepSeek e outros.\n\n<CardGroup cols={2}>\n  <Card title=\"Compatível com OpenAI\" icon=\"plug\">\n    Substituição direta para a API da OpenAI. Basta alterar a URL base.\n  </Card>\n  <Card title=\"Mais de 300 Modelos\" icon=\"robot\">\n    Acesse GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora e muito mais.\n  </Card>\n  <Card title=\"Pagamento por Uso\" icon=\"coins\">\n    Sem assinaturas. Pague apenas pelo que usar com preços competitivos.\n  </Card>\n  <Card title=\"Alta Disponibilidade\" icon=\"shield-check\">\n    Failover automático e balanceamento de carga para um serviço confiável.\n  </Card>\n</CardGroup>\n\n## Capacidades Suportadas\n\n| Capacidade | Descrição | Modelos de Exemplo |\n|------------|-------------|----------------|\n| **Chat** | Geração de texto e conversação | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Visão** | Compreensão e análise de imagens | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Geração de Imagens** | Crie imagens a partir de texto | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Geração de Vídeos** | Crie vídeos a partir de texto/imagens | Sora, Runway Gen-3, Kling |\n| **Áudio** | Texto-para-fala e transcrição | Whisper, TTS-1, MiniMax |\n| **Embeddings** | Vetorização de texto | text-embedding-3-small |\n| **Música** | Geração de música por IA | Suno |\n| **Geração 3D** | Crie modelos 3D | Tripo3D |\n\n## Principais Recursos\n\n<CardGroup cols={2}>\n  <Card title=\"API Multi-formato\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Use os formatos nativos da OpenAI, Anthropic ou Gemini com uma única chave de API. Sem necessidade de trocar de SDK.\n  </Card>\n  <Card title=\"Cache Semântico\" icon=\"bolt\" href=\"/guides/caching\">\n    O cache inteligente reduz custos e latência. Consultas semelhantes atingem o cache automaticamente.\n  </Card>\n  <Card title=\"Roteamento Inteligente\" icon=\"route\">\n    O failover automático entre provedores garante alta disponibilidade e preços otimizados.\n  </Card>\n  <Card title=\"Pensamento Estendido\" icon=\"brain\">\n    Acesse o modo de pensamento estendido do Claude via suporte ao formato nativo da Anthropic.\n  </Card>\n</CardGroup>\n\n## Exemplo Rápido\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Próximos Passos\n\n<CardGroup cols={2}>\n  <Card title=\"Início Rápido\" icon=\"rocket\" href=\"/quickstart\">\n    Obtenha sua chave de API e faça sua primeira requisição em 2 minutos.\n  </Card>\n  <Card title=\"Referência da API\" icon=\"code\" href=\"/api-reference/introduction\">\n    Explore the complete API documentation.\n  </Card>\n  <Card title=\"Modelos\" icon=\"robot\" href=\"https://lemondata.cc/pt/models\">\n    Navegue por todos os mais de 300 modelos disponíveis.\n  </Card>\n  <Card title=\"Preços\" icon=\"dollar-sign\" href=\"https://lemondata.cc/pt/models\">\n    Visualize preços transparentes para todos os modelos.\n  </Card>\n</CardGroup>",
      "ar": "---\ntitle: \"مقدمة\"\ndescription: \"الوصول إلى أكثر من 300 نموذج ذكاء اصطناعي عبر API موحد\"\n---\n\n## ما هو LemonData؟\n\nLemonData هي منصة تجميع AI API توفر وصولاً موحداً إلى **أكثر من 300 نموذج ذكاء اصطناعي** من كبار المزودين بما في ذلك OpenAI و Anthropic و Google و DeepSeek والمزيد.\n\n<CardGroup cols={2}>\n  <Card title=\"متوافق مع OpenAI\" icon=\"plug\">\n    بديل مباشر لـ OpenAI API. ما عليك سوى تغيير `base URL`.\n  </Card>\n  <Card title=\"أكثر من 300 نموذج\" icon=\"robot\">\n    الوصول إلى GPT-4o و Claude و Gemini و DeepSeek و Midjourney و Sora والمزيد.\n  </Card>\n  <Card title=\"الدفع حسب الاستخدام\" icon=\"coins\">\n    لا توجد اشتراكات. ادفع فقط مقابل ما تستخدمه بأسعار تنافسية.\n  </Card>\n  <Card title=\"توفر عالٍ\" icon=\"shield-check\">\n    تبديل تلقائي للفشل وموازنة التحميل لخدمة موثوقة.\n  </Card>\n</CardGroup>\n\n## القدرات المدعومة\n\n| القدرة | الوصف | نماذج أمثلة |\n|------------|-------------|----------------|\n| **الدردشة** | توليد النصوص والمحادثة | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **الرؤية** | فهم الصور وتحليلها | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **توليد الصور** | إنشاء صور من النصوص | DALL-E 3, Midjourney, Flux, Ideogram |\n| **توليد الفيديو** | إنشاء فيديوهات من النصوص/الصور | Sora, Runway Gen-3, Kling |\n| **الصوت** | تحويل النص إلى كلام والنسخ الصوتي | Whisper, TTS-1, MiniMax |\n| **Embeddings** | تمثيل النصوص متجهياً | text-embedding-3-small |\n| **الموسيقى** | توليد الموسيقى بالذكاء الاصطناعي | Suno |\n| **توليد ثلاثي الأبعاد** | إنشاء نماذج ثلاثية الأبعاد | Tripo3D |\n\n## الميزات الرئيسية\n\n<CardGroup cols={2}>\n  <Card title=\"API متعدد التنسيقات\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    استخدم تنسيقات OpenAI أو Anthropic أو Gemini الأصلية باستخدام API key واحد. لا حاجة لتبديل SDK.\n  </Card>\n  <Card title=\"التخزين المؤقت الدلالي\" icon=\"bolt\" href=\"/guides/caching\">\n    يقلل التخزين المؤقت الذكي (Semantic Caching) من التكاليف وزمن الاستجابة. الاستعلامات المتشابهة تستخدم التخزين المؤقت تلقائياً.\n  </Card>\n  <Card title=\"التوجيه الذكي\" icon=\"route\">\n    يضمن التبديل التلقائي للفشل بين المزودين توفراً عالياً وأسعاراً مثالية.\n  </Card>\n  <Card title=\"التفكير الممتد\" icon=\"brain\">\n    الوصول إلى وضع التفكير الممتد (Extended Thinking) لـ Claude عبر دعم تنسيق Anthropic الأصلي.\n  </Card>\n</CardGroup>\n\n## مثال سريع\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## الخطوات التالية\n\n<CardGroup cols={2}>\n  <Card title=\"البدء السريع\" icon=\"rocket\" href=\"/quickstart\">\n    احصل على API key الخاص بك وقم بإجراء طلبك الأول في دقيقتين.\n  </Card>\n  <Card title=\"مرجع API\" icon=\"code\" href=\"/api-reference/introduction\">\n    استكشف توثيق API الكامل.\n  </Card>\n  <Card title=\"النماذج\" icon=\"robot\" href=\"https://lemondata.cc/ar/models\">\n    تصفح جميع النماذج المتاحة التي تزيد عن 300 نموذج.\n  </Card>\n  <Card title=\"الأسعار\" icon=\"dollar-sign\" href=\"https://lemondata.cc/ar/models\">\n    عرض أسعار شفافة لجميع النماذج.\n  </Card>\n</CardGroup>",
      "vi": "---\ntitle: \"Giới thiệu\"\ndescription: \"Truy cập hơn 300 mô hình AI thông qua một API thống nhất\"\n---\n\n## LemonData là gì?\n\nLemonData là một nền tảng tổng hợp AI API cung cấp quyền truy cập thống nhất vào **hơn 300 mô hình AI** từ các nhà cung cấp hàng đầu bao gồm OpenAI, Anthropic, Google, DeepSeek và nhiều đơn vị khác.\n\n<CardGroup cols={2}>\n  <Card title=\"Tương thích với OpenAI\" icon=\"plug\">\n    Thay thế trực tiếp cho OpenAI API. Chỉ cần thay đổi base URL.\n  </Card>\n  <Card title=\"Hơn 300 mô hình\" icon=\"robot\">\n    Truy cập GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora và nhiều hơn nữa.\n  </Card>\n  <Card title=\"Thanh toán theo mức sử dụng\" icon=\"coins\">\n    Không cần đăng ký gói cố định. Chỉ trả tiền cho những gì bạn sử dụng với mức giá cạnh tranh.\n  </Card>\n  <Card title=\"Độ khả dụng cao\" icon=\"shield-check\">\n    Tự động chuyển vùng lỗi (failover) và cân bằng tải để đảm bảo dịch vụ tin cậy.\n  </Card>\n</CardGroup>\n\n## Các khả năng được hỗ trợ\n\n| Khả năng | Mô tả | Mô hình ví dụ |\n|------------|-------------|----------------|\n| **Chat** | Tạo văn bản và hội thoại | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Vision** | Hiểu và phân tích hình ảnh | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Tạo hình ảnh** | Tạo hình ảnh từ văn bản | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Tạo video** | Tạo video từ văn bản/hình ảnh | Sora, Runway Gen-3, Kling |\n| **Âm thanh** | Chuyển văn bản thành giọng nói và phiên âm | Whisper, TTS-1, MiniMax |\n| **Embeddings** | Vector hóa văn bản | text-embedding-3-small |\n| **Âm nhạc** | Tạo nhạc bằng AI | Suno |\n| **Tạo 3D** | Tạo mô hình 3D | Tripo3D |\n\n## Các tính năng chính\n\n<CardGroup cols={2}>\n  <Card title=\"API đa định dạng\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Sử dụng các định dạng gốc của OpenAI, Anthropic hoặc Gemini với một API key duy nhất. Không cần chuyển đổi SDK.\n  </Card>\n  <Card title=\"Bộ nhớ đệm ngữ nghĩa\" icon=\"bolt\" href=\"/guides/caching\">\n    Bộ nhớ đệm thông minh giúp giảm chi phí và độ trễ. Các truy vấn tương tự sẽ tự động truy xuất từ bộ nhớ đệm.\n  </Card>\n  <Card title=\"Định tuyến thông minh\" icon=\"route\">\n    Tự động chuyển đổi giữa các nhà cung cấp để đảm bảo độ khả dụng cao và mức giá tối ưu.\n  </Card>\n  <Card title=\"Suy nghĩ mở rộng\" icon=\"brain\">\n    Truy cập chế độ suy nghĩ mở rộng của Claude thông qua hỗ trợ định dạng Anthropic gốc.\n  </Card>\n</CardGroup>\n\n## Ví dụ nhanh\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Các bước tiếp theo\n\n<CardGroup cols={2}>\n  <Card title=\"Bắt đầu nhanh\" icon=\"rocket\" href=\"/quickstart\">\n    Nhận API key của bạn và thực hiện yêu cầu đầu tiên trong vòng 2 phút.\n  </Card>\n  <Card title=\"Tài liệu tham khảo API\" icon=\"code\" href=\"/api-reference/introduction\">\n    Khám phá tài liệu API đầy đủ.\n  </Card>\n  <Card title=\"Mô hình\" icon=\"robot\" href=\"https://lemondata.cc/vi/models\">\n    Duyệt qua tất cả hơn 300 mô hình hiện có.\n  </Card>\n  <Card title=\"Bảng giá\" icon=\"dollar-sign\" href=\"https://lemondata.cc/vi/models\">\n    Xem bảng giá minh bạch cho tất cả các mô hình.\n  </Card>\n</CardGroup>",
      "id": "---\ntitle: \"Pendahuluan\"\ndescription: \"Akses 300+ model AI melalui satu API terpadu\"\n---\n\n## Apa itu LemonData?\n\nLemonData adalah platform agregasi API AI yang menyediakan akses terpadu ke **300+ model AI** dari penyedia terkemuka termasuk OpenAI, Anthropic, Google, DeepSeek, dan banyak lagi.\n\n<CardGroup cols={2}>\n  <Card title=\"Kompatibel dengan OpenAI\" icon=\"plug\">\n    Pengganti langsung untuk API OpenAI. Cukup ubah base URL.\n  </Card>\n  <Card title=\"300+ Model\" icon=\"robot\">\n    Akses GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora, dan lainnya.\n  </Card>\n  <Card title=\"Bayar Sesuai Pemakaian\" icon=\"coins\">\n    Tanpa langganan. Hanya bayar untuk apa yang Anda gunakan dengan harga yang kompetitif.\n  </Card>\n  <Card title=\"Ketersediaan Tinggi\" icon=\"shield-check\">\n    Failover otomatis dan load balancing untuk layanan yang andal.\n  </Card>\n</CardGroup>\n\n## Kemampuan yang Didukung\n\n| Kemampuan | Deskripsi | Contoh Model |\n|------------|-------------|----------------|\n| **Chat** | Generasi teks dan percakapan | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Vision** | Pemahaman dan analisis gambar | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Generasi Gambar** | Buat gambar dari teks | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Generasi Video** | Buat video dari teks/gambar | Sora, Runway Gen-3, Kling |\n| **Audio** | Text-to-speech dan transkripsi | Whisper, TTS-1, MiniMax |\n| **Embeddings** | Vektorisasi teks | text-embedding-3-small |\n| **Musik** | Generasi musik AI | Suno |\n| **Generasi 3D** | Buat model 3D | Tripo3D |\n\n## Fitur Utama\n\n<CardGroup cols={2}>\n  <Card title=\"API Multi-Format\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Gunakan format asli OpenAI, Anthropic, atau Gemini dengan satu API key. Tidak perlu mengganti SDK.\n  </Card>\n  <Card title=\"Caching Semantik\" icon=\"bolt\" href=\"/guides/caching\">\n    Caching cerdas mengurangi biaya dan latensi. Kueri yang serupa akan masuk ke cache secara otomatis.\n  </Card>\n  <Card title=\"Routing Cerdas\" icon=\"route\">\n    Failover otomatis antar penyedia memastikan ketersediaan tinggi dan harga yang optimal.\n  </Card>\n  <Card title=\"Extended Thinking\" icon=\"brain\">\n    Akses mode extended thinking Claude melalui dukungan format asli Anthropic.\n  </Card>\n</CardGroup>\n\n## Contoh Cepat\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Langkah Selanjutnya\n\n<CardGroup cols={2}>\n  <Card title=\"Quickstart\" icon=\"rocket\" href=\"/quickstart\">\n    Dapatkan API key Anda dan buat permintaan pertama Anda dalam 2 menit.\n  </Card>\n  <Card title=\"Referensi API\" icon=\"code\" href=\"/api-reference/introduction\">\n    Jelajahi dokumentasi API yang lengkap.\n  </Card>\n  <Card title=\"Model\" icon=\"robot\" href=\"https://lemondata.cc/id/models\">\n    Telusuri semua 300+ model yang tersedia.\n  </Card>\n  <Card title=\"Harga\" icon=\"dollar-sign\" href=\"https://lemondata.cc/id/models\">\n    Lihat harga yang transparan untuk semua model.\n  </Card>\n</CardGroup>",
      "tr": "---\ntitle: \"Giriş\"\ndescription: \"Birleşik bir API üzerinden 300'den fazla yapay zeka modeline erişin\"\n---\n\n## LemonData Nedir?\n\nLemonData; OpenAI, Anthropic, Google, DeepSeek ve daha fazlası dahil olmak üzere önde gelen sağlayıcılardan **300'den fazla yapay zeka modeline** birleşik erişim sağlayan bir yapay zeka API toplama platformudur.\n\n<CardGroup cols={2}>\n  <Card title=\"OpenAI Uyumlu\" icon=\"plug\">\n    OpenAI API için doğrudan değişim. Sadece temel URL'yi değiştirin.\n  </Card>\n  <Card title=\"300+ Model\" icon=\"robot\">\n    GPT-4o, Claude, Gemini, DeepSeek, Midjourney, Sora ve daha fazlasına erişin.\n  </Card>\n  <Card title=\"Kullandıkça Öde\" icon=\"coins\">\n    Abonelik yok. Rekabetçi fiyatlandırma ile sadece kullandığınız kadar ödeyin.\n  </Card>\n  <Card title=\"Yüksek Erişilebilirlik\" icon=\"shield-check\">\n    Güvenilir hizmet için otomatik yük devretme ve yük dengeleme.\n  </Card>\n</CardGroup>\n\n## Desteklenen Yetenekler\n\n| Yetenek | Açıklama | Örnek Modeller |\n|------------|-------------|----------------|\n| **Sohbet** | Metin oluşturma ve diyalog | GPT-4o, Claude 3.5, Gemini 2.0 |\n| **Görüntü İşleme** | Görüntü anlama ve analizi | GPT-4o, Claude 3.5 Sonnet, Gemini |\n| **Görüntü Oluşturma** | Metinden görüntü oluşturma | DALL-E 3, Midjourney, Flux, Ideogram |\n| **Video Oluşturma** | Metinden/görüntüden video oluşturma | Sora, Runway Gen-3, Kling |\n| **Ses** | Metinden sese ve transkripsiyon | Whisper, TTS-1, MiniMax |\n| **Vektör Temsilleri (Embeddings)** | Metin vektörleştirme | text-embedding-3-small |\n| **Müzik** | Yapay zeka ile müzik oluşturma | Suno |\n| **3D Oluşturma** | 3D modeller oluşturma | Tripo3D |\n\n## Temel Özellikler\n\n<CardGroup cols={2}>\n  <Card title=\"Çoklu Formatlı API\" icon=\"shuffle\" href=\"/guides/api-formats\">\n    Tek bir API anahtarı ile OpenAI, Anthropic veya Gemini yerel formatlarını kullanın. SDK değiştirmeye gerek yok.\n  </Card>\n  <Card title=\"Semantik Önbelleğe Alma\" icon=\"bolt\" href=\"/guides/caching\">\n    Akıllı önbelleğe alma maliyetleri ve gecikmeyi azaltır. Benzer sorgular otomatik olarak önbellekten yanıtlanır.\n  </Card>\n  <Card title=\"Akıllı Yönlendirme\" icon=\"route\">\n    Sağlayıcılar arasında otomatik yük devretme, yüksek erişilebilirlik ve en uygun fiyatlandırmayı sağlar.\n  </Card>\n  <Card title=\"Genişletilmiş Düşünme\" icon=\"brain\">\n    Yerel Anthropic format desteği aracılığıyla Claude'un genişletilmiş düşünme moduna erişin.\n  </Card>\n</CardGroup>\n\n## Hızlı Örnek\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.choices[0].message.content)\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [{ role: 'user', content: 'Hello!' }]\n});\n\nconsole.log(response.choices[0].message.content);\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, _ := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleUser, Content: \"Hello!\"},\n            },\n        },\n    )\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'user', 'content' => 'Hello!']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n```\n\n</CodeGroup>\n\n## Sonraki Adımlar\n\n<CardGroup cols={2}>\n  <Card title=\"Hızlı Başlangıç\" icon=\"rocket\" href=\"/quickstart\">\n    API anahtarınızı alın ve 2 dakika içinde ilk isteğinizi yapın.\n  </Card>\n  <Card title=\"API Referansı\" icon=\"code\" href=\"/api-reference/introduction\">\n    Eksiksiz API dokümantasyonunu inceleyin.\n  </Card>\n  <Card title=\"Modeller\" icon=\"robot\" href=\"https://lemondata.cc/tr/models\">\n    Mevcut 300'den fazla modelin tamamına göz atın.\n  </Card>\n  <Card title=\"Fiyatlandırma\" icon=\"dollar-sign\" href=\"https://lemondata.cc/tr/models\">\n    Tüm modeller için şeffaf fiyatlandırmayı görüntüleyin.\n  </Card>\n</CardGroup>"
    },
    "updatedAt": "2026-01-26T05:41:25.864Z"
  },
  "quickstart.mdx": {
    "sourceHash": "e560a690a00548c5",
    "translations": {
      "zh": "---\ntitle: \"快速入门\"\ndescription: \"在 2 分钟内快速上手 LemonData API\"\n---\n\n## 第一步：获取您的 API Key\n\n<Steps>\n  <Step title=\"创建账户\">\n    使用您的电子邮件在 [lemondata.cc](https://lemondata.cc) 注册。\n  </Step>\n  <Step title=\"充值额度\">\n    前往控制面板并为您的账户充值。按量计费，无最低消费限制。\n  </Step>\n  <Step title=\"创建 API key\">\n    前往 **Dashboard → API Keys** 并创建一个新密钥。请安全地复制并保存它——它仅会显示一次。\n  </Step>\n</Steps>\n\n<Warning>\n  请妥善保管您的 API key。切勿将其暴露在客户端代码或公共代码库中。\n</Warning>\n\n## 第二步：安装 SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 第三步：发送您的首个请求\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## 尝试不同的模型\n\nLemonData 支持 300 多种模型。只需更改 `model` 参数即可：\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## 启用流式传输\n\n如需实时响应，请启用流式传输：\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## 下一步？\n\n<CardGroup cols={2}>\n  <Card title=\"身份验证\" icon=\"key\" href=\"/authentication\">\n    了解 API key 管理和安全性。\n  </Card>\n  <Card title=\"模型\" icon=\"robot\" href=\"https://lemondata.cc/zh/models\">\n    探索所有可用模型及其功能。\n  </Card>\n  <Card title=\"流式传输\" icon=\"bolt\" href=\"/guides/streaming\">\n    实现实时流式响应。\n  </Card>\n  <Card title=\"错误处理\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    在您的应用程序中优雅地处理错误。\n  </Card>\n</CardGroup>",
      "zh-TW": "---\ntitle: \"快速入門\"\ndescription: \"在 2 分鐘內快速上手 LemonData API\"\n---\n\n## 步驟 1：獲取您的 API Key\n\n<Steps>\n  <Step title=\"建立帳戶\">\n    使用您的電子郵件在 [lemondata.cc](https://lemondata.cc) 註冊。\n  </Step>\n  <Step title=\"儲值額度\">\n    前往控制台並為您的帳戶儲值額度。採用按量計費模式，無最低消費限制。\n  </Step>\n  <Step title=\"建立 API Key\">\n    前往 **Dashboard → API Keys** 並建立一個新金鑰。請安全地複製並保存它 —— 它僅會顯示一次。\n  </Step>\n</Steps>\n\n<Warning>\n  請妥善保管您的 API Key。切勿將其暴露在客戶端程式碼或公開的程式碼庫中。\n</Warning>\n\n## 步驟 2：安裝 SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 步驟 3：發送您的第一個請求\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## 嘗試不同的模型\n\nLemonData 支援 300 多種模型。只需更改 `model` 參數即可：\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## 啟用串流 (Streaming)\n\n如需即時回應，請啟用串流：\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## 下一步？\n\n<CardGroup cols={2}>\n  <Card title=\"身分驗證\" icon=\"key\" href=\"/authentication\">\n    了解 API Key 管理與安全性。\n  </Card>\n  <Card title=\"模型\" icon=\"robot\" href=\"https://lemondata.cc/zh-TW/models\">\n    探索所有可用模型及其功能。\n  </Card>\n  <Card title=\"串流\" icon=\"bolt\" href=\"/guides/streaming\">\n    實作即時串流回應。\n  </Card>\n  <Card title=\"錯誤處理\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    在您的應用程式中優雅地處理錯誤。\n  </Card>\n</CardGroup>",
      "ja": "---\ntitle: \"クイックスタート\"\ndescription: \"LemonData APIを2分で使い始める\"\n---\n\n## ステップ 1: APIキーの取得\n\n<Steps>\n  <Step title=\"アカウントの作成\">\n    [lemondata.cc](https://lemondata.cc) でメールアドレスを使用してサインアップします。\n  </Step>\n  <Step title=\"クレジットの追加\">\n    ダッシュボードに移動し、アカウントにクレジットを追加します。最低利用料金のない従量課金制です。\n  </Step>\n  <Step title=\"APIキーの作成\">\n    **Dashboard → API Keys** に移動し、新しいキーを作成します。一度しか表示されないため、安全にコピーして保管してください。\n  </Step>\n</Steps>\n\n<Warning>\n  APIキーを安全に保管してください。クライアント側のコードや公開リポジトリに公開しないでください。\n</Warning>\n\n## ステップ 2: SDKのインストール\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## ステップ 3: 最初のリクエストを送信する\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## さまざまなモデルを試す\n\nLemonDataは300以上のモデルをサポートしています。`model` パラメータを変更するだけです。\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## ストリーミングを有効にする\n\nリアルタイムのレスポンスを得るには、ストリーミングを有効にします。\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## 次のステップ\n\n<CardGroup cols={2}>\n  <Card title=\"認証\" icon=\"key\" href=\"/authentication\">\n    APIキーの管理とセキュリティについて学びます。\n  </Card>\n  <Card title=\"モデル\" icon=\"robot\" href=\"https://lemondata.cc/ja/models\">\n    利用可能なすべてのモデルとその機能を確認します。\n  </Card>\n  <Card title=\"ストリーミング\" icon=\"bolt\" href=\"/guides/streaming\">\n    リアルタイムのストリーミングレスポンスを実装します。\n  </Card>\n  <Card title=\"エラーハンドリング\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    アプリケーションでエラーを適切に処理します。\n  </Card>\n</CardGroup>",
      "ko": "---\ntitle: \"빠른 시작\"\ndescription: \"2분 만에 LemonData API 시작하기\"\n---\n\n## 1단계: API 키 발급받기\n\n<Steps>\n  <Step title=\"계정 생성하기\">\n    이메일을 사용하여 [lemondata.cc](https://lemondata.cc)에서 가입하세요.\n  </Step>\n  <Step title=\"크레딧 충전하기\">\n    대시보드로 이동하여 계정에 크레딧을 추가하세요. 최소 금액 제한이 없는 종량제(Pay-as-you-go) 요금제가 적용됩니다.\n  </Step>\n  <Step title=\"API 키 생성하기\">\n    **대시보드 → API Keys**로 이동하여 새 키를 생성하세요. 키는 한 번만 표시되므로 안전하게 복사하여 보관하세요.\n  </Step>\n</Steps>\n\n<Warning>\n  API 키를 안전하게 보관하세요. 클라이언트 측 코드나 공개 리포지토리에 노출되지 않도록 주의하십시오.\n</Warning>\n\n## 2단계: SDK 설치하기\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## 3단계: 첫 번째 요청 보내기\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions",
      "de": "---\ntitle: \"Schnellstart\"\ndescription: \"Starten Sie mit der LemonData API in 2 Minuten\"\n---\n\n## Schritt 1: Holen Sie sich Ihren API-Key\n\n<Steps>\n  <Step title=\"Konto erstellen\">\n    Registrieren Sie sich auf [lemondata.cc](https://lemondata.cc) mit Ihrer E-Mail-Adresse.\n  </Step>\n  <Step title=\"Guthaben aufladen\">\n    Navigieren Sie zum Dashboard und laden Sie Guthaben auf Ihr Konto auf. Pay-as-you-go-Preise ohne Mindestumsatz.\n  </Step>\n  <Step title=\"API-Key erstellen\">\n    Gehen Sie zu **Dashboard → API Keys** und erstellen Sie einen neuen Key. Kopieren Sie ihn sicher – er wird nur einmal angezeigt.\n  </Step>\n</Steps>\n\n<Warning>\n  Bewahren Sie Ihren API-Key sicher auf. Veröffentlichen Sie ihn niemals in Client-seitigem Code oder öffentlichen Repositories.\n</Warning>\n\n## Schritt 2: SDK installieren\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Schritt 3: Erstellen Sie Ihre erste Anfrage\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## Verschiedene Modelle ausprobieren\n\nLemonData unterstützt über 300 Modelle. Ändern Sie einfach den Parameter `model`:\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Streaming aktivieren\n\nFür Antworten in Echtzeit aktivieren Sie Streaming:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Was kommt als Nächstes?\n\n<CardGroup cols={2}>\n  <Card title=\"Authentifizierung\" icon=\"key\" href=\"/authentication\">\n    Erfahren Sie mehr über die Verwaltung von API-Keys und Sicherheit.\n  </Card>\n  <Card title=\"Modelle\" icon=\"robot\" href=\"https://lemondata.cc/de/models\">\n    Entdecken Sie alle verfügbaren Modelle und deren Funktionen.\n  </Card>\n  <Card title=\"Streaming\" icon=\"bolt\" href=\"/guides/streaming\">\n    Implementieren Sie Streaming-Antworten in Echtzeit.\n  </Card>\n  <Card title=\"Fehlerbehandlung\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    Behandeln Sie Fehler in Ihrer Anwendung professionell.\n  </Card>\n</CardGroup>",
      "fr": "---\ntitle: \"Démarrage rapide\"\ndescription: \"Commencez avec l'API LemonData en 2 minutes\"\n---\n\n## Étape 1 : Obtenez votre clé API\n\n<Steps>\n  <Step title=\"Créez un compte\">\n    Inscrivez-vous sur [lemondata.cc](https://lemondata.cc) en utilisant votre e-mail.\n  </Step>\n  <Step title=\"Ajoutez des crédits\">\n    Accédez au dashboard et ajoutez des crédits à votre compte. Tarification pay-as-you-go sans minimum.\n  </Step>\n  <Step title=\"Créez une clé API\">\n    Allez dans **Dashboard → API Keys** et créez une nouvelle clé. Copiez-la en toute sécurité - elle ne s'affiche qu'une seule fois.\n  </Step>\n</Steps>\n\n<Warning>\n  Gardez votre clé API en sécurité. Ne l'exposez jamais dans le code côté client ou dans des dépôts publics.\n</Warning>\n\n## Étape 2 : Installez le SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Étape 3 : Effectuez votre première requête\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.com",
      "es": "ll keep it in mind.\n\n    *Final check on \"model\":*\n    I will keep it as `model` because it's a parameter name.\n\n    *Final check on \"stream\":*\n    I will keep it as `stream` if it's a parameter name, but in the text \"enable streaming\", I'll translate to \"habilite el streaming\".\n\n    *Final check on \"real-time\":*\n    \"en tiempo real\".\n\n    *Final check on \"gracefully\":*\n    \"de forma adecuada\".\n\n    *Final check on \"300+ models\":*\n    \"más de 300 modelos\".\n\n    *Final check on \"Just change\":*\n    \"Simplemente cambie\".\n\n    *Final check on \"For real-time responses\":*\n    \"Para respuestas en tiempo real\".\n\n    *Final check on \"Tell me a story\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"What is the capital of France?\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"The capital of France is Paris.\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"You are a helpful assistant.\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"sk-your-api-key\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"gpt-4o\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"claude-sonnet-4-5\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"gemini-2.5-flash\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"deepseek-r1\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"Python\", \"JavaScript\", \"Go\", \"PHP\", \"cURL\":*\n    These are technical identifiers/language names. Keep as is.\n\n    *Final check on \"pip install openai\", \"npm install openai\", etc.:*\n    Inside code blocks. Leave as is.\n\n    *Final check on \"composer require openai-php/client\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"go get github.com/sashabaranov/go-openai\":*\n    Inside code block. Leave as is.\n\n    *Final check on \"import OpenAI from 'openai';\", etc.:*\n    Inside code blocks. Leave as is.\n\n    *Final check on \"console.log\", \"print\", \"echo\", \"fmt.Println\":*\n    Inside code blocks. Leave as is.\n\n    *Final check on \"choices\", \"message\", \"content\", \"delta\":*\n    Inside code blocks. Leave as is.\n\n    *Final check on \"Authorization\", \"Bearer\", \"Content-Type\", \"application/json\":*\n    Inside",
      "pt": "---\ntitle: \"Início Rápido\"\ndescription: \"Comece a usar a LemonData API em 2 minutos\"\n---\n\n## Passo 1: Obtenha sua API Key\n\n<Steps>\n  <Step title=\"Crie uma conta\">\n    Cadastre-se em [lemondata.cc](https://lemondata.cc) usando seu e-mail.\n  </Step>\n  <Step title=\"Adicione créditos\">\n    Navegue até o dashboard e adicione créditos à sua conta. Preços no modelo pay-as-you-go sem valor mínimo.\n  </Step>\n  <Step title=\"Crie uma API key\">\n    Vá em **Dashboard → API Keys** e crie uma nova chave. Copie-a com segurança - ela é exibida apenas uma vez.\n  </Step>\n</Steps>\n\n<Warning>\n  Mantenha sua API key segura. Nunca a exponha em código do lado do cliente ou em repositórios públicos.\n</Warning>\n\n## Passo 2: Instale o SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Passo 3: Faça sua Primeira Requisição\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## Experimente Diferentes Modelos\n\nA LemonData suporta mais de 300 modelos. Basta alterar o parâmetro `model`:\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Habilite o Streaming\n\nPara respostas em tempo real, habilite o streaming:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Próximos Passos\n\n<CardGroup cols={2}>\n  <Card title=\"Autenticação\" icon=\"key\" href=\"/authentication\">\n    Saiba mais sobre o gerenciamento e a segurança de API keys.\n  </Card>\n  <Card title=\"Modelos\" icon=\"robot\" href=\"https://lemondata.cc/pt/models\">\n    Explore todos os modelos disponíveis e suas capacidades.\n  </Card>\n  <Card title=\"Streaming\" icon=\"bolt\" href=\"/guides/streaming\">\n    Implemente respostas em streaming em tempo real.\n  </Card>\n  <Card title=\"Tratamento de Erros\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    Trate erros de forma adequada em sua aplicação.\n  </Card>\n</CardGroup>",
      "ar": "---\ntitle: \"البدء السريع\"\ndescription: \"ابدأ استخدام LemonData API في دقيقتين\"\n---\n\n## الخطوة 1: الحصول على مفتاح API الخاص بك\n\n<Steps>\n  <Step title=\"إنشاء حساب\">\n    قم بالتسجيل في [lemondata.cc](https://lemondata.cc) باستخدام بريدك الإلكتروني.\n  </Step>\n  <Step title=\"إضافة رصيد\">\n    انتقل إلى لوحة التحكم وأضف رصيداً إلى حسابك. تسعير حسب الاستخدام (Pay-as-you-go) بدون حد أدنى.\n  </Step>\n  <Step title=\"إنشاء مفتاح API\">\n    انتقل إلى **Dashboard → API Keys** وقم بإنشاء مفتاح جديد. انسخه بأمان - يتم عرضه مرة واحدة فقط.\n  </Step>\n</Steps>\n\n<Warning>\n  حافظ على أمان مفتاح API الخاص بك. لا تقم أبداً بكشفه في كود جانب العميل (client-side) أو المستودعات العامة.\n</Warning>\n\n## الخطوة 2: تثبيت SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## الخطوة 3: إجراء طلبك الأول\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## تجربة نماذج مختلفة\n\nيدعم LemonData أكثر من 300 نموذج. ما عليك سوى تغيير بارامتر `model`:\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## تفعيل البث (Streaming)\n\nللحصول على استجابات في الوقت الفعلي، قم بتفعيل البث:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## ماذا بعد؟\n\n<CardGroup cols={2}>\n  <Card title=\"المصادقة (Authentication)\" icon=\"key\" href=\"/authentication\">\n    تعرف على إدارة مفاتيح API والأمان.\n  </Card>\n  <Card title=\"النماذج (Models)\" icon=\"robot\" href=\"https://lemondata.cc/ar/models\">\n    استكشف جميع النماذج المتاحة وقدراتها.\n  </Card>\n  <Card title=\"البث (Streaming)\" icon=\"bolt\" href=\"/guides/streaming\">\n    قم بتنفيذ استجابات البث في الوقت الفعلي.\n  </Card>\n  <Card title=\"معالجة الأخطاء\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    تعامل مع الأخطاء بسلاسة في تطبيقك.\n  </Card>\n</CardGroup>",
      "vi": "---\ntitle: \"Bắt đầu nhanh\"\ndescription: \"Bắt đầu với LemonData API trong 2 phút\"\n---\n\n## Bước 1: Lấy API Key của bạn\n\n<Steps>\n  <Step title=\"Tạo tài khoản\">\n    Đăng ký tại [lemondata.cc](https://lemondata.cc) bằng email của bạn.\n  </Step>\n  <Step title=\"Nạp credit\">\n    Đi tới dashboard và nạp credit vào tài khoản của bạn. Mô hình thanh toán pay-as-you-go không yêu cầu mức tối thiểu.\n  </Step>\n  <Step title=\"Tạo API key\">\n    Đi tới **Dashboard → API Keys** và tạo một key mới. Hãy sao chép và lưu trữ bảo mật - nó chỉ được hiển thị một lần duy nhất.\n  </Step>\n</Steps>\n\n<Warning>\n  Giữ API key của bạn an toàn. Tuyệt đối không để lộ trong mã nguồn phía client hoặc các kho lưu trữ công khai.\n</Warning>\n\n## Bước 2: Cài đặt SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Bước 3: Thực hiện yêu cầu đầu tiên\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## Thử các Model khác nhau\n\nLemonData hỗ trợ hơn 300 model. Chỉ cần thay đổi tham số `model`:\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Bật Streaming\n\nĐể nhận phản hồi theo thời gian thực, hãy bật tính năng streaming:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Tiếp theo là gì?\n\n<CardGroup cols={2}>\n  <Card title=\"Xác thực\" icon=\"key\" href=\"/authentication\">\n    Tìm hiểu về quản lý API key và bảo mật.\n  </Card>\n  <Card title=\"Model\" icon=\"robot\" href=\"https://lemondata.cc/vi/models\">\n    Khám phá tất cả các model hiện có và khả năng của chúng.\n  </Card>\n  <Card title=\"Streaming\" icon=\"bolt\" href=\"/guides/streaming\">\n    Triển khai phản hồi streaming theo thời gian thực.\n  </Card>\n  <Card title=\"Xử lý lỗi\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    Xử lý lỗi một cách hiệu quả trong ứng dụng của bạn.\n  </Card>\n</CardGroup>",
      "id": "---\ntitle: \"Mulai Cepat\"\ndescription: \"Mulai menggunakan LemonData API dalam 2 menit\"\n---\n\n## Langkah 1: Dapatkan API Key Anda\n\n<Steps>\n  <Step title=\"Buat akun\">\n    Daftar di [lemondata.cc](https://lemondata.cc) menggunakan email Anda.\n  </Step>\n  <Step title=\"Tambah kredit\">\n    Buka dashboard dan tambahkan kredit ke akun Anda. Harga pay-as-you-go tanpa minimum.\n  </Step>\n  <Step title=\"Buat API key\">\n    Buka **Dashboard → API Keys** dan buat kunci baru. Salin dengan aman - kunci ini hanya ditampilkan sekali.\n  </Step>\n</Steps>\n\n<Warning>\n  Jaga keamanan API key Anda. Jangan pernah membagikannya di kode sisi klien atau repositori publik.\n</Warning>\n\n## Langkah 2: Instal SDK\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Langkah 3: Buat Permintaan Pertama Anda\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n",
      "tr": "---\ntitle: \"Hızlı Başlangıç\"\ndescription: \"LemonData API'yi 2 dakika içinde kullanmaya başlayın\"\n---\n\n## Adım 1: API Anahtarınızı Alın\n\n<Steps>\n  <Step title=\"Hesap oluşturun\">\n    E-postanızı kullanarak [lemondata.cc](https://lemondata.cc) adresinden kaydolun.\n  </Step>\n  <Step title=\"Kredi ekleyin\">\n    Dashboard'a gidin ve hesabınıza kredi ekleyin. Minimum tutar olmaksızın kullandıkça öde fiyatlandırması.\n  </Step>\n  <Step title=\"API anahtarı oluşturun\">\n    **Dashboard → API Keys** bölümüne gidin ve yeni bir anahtar oluşturun. Güvenli bir şekilde kopyalayın - yalnızca bir kez gösterilir.\n  </Step>\n</Steps>\n\n<Warning>\n  API anahtarınızı güvende tutun. İstemci tarafı kodlarında veya herkese açık depolarda asla paylaşmayın.\n</Warning>\n\n## Adım 2: SDK'yı Yükleyin\n\n<CodeGroup>\n\n```bash Python\npip install openai\n```\n\n```bash JavaScript\nnpm install openai\n```\n\n```bash Go\ngo get github.com/sashabaranov/go-openai\n```\n\n```bash PHP\ncomposer require openai-php/client\n```\n\n</CodeGroup>\n\n## Adım 3: İlk İsteğinizi Yapın\n\n<CodeGroup>\n\n```bash cURL\ncurl https://api.lemondata.cc/v1/chat/completions \\\n  -H \"Authorization: Bearer sk-your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }'\n```\n\n```python Python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"sk-your-api-key\",\n    base_url=\"https://api.lemondata.cc/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n# Output: The capital of France is Paris.\n```\n\n```javascript JavaScript\nimport OpenAI from 'openai';\n\nconst client = new OpenAI({\n  apiKey: 'sk-your-api-key',\n  baseURL: 'https://api.lemondata.cc/v1'\n});\n\nconst response = await client.chat.completions.create({\n  model: 'gpt-4o',\n  messages: [\n    { role: 'system', content: 'You are a helpful assistant.' },\n    { role: 'user', content: 'What is the capital of France?' }\n  ]\n});\n\nconsole.log(response.choices[0].message.content);\n// Output: The capital of France is Paris.\n```\n\n```go Go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"github.com/sashabaranov/go-openai\"\n)\n\nfunc main() {\n    config := openai.DefaultConfig(\"sk-your-api-key\")\n    config.BaseURL = \"https://api.lemondata.cc/v1\"\n\n    client := openai.NewClientWithConfig(config)\n\n    resp, err := client.CreateChatCompletion(\n        context.Background(),\n        openai.ChatCompletionRequest{\n            Model: \"gpt-4o\",\n            Messages: []openai.ChatCompletionMessage{\n                {Role: openai.ChatMessageRoleSystem, Content: \"You are a helpful assistant.\"},\n                {Role: openai.ChatMessageRoleUser, Content: \"What is the capital of France?\"},\n            },\n        },\n    )\n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(resp.Choices[0].Message.Content)\n}\n```\n\n```php PHP\n<?php\n$ch = curl_init('https://api.lemondata.cc/v1/chat/completions');\n\ncurl_setopt_array($ch, [\n    CURLOPT_RETURNTRANSFER => true,\n    CURLOPT_POST => true,\n    CURLOPT_HTTPHEADER => [\n        'Content-Type: application/json',\n        'Authorization: Bearer sk-your-api-key'\n    ],\n    CURLOPT_POSTFIELDS => json_encode([\n        'model' => 'gpt-4o',\n        'messages' => [\n            ['role' => 'system', 'content' => 'You are a helpful assistant.'],\n            ['role' => 'user', 'content' => 'What is the capital of France?']\n        ]\n    ])\n]);\n\n$response = curl_exec($ch);\ncurl_close($ch);\n\n$data = json_decode($response, true);\necho $data['choices'][0]['message']['content'];\n// Output: The capital of France is Paris.\n```\n\n</CodeGroup>\n\n## Farklı Modelleri Deneyin\n\nLemonData 300'den fazla modeli destekler. Sadece `model` parametresini değiştirmeniz yeterlidir:\n\n```python\n# OpenAI GPT-4o\nresponse = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n\n# Anthropic Claude Sonnet 4.5\nresponse = client.chat.completions.create(model=\"claude-sonnet-4-5\", messages=messages)\n\n# Google Gemini 2.5 Flash\nresponse = client.chat.completions.create(model=\"gemini-2.5-flash\", messages=messages)\n\n# DeepSeek R1\nresponse = client.chat.completions.create(model=\"deepseek-r1\", messages=messages)\n```\n\n## Streaming'i Etkinleştirin\n\nGerçek zamanlı yanıtlar için streaming'i etkinleştirin:\n\n```python\nstream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n```\n\n## Sırada Ne Var?\n\n<CardGroup cols={2}>\n  <Card title=\"Kimlik Doğrulama\" icon=\"key\" href=\"/authentication\">\n    API anahtarı yönetimi ve güvenliği hakkında bilgi edinin.\n  </Card>\n  <Card title=\"Modeller\" icon=\"robot\" href=\"https://lemondata.cc/tr/models\">\n    Mevcut tüm modelleri ve yeteneklerini keşfedin.\n  </Card>\n  <Card title=\"Streaming\" icon=\"bolt\" href=\"/guides/streaming\">\n    Gerçek zamanlı streaming yanıtlarını uygulayın.\n  </Card>\n  <Card title=\"Hata Yönetimi\" icon=\"triangle-exclamation\" href=\"/guides/error-handling\">\n    Uygulamanızdaki hataları sorunsuz bir şekilde yönetin.\n  </Card>\n</CardGroup>"
    },
    "updatedAt": "2026-01-26T05:43:16.832Z"
  }
}
