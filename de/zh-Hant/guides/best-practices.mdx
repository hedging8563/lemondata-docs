---
title: "Best Practices"
description: "Optimieren Sie Ihre Nutzung der LemonData API für Kosteneffizienz, Performance und Zuverlässigkeit"
---

## Modellauswahl

Die Wahl des richtigen Modells hat erheblichen Einfluss auf Kosten und Qualität.

### Aufgabenbasierte Empfehlungen

| Aufgabe | Empfohlenes Modell | Grund |
|------|-------------------|-----------|
| **Einfache Fragen & Antworten** | `gpt-4o-mini`, `gemini-2.5-flash` | Schnell, günstig, ausreichend gut |
| **Komplexes Reasoning** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Bessere Logik- und Planungsfähigkeiten |
| **Programmierung** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimiert für Code |
| **Kreatives Schreiben** | `claude-sonnet-4-5`, `gpt-4o` | Bessere Prosa-Qualität |
| **Vision/Bild** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Native Vision-Unterstützung |
| **Langer Kontext** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 1M+ Token-Fenster |
| **Kostensensibel** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Bestes Preis-Leistungs-Verhältnis |

### Kostenstufen

```
$$$$ Premium: o3, claude-opus-4-5, gpt-4o
$$$  Standard: claude-sonnet-4-5, gpt-4o
$$   Budget:   gpt-4o-mini, gemini-2.5-flash
$    Economy:  deepseek-v3.2, deepseek-r1
```

## Kostenoptimierung

### 1. Kleine Modelle bevorzugen

```python
def smart_query(question: str, complexity: str = "auto"):
    """Use cheaper models for simple tasks."""

    if complexity == "simple":
        model = "gpt-4o-mini"
    elif complexity == "complex":
        model = "gpt-4o"
    else:
        # Start cheap, escalate if needed
        model = "gpt-4o-mini"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": question}]
    )
    return response
```

### 2. max_tokens festlegen

Legen Sie unbedingt ein vernünftiges `max_tokens`-Limit fest:

```python
# ❌ 錯誤：未設限制，可能會產生數千個 token
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}]
)

# ✅ 正確：限制回應長度
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}],
    max_tokens=500  # 摘要的合理限制
)
```

### 3. Prompts optimieren

```python
# ❌ 冗長的 prompt (消耗更多 input token)
prompt = """
I would like you to please help me by analyzing the following text
and providing a comprehensive summary of the main points. Please be
thorough but also concise in your response. The text is as follows:
{text}
"""

# ✅ 簡潔的 prompt (消耗較少 token)
prompt = "Summarize the key points:\n{text}"
```

### 4. Caching aktivieren

Nutzen Sie [semantisches Caching](/guides/caching):

```python
# 對於重複的相似查詢，快取可大幅節省成本
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is machine learning?"}],
    temperature=0  # 確定性 = 更好的快取命中率
)
```

### 5. Ähnliche Anfragen stapeln (Batching)

```python
# ❌ 許多小型請求
for question in questions:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": question}]
    )

# ✅ 較少的大型請求
combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"Answer each question:\n{combined_prompt}"}]
)
```

## Performance-Optimierung

### 1. Streaming für eine bessere Benutzererfahrung nutzen

Streaming kann die wahrgenommene Performance verbessern:

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 2. Schnelle Modelle für interaktive Zwecke wählen

| Anwendungsfall | Empfohlenes Modell | Latenz |
|----------|-------------|---------|
| Chat-Interface | `gpt-4o-mini`, `gemini-2.5-flash` | Erster Token ca. 200ms |
| Autovervollständigung | `claude-haiku-4-5` | Erster Token ca. 150ms |
| Hintergrundverarbeitung | `gpt-4o`, `claude-sonnet-4-5` | Erster Token ca. 500ms |

### 3. Timeouts festlegen

```python
client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    timeout=60.0  # 60 秒逾時
)
```

## Zuverlässigkeit

### 1. Retry-Mechanismus implementieren

```python
import time
from openai import RateLimitError, APIError

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError:
            wait = 2 ** attempt
            print(f"Rate limited, waiting {wait}s...")
            time.sleep(wait)
        except APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(1)
    raise Exception("Max retries exceeded")
```

### 2. Fehler elegant behandeln

```python
from openai import APIError, AuthenticationError, RateLimitError

try:
    response = client.chat.completions.create(...)
except AuthenticationError:
    # 檢查 API key
    notify_admin("Invalid API key")
except RateLimitError:
    # 加入隊列稍後處理或使用備援
    add_to_queue(request)
except APIError as e:
    if e.status_code == 402:
        notify_admin("Balance low")
    elif e.status_code >= 500:
        # 伺服器錯誤，稍後重試
        schedule_retry(request)
```

### 3. Fallback-Modelle verwenden

```python
FALLBACK_CHAIN = ["gpt-4o", "claude-sonnet-4-5", "gemini-2.5-flash"]

def chat_with_fallback(messages):
    for model in FALLBACK_CHAIN:
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages
            )
        except APIError:
            continue
    raise Exception("All models failed")
```

## Sicherheit

### 1. API-Keys schützen

```python
# ❌ 切勿將金鑰寫死在程式碼中
client = OpenAI(api_key="sk-abc123...")

# ✅ 使用環境變數
import os
client = OpenAI(api_key=os.environ["LEMONDATA_API_KEY"])
```

### 2. Benutzereingaben validieren

```python
def validate_message(content: str) -> bool:
    """Validieren Sie Benutzereingaben, bevor Sie diese an die API senden."""
    if len(content) > 100000:
        raise ValueError("Message too long")
    # 根據需要添加其他驗證
    return True
```

### 3. API-Key-Beschränkungen festlegen

Erstellen Sie separate API-Keys mit Ausgabenlimits für:
- Entwicklung/Test
- Produktionsumgebung
- Verschiedene Anwendungen

## Monitoring

### 1. Nutzung verfolgen

Überprüfen Sie regelmäßig Ihr Dashboard auf:
- Token-Verbrauch pro Modell
- Kostenaufschlüsselung
- Cache-Trefferquote
- Fehlerrate

### 2. Wichtige Metriken protokollieren

```python
import logging

response = client.chat.completions.create(...)

logging.info({
    "model": response.model,
    "prompt_tokens": response.usage.prompt_tokens,
    "completion_tokens": response.usage.completion_tokens,
    "total_tokens": response.usage.total_tokens,
})
```

### 3. Warnungen einrichten

Konfigurieren Sie Warnungen bei niedrigem Guthaben im Dashboard, um Dienstunterbrechungen zu vermeiden.

## Checkliste

<AccordionGroup>
  <Accordion title="Kostenoptimierung">
    - [ ] Passendes Modell für jede Aufgabe verwenden
    - [ ] max_tokens-Limit festlegen
    - [ ] Prompts prägnant halten
    - [ ] Caching an geeigneten Stellen aktivieren
    - [ ] Ähnliche Anfragen stapeln (Batching)
  </Accordion>

  <Accordion title="Performance">
    - [ ] Streaming für interaktive Erlebnisse nutzen
    - [ ] Schnelle Modelle für Echtzeitzwecke verwenden
    - [ ] Timeout-Einstellungen konfiguriert
  </Accordion>

  <Accordion title="Zuverlässigkeit">
    - [ ] Retry-Logik implementiert
    - [ ] Fehlerbehandlung bereitgestellt
    - [ ] Fallback-Modelle konfiguriert
  </Accordion>

  <Accordion title="Sicherheit">
    - [ ] API-Keys in Umgebungsvariablen gespeichert
    - [ ] Eingabevalidierung
    - [ ] Separate Keys für Entwicklung/Produktion
    - [ ] Ausgabenlimits festgelegt
  </Accordion>
</AccordionGroup>