---
title: "✨ Intelligentes Caching"
description: "Reduzieren Sie Kosten und Latenz durch kontextbewusstes semantisches Caching"
---

## Übersicht

LemonData bietet ein intelligentes Caching-System, das Ihre API-Kosten und Antwortlatenzen erheblich reduziert. Unser Cache geht über einfaches Request-Matching hinaus; er versteht die **semantische Bedeutung (semantic meaning)** Ihrer Prompts.

<CardGroup cols={2}>
  <Card title="Kostenersparnis" icon="piggy-bank">
    Cache-Hits werden nur mit einem Bruchteil der regulären Kosten berechnet.
  </Card>
  <Card title="Schnellere Antworten" icon="bolt">
    Gecachte Antworten werden sofort zurückgegeben, ohne dass eine Modell-Inferenz erforderlich ist.
  </Card>
  <Card title="Kontextbewusst" icon="brain">
    Semantisches Matching findet ähnliche Anfragen, selbst wenn diese unterschiedlich formuliert sind.
  </Card>
  <Card title="Datenschutzkontrolle" icon="shield">
    Vollständige Kontrolle darüber, was gecacht und geteilt wird.
  </Card>
</CardGroup>

## Funktionsweise

LemonData verwendet ein zweistufiges Caching-System:

### Ebene 1: Response-Caching (Exakter Match)

Für deterministische Anfragen (`temperature=0`) cachen wir die exakte Antwort:

- **Matching-Bedingungen**: Gleiches Modell, gleiche Nachrichten und Parameter
- **Geschwindigkeit**: Sofort (im Mikrosekundenbereich)
- **Geeignet für**: Wiederholte identische Abfragen

### Ebene 2: Semantisches Caching (Ähnlichkeits-Match)

Für alle Anfragen prüfen wir zudem die semantische Ähnlichkeit mithilfe eines zweistufigen Matching-Algorithmus:

- **Stufe 1 (Nur Abfrage)**: Ähnlichkeit der Benutzerabfrage ≥95%
- **Stufe 2 (Vollständiger Kontext)**: Ähnlichkeit inklusive Konversationskontext ≥85%
- **Geeignet für**: FAQ-ähnliche Abfragen, häufig gestellte Fragen

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
→ Same cached response (high semantic similarity)
```

## Cache-Steuerung

### Steuerung auf Request-Ebene

Verwenden Sie den Parameter `cache_control` im Request-Body, um das Caching-Verhalten für jede Anfrage zu steuern:

```bash
# Cache-Abfrage überspringen, immer eine neue Antwort generieren
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello"}],
    "cache_control": {"type": "no_cache"}
  }'
```

| Typ | Effekt |
|------|------|
| `no_cache` | Cache-Abfrage überspringen, immer eine neue Antwort generieren |
| `no_store` | Diese Antwort nicht im Cache speichern |
| `response_only` | Nur exaktes Matching-Caching verwenden (semantisches Caching überspringen) |
| `semantic_only` | Nur semantisches Caching verwenden (exaktes Matching überspringen) |

### Response-Header

Jede Antwort enthält einen Cache-Status:

```
X-Cache-Status: HIT    # Antwort stammt aus dem Cache
X-Cache-Status: MISS   # Neue Antwort vom Modell
```

## Cache-Status prüfen

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# Cache-Status aus den Response-Headern prüfen
# (Verfügbar in der rohen HTTP-Antwort)
print(f"Cache: {response._raw_response.headers.get('X-Cache-Status')}")
```

## Cache-Abrechnung

Die Kosten für Cache-Hits sind deutlich niedriger als für neue Anfragen:

| Typ | Kosten |
|------|------|
| Cache-Hit (HIT) | **90% Rabatt** |
| Cache-Miss (MISS) | Originalpreis |

Der genaue Rabatt wird in Ihren Nutzungslogs im Dashboard angezeigt.

## Datenschutzkontrolle

### Organisations- / Benutzerebene

Konfigurieren Sie das Caching-Verhalten in den Dashboard-Einstellungen:

| Modus | Beschreibung |
|------|-------------|
| **Shared (Geteilt)** | Caching aktiviert, Antworten können benutzerübergreifend geteilt werden (Standard für Privatkonten) |
| **Isolated (Isoliert)** | Caching aktiviert, aber Antworten sind privat für Ihre Organisation (Standard für Organisationskonten) |
| **Deaktiviert (Disabled)** | Caching wird überhaupt nicht verwendet |

Weitere konfigurierbare Optionen:
- **Ähnlichkeitsschwellenwert**: Empfindlichkeit des semantischen Matchings anpassen (Standard: 92%)
- **Benutzerdefinierte TTL**: Cache-Ablaufzeit überschreiben
- **Modelle ausschließen**: Caching für bestimmte Modelle deaktivieren

### Request-Ebene

Verwenden Sie den Parameter `cache_control`, um die Einstellungen für eine einzelne Anfrage zu überschreiben:

```bash
# Caching für diese Anfrage deaktivieren
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "..."}],
    "cache_control": {"type": "no_store"}
  }'
```

## Cache-Feedback

Wenn Sie eine fehlerhafte gecachte Antwort erhalten, können Sie dies melden:

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**Feedback-Typen:**
- `wrong_answer` - Faktischer Fehler
- `outdated` - Informationen veraltet
- `irrelevant` - Entspricht nicht der Frage
- `other` - Sonstige Probleme

Wenn ein Cache-Eintrag ausreichend negatives Feedback erhält, wird er automatisch ungültig.

## Best Practices

<AccordionGroup>
  <Accordion title="Verwenden Sie temperature=0 für cachbare Abfragen">
    Deterministische Einstellungen maximieren die Cache-Hit-Rate.
  </Accordion>

  <Accordion title="Standardisieren Sie das Prompt-Format">
    Konsistente Formatierung verbessert das semantische Matching.
  </Accordion>

  <Accordion title="Verwenden Sie no-cache für zeitkritische Abfragen">
    Aktuelle Ereignisse und Echtzeitdaten sollten den Cache überspringen.
  </Accordion>

  <Accordion title="Überwachen Sie die Cache-Hit-Rate">
    Sehen Sie Cache-Statistiken und Ersparnisse im Dashboard ein.
  </Accordion>
</AccordionGroup>

## Wann Caching nicht verwendet werden sollte

Deaktivieren Sie das Caching in folgenden Fällen:

- **Echtzeit-Informationen**: Aktienkurse, Wetter, Nachrichten
- **Personalisierte Inhalte**: Empfehlungen für bestimmte Benutzer
- **Kreative Aufgaben**: Wenn Variation erforderlich ist
- **Sensible Daten**: Vertrauliche Informationen

```python
# Für zeitkritische Abfragen
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_body={"cache_control": {"type": "no_cache"}}
)
```