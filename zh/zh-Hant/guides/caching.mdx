---
title: "✨ 智能缓存"
description: "通过上下文感知的语义缓存降低成本与延迟"
---

## 总览

LemonData 提供智能缓存系统，可显著降低您的 API 成本与响应延迟。我们的缓存不仅仅是简单的请求匹配，它还能理解您提示词（prompts）的**语义（semantic meaning）**。

<CardGroup cols={2}>
  <Card title="节省成本" icon="piggy-bank">
    缓存命中（Cache hits）仅按正常成本的一小部分计费。
  </Card>
  <Card title="更快速的响应" icon="bolt">
    缓存的响应会立即返回，无需进行模型推理。
  </Card>
  <Card title="上下文感知" icon="brain">
    语义匹配即使在措辞不同的情况下也能找到相似的请求。
  </Card>
  <Card title="隐私控制" icon="shield">
    完全控制缓存与分享的内容。
  </Card>
</CardGroup>

## 运作原理

LemonData 使用双层缓存系统：

### 第一层：响应缓存（精确匹配）

对于确定性请求（`temperature=0`），我们缓存精确的响应：

- **匹配条件**：相同的模型、消息与参数
- **速度**：即时（微秒级）
- **适用于**：重复的相同查询

### 第二层：语义缓存（相似度匹配）

对于所有请求，我们还会使用两阶段匹配算法检查语义相似度：

- **第一阶段（仅查询）**：用户查询相似度 ≥95%
- **第二阶段（完整上下文）**：包含对话上下文的相似度 ≥85%
- **适用于**：FAQ 类型的查询、常见问题

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
→ Same cached response (high semantic similarity)
```

## 缓存控制

### 请求级别控制

使用请求体中的 `cache_control` 参数控制每个请求的缓存行为：

```bash
# 跳过缓存查询，始终调用模型
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello"}],
    "cache_control": {"type": "no_cache"}
  }'
```

| 类型 | 效果 |
|------|------|
| `no_cache` | 跳过缓存查询，始终获取全新响应 |
| `no_store` | 不将此响应存入缓存 |
| `response_only` | 仅使用精确匹配缓存（跳过语义缓存） |
| `semantic_only` | 仅使用语义缓存（跳过精确匹配） |

### 响应标头

每个响应都包含缓存状态：

```
X-Cache-Status: HIT    # 响应来自缓存
X-Cache-Status: MISS   # 来自模型的全新响应
```

## 检查缓存状态

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# 从响应标头检查缓存状态
# (在原始 HTTP 响应中可用)
print(f"Cache: {response._raw_response.headers.get('X-Cache-Status')}")
```

## 缓存计费

缓存命中的费用显著低于全新请求：

| 类型 | 成本 |
|------|------|
| 缓存命中 (HIT) | **2 折 (80% off)** |
| 缓存未命中 (MISS) | 原价 |

确切的折扣显示在您的仪表板使用日志中。

## 隐私控制

### 组织 / 用户层级

在仪表板设置中配置缓存行为：

| 模式 | 描述 |
|------|-------------|
| **共享 (Shared)** | 启用缓存，响应可能跨用户共享（个人账户默认） |
| **隔离 (Isolated)** | 启用缓存，但响应仅限您的组织私有（组织账户默认） |
| **停用 (Disabled)** | 完全不使用缓存 |

其他可配置项：
- **相似度阈值**：调整语义匹配灵敏度（默认：92%）
- **自定义 TTL**：覆盖缓存过期时间
- **排除模型**：为特定模型停用缓存

### 请求层级

使用 `cache_control` 参数覆盖单个请求的设置：

```bash
# 为此请求停用缓存
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "..."}],
    "cache_control": {"type": "no_store"}
  }'
```

## 缓存反馈

如果您收到错误的缓存响应，可以进行举报：

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**反馈类型：**
- `wrong_answer` - 事实错误
- `outdated` - 信息已过时
- `irrelevant` - 与问题不符
- `other` - 其他问题

当缓存项目收到足够的负面反馈时，它将自动失效。

## 最佳实践

<AccordionGroup>
  <Accordion title="对可缓存的查询使用 temperature=0">
    确定性设置可最大化缓存命中率。
  </Accordion>

  <Accordion title="标准化提示词格式">
    一致的格式化可改善语义匹配。
  </Accordion>

  <Accordion title="对时效性查询使用 no-cache">
    时事、实时数据应跳过缓存。
  </Accordion>

  <Accordion title="监控缓存命中率">
    在仪表板中查看缓存统计数据与节省金额。
  </Accordion>
</AccordionGroup>

## 何时不应使用缓存

针对以下情况停用缓存：

- **即时信息**：股票价格、天气、新闻
- **个性化内容**：针对特定用户的推荐
- **创意任务**：当需要多样性时
- **敏感数据**：机密信息

```python
# 对于时间敏感的查询
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_body={"cache_control": {"type": "no_cache"}}
)
```