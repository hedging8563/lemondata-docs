---
title: "✨ 智能缓存"
description: "通过上下文感知的语义缓存降低成本和延迟"
---

## 概览

LemonData 提供了一个智能缓存系统，可以显著降低您的 API 成本和响应延迟。我们的缓存不仅限于简单的请求匹配——它还能理解 Prompt 的**语义含义**。

<CardGroup cols={2}>
  <Card title="节省成本" icon="piggy-bank">
    缓存命中仅按正常成本的一小部分计费。
  </Card>
  <Card title="更快的响应" icon="bolt">
    缓存的响应会立即返回，无需模型推理。
  </Card>
  <Card title="上下文感知" icon="brain">
    语义匹配即使在措辞不同的情况下也能找到相似的请求。
  </Card>
  <Card title="隐私控制" icon="shield">
    完全控制缓存和共享的内容。
  </Card>
</CardGroup>

## 工作原理

LemonData 使用两层缓存系统：

### 第一层：响应缓存（精确匹配）

对于确定性请求（`temperature=0`），我们缓存精确的响应：

- **匹配**：相同的 `model`、`messages` 和参数
- **速度**：瞬时（微秒级）
- **最适用于**：重复的相同查询

### 第二层：语义缓存（相似度匹配）

对于所有请求，我们还使用两阶段匹配算法检查语义相似度：

- **第一阶段（仅查询）**：用户查询相似度 ≥95%
- **第二阶段（全上下文）**：包含对话上下文的相似度 ≥85%
- **最适用于**：FAQ 风格的查询、常见问题

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
→ Same cached response (high semantic similarity)
```

## 缓存控制

### 请求级别控制

使用请求体中的 `cache_control` 参数控制每个请求的缓存行为：

```bash
# 跳过缓存查询，始终调用模型
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello"}],
    "cache_control": {"type": "no_cache"}
  }'
```

| 类型 | 效果 |
|------|------|
| `no_cache` | 跳过缓存查询，始终获取新鲜响应 |
| `no_store` | 不将此响应存入缓存 |
| `response_only` | 仅使用精确匹配缓存（跳过语义缓存） |
| `semantic_only` | 仅使用语义缓存（跳过精确匹配） |

### 响应 Header

每个响应都包含缓存状态：

```
X-Cache-Status: HIT    # 响应来自缓存
X-Cache-Status: MISS   # 来自模型的新鲜响应
```

## 检查缓存状态

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# 从响应头检查缓存状态
# (在原始 HTTP 响应中可用)
print(f"Cache: {response._raw_response.headers.get('X-Cache-Status')}")
```

## 缓存计费

缓存命中的价格显著低于新鲜请求：

| 类型 | 成本 |
|------|------|
| 缓存 HIT | **2折 (80% off)** |
| 缓存 MISS | 全价 |

具体的折扣显示在您的仪表板使用日志中。

## 隐私控制

### 组织 / 用户级别

在仪表板设置中配置缓存行为：

| 模式 | 描述 |
|------|-------------|
| **共享 (Shared)** | 启用缓存，响应可能跨用户共享（个人账户默认） |
| **隔离 (Isolated)** | 启用缓存，但响应仅限您的组织私有（组织账户默认） |
| **禁用 (Disabled)** | 完全不缓存 |

其他可配置项：
- **相似度阈值**：调整语义匹配灵敏度（默认：92%）
- **自定义 TTL**：覆盖缓存过期时间
- **排除模型**：为特定模型禁用缓存

### 请求级别

使用 `cache_control` 参数按请求覆盖：

```bash
# 为此请求禁用缓存
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "..."}],
    "cache_control": {"type": "no_store"}
  }'
```

## 缓存反馈

如果您收到错误的缓存响应，可以进行报告：

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**反馈类型：**
- `wrong_answer` - 事实错误
- `outdated` - 信息陈旧
- `irrelevant` - 与问题不匹配
- `other` - 其他问题

当一个缓存条目收到足够的负面反馈时，它会自动失效。

## 最佳实践

<AccordionGroup>
  <Accordion title="对可缓存的查询使用 temperature=0">
    确定性设置可最大化缓存命中率。
  </Accordion>

  <Accordion title="标准化 Prompt 格式">
    一致的格式可以提高语义匹配效果。
  </Accordion>

  <Accordion title="对时间敏感的查询使用 no-cache">
    时事、实时数据应跳过缓存。
  </Accordion>

  <Accordion title="监控缓存命中率">
    在仪表板中查看缓存统计信息和节省情况。
  </Accordion>
</AccordionGroup>

## 何时不应使用缓存

为以下情况禁用缓存：

- **实时信息**：股票价格、天气、新闻
- **个性化内容**：针对特定用户的推荐
- **创意任务**：当需要多样性时
- **敏感数据**：机密信息

```python
# 对于时间敏感的查询
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_body={"cache_control": {"type": "no_cache"}}
)
```