---
title: "速率限制"
description: "了解并处理速率限制"
---

## 概览

LemonData 实施速率限制以确保公平使用和平台稳定性。限制因账户层级而异。

## 速率限制层级

| 层级 | 请求数/分钟 | 描述 |
|------|-------------|-------------|
| **User** | 60 | 所有账户的默认层级 |
| **Partner** | 300 | 适用于合作伙伴 |
| **VIP** | 1,000 | 高用量用户 |

<Note>
  速率限制可能会发生变化。如需自定义限制，请联系 support@lemondata.cc。
</Note>

## 速率限制响应

当您超过速率限制时，API 将返回 `429` 状态码，并带有 `Retry-After` 响应头，指示在重试前需要等待多长时间。

## 速率限制已超出

当您超出限制时，将收到 `429` 响应：

```json
{
  "error": {
    "message": "Rate limit exceeded. Please retry later.",
    "type": "rate_limit_error",
    "code": "rate_limit_exceeded"
  }
}
```

响应包含一个 `Retry-After` 响应头：
```
Retry-After: 60  # Seconds to wait before retrying
```

## 处理速率限制

### 指数退避

为自动重试实现指数退避：

```python
import time
from openai import OpenAI, RateLimitError

client = OpenAI(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc/v1"
)

def make_request_with_backoff(messages, max_retries=5):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError as e:
            if attempt == max_retries - 1:
                raise

            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds
            print(f"Rate limited. Waiting {wait_time}s...")
            time.sleep(wait_time)
```

### 请求队列

对于高并发应用，请实现请求队列：

```python
import asyncio
from collections import deque

class RateLimitedClient:
    def __init__(self, requests_per_minute=60):
        self.rpm = requests_per_minute
        self.interval = 60 / requests_per_minute
        self.last_request = 0

    async def request(self, messages):
        # Wait if needed to respect rate limit
        now = asyncio.get_event_loop().time()
        wait_time = max(0, self.last_request + self.interval - now)
        if wait_time > 0:
            await asyncio.sleep(wait_time)

        self.last_request = asyncio.get_event_loop().time()
        return await self.client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )
```

### 批量处理

对于批量操作，请通过延迟进行分批处理：

```python
def process_batch(items, batch_size=50, delay=1):
    results = []
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        for item in batch:
            result = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": item}]
            )
            results.append(result)
        time.sleep(delay)  # Pause between batches
    return results
```

## 最佳实践

<AccordionGroup>
  <Accordion title="监控您的用量">
    跟踪速率限制响应头，主动保持在限制范围内。
  </Accordion>

  <Accordion title="实现缓存">
    对相同请求的响应进行缓存，以减少 API 调用。
  </Accordion>

  <Accordion title="使用合适的模型">
    更快的模型（如 gpt-4o-mini）允许更高的吞吐量。
  </Accordion>

  <Accordion title="联系我们获取更高限制">
    如果您需要更高的限制，请联系 support@lemondata.cc。
  </Accordion>
</AccordionGroup>

## 升级您的层级

如需申请层级升级：

1. 登录您的 [控制面板](https://lemondata.cc/dashboard)
2. 前往 **Settings → Account**
3. 联系支持团队并说明您的使用场景

或者发送电子邮件至 support@lemondata.cc，并提供以下信息：
- 您的账户邮箱
- 预期的请求量
- 使用场景描述