---
title: "✨ 智慧快取"
description: "透過上下文感知的語義快取降低成本與延遲"
---

## 總覽

LemonData 提供智慧快取系統，可顯著降低您的 API 成本與回應延遲。我們的快取不僅僅是簡單的請求匹配，它還能理解您提示詞（prompts）的**語義（semantic meaning）**。

<CardGroup cols={2}>
  <Card title="節省成本" icon="piggy-bank">
    快取命中（Cache hits）僅按正常成本的一小部分計費。
  </Card>
  <Card title="更快速的回應" icon="bolt">
    快取的回應會立即回傳，無需進行模型推論。
  </Card>
  <Card title="上下文感知" icon="brain">
    語義匹配即使在措辭不同的情況下也能找到相似的請求。
  </Card>
  <Card title="隱私控制" icon="shield">
    完全控制快取與分享的內容。
  </Card>
</CardGroup>

## 運作原理

LemonData 使用雙層快取系統：

### 第一層：回應快取（精確匹配）

對於確定性請求（`temperature=0`），我們快取精確的回應：

- **匹配條件**：相同的模型、訊息與參數
- **速度**：即時（微秒級）
- **適用於**：重複的相同查詢

### 第二層：語義快取（相似度匹配）

對於所有請求，我們還會使用兩階段匹配演算法檢查語義相似度：

- **第一階段（僅查詢）**：使用者查詢相似度 ≥95%
- **第二階段（完整上下文）**：包含對話上下文的相似度 ≥85%
- **適用於**：FAQ 類型的查詢、常見問題

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
→ Same cached response (high semantic similarity)
```

## 快取標頭

### 請求標頭

控制每個請求的快取行為：

```bash
# Skip cache lookup, always call the model
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Cache-Control: no-cache" \
  -d '{"model": "gpt-4o", "messages": [...]}'
```

| 標頭 | 數值 | 效果 |
|--------|-------|--------|
| `Cache-Control: no-cache` | - | 跳過快取，獲取全新回應 |
| `Cache-Control: no-store` | - | 不要快取此回應 |

### 回應標頭

每個回應都包含快取狀態：

```
X-Cache: HIT           # 回應來自快取
X-Cache: MISS          # 來自模型的全新回應
X-Cache-Entry-Id: abc  # 快取項目 ID（用於回饋）
```

## 檢查快取狀態

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# Check cache status from response headers
# (Available in raw HTTP response)
print(f"Cache: {response._raw_response.headers.get('X-Cache')}")
```

## 快取計費

快取命中的費用顯著低於全新請求：

| 類型 | 成本 |
|------|------|
| 快取命中 (HIT) | **2 折 (90% off)** |
| 快取未命中 (MISS) | 原價 |

確切的折扣顯示在您的儀表板使用日誌中。

## 隱私控制

### API Key 層級

在儀表板中為每個 API Key 設定快取行為：

| 模式 | 描述 |
|------|-------------|
| **預設** | 啟用快取，可能與相似請求共享 |
| **不共享** | 啟用快取，但回應僅限您的帳戶私有 |
| **已停用** | 完全不使用快取 |

### 請求層級

覆蓋單個請求的設定：

```bash
# Disable caching for this request
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Cache-Control: no-store" \
  -d '...'
```

## 快取回饋

如果您收到錯誤的快取回應，可以進行回報：

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**回饋類型：**
- `wrong_answer` - 事實錯誤
- `outdated` - 資訊已過時
- `irrelevant` - 與問題不符
- `other` - 其他問題

當快取項目收到足夠的負面回饋時，它將自動失效。

## 最佳實踐

<AccordionGroup>
  <Accordion title="對可快取的查詢使用 temperature=0">
    確定性設定可最大化快取命中率。
  </Accordion>

  <Accordion title="標準化提示詞格式">
    一致的格式化可改善語義匹配。
  </Accordion>

  <Accordion title="對時效性查詢使用 no-cache">
    時事、即時數據應跳過快取。
  </Accordion>

  <Accordion title="監控快取命中率">
    在儀表板中查看快取統計數據與節省金額。
  </Accordion>
</AccordionGroup>

## 何時不應使用快取

針對以下情況停用快取：

- **即時資訊**：股票價格、天氣、新聞
- **個人化內容**：針對特定使用者的推薦
- **創意任務**：當需要多樣性時
- **敏感數據**：機密資訊

```python
# For time-sensitive queries
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_headers={"Cache-Control": "no-cache"}
)
```