---
title: "Mejores Prácticas"
description: "Optimice su uso de la API de LemonData para mejorar el costo, el rendimiento y la confiabilidad"
---

## Selección de Modelos

Elegir el modelo adecuado puede impactar significativamente en el costo y la calidad.

### Recomendaciones Basadas en Tareas

| Tarea | Modelos Recomendados | Razonamiento |
|------|-------------------|-----------|
| **Preguntas y respuestas simples** | `gpt-4o-mini`, `gemini-2.5-flash` | Rápido, económico, suficientemente bueno |
| **Razonamiento complejo** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Mejor lógica y planificación |
| **Programación** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Optimizado para código |
| **Escritura creativa** | `claude-sonnet-4-5`, `gpt-4o` | Mejor calidad de prosa |
| **Visión/Imágenes** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Soporte nativo de visión |
| **Contexto largo** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Ventanas de más de 1M de tokens |
| **Sensible al costo** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Mejor relación calidad-precio |

### Niveles de Costo

```
$$$$ Premium: o3, claude-opus-4-5, gpt-4o
$$$  Standard: claude-sonnet-4-5, gpt-4o
$$   Budget:   gpt-4o-mini, gemini-2.5-flash
$    Economy:  deepseek-v3.2, deepseek-r1
```

## Optimización de Costos

### 1. Use Modelos más Pequeños Primero

```python
def smart_query(question: str, complexity: str = "auto"):
    """Use modelos más económicos para tareas simples."""

    if complexity == "simple":
        model = "gpt-4o-mini"
    elif complexity == "complex":
        model = "gpt-4o"
    else:
        # Comenzar con lo económico, escalar si es necesario
        model = "gpt-4o-mini"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": question}]
    )
    return response
```

### 2. Configure max_tokens

Configure siempre un límite razonable de `max_tokens`:

```python
# ❌ Mal: Sin límite, podría generar miles de tokens
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}]
)

# ✅ Bien: Limitar la longitud de la respuesta
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}],
    max_tokens=500  # Límite razonable para un resumen
)
```

### 3. Optimice los Prompts

```python
# ❌ Prompt detallado (más tokens de entrada)
prompt = """
I would like you to please help me by analyzing the following text
and providing a comprehensive summary of the main points. Please be
thorough but also concise in your response. The text is as follows:
{text}
"""

# ✅ Prompt conciso (menos tokens)
prompt = "Summarize the key points:\n{text}"
```

### 4. Habilite el Caching

Aproveche el [caching semántico](/guides/caching):

```python
# Para consultas similares repetidas, el caching proporciona grandes ahorros
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is machine learning?"}],
    temperature=0  # Determinista = mejores aciertos de caché
)
```

### 5. Agrupe Solicitudes Similares (Batching)

```python
# ❌ Muchas solicitudes pequeñas
for question in questions:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": question}]
    )

# ✅ Menos solicitudes pero más grandes
combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"Answer each question:\n{combined_prompt}"}]
)
```

## Optimización del Rendimiento

### 1. Use Streaming para la UX

El streaming mejora el rendimiento percibido:

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 2. Elija Modelos Rápidos para Uso Interactivo

| Caso de Uso | Recomendado | Latencia |
|----------|-------------|---------|
| Interfaz de Chat | `gpt-4o-mini`, `gemini-2.5-flash` | ~200ms primer token |
| Autocompletado | `claude-haiku-4-5` | ~150ms primer token |
| Procesamiento en segundo plano | `gpt-4o`, `claude-sonnet-4-5` | ~500ms primer token |

### 3. Configure Timeouts

```python
client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    timeout=60.0  # Timeout de 60 segundos
)
```

## Confiabilidad

### 1. Implemente Reintentos

```python
import time
from openai import RateLimitError, APIError

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError:
            wait = 2 ** attempt
            print(f"Límite de tasa alcanzado, esperando {wait}s...")
            time.sleep(wait)
        except APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(1)
    raise Exception("Se excedió el máximo de reintentos")
```

### 2. Gestione los Errores con Elegancia

```python
from openai import APIError, AuthenticationError, RateLimitError

try:
    response = client.chat.completions.create(...)
except AuthenticationError:
    # Verificar la API key
    notify_admin("API key inválida")
except RateLimitError:
    # Encolar para más tarde o usar respaldo
    add_to_queue(request)
except APIError as e:
    if e.status_code == 402:
        notify_admin("Saldo bajo")
    elif e.status_code >= 500:
        # Error del servidor, reintentar más tarde
        schedule_retry(request)
```

### 3. Use Modelos de Respaldo (Fallback)

```python
FALLBACK_CHAIN = ["gpt-4o", "claude-sonnet-4-5", "gemini-2.5-flash"]

def chat_with_fallback(messages):
    for model in FALLBACK_CHAIN:
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages
            )
        except APIError:
            continue
    raise Exception("Todos los modelos fallaron")
```

## Seguridad

### 1. Proteja sus API Keys

```python
# ❌ Nunca incluya las llaves directamente en el código
client = OpenAI(api_key="sk-abc123...")

# ✅ Use variables de entorno
import os
client = OpenAI(api_key=os.environ["LEMONDATA_API_KEY"])
```

### 2. Valide la Entrada del Usuario

```python
def validate_message(content: str) -> bool:
    """Valida la entrada del usuario antes de enviarla a la API."""
    if len(content) > 100000:
        raise ValueError("Mensaje demasiado largo")
    # Agregar otras validaciones según sea necesario
    return True
```

### 3. Establezca Límites para las API Keys

Cree API keys separadas con límites de gasto para:
- Desarrollo/pruebas
- Producción
- Diferentes aplicaciones

## Monitoreo

### 1. Rastree el Uso

Revise su panel de control regularmente para:
- Uso de tokens por modelo
- Desglose de costos
- Tasas de acierto de caché
- Tasas de error

### 2. Registre Métricas Importantes

```python
import logging

response = client.chat.completions.create(...)

logging.info({
    "model": response.model,
    "prompt_tokens": response.usage.prompt_tokens,
    "completion_tokens": response.usage.completion_tokens,
    "total_tokens": response.usage.total_tokens,
})
```

### 3. Configure Alertas

Configure alertas de saldo bajo en su panel de control para evitar interrupciones del servicio.

## Lista de Verificación

<AccordionGroup>
  <Accordion title="Optimización de costos">
    - [ ] Uso del modelo apropiado para cada tarea
    - [ ] Configuración de límites de max_tokens
    - [ ] Los prompts son concisos
    - [ ] Caching habilitado donde sea apropiado
    - [ ] Agrupación de solicitudes similares
  </Accordion>

  <Accordion title="Rendimiento">
    - [ ] Streaming para UX interactiva
    - [ ] Modelos rápidos para uso en tiempo real
    - [ ] Timeouts configurados
  </Accordion>

  <Accordion title="Confiabilidad">
    - [ ] Lógica de reintento implementada
    - [ ] Gestión de errores implementada
    - [ ] Modelos de respaldo configurados
  </Accordion>

  <Accordion title="Seguridad">
    - [ ] API keys en variables de entorno
    - [ ] Validación de entrada
    - [ ] Llaves separadas para desarrollo/producción
    - [ ] Límites de gasto establecidos
  </Accordion>
</AccordionGroup>