---
title: "모범 사례"
description: "비용, 성능 및 안정성을 위해 LemonData API 사용을 최적화하세요"
---

## 모델 선택

적절한 모델을 선택하면 비용과 품질에 큰 영향을 미칠 수 있습니다.

### 작업별 권장 사항

| 작업 | 권장 모델 | 이유 |
|------|-------------------|-----------|
| **단순 Q&A** | `gpt-4o-mini`, `gemini-2.5-flash` | 빠르고 저렴하며 충분한 성능 |
| **복잡한 추론** | `o3`, `claude-opus-4-5`, `deepseek-r1` | 더 나은 논리 및 계획 능력 |
| **코딩** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | 코드에 최적화됨 |
| **창의적 글쓰기** | `claude-sonnet-4-5`, `gpt-4o` | 더 나은 문장 품질 |
| **비전/이미지** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | 네이티브 비전 지원 |
| **긴 컨텍스트** | `gemini-2.5-pro`, `claude-sonnet-4-5` | 100만 개 이상의 토큰 윈도우 |
| **비용 효율성 중시** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | 최고의 가성비 |

### 비용 등급

```
$$$$ 프리미엄: o3, claude-opus-4-5, gpt-4o
$$$  표준:    claude-sonnet-4-5, gpt-4o
$$   예산형:  gpt-4o-mini, gemini-2.5-flash
$    절약형:  deepseek-v3.2, deepseek-r1
```

## 비용 최적화

### 1. 더 작은 모델부터 사용하기

```python
def smart_query(question: str, complexity: str = "auto"):
    """Use cheaper models for simple tasks."""

    if complexity == "simple":
        model = "gpt-4o-mini"
    elif complexity == "complex":
        model = "gpt-4o"
    else:
        # Start cheap, escalate if needed
        model = "gpt-4o-mini"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": question}]
    )
    return response
```

### 2. max_tokens 설정하기

항상 합리적인 `max_tokens` 제한을 설정하세요:

```python
# ❌ 나쁨: 제한이 없어 수천 개의 토큰이 생성될 수 있음
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}]
)

# ✅ 좋음: 응답 길이 제한
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}],
    max_tokens=500  # 요약에 적절한 제한
)
```

### 3. 프롬프트 최적화

```python
# ❌ 장황한 프롬프트 (더 많은 입력 토큰 발생)
prompt = """
I would like you to please help me by analyzing the following text
and providing a comprehensive summary of the main points. Please be
thorough but also concise in your response. The text is as follows:
{text}
"""

# ✅ 간결한 프롬프트 (더 적은 토큰 발생)
prompt = "Summarize the key points:\n{text}"
```

### 4. 캐싱 활성화

[시맨틱 캐싱](/guides/caching)을 활용하세요:

```python
# 반복되는 유사한 쿼리의 경우, 캐싱을 통해 비용을 크게 절감할 수 있습니다.
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is machine learning?"}],
    temperature=0  # 결정론적 응답 = 더 나은 캐시 히트율
)
```

### 5. 유사한 요청 일괄 처리(Batching)

```python
# ❌ 다수의 작은 요청
for question in questions:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": question}]
    )

# ✅ 적은 수의 큰 요청
combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"Answer each question:\n{combined_prompt}"}]
)
```

## 성능 최적화

### 1. 사용자 경험(UX)을 위한 스트리밍 사용

스트리밍은 체감 성능을 향상시킵니다:

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 2. 대화형 사용을 위한 빠른 모델 선택

| 사용 사례 | 권장 모델 | 지연 시간(Latency) |
|----------|-------------|---------|
| 채팅 UI | `gpt-4o-mini`, `gemini-2.5-flash` | 첫 토큰까지 약 200ms |
| 탭 완성 | `claude-haiku-4-5` | 첫 토큰까지 약 150ms |
| 백그라운드 처리 | `gpt-4o`, `claude-sonnet-4-5` | 첫 토큰까지 약 500ms |

### 3. 타임아웃 설정

```python
client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    timeout=60.0  # 60초 타임아웃
)
```

## 안정성

### 1. 재시도 로직 구현

```python
import time
from openai import RateLimitError, APIError

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError:
            wait = 2 ** attempt
            print(f"Rate limited, waiting {wait}s...")
            time.sleep(wait)
        except APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(1)
    raise Exception("Max retries exceeded")
```

### 2. 우아한 에러 처리

```python
from openai import APIError, AuthenticationError, RateLimitError

try:
    response = client.chat.completions.create(...)
except AuthenticationError:
    # API 키 확인
    notify_admin("Invalid API key")
except RateLimitError:
    # 나중에 처리하도록 큐에 추가하거나 백업 사용
    add_to_queue(request)
except APIError as e:
    if e.status_code == 402:
        notify_admin("Balance low")
    elif e.status_code >= 500:
        # 서버 에러, 나중에 재시도 예약
        schedule_retry(request)
```

### 3. 폴백(Fallback) 모델 사용

```python
FALLBACK_CHAIN = ["gpt-4o", "claude-sonnet-4-5", "gemini-2.5-flash"]

def chat_with_fallback(messages):
    for model in FALLBACK_CHAIN:
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages
            )
        except APIError:
            continue
    raise Exception("All models failed")
```

## 보안

### 1. API 키 보호

```python
# ❌ 키를 코드에 직접 작성하지 마세요
client = OpenAI(api_key="sk-abc123...")

# ✅ 환경 변수를 사용하세요
import os
client = OpenAI(api_key=os.environ["LEMONDATA_API_KEY"])
```

### 2. 사용자 입력 검증

```python
def validate_message(content: str) -> bool:
    """Validate user input before sending to API."""
    if len(content) > 100000:
        raise ValueError("Message too long")
    # 필요한 다른 검증 로직 추가
    return True
```

### 3. API 키 제한 설정

다음을 위해 지출 제한이 설정된 별도의 API 키를 생성하세요:
- 개발/테스트
- 운영(Production)
- 서로 다른 애플리케이션

## 모니터링

### 1. 사용량 추적

대시보드에서 다음 사항을 정기적으로 확인하세요:
- 모델별 토큰 사용량
- 비용 내역
- 캐시 히트율
- 에러율

### 2. 주요 지표 로깅

```python
import logging

response = client.chat.completions.create(...)

logging.info({
    "model": response.model,
    "prompt_tokens": response.usage.prompt_tokens,
    "completion_tokens": response.usage.completion_tokens,
    "total_tokens": response.usage.total_tokens,
})
```

### 3. 알림 설정

서비스 중단을 방지하기 위해 대시보드에서 잔액 부족 알림을 구성하세요.

## 체크리스트

<AccordionGroup>
  <Accordion title="비용 최적화">
    - [ ] 각 작업에 적절한 모델 사용 중
    - [ ] max_tokens 제한 설정됨
    - [ ] 프롬프트가 간결함
    - [ ] 적절한 경우 캐싱 활성화됨
    - [ ] 유사한 요청을 일괄 처리함
  </Accordion>

  <Accordion title="성능">
    - [ ] 대화형 UX를 위한 스트리밍 사용
    - [ ] 실시간 사용을 위한 빠른 모델 사용
    - [ ] 타임아웃 구성됨
  </Accordion>

  <Accordion title="안정성">
    - [ ] 재시도 로직 구현됨
    - [ ] 에러 처리 적용됨
    - [ ] 폴백 모델 구성됨
  </Accordion>

  <Accordion title="보안">
    - [ ] 환경 변수에 API 키 저장
    - [ ] 입력 검증 수행
    - [ ] 개발/운영용 키 분리
    - [ ] 지출 제한 설정됨
  </Accordion>
</AccordionGroup>