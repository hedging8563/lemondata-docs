---
title: "✨ 스마트 캐시"
description: "컨텍스트 인식 의미론적 캐싱을 통해 비용과 지연 시간 단축"
---

## 개요

LemonData는 API 비용과 응답 지연 시간을 획기적으로 줄여주는 스마트 캐싱 시스템을 제공합니다. 당사의 캐싱은 단순한 요청 매칭을 넘어 프롬프트(prompts)의 **의미(semantic meaning)**를 이해합니다.

<CardGroup cols={2}>
  <Card title="비용 절감" icon="piggy-bank">
    캐시 히트(Cache hits)는 정상 비용의 극히 일부만 청구됩니다.
  </Card>
  <Card title="더 빠른 응답" icon="bolt">
    캐싱된 응답은 모델 추론 없이 즉시 반환됩니다.
  </Card>
  <Card title="컨텍스트 인식" icon="brain">
    의미론적 매칭은 표현이 다르더라도 유사한 요청을 찾아냅니다.
  </Card>
  <Card title="개인정보 제어" icon="shield">
    캐싱 및 공유되는 콘텐츠를 완전히 제어할 수 있습니다.
  </Card>
</CardGroup>

## 작동 원리

LemonData는 2계층 캐싱 시스템을 사용합니다:

### 제1계층: 응답 캐시 (정확한 매칭)

결정론적 요청(`temperature=0`)에 대해 정확한 응답을 캐싱합니다:

- **매칭 조건**: 동일한 모델, 메시지 및 파라미터
- **속도**: 즉시 (마이크로초 단위)
- **용도**: 반복되는 동일한 쿼리

### 제2계층: 의미론적 캐시 (유사도 매칭)

모든 요청에 대해 2단계 매칭 알고리즘을 사용하여 의미론적 유사도를 확인합니다:

- **1단계 (쿼리 전용)**: 사용자 쿼리 유사도 ≥95%
- **2단계 (전체 컨텍스트)**: 대화 컨텍스트를 포함한 유사도 ≥85%
- **용도**: FAQ 유형의 쿼리, 일반적인 질문

```
User A: "What is the capital of France?"
User B: "Tell me the capital city of France"
→ Same cached response (high semantic similarity)
```

## 캐시 제어

### 요청 레벨 제어

요청 본문의 `cache_control` 파라미터를 사용하여 각 요청의 캐싱 동작을 제어합니다:

```bash
# 캐시 조회를 건너뛰고 항상 모델을 호출합니다
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello"}],
    "cache_control": {"type": "no_cache"}
  }'
```

| 유형 | 효과 |
|------|------|
| `no_cache` | 캐시 조회를 건너뛰고 항상 새로운 응답을 가져옴 |
| `no_store` | 이 응답을 캐시에 저장하지 않음 |
| `response_only` | 정확한 매칭 캐시만 사용 (의미론적 캐시 건너뜀) |
| `semantic_only` | 의미론적 캐시만 사용 (정확한 매칭 건너뜀) |

### 응답 헤더

각 응답에는 캐시 상태가 포함됩니다:

```
X-Cache-Status: HIT    # 캐시에서 반환된 응답
X-Cache-Status: MISS   # 모델로부터 생성된 새로운 응답
```

## 캐시 상태 확인

```python
from openai import OpenAI

client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is 2+2?"}]
)

# 응답 헤더에서 캐시 상태 확인
# (원시 HTTP 응답에서 사용 가능)
print(f"Cache: {response._raw_response.headers.get('X-Cache-Status')}")
```

## 캐시 과금

캐시 히트 비용은 새로운 요청보다 훨씬 저렴합니다:

| 유형 | 비용 |
|------|------|
| 캐시 히트 (HIT) | **80% 할인 (20% 가격)** |
| 캐시 미스 (MISS) | 원래 가격 |

정확한 할인 내역은 대시보드의 사용 로그에 표시됩니다.

## 개인정보 제어

### 조직 / 사용자 레벨

대시보드 설정에서 캐싱 동작을 구성합니다:

| 모드 | 설명 |
|------|-------------|
| **공유 (Shared)** | 캐싱을 활성화하며, 응답이 사용자 간에 공유될 수 있음 (개인 계정 기본값) |
| **격리 (Isolated)** | 캐싱을 활성화하지만, 응답은 귀하의 조직 내에서만 비공개로 유지됨 (조직 계정 기본값) |
| **비활성화 (Disabled)** | 캐싱을 전혀 사용하지 않음 |

기타 구성 가능 항목:
- **유사도 임계값**: 의미론적 매칭 민감도 조정 (기본값: 92%)
- **사용자 정의 TTL**: 캐시 만료 시간 재정의
- **모델 제외**: 특정 모델에 대해 캐싱 비활성화

### 요청 레벨

`cache_control` 파라미터를 사용하여 개별 요청의 설정을 재정의합니다:

```bash
# 이 요청에 대해 캐싱 비활성화
curl https://api.lemondata.cc/v1/chat/completions \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "..."}],
    "cache_control": {"type": "no_store"}
  }'
```

## 캐시 피드백

잘못된 캐시 응답을 받은 경우 보고할 수 있습니다:

```bash
curl -X POST https://api.lemondata.cc/v1/cache/feedback \
  -H "Authorization: Bearer sk-your-key" \
  -H "Content-Type: application/json" \
  -d '{
    "cache_entry_id": "abc123",
    "feedback_type": "wrong_answer",
    "description": "Response was outdated"
  }'
```

**피드백 유형:**
- `wrong_answer` - 사실 오류
- `outdated` - 정보가 오래됨
- `irrelevant` - 질문과 관련 없음
- `other` - 기타 문제

캐시 항목에 부정적인 피드백이 충분히 쌓이면 자동으로 무효화됩니다.

## 권장 사항

<AccordionGroup>
  <Accordion title="캐싱 가능한 쿼리에 temperature=0 사용">
    결정론적 설정은 캐시 히트율을 극대화합니다.
  </Accordion>

  <Accordion title="프롬프트 형식 표준화">
    일관된 포맷팅은 의미론적 매칭 성능을 향상시킵니다.
  </Accordion>

  <Accordion title="시의성 있는 쿼리에 no-cache 사용">
    시사 뉴스, 실시간 데이터 등은 캐시를 건너뛰어야 합니다.
  </Accordion>

  <Accordion title="캐시 히트율 모니터링">
    대시보드에서 캐시 통계와 절감액을 확인하세요.
  </Accordion>
</AccordionGroup>

## 캐시를 사용하지 말아야 할 경우

다음과 같은 상황에서는 캐싱을 비활성화하세요:

- **실시간 정보**: 주가, 날씨, 뉴스
- **개인화된 콘텐츠**: 특정 사용자를 위한 추천
- **창의적인 작업**: 다양성이 필요한 경우
- **민감한 데이터**: 기밀 정보

```python
# 시간 민감도가 높은 쿼리의 경우
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What's the current stock price of AAPL?"}],
    extra_body={"cache_control": {"type": "no_cache"}}
)
```