---
title: "LlamaIndex"
description: "Tích hợp LemonData với LlamaIndex cho các ứng dụng RAG"
---

## Tổng quan

LlamaIndex là một khung dữ liệu (data framework) cho các ứng dụng LLM, đặc biệt mạnh mẽ trong việc xây dựng các hệ thống RAG (Retrieval Augmented Generation). LemonData hoạt động mượt mà với tích hợp OpenAI của LlamaIndex.

## Cài đặt

```bash
pip install llama-index llama-index-llms-openai llama-index-embeddings-openai
```

## Cấu hình cơ bản

```python
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# Configure LLM
llm = OpenAI(
    model="gpt-4o",
    api_key="sk-your-lemondata-key",
    api_base="https://api.lemondata.cc/v1"
)

# Set as default
Settings.llm = llm

# Simple query
response = llm.complete("What is LemonData?")
print(response.text)
```

## Sử dụng các mô hình khác nhau

```python
# OpenAI GPT-4o
gpt4 = OpenAI(
    model="gpt-4o",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)

# Anthropic Claude (via OpenAI-compatible endpoint)
claude = OpenAI(
    model="claude-sonnet-4-5",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)

# Google Gemini
gemini = OpenAI(
    model="gemini-2.5-flash",
    api_key="sk-your-key",
    api_base="https://api.lemondata.cc/v1"
)
```

## Giao diện trò chuyện

```python
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(role="system", content="You are a helpful assistant."),
    ChatMessage(role="user", content="What is the capital of France?")
]

response = llm.chat(messages)
print(response.message.content)
```

## Streaming

```python
# Streaming completion
for chunk in llm.stream_complete("Write a poem about AI"):
    print(chunk.delta, end="", flush=True)

# Streaming chat
for chunk in llm.stream_chat(messages):
    print(chunk.delta, end="", flush=True)
```

## Embeddings

```python
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key="sk-your-lemondata-key",
    api_base="https://api.lemondata.cc/v1"
)

# Set as default
Settings.embed_model = embed_model

# Get embeddings
embeddings = embed_model.get_text_embedding("Hello, world!")
print(f"Embedding dimension: {len(embeddings)}")
```

## RAG với tài liệu

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Configure settings
Settings.llm = llm
Settings.embed_model = embed_model

# Load documents
documents = SimpleDirectoryReader("./data").load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Query
query_engine = index.as_query_engine()
response = query_engine.query("What is in my documents?")
print(response)
```

## Chat Engine

```python
# Create chat engine with memory
chat_engine = index.as_chat_engine(chat_mode="condense_question")

# Multi-turn conversation
response = chat_engine.chat("What is LemonData?")
print(response)

response = chat_engine.chat("How many models does it support?")
print(response)
```

## Sử dụng bất đồng bộ

```python
import asyncio

async def main():
    response = await llm.acomplete("Hello!")
    print(response.text)

asyncio.run(main())
```

## Biến môi trường

Để mã nguồn gọn gàng hơn, hãy sử dụng các biến môi trường:

```bash
export OPENAI_API_KEY="sk-your-lemondata-key"
export OPENAI_API_BASE="https://api.lemondata.cc/v1"
```

```python
from llama_index.llms.openai import OpenAI

# Will automatically use environment variables
llm = OpenAI(model="gpt-4o")
```

## Thực hành tốt nhất

<AccordionGroup>
  <Accordion title="Chọn mô hình phù hợp">
    Sử dụng các mô hình nhanh hơn (GPT-4o-mini) cho các tác vụ Embedding và tóm tắt, dành các mô hình mạnh mẽ (GPT-4o, Claude) cho phản hồi cuối cùng.
  </Accordion>

  <Accordion title="Tối ưu hóa kích thước chunk">
    Điều chỉnh kích thước chunk (`chunk size`) dựa trên loại tài liệu của bạn. Sử dụng các chunk nhỏ hơn cho các tài liệu kỹ thuật dày đặc và các chunk lớn hơn cho nội dung tự sự.
  </Accordion>

  <Accordion title="Sử dụng bộ nhớ đệm">
    Kích hoạt tính năng bộ nhớ đệm của LlamaIndex để tránh các lệnh gọi API dư thừa trong quá trình phát triển.
  </Accordion>
</AccordionGroup>