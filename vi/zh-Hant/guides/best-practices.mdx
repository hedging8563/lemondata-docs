---
title: "Thực hành tốt nhất"
description: "Tối ưu hóa việc sử dụng LemonData API của bạn để nâng cao hiệu quả chi phí, hiệu suất và độ tin cậy"
---

## Lựa chọn mô hình

Việc chọn mô hình phù hợp sẽ ảnh hưởng đáng kể đến chi phí và chất lượng.

### Khuyến nghị dựa trên nhiệm vụ

| Nhiệm vụ | Mô hình đề xuất | Lý do |
|------|-------------------|-----------|
| **Hỏi đáp đơn giản** | `gpt-4o-mini`, `gemini-2.5-flash` | Nhanh, rẻ, đủ dùng |
| **Suy luận phức tạp** | `o3`, `claude-opus-4-5`, `deepseek-r1` | Khả năng logic và lập kế hoạch tốt hơn |
| **Lập trình** | `claude-sonnet-4-5`, `gpt-4o`, `deepseek-v3.2` | Được tối ưu hóa cho mã nguồn |
| **Viết lách sáng tạo** | `claude-sonnet-4-5`, `gpt-4o` | Chất lượng văn bản tốt hơn |
| **Thị giác/Hình ảnh** | `gpt-4o`, `claude-sonnet-4-5`, `gemini-2.5-flash` | Hỗ trợ thị giác gốc |
| **Ngữ cảnh dài** | `gemini-2.5-pro`, `claude-sonnet-4-5` | Cửa sổ 1M+ token |
| **Nhạy cảm về chi phí** | `gpt-4o-mini`, `gemini-2.5-flash`, `deepseek-v3.2` | Hiệu năng trên giá thành tốt nhất |

### Phân cấp chi phí

```
$$$$ Premium: o3, claude-opus-4-5, gpt-4o
$$$  Standard: claude-sonnet-4-5, gpt-4o
$$   Budget:   gpt-4o-mini, gemini-2.5-flash
$    Economy:  deepseek-v3.2, deepseek-r1
```

## Tối ưu hóa chi phí

### 1. Ưu tiên sử dụng các mô hình nhỏ

```python
def smart_query(question: str, complexity: str = "auto"):
    """Use cheaper models for simple tasks."""

    if complexity == "simple":
        model = "gpt-4o-mini"
    elif complexity == "complex":
        model = "gpt-4o"
    else:
        # Start cheap, escalate if needed
        model = "gpt-4o-mini"

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": question}]
    )
    return response
```

### 2. Thiết lập max_tokens

Luôn thiết lập giới hạn `max_tokens` hợp lý:

```python
# ❌ Sai: Không thiết lập giới hạn, có thể tạo ra hàng nghìn token
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}]
)

# ✅ Đúng: Giới hạn độ dài phản hồi
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Summarize this article"}],
    max_tokens=500  # Giới hạn hợp lý cho bản tóm tắt
)
```

### 3. Tối ưu hóa Prompt

```python
# ❌ Prompt dài dòng (tiêu tốn nhiều input token hơn)
prompt = """
I would like you to please help me by analyzing the following text
and providing a comprehensive summary of the main points. Please be
thorough but also concise in your response. The text is as follows:
{text}
"""

# ✅ Prompt ngắn gọn (tiêu tốn ít token hơn)
prompt = "Summarize the key points:\n{text}"
```

### 4. Kích hoạt bộ nhớ đệm (Caching)

Tận dụng [Caching ngữ nghĩa](/guides/caching):

```python
# Đối với các truy vấn tương tự lặp lại, caching có thể tiết kiệm chi phí đáng kể
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "What is machine learning?"}],
    temperature=0  # Tính xác định = tỷ lệ khớp cache tốt hơn
)
```

### 5. Xử lý hàng loạt (Batching) các yêu cầu tương tự

```python
# ❌ Nhiều yêu cầu nhỏ
for question in questions:
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": question}]
    )

# ✅ Ít yêu cầu lớn hơn
combined_prompt = "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": f"Answer each question:\n{combined_prompt}"}]
)
```

## Tối ưu hóa hiệu suất

### 1. Sử dụng Streaming để cải thiện trải nghiệm người dùng

Streaming có thể cải thiện hiệu suất cảm nhận được:

```python
stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Write a long essay"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### 2. Chọn mô hình nhanh cho các mục đích tương tác

| Trường hợp sử dụng | Mô hình đề xuất | Độ trễ |
|----------|-------------|---------|
| Giao diện trò chuyện | `gpt-4o-mini`, `gemini-2.5-flash` | Token đầu tiên khoảng 200ms |
| Tự động hoàn thành | `claude-haiku-4-5` | Token đầu tiên khoảng 150ms |
| Xử lý nền | `gpt-4o`, `claude-sonnet-4-5` | Token đầu tiên khoảng 500ms |

### 3. Thiết lập thời gian chờ (Timeouts)

```python
client = OpenAI(
    api_key="sk-your-key",
    base_url="https://api.lemondata.cc/v1",
    timeout=60.0  # 60 giây timeout
)
```

## Độ tin cậy

### 1. Triển khai cơ chế thử lại (Retry)

```python
import time
from openai import RateLimitError, APIError

def chat_with_retry(messages, max_retries=3):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError:
            wait = 2 ** attempt
            print(f"Rate limited, waiting {wait}s...")
            time.sleep(wait)
        except APIError as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(1)
    raise Exception("Max retries exceeded")
```

### 2. Xử lý lỗi một cách khéo léo

```python
from openai import APIError, AuthenticationError, RateLimitError

try:
    response = client.chat.completions.create(...)
except AuthenticationError:
    # Kiểm tra API key
    notify_admin("Invalid API key")
except RateLimitError:
    # Thêm vào hàng đợi để xử lý sau hoặc sử dụng dự phòng
    add_to_queue(request)
except APIError as e:
    if e.status_code == 402:
        notify_admin("Balance low")
    elif e.status_code >= 500:
        # Lỗi máy chủ, thử lại sau
        schedule_retry(request)
```

### 3. Sử dụng mô hình dự phòng (Fallback Models)

```python
FALLBACK_CHAIN = ["gpt-4o", "claude-sonnet-4-5", "gemini-2.5-flash"]

def chat_with_fallback(messages):
    for model in FALLBACK_CHAIN:
        try:
            return client.chat.completions.create(
                model=model,
                messages=messages
            )
        except APIError:
            continue
    raise Exception("All models failed")
```

## Bảo mật

### 1. Bảo vệ API Key

```python
# ❌ Tuyệt đối không để lộ key trực tiếp trong mã nguồn
client = OpenAI(api_key="sk-abc123...")

# ✅ Sử dụng biến môi trường
import os
client = OpenAI(api_key=os.environ["LEMONDATA_API_KEY"])
```

### 2. Xác thực đầu vào của người dùng

```python
def validate_message(content: str) -> bool:
    """Xác thực đầu vào của người dùng trước khi gửi đến API."""
    if len(content) > 100000:
        raise ValueError("Message too long")
    # Thêm các xác thực khác nếu cần
    return True
```

### 3. Thiết lập giới hạn API Key

Tạo các API Key riêng biệt với giới hạn chi tiêu cho các mục đích sau:
- Phát triển/Kiểm thử
- Môi trường chính thức (Production)
- Các ứng dụng khác nhau

## Giám sát

### 1. Theo dõi lượng sử dụng

Kiểm tra bảng điều khiển của bạn thường xuyên để biết:
- Lượng sử dụng token của từng mô hình
- Chi tiết chi phí
- Tỷ lệ khớp cache
- Tỷ lệ lỗi

### 2. Ghi lại các chỉ số quan trọng

```python
import logging

response = client.chat.completions.create(...)

logging.info({
    "model": response.model,
    "prompt_tokens": response.usage.prompt_tokens,
    "completion_tokens": response.usage.completion_tokens,
    "total_tokens": response.usage.total_tokens,
})
```

### 3. Thiết lập cảnh báo

Cấu hình cảnh báo số dư thấp trong bảng điều khiển để tránh gián đoạn dịch vụ.

## Danh sách kiểm tra

<AccordionGroup>
  <Accordion title="Tối ưu hóa chi phí">
    - [ ] Sử dụng mô hình phù hợp cho mỗi nhiệm vụ
    - [ ] Thiết lập giới hạn max_tokens
    - [ ] Giữ Prompt ngắn gọn
    - [ ] Kích hoạt cache ở những nơi thích hợp
    - [ ] Xử lý hàng loạt các yêu cầu tương tự
  </Accordion>

  <Accordion title="Hiệu suất">
    - [ ] Sử dụng streaming cho trải nghiệm tương tác
    - [ ] Sử dụng mô hình nhanh cho các mục đích thời gian thực
    - [ ] Đã cấu hình thiết lập timeout
  </Accordion>

  <Accordion title="Độ tin cậy">
    - [ ] Đã triển khai logic thử lại
    - [ ] Xử lý lỗi đã sẵn sàng
    - [ ] Đã cấu hình mô hình dự phòng
  </Accordion>

  <Accordion title="Bảo mật">
    - [ ] API Key được lưu trữ trong biến môi trường
    - [ ] Xác thực đầu vào
    - [ ] Sử dụng key riêng biệt cho môi trường phát triển/chính thức
    - [ ] Đã thiết lập giới hạn chi tiêu
  </Accordion>
</AccordionGroup>