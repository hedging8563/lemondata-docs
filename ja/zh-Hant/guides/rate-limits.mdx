---
title: "レート制限"
description: "レート制限の理解と対処方法"
---

## 概要

LemonDataは、公平な利用とプラットフォームの安定性を確保するためにレート制限を実施しています。制限はアカウントのティアによって異なります。

## レート制限ティア

| ティア | リクエスト数/分 | 説明 |
|------|-------------|-------------|
| **User** | 60 | すべてのアカウントのデフォルトティア |
| **Partner** | 300 | 統合パートナー向け |
| **VIP** | 1,000 | 高ボリュームユーザー |

<Note>
  レート制限は変更される場合があります。カスタム制限については support@lemondata.cc までお問い合わせください。
</Note>

## レート制限ヘッダー

各 API レスポンスにはレート制限情報が含まれています：

```
X-RateLimit-Limit: 60          # Your limit per minute
X-RateLimit-Remaining: 55      # Requests remaining
X-RateLimit-Reset: 1234567890  # Unix timestamp when limit resets
```

## レート制限の超過

制限を超えると、`429` レスポンスが返されます：

```json
{
  "error": {
    "message": "Rate limit exceeded. Please slow down.",
    "type": "rate_limit_exceeded"
  }
}
```

追加のヘッダーが含まれます：
```
Retry-After: 60  # Seconds to wait before retrying
```

## レート制限の処理

### 指数バックオフ (Exponential Backoff)

自動再試行のために指数バックオフを実装します：

```python
import time
from openai import OpenAI, RateLimitError

client = OpenAI(
    api_key="sk-your-api-key",
    base_url="https://api.lemondata.cc/v1"
)

def make_request_with_backoff(messages, max_retries=5):
    for attempt in range(max_retries):
        try:
            return client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
        except RateLimitError as e:
            if attempt == max_retries - 1:
                raise

            wait_time = 2 ** attempt  # 1, 2, 4, 8, 16 seconds
            print(f"Rate limited. Waiting {wait_time}s...")
            time.sleep(wait_time)
```

### リクエストキューイング

高ボリュームのアプリケーションでは、リクエストキューイングを実装してください：

```python
import asyncio
from collections import deque

class RateLimitedClient:
    def __init__(self, requests_per_minute=60):
        self.rpm = requests_per_minute
        self.interval = 60 / requests_per_minute
        self.last_request = 0

    async def request(self, messages):
        # Wait if needed to respect rate limit
        now = asyncio.get_event_loop().time()
        wait_time = max(0, self.last_request + self.interval - now)
        if wait_time > 0:
            await asyncio.sleep(wait_time)

        self.last_request = asyncio.get_event_loop().time()
        return await self.client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )
```

### バッチ処理

大量の操作を行う場合は、遅延を伴うバッチ処理を使用してください：

```python
def process_batch(items, batch_size=50, delay=1):
    results = []
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        for item in batch:
            result = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": item}]
            )
            results.append(result)
        time.sleep(delay)  # Pause between batches
    return results
```

## ベストプラクティス

<AccordionGroup>
  <Accordion title="使用状況の監視">
    レート制限ヘッダーを追跡し、プロアクティブに制限内に収まるようにします。
  </Accordion>

  <Accordion title="キャッシュの実装">
    同じリクエストに対してレスポンスをキャッシュし、 API 呼び出しを削減します。
  </Accordion>

  <Accordion title="適切なモデルの使用">
    より高速なモデル（gpt-4o-miniなど）は、より高いスループットを提供します。
  </Accordion>

  <Accordion title="より高い制限についてはお問い合わせください">
    より高い制限が必要な場合は、support@lemondata.cc までご連絡ください。
  </Accordion>
</AccordionGroup>

## ティアのアップグレード

ティアのアップグレードを申請するには：

1. [コンソール](https://lemondata.cc/dashboard)にログインします
2. **Settings → Account** に移動します
3. サポートチームに連絡し、ユースケースを説明してください

または、以下の情報を添えて support@lemondata.cc までメールを送信してください：
- アカウントのメールアドレス
- 予想されるリクエスト量
- ユースケースの説明